{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.7819590532566236,
  "eval_steps": 500,
  "global_step": 1205000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.7636581656579423e-05,
      "grad_norm": 8.674405097961426,
      "learning_rate": 4.9999781747302004e-05,
      "loss": 7.5006,
      "step": 100
    },
    {
      "epoch": 3.5273163313158847e-05,
      "grad_norm": 7.975790977478027,
      "learning_rate": 4.999956129003129e-05,
      "loss": 7.0442,
      "step": 200
    },
    {
      "epoch": 5.290974496973827e-05,
      "grad_norm": 5.035762310028076,
      "learning_rate": 4.999934083276059e-05,
      "loss": 6.8428,
      "step": 300
    },
    {
      "epoch": 7.054632662631769e-05,
      "grad_norm": 4.29673957824707,
      "learning_rate": 4.9999120375489884e-05,
      "loss": 6.7108,
      "step": 400
    },
    {
      "epoch": 8.818290828289711e-05,
      "grad_norm": 5.048318386077881,
      "learning_rate": 4.999889991821917e-05,
      "loss": 6.6421,
      "step": 500
    },
    {
      "epoch": 0.00010581948993947655,
      "grad_norm": 5.230515003204346,
      "learning_rate": 4.999867946094847e-05,
      "loss": 6.4184,
      "step": 600
    },
    {
      "epoch": 0.00012345607159605597,
      "grad_norm": 10.959586143493652,
      "learning_rate": 4.999845900367776e-05,
      "loss": 6.4453,
      "step": 700
    },
    {
      "epoch": 0.00014109265325263539,
      "grad_norm": 6.939022541046143,
      "learning_rate": 4.999823854640705e-05,
      "loss": 6.4533,
      "step": 800
    },
    {
      "epoch": 0.0001587292349092148,
      "grad_norm": 4.942361831665039,
      "learning_rate": 4.999801808913635e-05,
      "loss": 6.3956,
      "step": 900
    },
    {
      "epoch": 0.00017636581656579423,
      "grad_norm": 4.991878986358643,
      "learning_rate": 4.9997797631865636e-05,
      "loss": 6.4136,
      "step": 1000
    },
    {
      "epoch": 0.00019400239822237367,
      "grad_norm": 5.187983989715576,
      "learning_rate": 4.999757717459493e-05,
      "loss": 6.336,
      "step": 1100
    },
    {
      "epoch": 0.0002116389798789531,
      "grad_norm": 4.421689033508301,
      "learning_rate": 4.999735671732422e-05,
      "loss": 6.3045,
      "step": 1200
    },
    {
      "epoch": 0.0002292755615355325,
      "grad_norm": 5.588916301727295,
      "learning_rate": 4.9997136260053516e-05,
      "loss": 6.245,
      "step": 1300
    },
    {
      "epoch": 0.00024691214319211193,
      "grad_norm": 5.696846008300781,
      "learning_rate": 4.9996915802782804e-05,
      "loss": 6.2579,
      "step": 1400
    },
    {
      "epoch": 0.00026454872484869135,
      "grad_norm": 3.893521785736084,
      "learning_rate": 4.99966953455121e-05,
      "loss": 6.2214,
      "step": 1500
    },
    {
      "epoch": 0.00028218530650527077,
      "grad_norm": 4.440462112426758,
      "learning_rate": 4.9996474888241395e-05,
      "loss": 6.2065,
      "step": 1600
    },
    {
      "epoch": 0.0002998218881618502,
      "grad_norm": 5.143251419067383,
      "learning_rate": 4.9996254430970684e-05,
      "loss": 6.1735,
      "step": 1700
    },
    {
      "epoch": 0.0003174584698184296,
      "grad_norm": 6.265223979949951,
      "learning_rate": 4.999603397369998e-05,
      "loss": 6.067,
      "step": 1800
    },
    {
      "epoch": 0.00033509505147500903,
      "grad_norm": 5.619327545166016,
      "learning_rate": 4.9995813516429275e-05,
      "loss": 6.1885,
      "step": 1900
    },
    {
      "epoch": 0.00035273163313158845,
      "grad_norm": 3.9725089073181152,
      "learning_rate": 4.9995593059158564e-05,
      "loss": 6.0171,
      "step": 2000
    },
    {
      "epoch": 0.0003703682147881679,
      "grad_norm": 6.067861557006836,
      "learning_rate": 4.999537260188786e-05,
      "loss": 6.1744,
      "step": 2100
    },
    {
      "epoch": 0.00038800479644474735,
      "grad_norm": 6.534496784210205,
      "learning_rate": 4.9995152144617155e-05,
      "loss": 6.0271,
      "step": 2200
    },
    {
      "epoch": 0.00040564137810132677,
      "grad_norm": 8.769402503967285,
      "learning_rate": 4.999493168734644e-05,
      "loss": 6.1037,
      "step": 2300
    },
    {
      "epoch": 0.0004232779597579062,
      "grad_norm": 7.326728820800781,
      "learning_rate": 4.999471123007574e-05,
      "loss": 5.9425,
      "step": 2400
    },
    {
      "epoch": 0.0004409145414144856,
      "grad_norm": 3.7227556705474854,
      "learning_rate": 4.999449077280503e-05,
      "loss": 6.0484,
      "step": 2500
    },
    {
      "epoch": 0.000458551123071065,
      "grad_norm": 6.81704568862915,
      "learning_rate": 4.999427031553432e-05,
      "loss": 6.0939,
      "step": 2600
    },
    {
      "epoch": 0.00047618770472764444,
      "grad_norm": 5.239577293395996,
      "learning_rate": 4.999404985826361e-05,
      "loss": 6.0073,
      "step": 2700
    },
    {
      "epoch": 0.0004938242863842239,
      "grad_norm": 6.487543106079102,
      "learning_rate": 4.999382940099291e-05,
      "loss": 6.0584,
      "step": 2800
    },
    {
      "epoch": 0.0005114608680408033,
      "grad_norm": 4.463504791259766,
      "learning_rate": 4.9993608943722196e-05,
      "loss": 5.8967,
      "step": 2900
    },
    {
      "epoch": 0.0005290974496973827,
      "grad_norm": 4.197649002075195,
      "learning_rate": 4.999338848645149e-05,
      "loss": 5.9182,
      "step": 3000
    },
    {
      "epoch": 0.0005467340313539622,
      "grad_norm": 4.731043338775635,
      "learning_rate": 4.999316802918079e-05,
      "loss": 6.0058,
      "step": 3100
    },
    {
      "epoch": 0.0005643706130105415,
      "grad_norm": 4.570987701416016,
      "learning_rate": 4.9992947571910075e-05,
      "loss": 5.9188,
      "step": 3200
    },
    {
      "epoch": 0.000582007194667121,
      "grad_norm": 6.139210224151611,
      "learning_rate": 4.999272711463937e-05,
      "loss": 5.8882,
      "step": 3300
    },
    {
      "epoch": 0.0005996437763237004,
      "grad_norm": 6.351330757141113,
      "learning_rate": 4.9992506657368666e-05,
      "loss": 5.901,
      "step": 3400
    },
    {
      "epoch": 0.0006172803579802799,
      "grad_norm": 4.869520664215088,
      "learning_rate": 4.9992286200097955e-05,
      "loss": 5.8469,
      "step": 3500
    },
    {
      "epoch": 0.0006349169396368592,
      "grad_norm": 4.413963794708252,
      "learning_rate": 4.999206574282725e-05,
      "loss": 5.8589,
      "step": 3600
    },
    {
      "epoch": 0.0006525535212934387,
      "grad_norm": 5.088119029998779,
      "learning_rate": 4.9991845285556546e-05,
      "loss": 5.9535,
      "step": 3700
    },
    {
      "epoch": 0.0006701901029500181,
      "grad_norm": 5.631072998046875,
      "learning_rate": 4.9991624828285835e-05,
      "loss": 5.8345,
      "step": 3800
    },
    {
      "epoch": 0.0006878266846065975,
      "grad_norm": 6.725576400756836,
      "learning_rate": 4.999140437101513e-05,
      "loss": 5.7932,
      "step": 3900
    },
    {
      "epoch": 0.0007054632662631769,
      "grad_norm": 5.372183322906494,
      "learning_rate": 4.999118391374442e-05,
      "loss": 5.8947,
      "step": 4000
    },
    {
      "epoch": 0.0007230998479197564,
      "grad_norm": 9.6747465133667,
      "learning_rate": 4.999096345647371e-05,
      "loss": 5.8888,
      "step": 4100
    },
    {
      "epoch": 0.0007407364295763359,
      "grad_norm": 4.5103678703308105,
      "learning_rate": 4.9990742999203e-05,
      "loss": 5.7969,
      "step": 4200
    },
    {
      "epoch": 0.0007583730112329152,
      "grad_norm": 4.959508895874023,
      "learning_rate": 4.99905225419323e-05,
      "loss": 5.8201,
      "step": 4300
    },
    {
      "epoch": 0.0007760095928894947,
      "grad_norm": 5.58948278427124,
      "learning_rate": 4.999030208466159e-05,
      "loss": 5.7805,
      "step": 4400
    },
    {
      "epoch": 0.0007936461745460741,
      "grad_norm": 5.993032932281494,
      "learning_rate": 4.999008162739088e-05,
      "loss": 5.7903,
      "step": 4500
    },
    {
      "epoch": 0.0008112827562026535,
      "grad_norm": 4.49887228012085,
      "learning_rate": 4.998986117012018e-05,
      "loss": 5.8724,
      "step": 4600
    },
    {
      "epoch": 0.0008289193378592329,
      "grad_norm": 4.770360946655273,
      "learning_rate": 4.998964071284947e-05,
      "loss": 5.8546,
      "step": 4700
    },
    {
      "epoch": 0.0008465559195158124,
      "grad_norm": 6.313211441040039,
      "learning_rate": 4.998942025557876e-05,
      "loss": 5.6316,
      "step": 4800
    },
    {
      "epoch": 0.0008641925011723917,
      "grad_norm": 5.281609535217285,
      "learning_rate": 4.998919979830806e-05,
      "loss": 5.7605,
      "step": 4900
    },
    {
      "epoch": 0.0008818290828289712,
      "grad_norm": 7.590467929840088,
      "learning_rate": 4.9988979341037346e-05,
      "loss": 5.6379,
      "step": 5000
    },
    {
      "epoch": 0.0008994656644855506,
      "grad_norm": 5.097375869750977,
      "learning_rate": 4.998875888376664e-05,
      "loss": 5.737,
      "step": 5100
    },
    {
      "epoch": 0.00091710224614213,
      "grad_norm": 6.036974906921387,
      "learning_rate": 4.998853842649594e-05,
      "loss": 5.723,
      "step": 5200
    },
    {
      "epoch": 0.0009347388277987094,
      "grad_norm": 4.835634708404541,
      "learning_rate": 4.9988317969225226e-05,
      "loss": 5.7046,
      "step": 5300
    },
    {
      "epoch": 0.0009523754094552889,
      "grad_norm": 5.26866340637207,
      "learning_rate": 4.998809751195452e-05,
      "loss": 5.5779,
      "step": 5400
    },
    {
      "epoch": 0.0009700119911118683,
      "grad_norm": 5.45151424407959,
      "learning_rate": 4.998787705468381e-05,
      "loss": 5.6703,
      "step": 5500
    },
    {
      "epoch": 0.0009876485727684477,
      "grad_norm": 5.229855060577393,
      "learning_rate": 4.99876565974131e-05,
      "loss": 5.7189,
      "step": 5600
    },
    {
      "epoch": 0.001005285154425027,
      "grad_norm": 5.009348392486572,
      "learning_rate": 4.9987436140142394e-05,
      "loss": 5.7282,
      "step": 5700
    },
    {
      "epoch": 0.0010229217360816067,
      "grad_norm": 3.8720805644989014,
      "learning_rate": 4.998721568287169e-05,
      "loss": 5.6642,
      "step": 5800
    },
    {
      "epoch": 0.001040558317738186,
      "grad_norm": 3.9226887226104736,
      "learning_rate": 4.998699522560098e-05,
      "loss": 5.7786,
      "step": 5900
    },
    {
      "epoch": 0.0010581948993947654,
      "grad_norm": 5.966726303100586,
      "learning_rate": 4.9986774768330274e-05,
      "loss": 5.7057,
      "step": 6000
    },
    {
      "epoch": 0.0010758314810513448,
      "grad_norm": 6.995871543884277,
      "learning_rate": 4.998655431105957e-05,
      "loss": 5.6612,
      "step": 6100
    },
    {
      "epoch": 0.0010934680627079244,
      "grad_norm": 4.731358528137207,
      "learning_rate": 4.998633385378886e-05,
      "loss": 5.6394,
      "step": 6200
    },
    {
      "epoch": 0.0011111046443645037,
      "grad_norm": 4.821954727172852,
      "learning_rate": 4.9986113396518154e-05,
      "loss": 5.7092,
      "step": 6300
    },
    {
      "epoch": 0.001128741226021083,
      "grad_norm": 7.6766276359558105,
      "learning_rate": 4.998589293924745e-05,
      "loss": 5.5392,
      "step": 6400
    },
    {
      "epoch": 0.0011463778076776625,
      "grad_norm": 3.5439999103546143,
      "learning_rate": 4.998567248197674e-05,
      "loss": 5.6736,
      "step": 6500
    },
    {
      "epoch": 0.001164014389334242,
      "grad_norm": 4.216145992279053,
      "learning_rate": 4.998545202470603e-05,
      "loss": 5.6266,
      "step": 6600
    },
    {
      "epoch": 0.0011816509709908214,
      "grad_norm": 5.996246337890625,
      "learning_rate": 4.998523156743533e-05,
      "loss": 5.6586,
      "step": 6700
    },
    {
      "epoch": 0.0011992875526474008,
      "grad_norm": 7.618131160736084,
      "learning_rate": 4.998501111016462e-05,
      "loss": 5.5135,
      "step": 6800
    },
    {
      "epoch": 0.0012169241343039801,
      "grad_norm": 4.624953746795654,
      "learning_rate": 4.9984790652893906e-05,
      "loss": 5.6364,
      "step": 6900
    },
    {
      "epoch": 0.0012345607159605597,
      "grad_norm": 8.728056907653809,
      "learning_rate": 4.99845701956232e-05,
      "loss": 5.626,
      "step": 7000
    },
    {
      "epoch": 0.001252197297617139,
      "grad_norm": 4.846568584442139,
      "learning_rate": 4.998434973835249e-05,
      "loss": 5.5988,
      "step": 7100
    },
    {
      "epoch": 0.0012698338792737184,
      "grad_norm": 4.62415885925293,
      "learning_rate": 4.9984129281081786e-05,
      "loss": 5.5219,
      "step": 7200
    },
    {
      "epoch": 0.001287470460930298,
      "grad_norm": 4.382715225219727,
      "learning_rate": 4.998390882381108e-05,
      "loss": 5.6196,
      "step": 7300
    },
    {
      "epoch": 0.0013051070425868774,
      "grad_norm": 6.508136749267578,
      "learning_rate": 4.998368836654037e-05,
      "loss": 5.5909,
      "step": 7400
    },
    {
      "epoch": 0.0013227436242434568,
      "grad_norm": 6.335553169250488,
      "learning_rate": 4.9983467909269665e-05,
      "loss": 5.558,
      "step": 7500
    },
    {
      "epoch": 0.0013403802059000361,
      "grad_norm": 5.572612285614014,
      "learning_rate": 4.998324745199896e-05,
      "loss": 5.4808,
      "step": 7600
    },
    {
      "epoch": 0.0013580167875566157,
      "grad_norm": 4.826536655426025,
      "learning_rate": 4.998302699472825e-05,
      "loss": 5.5779,
      "step": 7700
    },
    {
      "epoch": 0.001375653369213195,
      "grad_norm": 5.818243503570557,
      "learning_rate": 4.9982806537457545e-05,
      "loss": 5.5472,
      "step": 7800
    },
    {
      "epoch": 0.0013932899508697744,
      "grad_norm": 5.097725868225098,
      "learning_rate": 4.998258608018684e-05,
      "loss": 5.4937,
      "step": 7900
    },
    {
      "epoch": 0.0014109265325263538,
      "grad_norm": 4.904901504516602,
      "learning_rate": 4.998236562291613e-05,
      "loss": 5.5819,
      "step": 8000
    },
    {
      "epoch": 0.0014285631141829334,
      "grad_norm": 8.230917930603027,
      "learning_rate": 4.9982145165645425e-05,
      "loss": 5.6353,
      "step": 8100
    },
    {
      "epoch": 0.0014461996958395128,
      "grad_norm": 5.400548934936523,
      "learning_rate": 4.998192470837472e-05,
      "loss": 5.5269,
      "step": 8200
    },
    {
      "epoch": 0.0014638362774960921,
      "grad_norm": 6.4735541343688965,
      "learning_rate": 4.998170425110401e-05,
      "loss": 5.5306,
      "step": 8300
    },
    {
      "epoch": 0.0014814728591526717,
      "grad_norm": 6.2300705909729,
      "learning_rate": 4.99814837938333e-05,
      "loss": 5.5052,
      "step": 8400
    },
    {
      "epoch": 0.001499109440809251,
      "grad_norm": 6.880260467529297,
      "learning_rate": 4.998126333656259e-05,
      "loss": 5.4419,
      "step": 8500
    },
    {
      "epoch": 0.0015167460224658304,
      "grad_norm": 6.028846263885498,
      "learning_rate": 4.998104287929188e-05,
      "loss": 5.4475,
      "step": 8600
    },
    {
      "epoch": 0.0015343826041224098,
      "grad_norm": 4.08549690246582,
      "learning_rate": 4.998082242202118e-05,
      "loss": 5.4739,
      "step": 8700
    },
    {
      "epoch": 0.0015520191857789894,
      "grad_norm": 3.8181071281433105,
      "learning_rate": 4.998060196475047e-05,
      "loss": 5.5295,
      "step": 8800
    },
    {
      "epoch": 0.0015696557674355687,
      "grad_norm": 6.766882419586182,
      "learning_rate": 4.998038150747976e-05,
      "loss": 5.441,
      "step": 8900
    },
    {
      "epoch": 0.0015872923490921481,
      "grad_norm": 4.786388874053955,
      "learning_rate": 4.998016105020906e-05,
      "loss": 5.3898,
      "step": 9000
    },
    {
      "epoch": 0.0016049289307487275,
      "grad_norm": 5.398801326751709,
      "learning_rate": 4.997994059293835e-05,
      "loss": 5.507,
      "step": 9100
    },
    {
      "epoch": 0.001622565512405307,
      "grad_norm": 5.870822429656982,
      "learning_rate": 4.997972013566765e-05,
      "loss": 5.4506,
      "step": 9200
    },
    {
      "epoch": 0.0016402020940618864,
      "grad_norm": 5.102287769317627,
      "learning_rate": 4.9979499678396936e-05,
      "loss": 5.4701,
      "step": 9300
    },
    {
      "epoch": 0.0016578386757184658,
      "grad_norm": 5.091010570526123,
      "learning_rate": 4.997927922112623e-05,
      "loss": 5.2575,
      "step": 9400
    },
    {
      "epoch": 0.0016754752573750452,
      "grad_norm": 4.166958332061768,
      "learning_rate": 4.997905876385553e-05,
      "loss": 5.4012,
      "step": 9500
    },
    {
      "epoch": 0.0016931118390316247,
      "grad_norm": 4.2409281730651855,
      "learning_rate": 4.9978838306584816e-05,
      "loss": 5.4707,
      "step": 9600
    },
    {
      "epoch": 0.001710748420688204,
      "grad_norm": 5.773632526397705,
      "learning_rate": 4.9978617849314105e-05,
      "loss": 5.4007,
      "step": 9700
    },
    {
      "epoch": 0.0017283850023447835,
      "grad_norm": 4.753931522369385,
      "learning_rate": 4.99783973920434e-05,
      "loss": 5.3256,
      "step": 9800
    },
    {
      "epoch": 0.001746021584001363,
      "grad_norm": 7.970766067504883,
      "learning_rate": 4.997817693477269e-05,
      "loss": 5.4395,
      "step": 9900
    },
    {
      "epoch": 0.0017636581656579424,
      "grad_norm": 5.829692840576172,
      "learning_rate": 4.9977956477501984e-05,
      "loss": 5.2602,
      "step": 10000
    },
    {
      "epoch": 0.0017812947473145218,
      "grad_norm": 4.446383476257324,
      "learning_rate": 4.997773602023128e-05,
      "loss": 5.3913,
      "step": 10100
    },
    {
      "epoch": 0.0017989313289711012,
      "grad_norm": 4.458619594573975,
      "learning_rate": 4.997751556296057e-05,
      "loss": 5.3801,
      "step": 10200
    },
    {
      "epoch": 0.0018165679106276807,
      "grad_norm": 7.238236904144287,
      "learning_rate": 4.9977295105689864e-05,
      "loss": 5.3768,
      "step": 10300
    },
    {
      "epoch": 0.00183420449228426,
      "grad_norm": 5.995565414428711,
      "learning_rate": 4.997707464841916e-05,
      "loss": 5.354,
      "step": 10400
    },
    {
      "epoch": 0.0018518410739408395,
      "grad_norm": 5.968627452850342,
      "learning_rate": 4.997685419114845e-05,
      "loss": 5.3322,
      "step": 10500
    },
    {
      "epoch": 0.0018694776555974188,
      "grad_norm": 6.624331474304199,
      "learning_rate": 4.9976633733877743e-05,
      "loss": 5.4142,
      "step": 10600
    },
    {
      "epoch": 0.0018871142372539984,
      "grad_norm": 5.205347061157227,
      "learning_rate": 4.997641327660704e-05,
      "loss": 5.3881,
      "step": 10700
    },
    {
      "epoch": 0.0019047508189105778,
      "grad_norm": 6.964667797088623,
      "learning_rate": 4.997619281933633e-05,
      "loss": 5.3515,
      "step": 10800
    },
    {
      "epoch": 0.0019223874005671571,
      "grad_norm": 7.251681804656982,
      "learning_rate": 4.997597236206562e-05,
      "loss": 5.399,
      "step": 10900
    },
    {
      "epoch": 0.0019400239822237365,
      "grad_norm": 7.089068412780762,
      "learning_rate": 4.997575190479492e-05,
      "loss": 5.3081,
      "step": 11000
    },
    {
      "epoch": 0.001957660563880316,
      "grad_norm": 5.666306972503662,
      "learning_rate": 4.997553144752421e-05,
      "loss": 5.2383,
      "step": 11100
    },
    {
      "epoch": 0.0019752971455368955,
      "grad_norm": 6.493663311004639,
      "learning_rate": 4.9975310990253496e-05,
      "loss": 5.2394,
      "step": 11200
    },
    {
      "epoch": 0.001992933727193475,
      "grad_norm": 7.027013301849365,
      "learning_rate": 4.997509053298279e-05,
      "loss": 5.2653,
      "step": 11300
    },
    {
      "epoch": 0.002010570308850054,
      "grad_norm": 9.617761611938477,
      "learning_rate": 4.997487007571208e-05,
      "loss": 5.3484,
      "step": 11400
    },
    {
      "epoch": 0.0020282068905066336,
      "grad_norm": 5.591448783874512,
      "learning_rate": 4.9974649618441376e-05,
      "loss": 5.2807,
      "step": 11500
    },
    {
      "epoch": 0.0020458434721632134,
      "grad_norm": 6.438699722290039,
      "learning_rate": 4.997442916117067e-05,
      "loss": 5.3697,
      "step": 11600
    },
    {
      "epoch": 0.0020634800538197927,
      "grad_norm": 4.5943145751953125,
      "learning_rate": 4.997420870389996e-05,
      "loss": 5.3785,
      "step": 11700
    },
    {
      "epoch": 0.002081116635476372,
      "grad_norm": 5.975540637969971,
      "learning_rate": 4.9973988246629255e-05,
      "loss": 5.3441,
      "step": 11800
    },
    {
      "epoch": 0.0020987532171329515,
      "grad_norm": 5.038610458374023,
      "learning_rate": 4.997376778935855e-05,
      "loss": 5.3073,
      "step": 11900
    },
    {
      "epoch": 0.002116389798789531,
      "grad_norm": 4.525116920471191,
      "learning_rate": 4.997354733208784e-05,
      "loss": 5.2956,
      "step": 12000
    },
    {
      "epoch": 0.00213402638044611,
      "grad_norm": 5.525790691375732,
      "learning_rate": 4.9973326874817135e-05,
      "loss": 5.383,
      "step": 12100
    },
    {
      "epoch": 0.0021516629621026895,
      "grad_norm": 4.8318562507629395,
      "learning_rate": 4.997310641754643e-05,
      "loss": 5.1812,
      "step": 12200
    },
    {
      "epoch": 0.002169299543759269,
      "grad_norm": 5.084990978240967,
      "learning_rate": 4.997288596027572e-05,
      "loss": 5.2505,
      "step": 12300
    },
    {
      "epoch": 0.0021869361254158487,
      "grad_norm": 5.407971382141113,
      "learning_rate": 4.9972665503005014e-05,
      "loss": 5.241,
      "step": 12400
    },
    {
      "epoch": 0.002204572707072428,
      "grad_norm": 10.228340148925781,
      "learning_rate": 4.99724450457343e-05,
      "loss": 5.3246,
      "step": 12500
    },
    {
      "epoch": 0.0022222092887290074,
      "grad_norm": 8.045413970947266,
      "learning_rate": 4.99722245884636e-05,
      "loss": 5.3215,
      "step": 12600
    },
    {
      "epoch": 0.002239845870385587,
      "grad_norm": 7.497475624084473,
      "learning_rate": 4.997200413119289e-05,
      "loss": 5.2843,
      "step": 12700
    },
    {
      "epoch": 0.002257482452042166,
      "grad_norm": 8.329215049743652,
      "learning_rate": 4.997178367392218e-05,
      "loss": 5.2645,
      "step": 12800
    },
    {
      "epoch": 0.0022751190336987455,
      "grad_norm": 6.3322834968566895,
      "learning_rate": 4.997156321665147e-05,
      "loss": 5.2374,
      "step": 12900
    },
    {
      "epoch": 0.002292755615355325,
      "grad_norm": 5.449280738830566,
      "learning_rate": 4.997134275938077e-05,
      "loss": 5.3163,
      "step": 13000
    },
    {
      "epoch": 0.0023103921970119047,
      "grad_norm": 5.0756072998046875,
      "learning_rate": 4.997112230211006e-05,
      "loss": 5.1967,
      "step": 13100
    },
    {
      "epoch": 0.002328028778668484,
      "grad_norm": 5.111550807952881,
      "learning_rate": 4.997090184483935e-05,
      "loss": 5.3,
      "step": 13200
    },
    {
      "epoch": 0.0023456653603250634,
      "grad_norm": 5.0188822746276855,
      "learning_rate": 4.9970681387568647e-05,
      "loss": 5.2172,
      "step": 13300
    },
    {
      "epoch": 0.002363301941981643,
      "grad_norm": 5.534628868103027,
      "learning_rate": 4.997046093029794e-05,
      "loss": 5.2376,
      "step": 13400
    },
    {
      "epoch": 0.002380938523638222,
      "grad_norm": 6.3988213539123535,
      "learning_rate": 4.997024047302723e-05,
      "loss": 5.2703,
      "step": 13500
    },
    {
      "epoch": 0.0023985751052948015,
      "grad_norm": 4.911125183105469,
      "learning_rate": 4.9970020015756526e-05,
      "loss": 5.2733,
      "step": 13600
    },
    {
      "epoch": 0.002416211686951381,
      "grad_norm": 7.138851642608643,
      "learning_rate": 4.996979955848582e-05,
      "loss": 5.2349,
      "step": 13700
    },
    {
      "epoch": 0.0024338482686079603,
      "grad_norm": 7.004884243011475,
      "learning_rate": 4.996957910121511e-05,
      "loss": 5.2823,
      "step": 13800
    },
    {
      "epoch": 0.00245148485026454,
      "grad_norm": 6.368241786956787,
      "learning_rate": 4.9969358643944406e-05,
      "loss": 5.2602,
      "step": 13900
    },
    {
      "epoch": 0.0024691214319211194,
      "grad_norm": 9.491403579711914,
      "learning_rate": 4.9969138186673695e-05,
      "loss": 5.1641,
      "step": 14000
    },
    {
      "epoch": 0.002486758013577699,
      "grad_norm": 7.36294412612915,
      "learning_rate": 4.996891772940298e-05,
      "loss": 5.1427,
      "step": 14100
    },
    {
      "epoch": 0.002504394595234278,
      "grad_norm": 6.707412242889404,
      "learning_rate": 4.996869727213228e-05,
      "loss": 5.16,
      "step": 14200
    },
    {
      "epoch": 0.0025220311768908575,
      "grad_norm": 4.816512107849121,
      "learning_rate": 4.9968476814861574e-05,
      "loss": 5.1291,
      "step": 14300
    },
    {
      "epoch": 0.002539667758547437,
      "grad_norm": 4.682504177093506,
      "learning_rate": 4.996825635759086e-05,
      "loss": 5.2474,
      "step": 14400
    },
    {
      "epoch": 0.0025573043402040163,
      "grad_norm": 5.089374542236328,
      "learning_rate": 4.996803590032016e-05,
      "loss": 5.2447,
      "step": 14500
    },
    {
      "epoch": 0.002574940921860596,
      "grad_norm": 7.138289928436279,
      "learning_rate": 4.9967815443049454e-05,
      "loss": 5.1682,
      "step": 14600
    },
    {
      "epoch": 0.0025925775035171754,
      "grad_norm": 4.519061088562012,
      "learning_rate": 4.996759498577874e-05,
      "loss": 5.2263,
      "step": 14700
    },
    {
      "epoch": 0.002610214085173755,
      "grad_norm": 6.793076992034912,
      "learning_rate": 4.996737452850804e-05,
      "loss": 5.2015,
      "step": 14800
    },
    {
      "epoch": 0.002627850666830334,
      "grad_norm": 8.205930709838867,
      "learning_rate": 4.9967154071237333e-05,
      "loss": 5.1919,
      "step": 14900
    },
    {
      "epoch": 0.0026454872484869135,
      "grad_norm": 5.231980800628662,
      "learning_rate": 4.996693361396662e-05,
      "loss": 5.2197,
      "step": 15000
    },
    {
      "epoch": 0.002663123830143493,
      "grad_norm": 5.443431854248047,
      "learning_rate": 4.996671315669592e-05,
      "loss": 5.1002,
      "step": 15100
    },
    {
      "epoch": 0.0026807604118000723,
      "grad_norm": 9.753478050231934,
      "learning_rate": 4.996649269942521e-05,
      "loss": 5.116,
      "step": 15200
    },
    {
      "epoch": 0.002698396993456652,
      "grad_norm": 6.606389045715332,
      "learning_rate": 4.99662722421545e-05,
      "loss": 5.1662,
      "step": 15300
    },
    {
      "epoch": 0.0027160335751132314,
      "grad_norm": 6.049864292144775,
      "learning_rate": 4.99660517848838e-05,
      "loss": 5.1641,
      "step": 15400
    },
    {
      "epoch": 0.002733670156769811,
      "grad_norm": 6.744314193725586,
      "learning_rate": 4.9965831327613086e-05,
      "loss": 5.1827,
      "step": 15500
    },
    {
      "epoch": 0.00275130673842639,
      "grad_norm": 7.461085796356201,
      "learning_rate": 4.9965610870342375e-05,
      "loss": 5.2336,
      "step": 15600
    },
    {
      "epoch": 0.0027689433200829695,
      "grad_norm": 6.836460113525391,
      "learning_rate": 4.996539041307167e-05,
      "loss": 5.221,
      "step": 15700
    },
    {
      "epoch": 0.002786579901739549,
      "grad_norm": 6.094715118408203,
      "learning_rate": 4.9965169955800966e-05,
      "loss": 5.1093,
      "step": 15800
    },
    {
      "epoch": 0.0028042164833961282,
      "grad_norm": 7.552525043487549,
      "learning_rate": 4.9964949498530254e-05,
      "loss": 5.0309,
      "step": 15900
    },
    {
      "epoch": 0.0028218530650527076,
      "grad_norm": 4.921248912811279,
      "learning_rate": 4.996472904125955e-05,
      "loss": 5.0354,
      "step": 16000
    },
    {
      "epoch": 0.0028394896467092874,
      "grad_norm": 4.9564714431762695,
      "learning_rate": 4.9964508583988845e-05,
      "loss": 5.1178,
      "step": 16100
    },
    {
      "epoch": 0.0028571262283658668,
      "grad_norm": 6.184421062469482,
      "learning_rate": 4.9964288126718134e-05,
      "loss": 5.0627,
      "step": 16200
    },
    {
      "epoch": 0.002874762810022446,
      "grad_norm": 4.898995399475098,
      "learning_rate": 4.996406766944743e-05,
      "loss": 5.1597,
      "step": 16300
    },
    {
      "epoch": 0.0028923993916790255,
      "grad_norm": 8.523173332214355,
      "learning_rate": 4.9963847212176725e-05,
      "loss": 5.1771,
      "step": 16400
    },
    {
      "epoch": 0.002910035973335605,
      "grad_norm": 5.7460126876831055,
      "learning_rate": 4.9963626754906013e-05,
      "loss": 5.1343,
      "step": 16500
    },
    {
      "epoch": 0.0029276725549921842,
      "grad_norm": 6.55590295791626,
      "learning_rate": 4.996340629763531e-05,
      "loss": 5.0328,
      "step": 16600
    },
    {
      "epoch": 0.0029453091366487636,
      "grad_norm": 7.2641377449035645,
      "learning_rate": 4.9963185840364604e-05,
      "loss": 5.0706,
      "step": 16700
    },
    {
      "epoch": 0.0029629457183053434,
      "grad_norm": 7.400598049163818,
      "learning_rate": 4.996296538309389e-05,
      "loss": 5.0212,
      "step": 16800
    },
    {
      "epoch": 0.0029805822999619228,
      "grad_norm": 6.867231369018555,
      "learning_rate": 4.996274492582318e-05,
      "loss": 5.1677,
      "step": 16900
    },
    {
      "epoch": 0.002998218881618502,
      "grad_norm": 9.301424026489258,
      "learning_rate": 4.996252446855248e-05,
      "loss": 5.1054,
      "step": 17000
    },
    {
      "epoch": 0.0030158554632750815,
      "grad_norm": 7.5295538902282715,
      "learning_rate": 4.9962304011281766e-05,
      "loss": 5.1394,
      "step": 17100
    },
    {
      "epoch": 0.003033492044931661,
      "grad_norm": 7.241102695465088,
      "learning_rate": 4.996208355401106e-05,
      "loss": 5.0851,
      "step": 17200
    },
    {
      "epoch": 0.0030511286265882402,
      "grad_norm": 8.156607627868652,
      "learning_rate": 4.996186309674036e-05,
      "loss": 5.1271,
      "step": 17300
    },
    {
      "epoch": 0.0030687652082448196,
      "grad_norm": 5.5643463134765625,
      "learning_rate": 4.9961642639469646e-05,
      "loss": 4.9927,
      "step": 17400
    },
    {
      "epoch": 0.003086401789901399,
      "grad_norm": 6.248370170593262,
      "learning_rate": 4.996142218219894e-05,
      "loss": 5.1046,
      "step": 17500
    },
    {
      "epoch": 0.0031040383715579788,
      "grad_norm": 6.675862789154053,
      "learning_rate": 4.9961201724928237e-05,
      "loss": 5.0498,
      "step": 17600
    },
    {
      "epoch": 0.003121674953214558,
      "grad_norm": 7.199674129486084,
      "learning_rate": 4.9960981267657525e-05,
      "loss": 5.1136,
      "step": 17700
    },
    {
      "epoch": 0.0031393115348711375,
      "grad_norm": 8.935900688171387,
      "learning_rate": 4.996076081038682e-05,
      "loss": 5.1144,
      "step": 17800
    },
    {
      "epoch": 0.003156948116527717,
      "grad_norm": 4.737967491149902,
      "learning_rate": 4.9960540353116116e-05,
      "loss": 5.186,
      "step": 17900
    },
    {
      "epoch": 0.0031745846981842962,
      "grad_norm": 5.066159248352051,
      "learning_rate": 4.9960319895845405e-05,
      "loss": 5.0344,
      "step": 18000
    },
    {
      "epoch": 0.0031922212798408756,
      "grad_norm": 5.367943286895752,
      "learning_rate": 4.99600994385747e-05,
      "loss": 4.9996,
      "step": 18100
    },
    {
      "epoch": 0.003209857861497455,
      "grad_norm": 4.49295711517334,
      "learning_rate": 4.9959878981303996e-05,
      "loss": 5.0351,
      "step": 18200
    },
    {
      "epoch": 0.0032274944431540348,
      "grad_norm": 6.165286064147949,
      "learning_rate": 4.9959658524033284e-05,
      "loss": 5.1373,
      "step": 18300
    },
    {
      "epoch": 0.003245131024810614,
      "grad_norm": 7.739685535430908,
      "learning_rate": 4.995943806676257e-05,
      "loss": 5.0865,
      "step": 18400
    },
    {
      "epoch": 0.0032627676064671935,
      "grad_norm": 7.329651355743408,
      "learning_rate": 4.995921760949187e-05,
      "loss": 4.9588,
      "step": 18500
    },
    {
      "epoch": 0.003280404188123773,
      "grad_norm": 8.505498886108398,
      "learning_rate": 4.995899715222116e-05,
      "loss": 5.0921,
      "step": 18600
    },
    {
      "epoch": 0.0032980407697803522,
      "grad_norm": 8.016753196716309,
      "learning_rate": 4.995877669495045e-05,
      "loss": 5.0328,
      "step": 18700
    },
    {
      "epoch": 0.0033156773514369316,
      "grad_norm": 5.991211891174316,
      "learning_rate": 4.995855623767975e-05,
      "loss": 4.981,
      "step": 18800
    },
    {
      "epoch": 0.003333313933093511,
      "grad_norm": 5.643515586853027,
      "learning_rate": 4.995833578040904e-05,
      "loss": 5.0421,
      "step": 18900
    },
    {
      "epoch": 0.0033509505147500903,
      "grad_norm": 6.363032341003418,
      "learning_rate": 4.995811532313833e-05,
      "loss": 5.0109,
      "step": 19000
    },
    {
      "epoch": 0.00336858709640667,
      "grad_norm": 6.026537895202637,
      "learning_rate": 4.995789486586763e-05,
      "loss": 5.0721,
      "step": 19100
    },
    {
      "epoch": 0.0033862236780632495,
      "grad_norm": 6.658654689788818,
      "learning_rate": 4.9957674408596917e-05,
      "loss": 4.9496,
      "step": 19200
    },
    {
      "epoch": 0.003403860259719829,
      "grad_norm": 7.024562835693359,
      "learning_rate": 4.995745395132621e-05,
      "loss": 4.8899,
      "step": 19300
    },
    {
      "epoch": 0.003421496841376408,
      "grad_norm": 5.792415142059326,
      "learning_rate": 4.995723349405551e-05,
      "loss": 4.9621,
      "step": 19400
    },
    {
      "epoch": 0.0034391334230329876,
      "grad_norm": 9.369799613952637,
      "learning_rate": 4.9957013036784796e-05,
      "loss": 5.089,
      "step": 19500
    },
    {
      "epoch": 0.003456770004689567,
      "grad_norm": 5.3195905685424805,
      "learning_rate": 4.995679257951409e-05,
      "loss": 5.0863,
      "step": 19600
    },
    {
      "epoch": 0.0034744065863461463,
      "grad_norm": 6.642240524291992,
      "learning_rate": 4.995657212224338e-05,
      "loss": 5.0443,
      "step": 19700
    },
    {
      "epoch": 0.003492043168002726,
      "grad_norm": 5.181208610534668,
      "learning_rate": 4.9956351664972676e-05,
      "loss": 5.0338,
      "step": 19800
    },
    {
      "epoch": 0.0035096797496593055,
      "grad_norm": 9.760100364685059,
      "learning_rate": 4.9956131207701965e-05,
      "loss": 5.0414,
      "step": 19900
    },
    {
      "epoch": 0.003527316331315885,
      "grad_norm": 5.537276744842529,
      "learning_rate": 4.995591075043126e-05,
      "loss": 5.0439,
      "step": 20000
    },
    {
      "epoch": 0.003544952912972464,
      "grad_norm": 4.421095848083496,
      "learning_rate": 4.9955690293160555e-05,
      "loss": 4.9585,
      "step": 20100
    },
    {
      "epoch": 0.0035625894946290436,
      "grad_norm": 4.991435527801514,
      "learning_rate": 4.9955469835889844e-05,
      "loss": 4.9283,
      "step": 20200
    },
    {
      "epoch": 0.003580226076285623,
      "grad_norm": 7.0234174728393555,
      "learning_rate": 4.995524937861914e-05,
      "loss": 5.0111,
      "step": 20300
    },
    {
      "epoch": 0.0035978626579422023,
      "grad_norm": 5.854907035827637,
      "learning_rate": 4.9955028921348435e-05,
      "loss": 4.9875,
      "step": 20400
    },
    {
      "epoch": 0.0036154992395987817,
      "grad_norm": 7.262376308441162,
      "learning_rate": 4.9954808464077724e-05,
      "loss": 4.9499,
      "step": 20500
    },
    {
      "epoch": 0.0036331358212553615,
      "grad_norm": 5.2209343910217285,
      "learning_rate": 4.995458800680702e-05,
      "loss": 5.0494,
      "step": 20600
    },
    {
      "epoch": 0.003650772402911941,
      "grad_norm": 6.800776481628418,
      "learning_rate": 4.9954367549536315e-05,
      "loss": 5.0525,
      "step": 20700
    },
    {
      "epoch": 0.00366840898456852,
      "grad_norm": 7.345460891723633,
      "learning_rate": 4.9954147092265603e-05,
      "loss": 4.9866,
      "step": 20800
    },
    {
      "epoch": 0.0036860455662250996,
      "grad_norm": 5.740683078765869,
      "learning_rate": 4.99539266349949e-05,
      "loss": 4.9514,
      "step": 20900
    },
    {
      "epoch": 0.003703682147881679,
      "grad_norm": 5.608081340789795,
      "learning_rate": 4.9953706177724194e-05,
      "loss": 5.0243,
      "step": 21000
    },
    {
      "epoch": 0.0037213187295382583,
      "grad_norm": 6.503810882568359,
      "learning_rate": 4.995348572045348e-05,
      "loss": 5.0573,
      "step": 21100
    },
    {
      "epoch": 0.0037389553111948377,
      "grad_norm": 8.174808502197266,
      "learning_rate": 4.995326526318277e-05,
      "loss": 5.0001,
      "step": 21200
    },
    {
      "epoch": 0.0037565918928514175,
      "grad_norm": 4.703897476196289,
      "learning_rate": 4.995304480591207e-05,
      "loss": 5.0372,
      "step": 21300
    },
    {
      "epoch": 0.003774228474507997,
      "grad_norm": 4.994741439819336,
      "learning_rate": 4.9952824348641356e-05,
      "loss": 4.9253,
      "step": 21400
    },
    {
      "epoch": 0.003791865056164576,
      "grad_norm": 7.20707368850708,
      "learning_rate": 4.995260389137065e-05,
      "loss": 4.9246,
      "step": 21500
    },
    {
      "epoch": 0.0038095016378211556,
      "grad_norm": 5.165259838104248,
      "learning_rate": 4.995238343409995e-05,
      "loss": 4.9197,
      "step": 21600
    },
    {
      "epoch": 0.003827138219477735,
      "grad_norm": 8.701043128967285,
      "learning_rate": 4.9952162976829236e-05,
      "loss": 4.9734,
      "step": 21700
    },
    {
      "epoch": 0.0038447748011343143,
      "grad_norm": 6.9191694259643555,
      "learning_rate": 4.995194251955853e-05,
      "loss": 4.9931,
      "step": 21800
    },
    {
      "epoch": 0.0038624113827908937,
      "grad_norm": 5.516060829162598,
      "learning_rate": 4.9951722062287826e-05,
      "loss": 4.9939,
      "step": 21900
    },
    {
      "epoch": 0.003880047964447473,
      "grad_norm": 6.225918292999268,
      "learning_rate": 4.9951501605017115e-05,
      "loss": 4.9655,
      "step": 22000
    },
    {
      "epoch": 0.003897684546104053,
      "grad_norm": 6.183194160461426,
      "learning_rate": 4.995128114774641e-05,
      "loss": 4.868,
      "step": 22100
    },
    {
      "epoch": 0.003915321127760632,
      "grad_norm": 5.0075178146362305,
      "learning_rate": 4.9951060690475706e-05,
      "loss": 4.9117,
      "step": 22200
    },
    {
      "epoch": 0.0039329577094172116,
      "grad_norm": 6.386738300323486,
      "learning_rate": 4.9950840233204995e-05,
      "loss": 4.9989,
      "step": 22300
    },
    {
      "epoch": 0.003950594291073791,
      "grad_norm": 6.0453715324401855,
      "learning_rate": 4.995061977593429e-05,
      "loss": 4.9019,
      "step": 22400
    },
    {
      "epoch": 0.00396823087273037,
      "grad_norm": 6.600708961486816,
      "learning_rate": 4.995039931866358e-05,
      "loss": 4.8772,
      "step": 22500
    },
    {
      "epoch": 0.00398586745438695,
      "grad_norm": 7.405257701873779,
      "learning_rate": 4.9950178861392874e-05,
      "loss": 4.8988,
      "step": 22600
    },
    {
      "epoch": 0.004003504036043529,
      "grad_norm": 4.4869384765625,
      "learning_rate": 4.994995840412216e-05,
      "loss": 4.8553,
      "step": 22700
    },
    {
      "epoch": 0.004021140617700108,
      "grad_norm": 5.7968974113464355,
      "learning_rate": 4.994973794685146e-05,
      "loss": 5.0274,
      "step": 22800
    },
    {
      "epoch": 0.004038777199356688,
      "grad_norm": 5.191774368286133,
      "learning_rate": 4.994951748958075e-05,
      "loss": 4.942,
      "step": 22900
    },
    {
      "epoch": 0.004056413781013267,
      "grad_norm": 7.691312789916992,
      "learning_rate": 4.994929703231004e-05,
      "loss": 4.9146,
      "step": 23000
    },
    {
      "epoch": 0.0040740503626698465,
      "grad_norm": 5.742895603179932,
      "learning_rate": 4.994907657503934e-05,
      "loss": 4.8459,
      "step": 23100
    },
    {
      "epoch": 0.004091686944326427,
      "grad_norm": 6.173206806182861,
      "learning_rate": 4.994885611776863e-05,
      "loss": 4.8443,
      "step": 23200
    },
    {
      "epoch": 0.004109323525983006,
      "grad_norm": 8.172006607055664,
      "learning_rate": 4.994863566049792e-05,
      "loss": 4.867,
      "step": 23300
    },
    {
      "epoch": 0.0041269601076395854,
      "grad_norm": 6.665229320526123,
      "learning_rate": 4.994841520322722e-05,
      "loss": 4.8718,
      "step": 23400
    },
    {
      "epoch": 0.004144596689296165,
      "grad_norm": 7.661552906036377,
      "learning_rate": 4.9948194745956507e-05,
      "loss": 4.9292,
      "step": 23500
    },
    {
      "epoch": 0.004162233270952744,
      "grad_norm": 4.587043285369873,
      "learning_rate": 4.99479742886858e-05,
      "loss": 4.841,
      "step": 23600
    },
    {
      "epoch": 0.0041798698526093235,
      "grad_norm": 6.419572353363037,
      "learning_rate": 4.99477538314151e-05,
      "loss": 4.967,
      "step": 23700
    },
    {
      "epoch": 0.004197506434265903,
      "grad_norm": 6.670077800750732,
      "learning_rate": 4.9947533374144386e-05,
      "loss": 4.9498,
      "step": 23800
    },
    {
      "epoch": 0.004215143015922482,
      "grad_norm": 6.170242786407471,
      "learning_rate": 4.994731291687368e-05,
      "loss": 4.894,
      "step": 23900
    },
    {
      "epoch": 0.004232779597579062,
      "grad_norm": 5.477519989013672,
      "learning_rate": 4.994709245960297e-05,
      "loss": 4.9037,
      "step": 24000
    },
    {
      "epoch": 0.004250416179235641,
      "grad_norm": 6.845274448394775,
      "learning_rate": 4.994687200233226e-05,
      "loss": 4.8925,
      "step": 24100
    },
    {
      "epoch": 0.00426805276089222,
      "grad_norm": 8.233709335327148,
      "learning_rate": 4.9946651545061554e-05,
      "loss": 4.9022,
      "step": 24200
    },
    {
      "epoch": 0.0042856893425488,
      "grad_norm": 5.934253215789795,
      "learning_rate": 4.994643108779085e-05,
      "loss": 4.9148,
      "step": 24300
    },
    {
      "epoch": 0.004303325924205379,
      "grad_norm": 6.258335113525391,
      "learning_rate": 4.994621063052014e-05,
      "loss": 4.8651,
      "step": 24400
    },
    {
      "epoch": 0.0043209625058619585,
      "grad_norm": 5.691579341888428,
      "learning_rate": 4.9945990173249434e-05,
      "loss": 4.9103,
      "step": 24500
    },
    {
      "epoch": 0.004338599087518538,
      "grad_norm": 5.352030277252197,
      "learning_rate": 4.994576971597873e-05,
      "loss": 4.8565,
      "step": 24600
    },
    {
      "epoch": 0.004356235669175118,
      "grad_norm": 5.397042751312256,
      "learning_rate": 4.994554925870802e-05,
      "loss": 4.8541,
      "step": 24700
    },
    {
      "epoch": 0.004373872250831697,
      "grad_norm": 7.008400917053223,
      "learning_rate": 4.9945328801437314e-05,
      "loss": 4.9081,
      "step": 24800
    },
    {
      "epoch": 0.004391508832488277,
      "grad_norm": 5.2067060470581055,
      "learning_rate": 4.994510834416661e-05,
      "loss": 4.8791,
      "step": 24900
    },
    {
      "epoch": 0.004409145414144856,
      "grad_norm": 9.102004051208496,
      "learning_rate": 4.99448878868959e-05,
      "loss": 4.8342,
      "step": 25000
    },
    {
      "epoch": 0.0044267819958014355,
      "grad_norm": 5.00399112701416,
      "learning_rate": 4.994466742962519e-05,
      "loss": 4.9686,
      "step": 25100
    },
    {
      "epoch": 0.004444418577458015,
      "grad_norm": 8.421855926513672,
      "learning_rate": 4.994444697235449e-05,
      "loss": 4.9671,
      "step": 25200
    },
    {
      "epoch": 0.004462055159114594,
      "grad_norm": 8.960397720336914,
      "learning_rate": 4.994422651508378e-05,
      "loss": 4.7965,
      "step": 25300
    },
    {
      "epoch": 0.004479691740771174,
      "grad_norm": 5.7137451171875,
      "learning_rate": 4.994400605781307e-05,
      "loss": 4.8386,
      "step": 25400
    },
    {
      "epoch": 0.004497328322427753,
      "grad_norm": 5.45203971862793,
      "learning_rate": 4.994378560054236e-05,
      "loss": 4.7609,
      "step": 25500
    },
    {
      "epoch": 0.004514964904084332,
      "grad_norm": 6.272965431213379,
      "learning_rate": 4.994356514327165e-05,
      "loss": 4.8293,
      "step": 25600
    },
    {
      "epoch": 0.004532601485740912,
      "grad_norm": 6.241883277893066,
      "learning_rate": 4.9943344686000946e-05,
      "loss": 4.721,
      "step": 25700
    },
    {
      "epoch": 0.004550238067397491,
      "grad_norm": 5.654331207275391,
      "learning_rate": 4.994312422873024e-05,
      "loss": 4.7269,
      "step": 25800
    },
    {
      "epoch": 0.0045678746490540705,
      "grad_norm": 6.504383563995361,
      "learning_rate": 4.994290377145953e-05,
      "loss": 4.843,
      "step": 25900
    },
    {
      "epoch": 0.00458551123071065,
      "grad_norm": 5.852560997009277,
      "learning_rate": 4.9942683314188825e-05,
      "loss": 4.9165,
      "step": 26000
    },
    {
      "epoch": 0.004603147812367229,
      "grad_norm": 5.33167839050293,
      "learning_rate": 4.994246285691812e-05,
      "loss": 4.8997,
      "step": 26100
    },
    {
      "epoch": 0.004620784394023809,
      "grad_norm": 5.343764781951904,
      "learning_rate": 4.994224239964741e-05,
      "loss": 4.7765,
      "step": 26200
    },
    {
      "epoch": 0.004638420975680389,
      "grad_norm": 9.532696723937988,
      "learning_rate": 4.9942021942376705e-05,
      "loss": 4.9343,
      "step": 26300
    },
    {
      "epoch": 0.004656057557336968,
      "grad_norm": 4.793680191040039,
      "learning_rate": 4.9941801485106e-05,
      "loss": 4.9423,
      "step": 26400
    },
    {
      "epoch": 0.0046736941389935475,
      "grad_norm": 6.637661457061768,
      "learning_rate": 4.994158102783529e-05,
      "loss": 4.7584,
      "step": 26500
    },
    {
      "epoch": 0.004691330720650127,
      "grad_norm": 7.36094331741333,
      "learning_rate": 4.9941360570564585e-05,
      "loss": 4.8588,
      "step": 26600
    },
    {
      "epoch": 0.004708967302306706,
      "grad_norm": 6.636943340301514,
      "learning_rate": 4.994114011329388e-05,
      "loss": 4.8401,
      "step": 26700
    },
    {
      "epoch": 0.004726603883963286,
      "grad_norm": 5.554035186767578,
      "learning_rate": 4.994091965602317e-05,
      "loss": 4.8158,
      "step": 26800
    },
    {
      "epoch": 0.004744240465619865,
      "grad_norm": 4.983875274658203,
      "learning_rate": 4.994069919875246e-05,
      "loss": 4.8642,
      "step": 26900
    },
    {
      "epoch": 0.004761877047276444,
      "grad_norm": 6.438251972198486,
      "learning_rate": 4.994047874148175e-05,
      "loss": 4.8574,
      "step": 27000
    },
    {
      "epoch": 0.004779513628933024,
      "grad_norm": 6.063092231750488,
      "learning_rate": 4.994025828421104e-05,
      "loss": 4.8177,
      "step": 27100
    },
    {
      "epoch": 0.004797150210589603,
      "grad_norm": 7.101676940917969,
      "learning_rate": 4.994003782694034e-05,
      "loss": 4.7957,
      "step": 27200
    },
    {
      "epoch": 0.0048147867922461824,
      "grad_norm": 7.293950080871582,
      "learning_rate": 4.993981736966963e-05,
      "loss": 4.8256,
      "step": 27300
    },
    {
      "epoch": 0.004832423373902762,
      "grad_norm": 6.981071472167969,
      "learning_rate": 4.993959691239892e-05,
      "loss": 4.9323,
      "step": 27400
    },
    {
      "epoch": 0.004850059955559341,
      "grad_norm": 4.997921943664551,
      "learning_rate": 4.993937645512822e-05,
      "loss": 4.9039,
      "step": 27500
    },
    {
      "epoch": 0.0048676965372159205,
      "grad_norm": 6.644820690155029,
      "learning_rate": 4.993915599785751e-05,
      "loss": 4.7962,
      "step": 27600
    },
    {
      "epoch": 0.004885333118872501,
      "grad_norm": 4.712808609008789,
      "learning_rate": 4.99389355405868e-05,
      "loss": 4.8685,
      "step": 27700
    },
    {
      "epoch": 0.00490296970052908,
      "grad_norm": 9.38625717163086,
      "learning_rate": 4.9938715083316096e-05,
      "loss": 4.8945,
      "step": 27800
    },
    {
      "epoch": 0.0049206062821856595,
      "grad_norm": 5.495824337005615,
      "learning_rate": 4.993849462604539e-05,
      "loss": 4.7532,
      "step": 27900
    },
    {
      "epoch": 0.004938242863842239,
      "grad_norm": 7.001979351043701,
      "learning_rate": 4.993827416877468e-05,
      "loss": 4.8096,
      "step": 28000
    },
    {
      "epoch": 0.004955879445498818,
      "grad_norm": 6.288053512573242,
      "learning_rate": 4.9938053711503976e-05,
      "loss": 4.7686,
      "step": 28100
    },
    {
      "epoch": 0.004973516027155398,
      "grad_norm": 6.399471282958984,
      "learning_rate": 4.993783325423327e-05,
      "loss": 4.655,
      "step": 28200
    },
    {
      "epoch": 0.004991152608811977,
      "grad_norm": 5.38668155670166,
      "learning_rate": 4.993761279696256e-05,
      "loss": 4.7505,
      "step": 28300
    },
    {
      "epoch": 0.005008789190468556,
      "grad_norm": 7.048568248748779,
      "learning_rate": 4.993739233969185e-05,
      "loss": 4.8569,
      "step": 28400
    },
    {
      "epoch": 0.005026425772125136,
      "grad_norm": 5.309178352355957,
      "learning_rate": 4.9937171882421144e-05,
      "loss": 4.8284,
      "step": 28500
    },
    {
      "epoch": 0.005044062353781715,
      "grad_norm": 5.98450231552124,
      "learning_rate": 4.993695142515043e-05,
      "loss": 4.7204,
      "step": 28600
    },
    {
      "epoch": 0.005061698935438294,
      "grad_norm": 8.222147941589355,
      "learning_rate": 4.993673096787973e-05,
      "loss": 4.7734,
      "step": 28700
    },
    {
      "epoch": 0.005079335517094874,
      "grad_norm": 6.196311950683594,
      "learning_rate": 4.9936510510609024e-05,
      "loss": 4.7416,
      "step": 28800
    },
    {
      "epoch": 0.005096972098751453,
      "grad_norm": 4.146739959716797,
      "learning_rate": 4.993629005333831e-05,
      "loss": 4.7192,
      "step": 28900
    },
    {
      "epoch": 0.0051146086804080325,
      "grad_norm": 6.485233783721924,
      "learning_rate": 4.993606959606761e-05,
      "loss": 4.6853,
      "step": 29000
    },
    {
      "epoch": 0.005132245262064613,
      "grad_norm": 6.656829833984375,
      "learning_rate": 4.9935849138796904e-05,
      "loss": 4.7552,
      "step": 29100
    },
    {
      "epoch": 0.005149881843721192,
      "grad_norm": 7.048202037811279,
      "learning_rate": 4.993562868152619e-05,
      "loss": 4.7714,
      "step": 29200
    },
    {
      "epoch": 0.0051675184253777715,
      "grad_norm": 6.19257926940918,
      "learning_rate": 4.993540822425549e-05,
      "loss": 4.7624,
      "step": 29300
    },
    {
      "epoch": 0.005185155007034351,
      "grad_norm": 6.968923568725586,
      "learning_rate": 4.993518776698478e-05,
      "loss": 4.72,
      "step": 29400
    },
    {
      "epoch": 0.00520279158869093,
      "grad_norm": 5.075300693511963,
      "learning_rate": 4.993496730971407e-05,
      "loss": 4.806,
      "step": 29500
    },
    {
      "epoch": 0.00522042817034751,
      "grad_norm": 10.682802200317383,
      "learning_rate": 4.993474685244337e-05,
      "loss": 4.7351,
      "step": 29600
    },
    {
      "epoch": 0.005238064752004089,
      "grad_norm": 5.177377700805664,
      "learning_rate": 4.9934526395172656e-05,
      "loss": 4.7019,
      "step": 29700
    },
    {
      "epoch": 0.005255701333660668,
      "grad_norm": 6.6545090675354,
      "learning_rate": 4.993430593790195e-05,
      "loss": 4.7797,
      "step": 29800
    },
    {
      "epoch": 0.005273337915317248,
      "grad_norm": 6.58167839050293,
      "learning_rate": 4.993408548063124e-05,
      "loss": 4.7531,
      "step": 29900
    },
    {
      "epoch": 0.005290974496973827,
      "grad_norm": 5.3109025955200195,
      "learning_rate": 4.9933865023360536e-05,
      "loss": 4.675,
      "step": 30000
    },
    {
      "epoch": 0.005308611078630406,
      "grad_norm": 7.183428764343262,
      "learning_rate": 4.9933644566089824e-05,
      "loss": 4.8605,
      "step": 30100
    },
    {
      "epoch": 0.005326247660286986,
      "grad_norm": 6.536733150482178,
      "learning_rate": 4.993342410881912e-05,
      "loss": 4.7942,
      "step": 30200
    },
    {
      "epoch": 0.005343884241943565,
      "grad_norm": 6.04018497467041,
      "learning_rate": 4.9933203651548415e-05,
      "loss": 4.6795,
      "step": 30300
    },
    {
      "epoch": 0.0053615208236001445,
      "grad_norm": 7.541137218475342,
      "learning_rate": 4.9932983194277704e-05,
      "loss": 4.696,
      "step": 30400
    },
    {
      "epoch": 0.005379157405256724,
      "grad_norm": 5.653073787689209,
      "learning_rate": 4.9932762737007e-05,
      "loss": 4.7214,
      "step": 30500
    },
    {
      "epoch": 0.005396793986913304,
      "grad_norm": 7.126154899597168,
      "learning_rate": 4.9932542279736295e-05,
      "loss": 4.6838,
      "step": 30600
    },
    {
      "epoch": 0.0054144305685698835,
      "grad_norm": 9.37454605102539,
      "learning_rate": 4.993232182246559e-05,
      "loss": 4.6754,
      "step": 30700
    },
    {
      "epoch": 0.005432067150226463,
      "grad_norm": 5.411036968231201,
      "learning_rate": 4.993210136519488e-05,
      "loss": 4.6745,
      "step": 30800
    },
    {
      "epoch": 0.005449703731883042,
      "grad_norm": 4.985154628753662,
      "learning_rate": 4.9931880907924175e-05,
      "loss": 4.7819,
      "step": 30900
    },
    {
      "epoch": 0.005467340313539622,
      "grad_norm": 5.366785526275635,
      "learning_rate": 4.993166045065347e-05,
      "loss": 4.8352,
      "step": 31000
    },
    {
      "epoch": 0.005484976895196201,
      "grad_norm": 6.698453903198242,
      "learning_rate": 4.993143999338276e-05,
      "loss": 4.7323,
      "step": 31100
    },
    {
      "epoch": 0.00550261347685278,
      "grad_norm": 5.553034782409668,
      "learning_rate": 4.993121953611205e-05,
      "loss": 4.7084,
      "step": 31200
    },
    {
      "epoch": 0.00552025005850936,
      "grad_norm": 7.018762111663818,
      "learning_rate": 4.993099907884134e-05,
      "loss": 4.6378,
      "step": 31300
    },
    {
      "epoch": 0.005537886640165939,
      "grad_norm": 7.043036460876465,
      "learning_rate": 4.993077862157063e-05,
      "loss": 4.8273,
      "step": 31400
    },
    {
      "epoch": 0.005555523221822518,
      "grad_norm": 5.976878643035889,
      "learning_rate": 4.993055816429993e-05,
      "loss": 4.8819,
      "step": 31500
    },
    {
      "epoch": 0.005573159803479098,
      "grad_norm": 6.679941177368164,
      "learning_rate": 4.993033770702922e-05,
      "loss": 4.6947,
      "step": 31600
    },
    {
      "epoch": 0.005590796385135677,
      "grad_norm": 8.526371955871582,
      "learning_rate": 4.993011724975851e-05,
      "loss": 4.6579,
      "step": 31700
    },
    {
      "epoch": 0.0056084329667922565,
      "grad_norm": 4.791290760040283,
      "learning_rate": 4.992989679248781e-05,
      "loss": 4.8,
      "step": 31800
    },
    {
      "epoch": 0.005626069548448836,
      "grad_norm": 5.097718238830566,
      "learning_rate": 4.99296763352171e-05,
      "loss": 4.7731,
      "step": 31900
    },
    {
      "epoch": 0.005643706130105415,
      "grad_norm": 8.102089881896973,
      "learning_rate": 4.992945587794639e-05,
      "loss": 4.7276,
      "step": 32000
    },
    {
      "epoch": 0.0056613427117619955,
      "grad_norm": 5.597367763519287,
      "learning_rate": 4.9929235420675686e-05,
      "loss": 4.6191,
      "step": 32100
    },
    {
      "epoch": 0.005678979293418575,
      "grad_norm": 7.396584987640381,
      "learning_rate": 4.992901496340498e-05,
      "loss": 4.7249,
      "step": 32200
    },
    {
      "epoch": 0.005696615875075154,
      "grad_norm": 6.664928436279297,
      "learning_rate": 4.992879450613427e-05,
      "loss": 4.5978,
      "step": 32300
    },
    {
      "epoch": 0.0057142524567317336,
      "grad_norm": 6.928153038024902,
      "learning_rate": 4.9928574048863566e-05,
      "loss": 4.7,
      "step": 32400
    },
    {
      "epoch": 0.005731889038388313,
      "grad_norm": 5.434366226196289,
      "learning_rate": 4.9928353591592855e-05,
      "loss": 4.7247,
      "step": 32500
    },
    {
      "epoch": 0.005749525620044892,
      "grad_norm": 12.978455543518066,
      "learning_rate": 4.992813313432215e-05,
      "loss": 4.7643,
      "step": 32600
    },
    {
      "epoch": 0.005767162201701472,
      "grad_norm": 5.953181266784668,
      "learning_rate": 4.992791267705144e-05,
      "loss": 4.7794,
      "step": 32700
    },
    {
      "epoch": 0.005784798783358051,
      "grad_norm": 5.412333011627197,
      "learning_rate": 4.9927692219780734e-05,
      "loss": 4.6503,
      "step": 32800
    },
    {
      "epoch": 0.00580243536501463,
      "grad_norm": 5.309633255004883,
      "learning_rate": 4.992747176251002e-05,
      "loss": 4.6656,
      "step": 32900
    },
    {
      "epoch": 0.00582007194667121,
      "grad_norm": 5.625385284423828,
      "learning_rate": 4.992725130523932e-05,
      "loss": 4.7171,
      "step": 33000
    },
    {
      "epoch": 0.005837708528327789,
      "grad_norm": 6.366658687591553,
      "learning_rate": 4.9927030847968614e-05,
      "loss": 4.728,
      "step": 33100
    },
    {
      "epoch": 0.0058553451099843685,
      "grad_norm": 6.548155784606934,
      "learning_rate": 4.99268103906979e-05,
      "loss": 4.7033,
      "step": 33200
    },
    {
      "epoch": 0.005872981691640948,
      "grad_norm": 7.3017897605896,
      "learning_rate": 4.99265899334272e-05,
      "loss": 4.7537,
      "step": 33300
    },
    {
      "epoch": 0.005890618273297527,
      "grad_norm": 8.251853942871094,
      "learning_rate": 4.9926369476156494e-05,
      "loss": 4.7177,
      "step": 33400
    },
    {
      "epoch": 0.005908254854954107,
      "grad_norm": 7.4376912117004395,
      "learning_rate": 4.992614901888578e-05,
      "loss": 4.6753,
      "step": 33500
    },
    {
      "epoch": 0.005925891436610687,
      "grad_norm": 8.994784355163574,
      "learning_rate": 4.992592856161508e-05,
      "loss": 4.721,
      "step": 33600
    },
    {
      "epoch": 0.005943528018267266,
      "grad_norm": 6.5303239822387695,
      "learning_rate": 4.992570810434437e-05,
      "loss": 4.6914,
      "step": 33700
    },
    {
      "epoch": 0.0059611645999238455,
      "grad_norm": 9.340738296508789,
      "learning_rate": 4.992548764707366e-05,
      "loss": 4.6881,
      "step": 33800
    },
    {
      "epoch": 0.005978801181580425,
      "grad_norm": 7.069095134735107,
      "learning_rate": 4.992526718980296e-05,
      "loss": 4.7229,
      "step": 33900
    },
    {
      "epoch": 0.005996437763237004,
      "grad_norm": 5.266623020172119,
      "learning_rate": 4.9925046732532246e-05,
      "loss": 4.661,
      "step": 34000
    },
    {
      "epoch": 0.006014074344893584,
      "grad_norm": 5.58901834487915,
      "learning_rate": 4.9924826275261535e-05,
      "loss": 4.738,
      "step": 34100
    },
    {
      "epoch": 0.006031710926550163,
      "grad_norm": 5.909383773803711,
      "learning_rate": 4.992460581799083e-05,
      "loss": 4.7246,
      "step": 34200
    },
    {
      "epoch": 0.006049347508206742,
      "grad_norm": 4.253833770751953,
      "learning_rate": 4.9924385360720126e-05,
      "loss": 4.7569,
      "step": 34300
    },
    {
      "epoch": 0.006066984089863322,
      "grad_norm": 5.390399932861328,
      "learning_rate": 4.9924164903449414e-05,
      "loss": 4.7545,
      "step": 34400
    },
    {
      "epoch": 0.006084620671519901,
      "grad_norm": 6.6153178215026855,
      "learning_rate": 4.992394444617871e-05,
      "loss": 4.662,
      "step": 34500
    },
    {
      "epoch": 0.0061022572531764805,
      "grad_norm": 7.390261173248291,
      "learning_rate": 4.9923723988908005e-05,
      "loss": 4.561,
      "step": 34600
    },
    {
      "epoch": 0.00611989383483306,
      "grad_norm": 8.330123901367188,
      "learning_rate": 4.9923503531637294e-05,
      "loss": 4.7609,
      "step": 34700
    },
    {
      "epoch": 0.006137530416489639,
      "grad_norm": 7.776208400726318,
      "learning_rate": 4.992328307436659e-05,
      "loss": 4.604,
      "step": 34800
    },
    {
      "epoch": 0.0061551669981462186,
      "grad_norm": 8.323336601257324,
      "learning_rate": 4.9923062617095885e-05,
      "loss": 4.7759,
      "step": 34900
    },
    {
      "epoch": 0.006172803579802798,
      "grad_norm": 6.131304740905762,
      "learning_rate": 4.9922842159825174e-05,
      "loss": 4.689,
      "step": 35000
    },
    {
      "epoch": 0.006190440161459378,
      "grad_norm": 8.214788436889648,
      "learning_rate": 4.992262170255447e-05,
      "loss": 4.6248,
      "step": 35100
    },
    {
      "epoch": 0.0062080767431159575,
      "grad_norm": 6.994889736175537,
      "learning_rate": 4.9922401245283765e-05,
      "loss": 4.6823,
      "step": 35200
    },
    {
      "epoch": 0.006225713324772537,
      "grad_norm": 7.763645172119141,
      "learning_rate": 4.992218078801305e-05,
      "loss": 4.6706,
      "step": 35300
    },
    {
      "epoch": 0.006243349906429116,
      "grad_norm": 7.231963634490967,
      "learning_rate": 4.992196033074235e-05,
      "loss": 4.6866,
      "step": 35400
    },
    {
      "epoch": 0.006260986488085696,
      "grad_norm": 5.561907768249512,
      "learning_rate": 4.992173987347164e-05,
      "loss": 4.6373,
      "step": 35500
    },
    {
      "epoch": 0.006278623069742275,
      "grad_norm": 6.487305641174316,
      "learning_rate": 4.9921519416200926e-05,
      "loss": 4.6361,
      "step": 35600
    },
    {
      "epoch": 0.006296259651398854,
      "grad_norm": 8.042118072509766,
      "learning_rate": 4.992129895893022e-05,
      "loss": 4.5888,
      "step": 35700
    },
    {
      "epoch": 0.006313896233055434,
      "grad_norm": 6.060412883758545,
      "learning_rate": 4.992107850165952e-05,
      "loss": 4.6368,
      "step": 35800
    },
    {
      "epoch": 0.006331532814712013,
      "grad_norm": 6.088676929473877,
      "learning_rate": 4.9920858044388806e-05,
      "loss": 4.6749,
      "step": 35900
    },
    {
      "epoch": 0.0063491693963685925,
      "grad_norm": 7.718728065490723,
      "learning_rate": 4.99206375871181e-05,
      "loss": 4.788,
      "step": 36000
    },
    {
      "epoch": 0.006366805978025172,
      "grad_norm": 5.756263256072998,
      "learning_rate": 4.99204171298474e-05,
      "loss": 4.7529,
      "step": 36100
    },
    {
      "epoch": 0.006384442559681751,
      "grad_norm": 7.391767501831055,
      "learning_rate": 4.9920196672576685e-05,
      "loss": 4.6284,
      "step": 36200
    },
    {
      "epoch": 0.0064020791413383306,
      "grad_norm": 6.685281753540039,
      "learning_rate": 4.991997621530598e-05,
      "loss": 4.6971,
      "step": 36300
    },
    {
      "epoch": 0.00641971572299491,
      "grad_norm": 6.337024211883545,
      "learning_rate": 4.9919755758035276e-05,
      "loss": 4.6962,
      "step": 36400
    },
    {
      "epoch": 0.006437352304651489,
      "grad_norm": 7.3639397621154785,
      "learning_rate": 4.9919535300764565e-05,
      "loss": 4.7332,
      "step": 36500
    },
    {
      "epoch": 0.0064549888863080695,
      "grad_norm": 6.193415641784668,
      "learning_rate": 4.991931484349386e-05,
      "loss": 4.5723,
      "step": 36600
    },
    {
      "epoch": 0.006472625467964649,
      "grad_norm": 6.006927013397217,
      "learning_rate": 4.9919094386223156e-05,
      "loss": 4.6254,
      "step": 36700
    },
    {
      "epoch": 0.006490262049621228,
      "grad_norm": 7.478738307952881,
      "learning_rate": 4.9918873928952445e-05,
      "loss": 4.6488,
      "step": 36800
    },
    {
      "epoch": 0.006507898631277808,
      "grad_norm": 8.770774841308594,
      "learning_rate": 4.9918653471681733e-05,
      "loss": 4.6707,
      "step": 36900
    },
    {
      "epoch": 0.006525535212934387,
      "grad_norm": 5.082668781280518,
      "learning_rate": 4.991843301441103e-05,
      "loss": 4.6021,
      "step": 37000
    },
    {
      "epoch": 0.006543171794590966,
      "grad_norm": 7.019100666046143,
      "learning_rate": 4.991821255714032e-05,
      "loss": 4.6153,
      "step": 37100
    },
    {
      "epoch": 0.006560808376247546,
      "grad_norm": 7.139100074768066,
      "learning_rate": 4.991799209986961e-05,
      "loss": 4.6301,
      "step": 37200
    },
    {
      "epoch": 0.006578444957904125,
      "grad_norm": 9.155359268188477,
      "learning_rate": 4.991777164259891e-05,
      "loss": 4.6734,
      "step": 37300
    },
    {
      "epoch": 0.0065960815395607044,
      "grad_norm": 7.246128559112549,
      "learning_rate": 4.99175511853282e-05,
      "loss": 4.5821,
      "step": 37400
    },
    {
      "epoch": 0.006613718121217284,
      "grad_norm": 7.529854774475098,
      "learning_rate": 4.991733072805749e-05,
      "loss": 4.5871,
      "step": 37500
    },
    {
      "epoch": 0.006631354702873863,
      "grad_norm": 5.488953590393066,
      "learning_rate": 4.991711027078679e-05,
      "loss": 4.6429,
      "step": 37600
    },
    {
      "epoch": 0.0066489912845304425,
      "grad_norm": 7.095541477203369,
      "learning_rate": 4.991688981351608e-05,
      "loss": 4.6552,
      "step": 37700
    },
    {
      "epoch": 0.006666627866187022,
      "grad_norm": 8.152517318725586,
      "learning_rate": 4.991666935624537e-05,
      "loss": 4.559,
      "step": 37800
    },
    {
      "epoch": 0.006684264447843601,
      "grad_norm": 6.291676998138428,
      "learning_rate": 4.991644889897467e-05,
      "loss": 4.5925,
      "step": 37900
    },
    {
      "epoch": 0.006701901029500181,
      "grad_norm": 7.524388790130615,
      "learning_rate": 4.9916228441703956e-05,
      "loss": 4.6724,
      "step": 38000
    },
    {
      "epoch": 0.006719537611156761,
      "grad_norm": 4.770275592803955,
      "learning_rate": 4.991600798443325e-05,
      "loss": 4.674,
      "step": 38100
    },
    {
      "epoch": 0.00673717419281334,
      "grad_norm": 8.840768814086914,
      "learning_rate": 4.991578752716255e-05,
      "loss": 4.5187,
      "step": 38200
    },
    {
      "epoch": 0.00675481077446992,
      "grad_norm": 8.746743202209473,
      "learning_rate": 4.9915567069891836e-05,
      "loss": 4.5819,
      "step": 38300
    },
    {
      "epoch": 0.006772447356126499,
      "grad_norm": 7.005561351776123,
      "learning_rate": 4.9915346612621125e-05,
      "loss": 4.5992,
      "step": 38400
    },
    {
      "epoch": 0.006790083937783078,
      "grad_norm": 9.272071838378906,
      "learning_rate": 4.991512615535042e-05,
      "loss": 4.5406,
      "step": 38500
    },
    {
      "epoch": 0.006807720519439658,
      "grad_norm": 6.319397449493408,
      "learning_rate": 4.991490569807971e-05,
      "loss": 4.6727,
      "step": 38600
    },
    {
      "epoch": 0.006825357101096237,
      "grad_norm": 5.495326519012451,
      "learning_rate": 4.9914685240809004e-05,
      "loss": 4.5923,
      "step": 38700
    },
    {
      "epoch": 0.006842993682752816,
      "grad_norm": 6.589612007141113,
      "learning_rate": 4.99144647835383e-05,
      "loss": 4.6351,
      "step": 38800
    },
    {
      "epoch": 0.006860630264409396,
      "grad_norm": 6.085010528564453,
      "learning_rate": 4.991424432626759e-05,
      "loss": 4.5888,
      "step": 38900
    },
    {
      "epoch": 0.006878266846065975,
      "grad_norm": 7.121387004852295,
      "learning_rate": 4.9914023868996884e-05,
      "loss": 4.5686,
      "step": 39000
    },
    {
      "epoch": 0.0068959034277225545,
      "grad_norm": 6.441159725189209,
      "learning_rate": 4.991380341172618e-05,
      "loss": 4.6722,
      "step": 39100
    },
    {
      "epoch": 0.006913540009379134,
      "grad_norm": 9.684300422668457,
      "learning_rate": 4.991358295445547e-05,
      "loss": 4.5291,
      "step": 39200
    },
    {
      "epoch": 0.006931176591035713,
      "grad_norm": 7.631382942199707,
      "learning_rate": 4.9913362497184764e-05,
      "loss": 4.6594,
      "step": 39300
    },
    {
      "epoch": 0.006948813172692293,
      "grad_norm": 7.671635627746582,
      "learning_rate": 4.991314203991406e-05,
      "loss": 4.6741,
      "step": 39400
    },
    {
      "epoch": 0.006966449754348872,
      "grad_norm": 5.935102939605713,
      "learning_rate": 4.991292158264335e-05,
      "loss": 4.6188,
      "step": 39500
    },
    {
      "epoch": 0.006984086336005452,
      "grad_norm": 6.026505470275879,
      "learning_rate": 4.991270112537264e-05,
      "loss": 4.5549,
      "step": 39600
    },
    {
      "epoch": 0.007001722917662032,
      "grad_norm": 6.463296413421631,
      "learning_rate": 4.991248066810193e-05,
      "loss": 4.6711,
      "step": 39700
    },
    {
      "epoch": 0.007019359499318611,
      "grad_norm": 6.662696361541748,
      "learning_rate": 4.991226021083123e-05,
      "loss": 4.5869,
      "step": 39800
    },
    {
      "epoch": 0.00703699608097519,
      "grad_norm": 7.472431659698486,
      "learning_rate": 4.9912039753560516e-05,
      "loss": 4.622,
      "step": 39900
    },
    {
      "epoch": 0.00705463266263177,
      "grad_norm": 6.792495250701904,
      "learning_rate": 4.991181929628981e-05,
      "loss": 4.6323,
      "step": 40000
    },
    {
      "epoch": 0.007072269244288349,
      "grad_norm": 7.969762802124023,
      "learning_rate": 4.99115988390191e-05,
      "loss": 4.5553,
      "step": 40100
    },
    {
      "epoch": 0.007089905825944928,
      "grad_norm": 7.615133762359619,
      "learning_rate": 4.9911378381748396e-05,
      "loss": 4.6685,
      "step": 40200
    },
    {
      "epoch": 0.007107542407601508,
      "grad_norm": 8.050171852111816,
      "learning_rate": 4.991115792447769e-05,
      "loss": 4.5701,
      "step": 40300
    },
    {
      "epoch": 0.007125178989258087,
      "grad_norm": 5.805362701416016,
      "learning_rate": 4.991093746720698e-05,
      "loss": 4.5814,
      "step": 40400
    },
    {
      "epoch": 0.0071428155709146665,
      "grad_norm": 8.756265640258789,
      "learning_rate": 4.9910717009936275e-05,
      "loss": 4.6501,
      "step": 40500
    },
    {
      "epoch": 0.007160452152571246,
      "grad_norm": 6.582508087158203,
      "learning_rate": 4.991049655266557e-05,
      "loss": 4.6351,
      "step": 40600
    },
    {
      "epoch": 0.007178088734227825,
      "grad_norm": 5.937710762023926,
      "learning_rate": 4.991027609539486e-05,
      "loss": 4.5647,
      "step": 40700
    },
    {
      "epoch": 0.007195725315884405,
      "grad_norm": 8.899298667907715,
      "learning_rate": 4.9910055638124155e-05,
      "loss": 4.6153,
      "step": 40800
    },
    {
      "epoch": 0.007213361897540984,
      "grad_norm": 6.543209075927734,
      "learning_rate": 4.990983518085345e-05,
      "loss": 4.6683,
      "step": 40900
    },
    {
      "epoch": 0.007230998479197563,
      "grad_norm": 7.59097146987915,
      "learning_rate": 4.990961472358274e-05,
      "loss": 4.6659,
      "step": 41000
    },
    {
      "epoch": 0.007248635060854144,
      "grad_norm": 5.589170455932617,
      "learning_rate": 4.9909394266312035e-05,
      "loss": 4.5547,
      "step": 41100
    },
    {
      "epoch": 0.007266271642510723,
      "grad_norm": 8.510977745056152,
      "learning_rate": 4.990917380904132e-05,
      "loss": 4.4989,
      "step": 41200
    },
    {
      "epoch": 0.007283908224167302,
      "grad_norm": 9.264147758483887,
      "learning_rate": 4.990895335177062e-05,
      "loss": 4.578,
      "step": 41300
    },
    {
      "epoch": 0.007301544805823882,
      "grad_norm": 9.042643547058105,
      "learning_rate": 4.990873289449991e-05,
      "loss": 4.6221,
      "step": 41400
    },
    {
      "epoch": 0.007319181387480461,
      "grad_norm": 9.71599006652832,
      "learning_rate": 4.99085124372292e-05,
      "loss": 4.5465,
      "step": 41500
    },
    {
      "epoch": 0.00733681796913704,
      "grad_norm": 7.768347263336182,
      "learning_rate": 4.99082919799585e-05,
      "loss": 4.6786,
      "step": 41600
    },
    {
      "epoch": 0.00735445455079362,
      "grad_norm": 6.931342601776123,
      "learning_rate": 4.990807152268779e-05,
      "loss": 4.5779,
      "step": 41700
    },
    {
      "epoch": 0.007372091132450199,
      "grad_norm": 4.8949174880981445,
      "learning_rate": 4.990785106541708e-05,
      "loss": 4.605,
      "step": 41800
    },
    {
      "epoch": 0.0073897277141067785,
      "grad_norm": 6.052083969116211,
      "learning_rate": 4.990763060814638e-05,
      "loss": 4.591,
      "step": 41900
    },
    {
      "epoch": 0.007407364295763358,
      "grad_norm": 6.620206832885742,
      "learning_rate": 4.990741015087567e-05,
      "loss": 4.6239,
      "step": 42000
    },
    {
      "epoch": 0.007425000877419937,
      "grad_norm": 4.787313938140869,
      "learning_rate": 4.990718969360496e-05,
      "loss": 4.5844,
      "step": 42100
    },
    {
      "epoch": 0.007442637459076517,
      "grad_norm": 6.432993412017822,
      "learning_rate": 4.990696923633426e-05,
      "loss": 4.5934,
      "step": 42200
    },
    {
      "epoch": 0.007460274040733096,
      "grad_norm": 5.482911109924316,
      "learning_rate": 4.9906748779063546e-05,
      "loss": 4.6016,
      "step": 42300
    },
    {
      "epoch": 0.007477910622389675,
      "grad_norm": 5.59726619720459,
      "learning_rate": 4.990652832179284e-05,
      "loss": 4.5602,
      "step": 42400
    },
    {
      "epoch": 0.007495547204046255,
      "grad_norm": 5.327610969543457,
      "learning_rate": 4.990630786452213e-05,
      "loss": 4.6193,
      "step": 42500
    },
    {
      "epoch": 0.007513183785702835,
      "grad_norm": 9.667963981628418,
      "learning_rate": 4.9906087407251426e-05,
      "loss": 4.5576,
      "step": 42600
    },
    {
      "epoch": 0.007530820367359414,
      "grad_norm": 7.759335041046143,
      "learning_rate": 4.9905866949980715e-05,
      "loss": 4.5523,
      "step": 42700
    },
    {
      "epoch": 0.007548456949015994,
      "grad_norm": 4.130037307739258,
      "learning_rate": 4.990564649271001e-05,
      "loss": 4.5009,
      "step": 42800
    },
    {
      "epoch": 0.007566093530672573,
      "grad_norm": 5.325657367706299,
      "learning_rate": 4.99054260354393e-05,
      "loss": 4.5603,
      "step": 42900
    },
    {
      "epoch": 0.007583730112329152,
      "grad_norm": 4.882652759552002,
      "learning_rate": 4.9905205578168594e-05,
      "loss": 4.553,
      "step": 43000
    },
    {
      "epoch": 0.007601366693985732,
      "grad_norm": 7.4212422370910645,
      "learning_rate": 4.990498512089789e-05,
      "loss": 4.5234,
      "step": 43100
    },
    {
      "epoch": 0.007619003275642311,
      "grad_norm": 5.020900249481201,
      "learning_rate": 4.990476466362718e-05,
      "loss": 4.5065,
      "step": 43200
    },
    {
      "epoch": 0.0076366398572988905,
      "grad_norm": 6.096640586853027,
      "learning_rate": 4.9904544206356474e-05,
      "loss": 4.5049,
      "step": 43300
    },
    {
      "epoch": 0.00765427643895547,
      "grad_norm": 5.649666786193848,
      "learning_rate": 4.990432374908577e-05,
      "loss": 4.5225,
      "step": 43400
    },
    {
      "epoch": 0.007671913020612049,
      "grad_norm": 8.559701919555664,
      "learning_rate": 4.990410329181506e-05,
      "loss": 4.5373,
      "step": 43500
    },
    {
      "epoch": 0.007689549602268629,
      "grad_norm": 7.267717361450195,
      "learning_rate": 4.9903882834544354e-05,
      "loss": 4.5449,
      "step": 43600
    },
    {
      "epoch": 0.007707186183925208,
      "grad_norm": 7.364350318908691,
      "learning_rate": 4.990366237727365e-05,
      "loss": 4.5872,
      "step": 43700
    },
    {
      "epoch": 0.007724822765581787,
      "grad_norm": 7.748714447021484,
      "learning_rate": 4.990344192000294e-05,
      "loss": 4.5438,
      "step": 43800
    },
    {
      "epoch": 0.007742459347238367,
      "grad_norm": 6.953908443450928,
      "learning_rate": 4.990322146273223e-05,
      "loss": 4.5211,
      "step": 43900
    },
    {
      "epoch": 0.007760095928894946,
      "grad_norm": 7.227991580963135,
      "learning_rate": 4.990300100546152e-05,
      "loss": 4.5363,
      "step": 44000
    },
    {
      "epoch": 0.007777732510551526,
      "grad_norm": 7.639138221740723,
      "learning_rate": 4.990278054819081e-05,
      "loss": 4.6456,
      "step": 44100
    },
    {
      "epoch": 0.007795369092208106,
      "grad_norm": 4.823576927185059,
      "learning_rate": 4.9902560090920106e-05,
      "loss": 4.4601,
      "step": 44200
    },
    {
      "epoch": 0.007813005673864684,
      "grad_norm": 6.039485454559326,
      "learning_rate": 4.99023396336494e-05,
      "loss": 4.5742,
      "step": 44300
    },
    {
      "epoch": 0.007830642255521264,
      "grad_norm": 8.269806861877441,
      "learning_rate": 4.990211917637869e-05,
      "loss": 4.5003,
      "step": 44400
    },
    {
      "epoch": 0.007848278837177843,
      "grad_norm": 8.17115306854248,
      "learning_rate": 4.9901898719107986e-05,
      "loss": 4.4765,
      "step": 44500
    },
    {
      "epoch": 0.007865915418834423,
      "grad_norm": 5.6808695793151855,
      "learning_rate": 4.990167826183728e-05,
      "loss": 4.5665,
      "step": 44600
    },
    {
      "epoch": 0.007883552000491002,
      "grad_norm": 4.7047224044799805,
      "learning_rate": 4.990145780456657e-05,
      "loss": 4.5244,
      "step": 44700
    },
    {
      "epoch": 0.007901188582147582,
      "grad_norm": 8.350459098815918,
      "learning_rate": 4.9901237347295865e-05,
      "loss": 4.509,
      "step": 44800
    },
    {
      "epoch": 0.007918825163804162,
      "grad_norm": 4.976613521575928,
      "learning_rate": 4.990101689002516e-05,
      "loss": 4.581,
      "step": 44900
    },
    {
      "epoch": 0.00793646174546074,
      "grad_norm": 6.782252311706543,
      "learning_rate": 4.990079643275445e-05,
      "loss": 4.6332,
      "step": 45000
    },
    {
      "epoch": 0.00795409832711732,
      "grad_norm": 6.38465690612793,
      "learning_rate": 4.9900575975483745e-05,
      "loss": 4.6083,
      "step": 45100
    },
    {
      "epoch": 0.0079717349087739,
      "grad_norm": 10.185528755187988,
      "learning_rate": 4.990035551821304e-05,
      "loss": 4.4859,
      "step": 45200
    },
    {
      "epoch": 0.00798937149043048,
      "grad_norm": 6.385829448699951,
      "learning_rate": 4.990013506094233e-05,
      "loss": 4.4551,
      "step": 45300
    },
    {
      "epoch": 0.008007008072087058,
      "grad_norm": 5.635735511779785,
      "learning_rate": 4.9899914603671625e-05,
      "loss": 4.5578,
      "step": 45400
    },
    {
      "epoch": 0.008024644653743638,
      "grad_norm": 5.3362226486206055,
      "learning_rate": 4.989969414640091e-05,
      "loss": 4.5499,
      "step": 45500
    },
    {
      "epoch": 0.008042281235400217,
      "grad_norm": 5.037675380706787,
      "learning_rate": 4.98994736891302e-05,
      "loss": 4.4857,
      "step": 45600
    },
    {
      "epoch": 0.008059917817056797,
      "grad_norm": 7.646221160888672,
      "learning_rate": 4.98992532318595e-05,
      "loss": 4.5421,
      "step": 45700
    },
    {
      "epoch": 0.008077554398713375,
      "grad_norm": 8.885113716125488,
      "learning_rate": 4.989903277458879e-05,
      "loss": 4.6427,
      "step": 45800
    },
    {
      "epoch": 0.008095190980369956,
      "grad_norm": 8.540061950683594,
      "learning_rate": 4.989881231731808e-05,
      "loss": 4.6135,
      "step": 45900
    },
    {
      "epoch": 0.008112827562026534,
      "grad_norm": 6.500186443328857,
      "learning_rate": 4.989859186004738e-05,
      "loss": 4.4733,
      "step": 46000
    },
    {
      "epoch": 0.008130464143683114,
      "grad_norm": 8.012733459472656,
      "learning_rate": 4.989837140277667e-05,
      "loss": 4.5396,
      "step": 46100
    },
    {
      "epoch": 0.008148100725339693,
      "grad_norm": 8.038573265075684,
      "learning_rate": 4.989815094550596e-05,
      "loss": 4.5484,
      "step": 46200
    },
    {
      "epoch": 0.008165737306996273,
      "grad_norm": 8.6069917678833,
      "learning_rate": 4.989793048823526e-05,
      "loss": 4.4774,
      "step": 46300
    },
    {
      "epoch": 0.008183373888652853,
      "grad_norm": 6.041851997375488,
      "learning_rate": 4.989771003096455e-05,
      "loss": 4.4278,
      "step": 46400
    },
    {
      "epoch": 0.008201010470309432,
      "grad_norm": 6.087915897369385,
      "learning_rate": 4.989748957369384e-05,
      "loss": 4.4759,
      "step": 46500
    },
    {
      "epoch": 0.008218647051966012,
      "grad_norm": 8.067267417907715,
      "learning_rate": 4.9897269116423136e-05,
      "loss": 4.4878,
      "step": 46600
    },
    {
      "epoch": 0.00823628363362259,
      "grad_norm": 8.26008129119873,
      "learning_rate": 4.989704865915243e-05,
      "loss": 4.4451,
      "step": 46700
    },
    {
      "epoch": 0.008253920215279171,
      "grad_norm": 5.337238788604736,
      "learning_rate": 4.989682820188172e-05,
      "loss": 4.5529,
      "step": 46800
    },
    {
      "epoch": 0.00827155679693575,
      "grad_norm": 5.584672451019287,
      "learning_rate": 4.989660774461101e-05,
      "loss": 4.6142,
      "step": 46900
    },
    {
      "epoch": 0.00828919337859233,
      "grad_norm": 5.4262166023254395,
      "learning_rate": 4.9896387287340305e-05,
      "loss": 4.5037,
      "step": 47000
    },
    {
      "epoch": 0.008306829960248908,
      "grad_norm": 5.828608512878418,
      "learning_rate": 4.989616683006959e-05,
      "loss": 4.4461,
      "step": 47100
    },
    {
      "epoch": 0.008324466541905488,
      "grad_norm": 5.3874311447143555,
      "learning_rate": 4.989594637279889e-05,
      "loss": 4.5264,
      "step": 47200
    },
    {
      "epoch": 0.008342103123562067,
      "grad_norm": 5.155858039855957,
      "learning_rate": 4.9895725915528184e-05,
      "loss": 4.3206,
      "step": 47300
    },
    {
      "epoch": 0.008359739705218647,
      "grad_norm": 6.5706095695495605,
      "learning_rate": 4.989550545825747e-05,
      "loss": 4.4848,
      "step": 47400
    },
    {
      "epoch": 0.008377376286875226,
      "grad_norm": 7.727005958557129,
      "learning_rate": 4.989528500098677e-05,
      "loss": 4.5509,
      "step": 47500
    },
    {
      "epoch": 0.008395012868531806,
      "grad_norm": 5.7901201248168945,
      "learning_rate": 4.9895064543716064e-05,
      "loss": 4.4434,
      "step": 47600
    },
    {
      "epoch": 0.008412649450188384,
      "grad_norm": 9.038567543029785,
      "learning_rate": 4.989484408644535e-05,
      "loss": 4.4599,
      "step": 47700
    },
    {
      "epoch": 0.008430286031844965,
      "grad_norm": 6.619800567626953,
      "learning_rate": 4.989462362917465e-05,
      "loss": 4.5227,
      "step": 47800
    },
    {
      "epoch": 0.008447922613501545,
      "grad_norm": 6.547466278076172,
      "learning_rate": 4.9894403171903944e-05,
      "loss": 4.3117,
      "step": 47900
    },
    {
      "epoch": 0.008465559195158123,
      "grad_norm": 8.163147926330566,
      "learning_rate": 4.989418271463323e-05,
      "loss": 4.4636,
      "step": 48000
    },
    {
      "epoch": 0.008483195776814704,
      "grad_norm": 7.535266399383545,
      "learning_rate": 4.989396225736253e-05,
      "loss": 4.4781,
      "step": 48100
    },
    {
      "epoch": 0.008500832358471282,
      "grad_norm": 4.88194465637207,
      "learning_rate": 4.989374180009182e-05,
      "loss": 4.5751,
      "step": 48200
    },
    {
      "epoch": 0.008518468940127862,
      "grad_norm": 7.280979633331299,
      "learning_rate": 4.989352134282111e-05,
      "loss": 4.5752,
      "step": 48300
    },
    {
      "epoch": 0.00853610552178444,
      "grad_norm": 7.350872993469238,
      "learning_rate": 4.98933008855504e-05,
      "loss": 4.4523,
      "step": 48400
    },
    {
      "epoch": 0.008553742103441021,
      "grad_norm": 6.153013229370117,
      "learning_rate": 4.9893080428279696e-05,
      "loss": 4.4803,
      "step": 48500
    },
    {
      "epoch": 0.0085713786850976,
      "grad_norm": 6.099572658538818,
      "learning_rate": 4.9892859971008985e-05,
      "loss": 4.5103,
      "step": 48600
    },
    {
      "epoch": 0.00858901526675418,
      "grad_norm": 5.4413580894470215,
      "learning_rate": 4.989263951373828e-05,
      "loss": 4.5085,
      "step": 48700
    },
    {
      "epoch": 0.008606651848410758,
      "grad_norm": 9.812232971191406,
      "learning_rate": 4.9892419056467576e-05,
      "loss": 4.3353,
      "step": 48800
    },
    {
      "epoch": 0.008624288430067338,
      "grad_norm": 5.334354400634766,
      "learning_rate": 4.9892198599196864e-05,
      "loss": 4.5472,
      "step": 48900
    },
    {
      "epoch": 0.008641925011723917,
      "grad_norm": 7.130921840667725,
      "learning_rate": 4.989197814192616e-05,
      "loss": 4.4394,
      "step": 49000
    },
    {
      "epoch": 0.008659561593380497,
      "grad_norm": 10.187826156616211,
      "learning_rate": 4.9891757684655455e-05,
      "loss": 4.4451,
      "step": 49100
    },
    {
      "epoch": 0.008677198175037076,
      "grad_norm": 7.520249366760254,
      "learning_rate": 4.9891537227384744e-05,
      "loss": 4.4459,
      "step": 49200
    },
    {
      "epoch": 0.008694834756693656,
      "grad_norm": 5.609415054321289,
      "learning_rate": 4.989131677011404e-05,
      "loss": 4.5783,
      "step": 49300
    },
    {
      "epoch": 0.008712471338350236,
      "grad_norm": 6.341971397399902,
      "learning_rate": 4.9891096312843335e-05,
      "loss": 4.5075,
      "step": 49400
    },
    {
      "epoch": 0.008730107920006815,
      "grad_norm": 7.3962554931640625,
      "learning_rate": 4.9890875855572624e-05,
      "loss": 4.5261,
      "step": 49500
    },
    {
      "epoch": 0.008747744501663395,
      "grad_norm": 5.490957260131836,
      "learning_rate": 4.989065539830192e-05,
      "loss": 4.5336,
      "step": 49600
    },
    {
      "epoch": 0.008765381083319973,
      "grad_norm": 5.715737342834473,
      "learning_rate": 4.989043494103121e-05,
      "loss": 4.5228,
      "step": 49700
    },
    {
      "epoch": 0.008783017664976554,
      "grad_norm": 6.307133197784424,
      "learning_rate": 4.98902144837605e-05,
      "loss": 4.3895,
      "step": 49800
    },
    {
      "epoch": 0.008800654246633132,
      "grad_norm": 9.363880157470703,
      "learning_rate": 4.988999402648979e-05,
      "loss": 4.5106,
      "step": 49900
    },
    {
      "epoch": 0.008818290828289712,
      "grad_norm": 4.979055881500244,
      "learning_rate": 4.988977356921909e-05,
      "loss": 4.442,
      "step": 50000
    },
    {
      "epoch": 0.00883592740994629,
      "grad_norm": 5.8339104652404785,
      "learning_rate": 4.9889553111948376e-05,
      "loss": 4.433,
      "step": 50100
    },
    {
      "epoch": 0.008853563991602871,
      "grad_norm": 6.423042297363281,
      "learning_rate": 4.988933265467767e-05,
      "loss": 4.4464,
      "step": 50200
    },
    {
      "epoch": 0.00887120057325945,
      "grad_norm": 5.222518444061279,
      "learning_rate": 4.988911219740697e-05,
      "loss": 4.5371,
      "step": 50300
    },
    {
      "epoch": 0.00888883715491603,
      "grad_norm": 5.75746488571167,
      "learning_rate": 4.9888891740136256e-05,
      "loss": 4.4966,
      "step": 50400
    },
    {
      "epoch": 0.008906473736572608,
      "grad_norm": 7.48262882232666,
      "learning_rate": 4.988867128286555e-05,
      "loss": 4.4255,
      "step": 50500
    },
    {
      "epoch": 0.008924110318229189,
      "grad_norm": 6.626788139343262,
      "learning_rate": 4.988845082559485e-05,
      "loss": 4.5125,
      "step": 50600
    },
    {
      "epoch": 0.008941746899885767,
      "grad_norm": 9.265020370483398,
      "learning_rate": 4.9888230368324135e-05,
      "loss": 4.5212,
      "step": 50700
    },
    {
      "epoch": 0.008959383481542347,
      "grad_norm": 8.00593376159668,
      "learning_rate": 4.988800991105343e-05,
      "loss": 4.4623,
      "step": 50800
    },
    {
      "epoch": 0.008977020063198927,
      "grad_norm": 8.271780014038086,
      "learning_rate": 4.9887789453782726e-05,
      "loss": 4.5533,
      "step": 50900
    },
    {
      "epoch": 0.008994656644855506,
      "grad_norm": 5.619403839111328,
      "learning_rate": 4.9887568996512015e-05,
      "loss": 4.4772,
      "step": 51000
    },
    {
      "epoch": 0.009012293226512086,
      "grad_norm": 6.369128227233887,
      "learning_rate": 4.988734853924131e-05,
      "loss": 4.4868,
      "step": 51100
    },
    {
      "epoch": 0.009029929808168665,
      "grad_norm": 6.334299087524414,
      "learning_rate": 4.98871280819706e-05,
      "loss": 4.4945,
      "step": 51200
    },
    {
      "epoch": 0.009047566389825245,
      "grad_norm": 6.321508884429932,
      "learning_rate": 4.9886907624699895e-05,
      "loss": 4.4351,
      "step": 51300
    },
    {
      "epoch": 0.009065202971481823,
      "grad_norm": 6.145515441894531,
      "learning_rate": 4.988668716742918e-05,
      "loss": 4.4554,
      "step": 51400
    },
    {
      "epoch": 0.009082839553138404,
      "grad_norm": 6.292781352996826,
      "learning_rate": 4.988646671015848e-05,
      "loss": 4.5102,
      "step": 51500
    },
    {
      "epoch": 0.009100476134794982,
      "grad_norm": 6.568816184997559,
      "learning_rate": 4.988624625288777e-05,
      "loss": 4.4434,
      "step": 51600
    },
    {
      "epoch": 0.009118112716451562,
      "grad_norm": 5.979086875915527,
      "learning_rate": 4.988602579561706e-05,
      "loss": 4.5702,
      "step": 51700
    },
    {
      "epoch": 0.009135749298108141,
      "grad_norm": 11.590471267700195,
      "learning_rate": 4.988580533834636e-05,
      "loss": 4.522,
      "step": 51800
    },
    {
      "epoch": 0.009153385879764721,
      "grad_norm": 8.692974090576172,
      "learning_rate": 4.9885584881075654e-05,
      "loss": 4.4943,
      "step": 51900
    },
    {
      "epoch": 0.0091710224614213,
      "grad_norm": 7.973832607269287,
      "learning_rate": 4.988536442380494e-05,
      "loss": 4.3708,
      "step": 52000
    },
    {
      "epoch": 0.00918865904307788,
      "grad_norm": 8.849494934082031,
      "learning_rate": 4.988514396653424e-05,
      "loss": 4.4746,
      "step": 52100
    },
    {
      "epoch": 0.009206295624734458,
      "grad_norm": 8.829238891601562,
      "learning_rate": 4.9884923509263533e-05,
      "loss": 4.3918,
      "step": 52200
    },
    {
      "epoch": 0.009223932206391039,
      "grad_norm": 5.532934188842773,
      "learning_rate": 4.988470305199282e-05,
      "loss": 4.419,
      "step": 52300
    },
    {
      "epoch": 0.009241568788047619,
      "grad_norm": 8.065301895141602,
      "learning_rate": 4.988448259472212e-05,
      "loss": 4.4559,
      "step": 52400
    },
    {
      "epoch": 0.009259205369704197,
      "grad_norm": 6.211464881896973,
      "learning_rate": 4.9884262137451406e-05,
      "loss": 4.5036,
      "step": 52500
    },
    {
      "epoch": 0.009276841951360778,
      "grad_norm": 6.506213665008545,
      "learning_rate": 4.98840416801807e-05,
      "loss": 4.5375,
      "step": 52600
    },
    {
      "epoch": 0.009294478533017356,
      "grad_norm": 5.887675762176514,
      "learning_rate": 4.988382122290999e-05,
      "loss": 4.4432,
      "step": 52700
    },
    {
      "epoch": 0.009312115114673936,
      "grad_norm": 5.06585168838501,
      "learning_rate": 4.9883600765639286e-05,
      "loss": 4.4187,
      "step": 52800
    },
    {
      "epoch": 0.009329751696330515,
      "grad_norm": 8.176244735717773,
      "learning_rate": 4.9883380308368575e-05,
      "loss": 4.4789,
      "step": 52900
    },
    {
      "epoch": 0.009347388277987095,
      "grad_norm": 5.674278259277344,
      "learning_rate": 4.988315985109787e-05,
      "loss": 4.4067,
      "step": 53000
    },
    {
      "epoch": 0.009365024859643674,
      "grad_norm": 6.186838626861572,
      "learning_rate": 4.9882939393827166e-05,
      "loss": 4.4651,
      "step": 53100
    },
    {
      "epoch": 0.009382661441300254,
      "grad_norm": 4.870802402496338,
      "learning_rate": 4.9882718936556454e-05,
      "loss": 4.4748,
      "step": 53200
    },
    {
      "epoch": 0.009400298022956832,
      "grad_norm": 7.458055019378662,
      "learning_rate": 4.988249847928575e-05,
      "loss": 4.3705,
      "step": 53300
    },
    {
      "epoch": 0.009417934604613412,
      "grad_norm": 7.465412139892578,
      "learning_rate": 4.9882278022015045e-05,
      "loss": 4.409,
      "step": 53400
    },
    {
      "epoch": 0.009435571186269991,
      "grad_norm": 7.091699600219727,
      "learning_rate": 4.9882057564744334e-05,
      "loss": 4.4363,
      "step": 53500
    },
    {
      "epoch": 0.009453207767926571,
      "grad_norm": 4.909925937652588,
      "learning_rate": 4.988183710747363e-05,
      "loss": 4.5093,
      "step": 53600
    },
    {
      "epoch": 0.00947084434958315,
      "grad_norm": 7.910037517547607,
      "learning_rate": 4.9881616650202925e-05,
      "loss": 4.4385,
      "step": 53700
    },
    {
      "epoch": 0.00948848093123973,
      "grad_norm": 5.846002578735352,
      "learning_rate": 4.9881396192932214e-05,
      "loss": 4.456,
      "step": 53800
    },
    {
      "epoch": 0.00950611751289631,
      "grad_norm": 6.643951416015625,
      "learning_rate": 4.988117573566151e-05,
      "loss": 4.4342,
      "step": 53900
    },
    {
      "epoch": 0.009523754094552889,
      "grad_norm": 4.984142303466797,
      "learning_rate": 4.98809552783908e-05,
      "loss": 4.4149,
      "step": 54000
    },
    {
      "epoch": 0.009541390676209469,
      "grad_norm": 8.017142295837402,
      "learning_rate": 4.9880734821120086e-05,
      "loss": 4.3802,
      "step": 54100
    },
    {
      "epoch": 0.009559027257866047,
      "grad_norm": 7.818729877471924,
      "learning_rate": 4.988051436384938e-05,
      "loss": 4.457,
      "step": 54200
    },
    {
      "epoch": 0.009576663839522628,
      "grad_norm": 6.469852924346924,
      "learning_rate": 4.988029390657868e-05,
      "loss": 4.3366,
      "step": 54300
    },
    {
      "epoch": 0.009594300421179206,
      "grad_norm": 8.600605964660645,
      "learning_rate": 4.9880073449307966e-05,
      "loss": 4.5246,
      "step": 54400
    },
    {
      "epoch": 0.009611937002835786,
      "grad_norm": 7.126296520233154,
      "learning_rate": 4.987985299203726e-05,
      "loss": 4.3992,
      "step": 54500
    },
    {
      "epoch": 0.009629573584492365,
      "grad_norm": 7.7921648025512695,
      "learning_rate": 4.987963253476656e-05,
      "loss": 4.3693,
      "step": 54600
    },
    {
      "epoch": 0.009647210166148945,
      "grad_norm": 6.702962398529053,
      "learning_rate": 4.9879412077495846e-05,
      "loss": 4.3862,
      "step": 54700
    },
    {
      "epoch": 0.009664846747805524,
      "grad_norm": 5.9316558837890625,
      "learning_rate": 4.987919162022514e-05,
      "loss": 4.4563,
      "step": 54800
    },
    {
      "epoch": 0.009682483329462104,
      "grad_norm": 7.035680770874023,
      "learning_rate": 4.9878971162954437e-05,
      "loss": 4.4031,
      "step": 54900
    },
    {
      "epoch": 0.009700119911118682,
      "grad_norm": 5.869793891906738,
      "learning_rate": 4.9878750705683725e-05,
      "loss": 4.3762,
      "step": 55000
    },
    {
      "epoch": 0.009717756492775263,
      "grad_norm": 7.039398670196533,
      "learning_rate": 4.987853024841302e-05,
      "loss": 4.5295,
      "step": 55100
    },
    {
      "epoch": 0.009735393074431841,
      "grad_norm": 8.317490577697754,
      "learning_rate": 4.9878309791142316e-05,
      "loss": 4.5244,
      "step": 55200
    },
    {
      "epoch": 0.009753029656088421,
      "grad_norm": 5.5408244132995605,
      "learning_rate": 4.9878089333871605e-05,
      "loss": 4.3519,
      "step": 55300
    },
    {
      "epoch": 0.009770666237745002,
      "grad_norm": 4.713628768920898,
      "learning_rate": 4.98778688766009e-05,
      "loss": 4.5201,
      "step": 55400
    },
    {
      "epoch": 0.00978830281940158,
      "grad_norm": 8.394753456115723,
      "learning_rate": 4.987764841933019e-05,
      "loss": 4.4282,
      "step": 55500
    },
    {
      "epoch": 0.00980593940105816,
      "grad_norm": 5.899433135986328,
      "learning_rate": 4.987742796205948e-05,
      "loss": 4.4886,
      "step": 55600
    },
    {
      "epoch": 0.009823575982714739,
      "grad_norm": 7.3206305503845215,
      "learning_rate": 4.987720750478877e-05,
      "loss": 4.4188,
      "step": 55700
    },
    {
      "epoch": 0.009841212564371319,
      "grad_norm": 6.984951019287109,
      "learning_rate": 4.987698704751807e-05,
      "loss": 4.4683,
      "step": 55800
    },
    {
      "epoch": 0.009858849146027897,
      "grad_norm": 4.954082489013672,
      "learning_rate": 4.987676659024736e-05,
      "loss": 4.2534,
      "step": 55900
    },
    {
      "epoch": 0.009876485727684478,
      "grad_norm": 6.374964237213135,
      "learning_rate": 4.987654613297665e-05,
      "loss": 4.3642,
      "step": 56000
    },
    {
      "epoch": 0.009894122309341056,
      "grad_norm": 18.291534423828125,
      "learning_rate": 4.987632567570595e-05,
      "loss": 4.441,
      "step": 56100
    },
    {
      "epoch": 0.009911758890997636,
      "grad_norm": 6.216943740844727,
      "learning_rate": 4.987610521843524e-05,
      "loss": 4.4592,
      "step": 56200
    },
    {
      "epoch": 0.009929395472654215,
      "grad_norm": 4.705249309539795,
      "learning_rate": 4.987588476116453e-05,
      "loss": 4.4507,
      "step": 56300
    },
    {
      "epoch": 0.009947032054310795,
      "grad_norm": 7.623227119445801,
      "learning_rate": 4.987566430389383e-05,
      "loss": 4.2968,
      "step": 56400
    },
    {
      "epoch": 0.009964668635967374,
      "grad_norm": 8.101297378540039,
      "learning_rate": 4.987544384662312e-05,
      "loss": 4.4288,
      "step": 56500
    },
    {
      "epoch": 0.009982305217623954,
      "grad_norm": 5.075204849243164,
      "learning_rate": 4.987522338935241e-05,
      "loss": 4.4041,
      "step": 56600
    },
    {
      "epoch": 0.009999941799280532,
      "grad_norm": 6.649347305297852,
      "learning_rate": 4.987500293208171e-05,
      "loss": 4.411,
      "step": 56700
    },
    {
      "epoch": 0.010017578380937113,
      "grad_norm": 6.221540451049805,
      "learning_rate": 4.9874782474810996e-05,
      "loss": 4.3556,
      "step": 56800
    },
    {
      "epoch": 0.010035214962593693,
      "grad_norm": 9.412664413452148,
      "learning_rate": 4.9874562017540285e-05,
      "loss": 4.446,
      "step": 56900
    },
    {
      "epoch": 0.010052851544250271,
      "grad_norm": 11.558176040649414,
      "learning_rate": 4.987434156026958e-05,
      "loss": 4.4176,
      "step": 57000
    },
    {
      "epoch": 0.010070488125906852,
      "grad_norm": 5.951226711273193,
      "learning_rate": 4.987412110299887e-05,
      "loss": 4.2834,
      "step": 57100
    },
    {
      "epoch": 0.01008812470756343,
      "grad_norm": 5.568721771240234,
      "learning_rate": 4.9873900645728165e-05,
      "loss": 4.4176,
      "step": 57200
    },
    {
      "epoch": 0.01010576128922001,
      "grad_norm": 7.42840576171875,
      "learning_rate": 4.987368018845746e-05,
      "loss": 4.3583,
      "step": 57300
    },
    {
      "epoch": 0.010123397870876589,
      "grad_norm": 8.071352005004883,
      "learning_rate": 4.987345973118675e-05,
      "loss": 4.3887,
      "step": 57400
    },
    {
      "epoch": 0.010141034452533169,
      "grad_norm": 6.445630073547363,
      "learning_rate": 4.9873239273916044e-05,
      "loss": 4.4247,
      "step": 57500
    },
    {
      "epoch": 0.010158671034189748,
      "grad_norm": 6.531445503234863,
      "learning_rate": 4.987301881664534e-05,
      "loss": 4.3109,
      "step": 57600
    },
    {
      "epoch": 0.010176307615846328,
      "grad_norm": 11.860954284667969,
      "learning_rate": 4.987279835937463e-05,
      "loss": 4.4744,
      "step": 57700
    },
    {
      "epoch": 0.010193944197502906,
      "grad_norm": 5.259406566619873,
      "learning_rate": 4.9872577902103924e-05,
      "loss": 4.3904,
      "step": 57800
    },
    {
      "epoch": 0.010211580779159487,
      "grad_norm": 6.757670879364014,
      "learning_rate": 4.987235744483322e-05,
      "loss": 4.3025,
      "step": 57900
    },
    {
      "epoch": 0.010229217360816065,
      "grad_norm": 7.671411037445068,
      "learning_rate": 4.987213698756251e-05,
      "loss": 4.3638,
      "step": 58000
    },
    {
      "epoch": 0.010246853942472645,
      "grad_norm": 6.584813594818115,
      "learning_rate": 4.9871916530291803e-05,
      "loss": 4.4218,
      "step": 58100
    },
    {
      "epoch": 0.010264490524129226,
      "grad_norm": 5.386585712432861,
      "learning_rate": 4.98716960730211e-05,
      "loss": 4.4729,
      "step": 58200
    },
    {
      "epoch": 0.010282127105785804,
      "grad_norm": 5.7223100662231445,
      "learning_rate": 4.987147561575039e-05,
      "loss": 4.4287,
      "step": 58300
    },
    {
      "epoch": 0.010299763687442384,
      "grad_norm": 6.230776786804199,
      "learning_rate": 4.9871255158479676e-05,
      "loss": 4.2496,
      "step": 58400
    },
    {
      "epoch": 0.010317400269098963,
      "grad_norm": 6.26111364364624,
      "learning_rate": 4.987103470120897e-05,
      "loss": 4.3981,
      "step": 58500
    },
    {
      "epoch": 0.010335036850755543,
      "grad_norm": 7.078213691711426,
      "learning_rate": 4.987081424393826e-05,
      "loss": 4.4392,
      "step": 58600
    },
    {
      "epoch": 0.010352673432412121,
      "grad_norm": 6.218893051147461,
      "learning_rate": 4.9870593786667556e-05,
      "loss": 4.41,
      "step": 58700
    },
    {
      "epoch": 0.010370310014068702,
      "grad_norm": 6.757347583770752,
      "learning_rate": 4.987037332939685e-05,
      "loss": 4.3163,
      "step": 58800
    },
    {
      "epoch": 0.01038794659572528,
      "grad_norm": 7.080426216125488,
      "learning_rate": 4.987015287212614e-05,
      "loss": 4.3139,
      "step": 58900
    },
    {
      "epoch": 0.01040558317738186,
      "grad_norm": 5.47819185256958,
      "learning_rate": 4.9869932414855436e-05,
      "loss": 4.3637,
      "step": 59000
    },
    {
      "epoch": 0.010423219759038439,
      "grad_norm": 5.654849529266357,
      "learning_rate": 4.986971195758473e-05,
      "loss": 4.4253,
      "step": 59100
    },
    {
      "epoch": 0.01044085634069502,
      "grad_norm": 6.81667947769165,
      "learning_rate": 4.986949150031402e-05,
      "loss": 4.3797,
      "step": 59200
    },
    {
      "epoch": 0.010458492922351598,
      "grad_norm": 7.08920955657959,
      "learning_rate": 4.9869271043043315e-05,
      "loss": 4.3531,
      "step": 59300
    },
    {
      "epoch": 0.010476129504008178,
      "grad_norm": 4.799642562866211,
      "learning_rate": 4.986905058577261e-05,
      "loss": 4.3824,
      "step": 59400
    },
    {
      "epoch": 0.010493766085664756,
      "grad_norm": 7.384620189666748,
      "learning_rate": 4.98688301285019e-05,
      "loss": 4.4431,
      "step": 59500
    },
    {
      "epoch": 0.010511402667321337,
      "grad_norm": 8.2163667678833,
      "learning_rate": 4.9868609671231195e-05,
      "loss": 4.3507,
      "step": 59600
    },
    {
      "epoch": 0.010529039248977917,
      "grad_norm": 6.948774337768555,
      "learning_rate": 4.9868389213960484e-05,
      "loss": 4.3934,
      "step": 59700
    },
    {
      "epoch": 0.010546675830634495,
      "grad_norm": 7.069557189941406,
      "learning_rate": 4.986816875668978e-05,
      "loss": 4.4119,
      "step": 59800
    },
    {
      "epoch": 0.010564312412291076,
      "grad_norm": 10.662056922912598,
      "learning_rate": 4.986794829941907e-05,
      "loss": 4.3331,
      "step": 59900
    },
    {
      "epoch": 0.010581948993947654,
      "grad_norm": 9.941441535949707,
      "learning_rate": 4.986772784214836e-05,
      "loss": 4.3426,
      "step": 60000
    },
    {
      "epoch": 0.010599585575604234,
      "grad_norm": 7.738245964050293,
      "learning_rate": 4.986750738487765e-05,
      "loss": 4.4387,
      "step": 60100
    },
    {
      "epoch": 0.010617222157260813,
      "grad_norm": 9.567100524902344,
      "learning_rate": 4.986728692760695e-05,
      "loss": 4.443,
      "step": 60200
    },
    {
      "epoch": 0.010634858738917393,
      "grad_norm": 8.223423957824707,
      "learning_rate": 4.986706647033624e-05,
      "loss": 4.3567,
      "step": 60300
    },
    {
      "epoch": 0.010652495320573972,
      "grad_norm": 5.9593095779418945,
      "learning_rate": 4.986684601306553e-05,
      "loss": 4.365,
      "step": 60400
    },
    {
      "epoch": 0.010670131902230552,
      "grad_norm": 5.637174606323242,
      "learning_rate": 4.986662555579483e-05,
      "loss": 4.3946,
      "step": 60500
    },
    {
      "epoch": 0.01068776848388713,
      "grad_norm": 6.765573501586914,
      "learning_rate": 4.986640509852412e-05,
      "loss": 4.2684,
      "step": 60600
    },
    {
      "epoch": 0.01070540506554371,
      "grad_norm": 9.356123924255371,
      "learning_rate": 4.986618464125341e-05,
      "loss": 4.3253,
      "step": 60700
    },
    {
      "epoch": 0.010723041647200289,
      "grad_norm": 6.557943820953369,
      "learning_rate": 4.9865964183982707e-05,
      "loss": 4.4153,
      "step": 60800
    },
    {
      "epoch": 0.01074067822885687,
      "grad_norm": 7.091341972351074,
      "learning_rate": 4.9865743726712e-05,
      "loss": 4.4091,
      "step": 60900
    },
    {
      "epoch": 0.010758314810513448,
      "grad_norm": 9.28513240814209,
      "learning_rate": 4.986552326944129e-05,
      "loss": 4.4493,
      "step": 61000
    },
    {
      "epoch": 0.010775951392170028,
      "grad_norm": 6.0974907875061035,
      "learning_rate": 4.9865302812170586e-05,
      "loss": 4.3418,
      "step": 61100
    },
    {
      "epoch": 0.010793587973826608,
      "grad_norm": 6.554758071899414,
      "learning_rate": 4.9865082354899875e-05,
      "loss": 4.4061,
      "step": 61200
    },
    {
      "epoch": 0.010811224555483187,
      "grad_norm": 5.684991359710693,
      "learning_rate": 4.986486189762917e-05,
      "loss": 4.2848,
      "step": 61300
    },
    {
      "epoch": 0.010828861137139767,
      "grad_norm": 7.678053855895996,
      "learning_rate": 4.986464144035846e-05,
      "loss": 4.3159,
      "step": 61400
    },
    {
      "epoch": 0.010846497718796345,
      "grad_norm": 7.818995475769043,
      "learning_rate": 4.9864420983087755e-05,
      "loss": 4.3627,
      "step": 61500
    },
    {
      "epoch": 0.010864134300452926,
      "grad_norm": 4.696332931518555,
      "learning_rate": 4.986420052581704e-05,
      "loss": 4.3841,
      "step": 61600
    },
    {
      "epoch": 0.010881770882109504,
      "grad_norm": 7.13564395904541,
      "learning_rate": 4.986398006854634e-05,
      "loss": 4.409,
      "step": 61700
    },
    {
      "epoch": 0.010899407463766084,
      "grad_norm": 5.8408355712890625,
      "learning_rate": 4.9863759611275634e-05,
      "loss": 4.453,
      "step": 61800
    },
    {
      "epoch": 0.010917044045422663,
      "grad_norm": 7.373345375061035,
      "learning_rate": 4.986353915400492e-05,
      "loss": 4.3508,
      "step": 61900
    },
    {
      "epoch": 0.010934680627079243,
      "grad_norm": 5.975674152374268,
      "learning_rate": 4.986331869673422e-05,
      "loss": 4.4476,
      "step": 62000
    },
    {
      "epoch": 0.010952317208735822,
      "grad_norm": 5.955574035644531,
      "learning_rate": 4.9863098239463514e-05,
      "loss": 4.3549,
      "step": 62100
    },
    {
      "epoch": 0.010969953790392402,
      "grad_norm": 8.045769691467285,
      "learning_rate": 4.98628777821928e-05,
      "loss": 4.3152,
      "step": 62200
    },
    {
      "epoch": 0.01098759037204898,
      "grad_norm": 6.526298522949219,
      "learning_rate": 4.98626573249221e-05,
      "loss": 4.4233,
      "step": 62300
    },
    {
      "epoch": 0.01100522695370556,
      "grad_norm": 5.8725504875183105,
      "learning_rate": 4.9862436867651393e-05,
      "loss": 4.2541,
      "step": 62400
    },
    {
      "epoch": 0.011022863535362139,
      "grad_norm": 8.305872917175293,
      "learning_rate": 4.986221641038068e-05,
      "loss": 4.4805,
      "step": 62500
    },
    {
      "epoch": 0.01104050011701872,
      "grad_norm": 6.637675762176514,
      "learning_rate": 4.986199595310998e-05,
      "loss": 4.3717,
      "step": 62600
    },
    {
      "epoch": 0.0110581366986753,
      "grad_norm": 8.031607627868652,
      "learning_rate": 4.9861775495839266e-05,
      "loss": 4.3439,
      "step": 62700
    },
    {
      "epoch": 0.011075773280331878,
      "grad_norm": 11.081757545471191,
      "learning_rate": 4.986155503856856e-05,
      "loss": 4.3599,
      "step": 62800
    },
    {
      "epoch": 0.011093409861988458,
      "grad_norm": 6.742763519287109,
      "learning_rate": 4.986133458129785e-05,
      "loss": 4.2595,
      "step": 62900
    },
    {
      "epoch": 0.011111046443645037,
      "grad_norm": 6.67362642288208,
      "learning_rate": 4.9861114124027146e-05,
      "loss": 4.3188,
      "step": 63000
    },
    {
      "epoch": 0.011128683025301617,
      "grad_norm": 7.3019561767578125,
      "learning_rate": 4.986089366675644e-05,
      "loss": 4.4054,
      "step": 63100
    },
    {
      "epoch": 0.011146319606958196,
      "grad_norm": 10.026187896728516,
      "learning_rate": 4.986067320948573e-05,
      "loss": 4.2966,
      "step": 63200
    },
    {
      "epoch": 0.011163956188614776,
      "grad_norm": 5.759890079498291,
      "learning_rate": 4.9860452752215026e-05,
      "loss": 4.3243,
      "step": 63300
    },
    {
      "epoch": 0.011181592770271354,
      "grad_norm": 5.955961227416992,
      "learning_rate": 4.986023229494432e-05,
      "loss": 4.3432,
      "step": 63400
    },
    {
      "epoch": 0.011199229351927934,
      "grad_norm": 6.018462181091309,
      "learning_rate": 4.986001183767361e-05,
      "loss": 4.3834,
      "step": 63500
    },
    {
      "epoch": 0.011216865933584513,
      "grad_norm": 9.587508201599121,
      "learning_rate": 4.9859791380402905e-05,
      "loss": 4.3505,
      "step": 63600
    },
    {
      "epoch": 0.011234502515241093,
      "grad_norm": 6.820469379425049,
      "learning_rate": 4.98595709231322e-05,
      "loss": 4.3137,
      "step": 63700
    },
    {
      "epoch": 0.011252139096897672,
      "grad_norm": 7.197722911834717,
      "learning_rate": 4.985935046586149e-05,
      "loss": 4.3419,
      "step": 63800
    },
    {
      "epoch": 0.011269775678554252,
      "grad_norm": 6.85371732711792,
      "learning_rate": 4.9859130008590785e-05,
      "loss": 4.2986,
      "step": 63900
    },
    {
      "epoch": 0.01128741226021083,
      "grad_norm": 9.175015449523926,
      "learning_rate": 4.9858909551320073e-05,
      "loss": 4.3556,
      "step": 64000
    },
    {
      "epoch": 0.01130504884186741,
      "grad_norm": 8.38261604309082,
      "learning_rate": 4.985868909404937e-05,
      "loss": 4.3675,
      "step": 64100
    },
    {
      "epoch": 0.011322685423523991,
      "grad_norm": 7.499587535858154,
      "learning_rate": 4.985846863677866e-05,
      "loss": 4.4097,
      "step": 64200
    },
    {
      "epoch": 0.01134032200518057,
      "grad_norm": 5.292964458465576,
      "learning_rate": 4.985824817950795e-05,
      "loss": 4.2977,
      "step": 64300
    },
    {
      "epoch": 0.01135795858683715,
      "grad_norm": 6.040182113647461,
      "learning_rate": 4.985802772223724e-05,
      "loss": 4.3403,
      "step": 64400
    },
    {
      "epoch": 0.011375595168493728,
      "grad_norm": 8.442967414855957,
      "learning_rate": 4.985780726496654e-05,
      "loss": 4.3019,
      "step": 64500
    },
    {
      "epoch": 0.011393231750150308,
      "grad_norm": 5.3741841316223145,
      "learning_rate": 4.985758680769583e-05,
      "loss": 4.3545,
      "step": 64600
    },
    {
      "epoch": 0.011410868331806887,
      "grad_norm": 8.374284744262695,
      "learning_rate": 4.985736635042512e-05,
      "loss": 4.3785,
      "step": 64700
    },
    {
      "epoch": 0.011428504913463467,
      "grad_norm": 7.76043176651001,
      "learning_rate": 4.985714589315442e-05,
      "loss": 4.3573,
      "step": 64800
    },
    {
      "epoch": 0.011446141495120046,
      "grad_norm": 6.670547962188721,
      "learning_rate": 4.985692543588371e-05,
      "loss": 4.3651,
      "step": 64900
    },
    {
      "epoch": 0.011463778076776626,
      "grad_norm": 7.641763210296631,
      "learning_rate": 4.9856704978613e-05,
      "loss": 4.3976,
      "step": 65000
    },
    {
      "epoch": 0.011481414658433204,
      "grad_norm": 8.233726501464844,
      "learning_rate": 4.9856484521342297e-05,
      "loss": 4.2974,
      "step": 65100
    },
    {
      "epoch": 0.011499051240089785,
      "grad_norm": 4.902022361755371,
      "learning_rate": 4.985626406407159e-05,
      "loss": 4.3147,
      "step": 65200
    },
    {
      "epoch": 0.011516687821746363,
      "grad_norm": 6.247907638549805,
      "learning_rate": 4.985604360680088e-05,
      "loss": 4.2686,
      "step": 65300
    },
    {
      "epoch": 0.011534324403402943,
      "grad_norm": 7.0272650718688965,
      "learning_rate": 4.9855823149530176e-05,
      "loss": 4.4151,
      "step": 65400
    },
    {
      "epoch": 0.011551960985059522,
      "grad_norm": 6.125346660614014,
      "learning_rate": 4.9855602692259465e-05,
      "loss": 4.3707,
      "step": 65500
    },
    {
      "epoch": 0.011569597566716102,
      "grad_norm": 6.0530242919921875,
      "learning_rate": 4.9855382234988754e-05,
      "loss": 4.3622,
      "step": 65600
    },
    {
      "epoch": 0.011587234148372682,
      "grad_norm": 5.951536178588867,
      "learning_rate": 4.985516177771805e-05,
      "loss": 4.3019,
      "step": 65700
    },
    {
      "epoch": 0.01160487073002926,
      "grad_norm": 4.871681213378906,
      "learning_rate": 4.9854941320447344e-05,
      "loss": 4.3644,
      "step": 65800
    },
    {
      "epoch": 0.011622507311685841,
      "grad_norm": 5.922115325927734,
      "learning_rate": 4.985472086317663e-05,
      "loss": 4.3193,
      "step": 65900
    },
    {
      "epoch": 0.01164014389334242,
      "grad_norm": 6.579107761383057,
      "learning_rate": 4.985450040590593e-05,
      "loss": 4.3623,
      "step": 66000
    },
    {
      "epoch": 0.011657780474999,
      "grad_norm": 5.850429058074951,
      "learning_rate": 4.9854279948635224e-05,
      "loss": 4.4079,
      "step": 66100
    },
    {
      "epoch": 0.011675417056655578,
      "grad_norm": 6.649912357330322,
      "learning_rate": 4.985405949136451e-05,
      "loss": 4.3896,
      "step": 66200
    },
    {
      "epoch": 0.011693053638312158,
      "grad_norm": 6.129563331604004,
      "learning_rate": 4.985383903409381e-05,
      "loss": 4.3878,
      "step": 66300
    },
    {
      "epoch": 0.011710690219968737,
      "grad_norm": 6.346358299255371,
      "learning_rate": 4.9853618576823104e-05,
      "loss": 4.3849,
      "step": 66400
    },
    {
      "epoch": 0.011728326801625317,
      "grad_norm": 6.581186771392822,
      "learning_rate": 4.985339811955239e-05,
      "loss": 4.3317,
      "step": 66500
    },
    {
      "epoch": 0.011745963383281896,
      "grad_norm": 7.861228942871094,
      "learning_rate": 4.985317766228169e-05,
      "loss": 4.4345,
      "step": 66600
    },
    {
      "epoch": 0.011763599964938476,
      "grad_norm": 6.2522406578063965,
      "learning_rate": 4.985295720501098e-05,
      "loss": 4.3334,
      "step": 66700
    },
    {
      "epoch": 0.011781236546595054,
      "grad_norm": 8.455177307128906,
      "learning_rate": 4.985273674774027e-05,
      "loss": 4.3382,
      "step": 66800
    },
    {
      "epoch": 0.011798873128251635,
      "grad_norm": 5.5565900802612305,
      "learning_rate": 4.985251629046957e-05,
      "loss": 4.3213,
      "step": 66900
    },
    {
      "epoch": 0.011816509709908213,
      "grad_norm": 9.958961486816406,
      "learning_rate": 4.9852295833198856e-05,
      "loss": 4.3733,
      "step": 67000
    },
    {
      "epoch": 0.011834146291564793,
      "grad_norm": 7.548517227172852,
      "learning_rate": 4.9852075375928145e-05,
      "loss": 4.2358,
      "step": 67100
    },
    {
      "epoch": 0.011851782873221374,
      "grad_norm": 12.290704727172852,
      "learning_rate": 4.985185491865744e-05,
      "loss": 4.3392,
      "step": 67200
    },
    {
      "epoch": 0.011869419454877952,
      "grad_norm": 6.543701648712158,
      "learning_rate": 4.9851634461386736e-05,
      "loss": 4.3228,
      "step": 67300
    },
    {
      "epoch": 0.011887056036534532,
      "grad_norm": 6.180972099304199,
      "learning_rate": 4.9851414004116025e-05,
      "loss": 4.2788,
      "step": 67400
    },
    {
      "epoch": 0.01190469261819111,
      "grad_norm": 7.853689193725586,
      "learning_rate": 4.985119354684532e-05,
      "loss": 4.2805,
      "step": 67500
    },
    {
      "epoch": 0.011922329199847691,
      "grad_norm": 5.122087478637695,
      "learning_rate": 4.9850973089574615e-05,
      "loss": 4.3062,
      "step": 67600
    },
    {
      "epoch": 0.01193996578150427,
      "grad_norm": 9.809356689453125,
      "learning_rate": 4.9850752632303904e-05,
      "loss": 4.3187,
      "step": 67700
    },
    {
      "epoch": 0.01195760236316085,
      "grad_norm": 10.08840274810791,
      "learning_rate": 4.98505321750332e-05,
      "loss": 4.3623,
      "step": 67800
    },
    {
      "epoch": 0.011975238944817428,
      "grad_norm": 7.067957401275635,
      "learning_rate": 4.9850311717762495e-05,
      "loss": 4.3608,
      "step": 67900
    },
    {
      "epoch": 0.011992875526474009,
      "grad_norm": 9.23125171661377,
      "learning_rate": 4.9850091260491784e-05,
      "loss": 4.3201,
      "step": 68000
    },
    {
      "epoch": 0.012010512108130587,
      "grad_norm": 6.79170036315918,
      "learning_rate": 4.984987080322108e-05,
      "loss": 4.2876,
      "step": 68100
    },
    {
      "epoch": 0.012028148689787167,
      "grad_norm": 7.431894779205322,
      "learning_rate": 4.9849650345950375e-05,
      "loss": 4.3552,
      "step": 68200
    },
    {
      "epoch": 0.012045785271443746,
      "grad_norm": 6.7565598487854,
      "learning_rate": 4.9849429888679663e-05,
      "loss": 4.31,
      "step": 68300
    },
    {
      "epoch": 0.012063421853100326,
      "grad_norm": 12.850951194763184,
      "learning_rate": 4.984920943140895e-05,
      "loss": 4.3702,
      "step": 68400
    },
    {
      "epoch": 0.012081058434756905,
      "grad_norm": 6.346343040466309,
      "learning_rate": 4.984898897413825e-05,
      "loss": 4.2269,
      "step": 68500
    },
    {
      "epoch": 0.012098695016413485,
      "grad_norm": 6.7758917808532715,
      "learning_rate": 4.9848768516867536e-05,
      "loss": 4.3192,
      "step": 68600
    },
    {
      "epoch": 0.012116331598070065,
      "grad_norm": 6.06728982925415,
      "learning_rate": 4.984854805959683e-05,
      "loss": 4.3409,
      "step": 68700
    },
    {
      "epoch": 0.012133968179726643,
      "grad_norm": 6.158467769622803,
      "learning_rate": 4.984832760232613e-05,
      "loss": 4.2587,
      "step": 68800
    },
    {
      "epoch": 0.012151604761383224,
      "grad_norm": 6.961314678192139,
      "learning_rate": 4.9848107145055416e-05,
      "loss": 4.2297,
      "step": 68900
    },
    {
      "epoch": 0.012169241343039802,
      "grad_norm": 9.427347183227539,
      "learning_rate": 4.984788668778471e-05,
      "loss": 4.3479,
      "step": 69000
    },
    {
      "epoch": 0.012186877924696382,
      "grad_norm": 8.07129955291748,
      "learning_rate": 4.984766623051401e-05,
      "loss": 4.3326,
      "step": 69100
    },
    {
      "epoch": 0.012204514506352961,
      "grad_norm": 5.300252437591553,
      "learning_rate": 4.9847445773243296e-05,
      "loss": 4.4038,
      "step": 69200
    },
    {
      "epoch": 0.012222151088009541,
      "grad_norm": 8.12580394744873,
      "learning_rate": 4.984722531597259e-05,
      "loss": 4.3073,
      "step": 69300
    },
    {
      "epoch": 0.01223978766966612,
      "grad_norm": 5.440061569213867,
      "learning_rate": 4.9847004858701886e-05,
      "loss": 4.3115,
      "step": 69400
    },
    {
      "epoch": 0.0122574242513227,
      "grad_norm": 6.402676105499268,
      "learning_rate": 4.9846784401431175e-05,
      "loss": 4.1925,
      "step": 69500
    },
    {
      "epoch": 0.012275060832979278,
      "grad_norm": 8.072514533996582,
      "learning_rate": 4.984656394416047e-05,
      "loss": 4.2775,
      "step": 69600
    },
    {
      "epoch": 0.012292697414635859,
      "grad_norm": 9.975335121154785,
      "learning_rate": 4.9846343486889766e-05,
      "loss": 4.3057,
      "step": 69700
    },
    {
      "epoch": 0.012310333996292437,
      "grad_norm": 7.407797336578369,
      "learning_rate": 4.9846123029619055e-05,
      "loss": 4.3468,
      "step": 69800
    },
    {
      "epoch": 0.012327970577949017,
      "grad_norm": 7.135295391082764,
      "learning_rate": 4.9845902572348344e-05,
      "loss": 4.3635,
      "step": 69900
    },
    {
      "epoch": 0.012345607159605596,
      "grad_norm": 5.836775302886963,
      "learning_rate": 4.984568211507764e-05,
      "loss": 4.2905,
      "step": 70000
    },
    {
      "epoch": 0.012363243741262176,
      "grad_norm": 7.754321575164795,
      "learning_rate": 4.984546165780693e-05,
      "loss": 4.3283,
      "step": 70100
    },
    {
      "epoch": 0.012380880322918756,
      "grad_norm": 8.41949462890625,
      "learning_rate": 4.984524120053622e-05,
      "loss": 4.3742,
      "step": 70200
    },
    {
      "epoch": 0.012398516904575335,
      "grad_norm": 6.333799839019775,
      "learning_rate": 4.984502074326552e-05,
      "loss": 4.3294,
      "step": 70300
    },
    {
      "epoch": 0.012416153486231915,
      "grad_norm": 6.922268390655518,
      "learning_rate": 4.984480028599481e-05,
      "loss": 4.3018,
      "step": 70400
    },
    {
      "epoch": 0.012433790067888494,
      "grad_norm": 5.549326419830322,
      "learning_rate": 4.98445798287241e-05,
      "loss": 4.1741,
      "step": 70500
    },
    {
      "epoch": 0.012451426649545074,
      "grad_norm": 7.662748336791992,
      "learning_rate": 4.98443593714534e-05,
      "loss": 4.2395,
      "step": 70600
    },
    {
      "epoch": 0.012469063231201652,
      "grad_norm": 5.972400665283203,
      "learning_rate": 4.984413891418269e-05,
      "loss": 4.3154,
      "step": 70700
    },
    {
      "epoch": 0.012486699812858233,
      "grad_norm": 7.03279447555542,
      "learning_rate": 4.984391845691198e-05,
      "loss": 4.3441,
      "step": 70800
    },
    {
      "epoch": 0.012504336394514811,
      "grad_norm": 6.812561988830566,
      "learning_rate": 4.984369799964128e-05,
      "loss": 4.3059,
      "step": 70900
    },
    {
      "epoch": 0.012521972976171391,
      "grad_norm": 7.744077682495117,
      "learning_rate": 4.9843477542370567e-05,
      "loss": 4.2213,
      "step": 71000
    },
    {
      "epoch": 0.01253960955782797,
      "grad_norm": 8.527499198913574,
      "learning_rate": 4.984325708509986e-05,
      "loss": 4.3578,
      "step": 71100
    },
    {
      "epoch": 0.01255724613948455,
      "grad_norm": 9.662544250488281,
      "learning_rate": 4.984303662782915e-05,
      "loss": 4.2914,
      "step": 71200
    },
    {
      "epoch": 0.012574882721141128,
      "grad_norm": 7.560918807983398,
      "learning_rate": 4.9842816170558446e-05,
      "loss": 4.3429,
      "step": 71300
    },
    {
      "epoch": 0.012592519302797709,
      "grad_norm": 6.3992180824279785,
      "learning_rate": 4.9842595713287735e-05,
      "loss": 4.1715,
      "step": 71400
    },
    {
      "epoch": 0.012610155884454287,
      "grad_norm": 10.508781433105469,
      "learning_rate": 4.984237525601703e-05,
      "loss": 4.2528,
      "step": 71500
    },
    {
      "epoch": 0.012627792466110867,
      "grad_norm": 11.097844123840332,
      "learning_rate": 4.984215479874632e-05,
      "loss": 4.2341,
      "step": 71600
    },
    {
      "epoch": 0.012645429047767448,
      "grad_norm": 5.41688346862793,
      "learning_rate": 4.9841934341475615e-05,
      "loss": 4.3535,
      "step": 71700
    },
    {
      "epoch": 0.012663065629424026,
      "grad_norm": 5.733083248138428,
      "learning_rate": 4.984171388420491e-05,
      "loss": 4.366,
      "step": 71800
    },
    {
      "epoch": 0.012680702211080606,
      "grad_norm": 7.6507978439331055,
      "learning_rate": 4.98414934269342e-05,
      "loss": 4.2654,
      "step": 71900
    },
    {
      "epoch": 0.012698338792737185,
      "grad_norm": 5.264863014221191,
      "learning_rate": 4.9841272969663494e-05,
      "loss": 4.1684,
      "step": 72000
    },
    {
      "epoch": 0.012715975374393765,
      "grad_norm": 8.622480392456055,
      "learning_rate": 4.984105251239279e-05,
      "loss": 4.3637,
      "step": 72100
    },
    {
      "epoch": 0.012733611956050344,
      "grad_norm": 8.517539978027344,
      "learning_rate": 4.984083205512208e-05,
      "loss": 4.3018,
      "step": 72200
    },
    {
      "epoch": 0.012751248537706924,
      "grad_norm": 5.752557277679443,
      "learning_rate": 4.9840611597851374e-05,
      "loss": 4.2167,
      "step": 72300
    },
    {
      "epoch": 0.012768885119363502,
      "grad_norm": 8.252609252929688,
      "learning_rate": 4.984039114058067e-05,
      "loss": 4.3286,
      "step": 72400
    },
    {
      "epoch": 0.012786521701020083,
      "grad_norm": 7.36110782623291,
      "learning_rate": 4.984017068330996e-05,
      "loss": 4.2445,
      "step": 72500
    },
    {
      "epoch": 0.012804158282676661,
      "grad_norm": 7.967659950256348,
      "learning_rate": 4.9839950226039253e-05,
      "loss": 4.3037,
      "step": 72600
    },
    {
      "epoch": 0.012821794864333241,
      "grad_norm": 7.110871315002441,
      "learning_rate": 4.983972976876854e-05,
      "loss": 4.129,
      "step": 72700
    },
    {
      "epoch": 0.01283943144598982,
      "grad_norm": 7.819331169128418,
      "learning_rate": 4.983950931149783e-05,
      "loss": 4.376,
      "step": 72800
    },
    {
      "epoch": 0.0128570680276464,
      "grad_norm": 8.894966125488281,
      "learning_rate": 4.9839288854227126e-05,
      "loss": 4.2446,
      "step": 72900
    },
    {
      "epoch": 0.012874704609302979,
      "grad_norm": 5.848570823669434,
      "learning_rate": 4.983906839695642e-05,
      "loss": 4.2416,
      "step": 73000
    },
    {
      "epoch": 0.012892341190959559,
      "grad_norm": 7.60205078125,
      "learning_rate": 4.983884793968572e-05,
      "loss": 4.2292,
      "step": 73100
    },
    {
      "epoch": 0.012909977772616139,
      "grad_norm": 4.310253143310547,
      "learning_rate": 4.9838627482415006e-05,
      "loss": 4.222,
      "step": 73200
    },
    {
      "epoch": 0.012927614354272718,
      "grad_norm": 8.168061256408691,
      "learning_rate": 4.98384070251443e-05,
      "loss": 4.3272,
      "step": 73300
    },
    {
      "epoch": 0.012945250935929298,
      "grad_norm": 6.277576923370361,
      "learning_rate": 4.98381865678736e-05,
      "loss": 4.2662,
      "step": 73400
    },
    {
      "epoch": 0.012962887517585876,
      "grad_norm": 10.536588668823242,
      "learning_rate": 4.9837966110602886e-05,
      "loss": 4.2221,
      "step": 73500
    },
    {
      "epoch": 0.012980524099242456,
      "grad_norm": 6.051902770996094,
      "learning_rate": 4.983774565333218e-05,
      "loss": 4.4233,
      "step": 73600
    },
    {
      "epoch": 0.012998160680899035,
      "grad_norm": 9.701619148254395,
      "learning_rate": 4.9837525196061476e-05,
      "loss": 4.2795,
      "step": 73700
    },
    {
      "epoch": 0.013015797262555615,
      "grad_norm": 7.5513529777526855,
      "learning_rate": 4.9837304738790765e-05,
      "loss": 4.4192,
      "step": 73800
    },
    {
      "epoch": 0.013033433844212194,
      "grad_norm": 5.8760576248168945,
      "learning_rate": 4.983708428152006e-05,
      "loss": 4.3373,
      "step": 73900
    },
    {
      "epoch": 0.013051070425868774,
      "grad_norm": 9.926383018493652,
      "learning_rate": 4.983686382424935e-05,
      "loss": 4.2421,
      "step": 74000
    },
    {
      "epoch": 0.013068707007525352,
      "grad_norm": 7.980915546417236,
      "learning_rate": 4.9836643366978645e-05,
      "loss": 4.1929,
      "step": 74100
    },
    {
      "epoch": 0.013086343589181933,
      "grad_norm": 5.6466064453125,
      "learning_rate": 4.9836422909707933e-05,
      "loss": 4.3481,
      "step": 74200
    },
    {
      "epoch": 0.013103980170838511,
      "grad_norm": 5.119506359100342,
      "learning_rate": 4.983620245243723e-05,
      "loss": 4.2808,
      "step": 74300
    },
    {
      "epoch": 0.013121616752495091,
      "grad_norm": 7.537578582763672,
      "learning_rate": 4.983598199516652e-05,
      "loss": 4.3049,
      "step": 74400
    },
    {
      "epoch": 0.01313925333415167,
      "grad_norm": 7.021448135375977,
      "learning_rate": 4.983576153789581e-05,
      "loss": 4.2354,
      "step": 74500
    },
    {
      "epoch": 0.01315688991580825,
      "grad_norm": 6.890361309051514,
      "learning_rate": 4.983554108062511e-05,
      "loss": 4.3397,
      "step": 74600
    },
    {
      "epoch": 0.01317452649746483,
      "grad_norm": 7.306060791015625,
      "learning_rate": 4.98353206233544e-05,
      "loss": 4.2817,
      "step": 74700
    },
    {
      "epoch": 0.013192163079121409,
      "grad_norm": 5.111050128936768,
      "learning_rate": 4.983510016608369e-05,
      "loss": 4.1816,
      "step": 74800
    },
    {
      "epoch": 0.013209799660777989,
      "grad_norm": 7.512477397918701,
      "learning_rate": 4.983487970881299e-05,
      "loss": 4.407,
      "step": 74900
    },
    {
      "epoch": 0.013227436242434568,
      "grad_norm": 9.80772876739502,
      "learning_rate": 4.983465925154228e-05,
      "loss": 4.394,
      "step": 75000
    },
    {
      "epoch": 0.013245072824091148,
      "grad_norm": 6.245039939880371,
      "learning_rate": 4.983443879427157e-05,
      "loss": 4.3808,
      "step": 75100
    },
    {
      "epoch": 0.013262709405747726,
      "grad_norm": 5.996768951416016,
      "learning_rate": 4.983421833700087e-05,
      "loss": 4.229,
      "step": 75200
    },
    {
      "epoch": 0.013280345987404307,
      "grad_norm": 6.843710899353027,
      "learning_rate": 4.9833997879730157e-05,
      "loss": 4.258,
      "step": 75300
    },
    {
      "epoch": 0.013297982569060885,
      "grad_norm": 7.116604328155518,
      "learning_rate": 4.983377742245945e-05,
      "loss": 4.2505,
      "step": 75400
    },
    {
      "epoch": 0.013315619150717465,
      "grad_norm": 7.3449835777282715,
      "learning_rate": 4.983355696518874e-05,
      "loss": 4.2712,
      "step": 75500
    },
    {
      "epoch": 0.013333255732374044,
      "grad_norm": 16.652421951293945,
      "learning_rate": 4.983333650791803e-05,
      "loss": 4.2471,
      "step": 75600
    },
    {
      "epoch": 0.013350892314030624,
      "grad_norm": 6.485400199890137,
      "learning_rate": 4.9833116050647325e-05,
      "loss": 4.2943,
      "step": 75700
    },
    {
      "epoch": 0.013368528895687203,
      "grad_norm": 5.419765949249268,
      "learning_rate": 4.983289559337662e-05,
      "loss": 4.2929,
      "step": 75800
    },
    {
      "epoch": 0.013386165477343783,
      "grad_norm": 7.058714866638184,
      "learning_rate": 4.983267513610591e-05,
      "loss": 4.2589,
      "step": 75900
    },
    {
      "epoch": 0.013403802059000361,
      "grad_norm": 8.8424711227417,
      "learning_rate": 4.9832454678835204e-05,
      "loss": 4.2905,
      "step": 76000
    },
    {
      "epoch": 0.013421438640656942,
      "grad_norm": 5.972256660461426,
      "learning_rate": 4.98322342215645e-05,
      "loss": 4.3007,
      "step": 76100
    },
    {
      "epoch": 0.013439075222313522,
      "grad_norm": 5.740483283996582,
      "learning_rate": 4.983201376429379e-05,
      "loss": 4.3663,
      "step": 76200
    },
    {
      "epoch": 0.0134567118039701,
      "grad_norm": 11.694348335266113,
      "learning_rate": 4.9831793307023084e-05,
      "loss": 4.1845,
      "step": 76300
    },
    {
      "epoch": 0.01347434838562668,
      "grad_norm": 5.579736232757568,
      "learning_rate": 4.983157284975238e-05,
      "loss": 4.2817,
      "step": 76400
    },
    {
      "epoch": 0.013491984967283259,
      "grad_norm": 6.606788158416748,
      "learning_rate": 4.983135239248167e-05,
      "loss": 4.2049,
      "step": 76500
    },
    {
      "epoch": 0.01350962154893984,
      "grad_norm": 5.388739585876465,
      "learning_rate": 4.9831131935210964e-05,
      "loss": 4.1313,
      "step": 76600
    },
    {
      "epoch": 0.013527258130596418,
      "grad_norm": 20.66738510131836,
      "learning_rate": 4.983091147794026e-05,
      "loss": 4.3014,
      "step": 76700
    },
    {
      "epoch": 0.013544894712252998,
      "grad_norm": 5.070841312408447,
      "learning_rate": 4.983069102066955e-05,
      "loss": 4.2554,
      "step": 76800
    },
    {
      "epoch": 0.013562531293909576,
      "grad_norm": 6.410280227661133,
      "learning_rate": 4.983047056339884e-05,
      "loss": 4.2903,
      "step": 76900
    },
    {
      "epoch": 0.013580167875566157,
      "grad_norm": 11.159833908081055,
      "learning_rate": 4.983025010612813e-05,
      "loss": 4.2796,
      "step": 77000
    },
    {
      "epoch": 0.013597804457222735,
      "grad_norm": 6.89277982711792,
      "learning_rate": 4.983002964885742e-05,
      "loss": 4.2038,
      "step": 77100
    },
    {
      "epoch": 0.013615441038879315,
      "grad_norm": 8.183292388916016,
      "learning_rate": 4.9829809191586716e-05,
      "loss": 4.3054,
      "step": 77200
    },
    {
      "epoch": 0.013633077620535894,
      "grad_norm": 9.891484260559082,
      "learning_rate": 4.982958873431601e-05,
      "loss": 4.3113,
      "step": 77300
    },
    {
      "epoch": 0.013650714202192474,
      "grad_norm": 5.9831953048706055,
      "learning_rate": 4.98293682770453e-05,
      "loss": 4.2319,
      "step": 77400
    },
    {
      "epoch": 0.013668350783849053,
      "grad_norm": 12.08278751373291,
      "learning_rate": 4.9829147819774596e-05,
      "loss": 4.1692,
      "step": 77500
    },
    {
      "epoch": 0.013685987365505633,
      "grad_norm": 5.957094192504883,
      "learning_rate": 4.982892736250389e-05,
      "loss": 4.244,
      "step": 77600
    },
    {
      "epoch": 0.013703623947162213,
      "grad_norm": 5.612415313720703,
      "learning_rate": 4.982870690523318e-05,
      "loss": 4.2058,
      "step": 77700
    },
    {
      "epoch": 0.013721260528818792,
      "grad_norm": 9.954780578613281,
      "learning_rate": 4.9828486447962475e-05,
      "loss": 4.3102,
      "step": 77800
    },
    {
      "epoch": 0.013738897110475372,
      "grad_norm": 6.9185404777526855,
      "learning_rate": 4.982826599069177e-05,
      "loss": 4.2671,
      "step": 77900
    },
    {
      "epoch": 0.01375653369213195,
      "grad_norm": 6.927863121032715,
      "learning_rate": 4.982804553342106e-05,
      "loss": 4.1394,
      "step": 78000
    },
    {
      "epoch": 0.01377417027378853,
      "grad_norm": 6.745497703552246,
      "learning_rate": 4.9827825076150355e-05,
      "loss": 4.2205,
      "step": 78100
    },
    {
      "epoch": 0.013791806855445109,
      "grad_norm": 7.19296407699585,
      "learning_rate": 4.982760461887965e-05,
      "loss": 4.1968,
      "step": 78200
    },
    {
      "epoch": 0.01380944343710169,
      "grad_norm": 7.88399600982666,
      "learning_rate": 4.982738416160894e-05,
      "loss": 4.2342,
      "step": 78300
    },
    {
      "epoch": 0.013827080018758268,
      "grad_norm": 5.953391075134277,
      "learning_rate": 4.982716370433823e-05,
      "loss": 4.2296,
      "step": 78400
    },
    {
      "epoch": 0.013844716600414848,
      "grad_norm": 5.687605857849121,
      "learning_rate": 4.9826943247067523e-05,
      "loss": 4.1917,
      "step": 78500
    },
    {
      "epoch": 0.013862353182071427,
      "grad_norm": 7.199227333068848,
      "learning_rate": 4.982672278979681e-05,
      "loss": 4.2567,
      "step": 78600
    },
    {
      "epoch": 0.013879989763728007,
      "grad_norm": 6.651280879974365,
      "learning_rate": 4.982650233252611e-05,
      "loss": 4.1257,
      "step": 78700
    },
    {
      "epoch": 0.013897626345384585,
      "grad_norm": 6.450899124145508,
      "learning_rate": 4.98262818752554e-05,
      "loss": 4.2544,
      "step": 78800
    },
    {
      "epoch": 0.013915262927041165,
      "grad_norm": 5.042211532592773,
      "learning_rate": 4.982606141798469e-05,
      "loss": 4.1915,
      "step": 78900
    },
    {
      "epoch": 0.013932899508697744,
      "grad_norm": 8.922880172729492,
      "learning_rate": 4.982584096071399e-05,
      "loss": 4.317,
      "step": 79000
    },
    {
      "epoch": 0.013950536090354324,
      "grad_norm": 5.995482444763184,
      "learning_rate": 4.982562050344328e-05,
      "loss": 4.2899,
      "step": 79100
    },
    {
      "epoch": 0.013968172672010904,
      "grad_norm": 6.669735431671143,
      "learning_rate": 4.982540004617257e-05,
      "loss": 4.2364,
      "step": 79200
    },
    {
      "epoch": 0.013985809253667483,
      "grad_norm": 7.401613712310791,
      "learning_rate": 4.982517958890187e-05,
      "loss": 4.1996,
      "step": 79300
    },
    {
      "epoch": 0.014003445835324063,
      "grad_norm": 5.552191734313965,
      "learning_rate": 4.982495913163116e-05,
      "loss": 4.2175,
      "step": 79400
    },
    {
      "epoch": 0.014021082416980642,
      "grad_norm": 5.873813629150391,
      "learning_rate": 4.982473867436045e-05,
      "loss": 4.296,
      "step": 79500
    },
    {
      "epoch": 0.014038718998637222,
      "grad_norm": 6.460977554321289,
      "learning_rate": 4.9824518217089746e-05,
      "loss": 4.2211,
      "step": 79600
    },
    {
      "epoch": 0.0140563555802938,
      "grad_norm": 10.517620086669922,
      "learning_rate": 4.982429775981904e-05,
      "loss": 4.3277,
      "step": 79700
    },
    {
      "epoch": 0.01407399216195038,
      "grad_norm": 7.0544819831848145,
      "learning_rate": 4.982407730254833e-05,
      "loss": 4.1953,
      "step": 79800
    },
    {
      "epoch": 0.01409162874360696,
      "grad_norm": 6.690151691436768,
      "learning_rate": 4.982385684527762e-05,
      "loss": 4.2747,
      "step": 79900
    },
    {
      "epoch": 0.01410926532526354,
      "grad_norm": 6.228731155395508,
      "learning_rate": 4.9823636388006915e-05,
      "loss": 4.1857,
      "step": 80000
    },
    {
      "epoch": 0.014126901906920118,
      "grad_norm": 5.394221305847168,
      "learning_rate": 4.9823415930736203e-05,
      "loss": 4.2775,
      "step": 80100
    },
    {
      "epoch": 0.014144538488576698,
      "grad_norm": 11.534717559814453,
      "learning_rate": 4.98231954734655e-05,
      "loss": 4.273,
      "step": 80200
    },
    {
      "epoch": 0.014162175070233277,
      "grad_norm": 6.224231719970703,
      "learning_rate": 4.9822975016194794e-05,
      "loss": 4.2441,
      "step": 80300
    },
    {
      "epoch": 0.014179811651889857,
      "grad_norm": 6.072531223297119,
      "learning_rate": 4.982275455892408e-05,
      "loss": 4.2016,
      "step": 80400
    },
    {
      "epoch": 0.014197448233546435,
      "grad_norm": 8.902822494506836,
      "learning_rate": 4.982253410165338e-05,
      "loss": 4.2697,
      "step": 80500
    },
    {
      "epoch": 0.014215084815203016,
      "grad_norm": 6.364502429962158,
      "learning_rate": 4.9822313644382674e-05,
      "loss": 4.1981,
      "step": 80600
    },
    {
      "epoch": 0.014232721396859596,
      "grad_norm": 6.1219377517700195,
      "learning_rate": 4.982209318711196e-05,
      "loss": 4.2218,
      "step": 80700
    },
    {
      "epoch": 0.014250357978516174,
      "grad_norm": 10.154072761535645,
      "learning_rate": 4.982187272984126e-05,
      "loss": 4.2702,
      "step": 80800
    },
    {
      "epoch": 0.014267994560172755,
      "grad_norm": 4.930331707000732,
      "learning_rate": 4.9821652272570554e-05,
      "loss": 4.2652,
      "step": 80900
    },
    {
      "epoch": 0.014285631141829333,
      "grad_norm": 6.582866668701172,
      "learning_rate": 4.982143181529984e-05,
      "loss": 4.2334,
      "step": 81000
    },
    {
      "epoch": 0.014303267723485913,
      "grad_norm": 7.414453983306885,
      "learning_rate": 4.982121135802914e-05,
      "loss": 4.2566,
      "step": 81100
    },
    {
      "epoch": 0.014320904305142492,
      "grad_norm": 7.768573760986328,
      "learning_rate": 4.9820990900758427e-05,
      "loss": 4.1138,
      "step": 81200
    },
    {
      "epoch": 0.014338540886799072,
      "grad_norm": 5.823853492736816,
      "learning_rate": 4.982077044348772e-05,
      "loss": 4.2108,
      "step": 81300
    },
    {
      "epoch": 0.01435617746845565,
      "grad_norm": 7.969454288482666,
      "learning_rate": 4.982054998621701e-05,
      "loss": 4.1936,
      "step": 81400
    },
    {
      "epoch": 0.01437381405011223,
      "grad_norm": 6.1071553230285645,
      "learning_rate": 4.9820329528946306e-05,
      "loss": 4.152,
      "step": 81500
    },
    {
      "epoch": 0.01439145063176881,
      "grad_norm": 7.525707721710205,
      "learning_rate": 4.9820109071675595e-05,
      "loss": 4.1772,
      "step": 81600
    },
    {
      "epoch": 0.01440908721342539,
      "grad_norm": 5.896087646484375,
      "learning_rate": 4.981988861440489e-05,
      "loss": 4.1621,
      "step": 81700
    },
    {
      "epoch": 0.014426723795081968,
      "grad_norm": 8.171152114868164,
      "learning_rate": 4.9819668157134186e-05,
      "loss": 4.2682,
      "step": 81800
    },
    {
      "epoch": 0.014444360376738548,
      "grad_norm": 6.849403381347656,
      "learning_rate": 4.9819447699863474e-05,
      "loss": 4.1446,
      "step": 81900
    },
    {
      "epoch": 0.014461996958395127,
      "grad_norm": 5.69243860244751,
      "learning_rate": 4.981922724259277e-05,
      "loss": 4.2409,
      "step": 82000
    },
    {
      "epoch": 0.014479633540051707,
      "grad_norm": 10.08165168762207,
      "learning_rate": 4.9819006785322065e-05,
      "loss": 4.2359,
      "step": 82100
    },
    {
      "epoch": 0.014497270121708287,
      "grad_norm": 12.483796119689941,
      "learning_rate": 4.9818786328051354e-05,
      "loss": 4.327,
      "step": 82200
    },
    {
      "epoch": 0.014514906703364866,
      "grad_norm": 7.895836353302002,
      "learning_rate": 4.981856587078065e-05,
      "loss": 4.1632,
      "step": 82300
    },
    {
      "epoch": 0.014532543285021446,
      "grad_norm": 6.183413028717041,
      "learning_rate": 4.9818345413509945e-05,
      "loss": 4.1835,
      "step": 82400
    },
    {
      "epoch": 0.014550179866678024,
      "grad_norm": 7.570639133453369,
      "learning_rate": 4.9818124956239234e-05,
      "loss": 4.1665,
      "step": 82500
    },
    {
      "epoch": 0.014567816448334605,
      "grad_norm": 7.296695709228516,
      "learning_rate": 4.981790449896853e-05,
      "loss": 4.2101,
      "step": 82600
    },
    {
      "epoch": 0.014585453029991183,
      "grad_norm": 13.065677642822266,
      "learning_rate": 4.981768404169782e-05,
      "loss": 4.0723,
      "step": 82700
    },
    {
      "epoch": 0.014603089611647763,
      "grad_norm": 6.450053691864014,
      "learning_rate": 4.9817463584427107e-05,
      "loss": 4.2441,
      "step": 82800
    },
    {
      "epoch": 0.014620726193304342,
      "grad_norm": 7.8519673347473145,
      "learning_rate": 4.98172431271564e-05,
      "loss": 4.2372,
      "step": 82900
    },
    {
      "epoch": 0.014638362774960922,
      "grad_norm": 6.683003902435303,
      "learning_rate": 4.98170226698857e-05,
      "loss": 4.2152,
      "step": 83000
    },
    {
      "epoch": 0.0146559993566175,
      "grad_norm": 5.30575704574585,
      "learning_rate": 4.9816802212614986e-05,
      "loss": 4.3713,
      "step": 83100
    },
    {
      "epoch": 0.01467363593827408,
      "grad_norm": 5.896632671356201,
      "learning_rate": 4.981658175534428e-05,
      "loss": 4.2252,
      "step": 83200
    },
    {
      "epoch": 0.01469127251993066,
      "grad_norm": 7.401766777038574,
      "learning_rate": 4.981636129807358e-05,
      "loss": 4.2014,
      "step": 83300
    },
    {
      "epoch": 0.01470890910158724,
      "grad_norm": 6.068694591522217,
      "learning_rate": 4.9816140840802866e-05,
      "loss": 4.3035,
      "step": 83400
    },
    {
      "epoch": 0.014726545683243818,
      "grad_norm": 11.531227111816406,
      "learning_rate": 4.981592038353216e-05,
      "loss": 4.1724,
      "step": 83500
    },
    {
      "epoch": 0.014744182264900398,
      "grad_norm": 8.652606010437012,
      "learning_rate": 4.981569992626146e-05,
      "loss": 4.2488,
      "step": 83600
    },
    {
      "epoch": 0.014761818846556978,
      "grad_norm": 8.049381256103516,
      "learning_rate": 4.9815479468990745e-05,
      "loss": 4.2332,
      "step": 83700
    },
    {
      "epoch": 0.014779455428213557,
      "grad_norm": 9.4996337890625,
      "learning_rate": 4.981525901172004e-05,
      "loss": 4.3019,
      "step": 83800
    },
    {
      "epoch": 0.014797092009870137,
      "grad_norm": 7.017955780029297,
      "learning_rate": 4.9815038554449336e-05,
      "loss": 4.252,
      "step": 83900
    },
    {
      "epoch": 0.014814728591526716,
      "grad_norm": 6.426937580108643,
      "learning_rate": 4.9814818097178625e-05,
      "loss": 4.2857,
      "step": 84000
    },
    {
      "epoch": 0.014832365173183296,
      "grad_norm": 10.988340377807617,
      "learning_rate": 4.981459763990792e-05,
      "loss": 4.176,
      "step": 84100
    },
    {
      "epoch": 0.014850001754839874,
      "grad_norm": 6.1161346435546875,
      "learning_rate": 4.981437718263721e-05,
      "loss": 4.281,
      "step": 84200
    },
    {
      "epoch": 0.014867638336496455,
      "grad_norm": 5.9001874923706055,
      "learning_rate": 4.9814156725366505e-05,
      "loss": 4.335,
      "step": 84300
    },
    {
      "epoch": 0.014885274918153033,
      "grad_norm": 5.887930870056152,
      "learning_rate": 4.9813936268095793e-05,
      "loss": 4.1769,
      "step": 84400
    },
    {
      "epoch": 0.014902911499809613,
      "grad_norm": 12.241363525390625,
      "learning_rate": 4.981371581082509e-05,
      "loss": 4.2902,
      "step": 84500
    },
    {
      "epoch": 0.014920548081466192,
      "grad_norm": 7.591212749481201,
      "learning_rate": 4.9813495353554384e-05,
      "loss": 4.2373,
      "step": 84600
    },
    {
      "epoch": 0.014938184663122772,
      "grad_norm": 5.6449761390686035,
      "learning_rate": 4.981327489628367e-05,
      "loss": 4.2341,
      "step": 84700
    },
    {
      "epoch": 0.01495582124477935,
      "grad_norm": 7.717013359069824,
      "learning_rate": 4.981305443901297e-05,
      "loss": 4.2694,
      "step": 84800
    },
    {
      "epoch": 0.014973457826435931,
      "grad_norm": 5.128050327301025,
      "learning_rate": 4.9812833981742264e-05,
      "loss": 4.1409,
      "step": 84900
    },
    {
      "epoch": 0.01499109440809251,
      "grad_norm": 7.795753002166748,
      "learning_rate": 4.981261352447155e-05,
      "loss": 4.1545,
      "step": 85000
    },
    {
      "epoch": 0.01500873098974909,
      "grad_norm": 5.460602283477783,
      "learning_rate": 4.981239306720085e-05,
      "loss": 4.2247,
      "step": 85100
    },
    {
      "epoch": 0.01502636757140567,
      "grad_norm": 7.1542582511901855,
      "learning_rate": 4.9812172609930144e-05,
      "loss": 4.2383,
      "step": 85200
    },
    {
      "epoch": 0.015044004153062248,
      "grad_norm": 7.330876350402832,
      "learning_rate": 4.981195215265943e-05,
      "loss": 4.199,
      "step": 85300
    },
    {
      "epoch": 0.015061640734718829,
      "grad_norm": 5.519903182983398,
      "learning_rate": 4.981173169538873e-05,
      "loss": 4.2392,
      "step": 85400
    },
    {
      "epoch": 0.015079277316375407,
      "grad_norm": 6.319302558898926,
      "learning_rate": 4.9811511238118016e-05,
      "loss": 4.1638,
      "step": 85500
    },
    {
      "epoch": 0.015096913898031987,
      "grad_norm": 6.075700759887695,
      "learning_rate": 4.9811290780847305e-05,
      "loss": 4.2773,
      "step": 85600
    },
    {
      "epoch": 0.015114550479688566,
      "grad_norm": 8.715178489685059,
      "learning_rate": 4.98110703235766e-05,
      "loss": 4.2025,
      "step": 85700
    },
    {
      "epoch": 0.015132187061345146,
      "grad_norm": 8.967350959777832,
      "learning_rate": 4.9810849866305896e-05,
      "loss": 4.1313,
      "step": 85800
    },
    {
      "epoch": 0.015149823643001725,
      "grad_norm": 6.1953582763671875,
      "learning_rate": 4.9810629409035185e-05,
      "loss": 4.242,
      "step": 85900
    },
    {
      "epoch": 0.015167460224658305,
      "grad_norm": 7.131707191467285,
      "learning_rate": 4.981040895176448e-05,
      "loss": 4.2834,
      "step": 86000
    },
    {
      "epoch": 0.015185096806314883,
      "grad_norm": 5.551503658294678,
      "learning_rate": 4.9810188494493776e-05,
      "loss": 4.2492,
      "step": 86100
    },
    {
      "epoch": 0.015202733387971464,
      "grad_norm": 8.088663101196289,
      "learning_rate": 4.9809968037223064e-05,
      "loss": 4.2484,
      "step": 86200
    },
    {
      "epoch": 0.015220369969628042,
      "grad_norm": 6.4527668952941895,
      "learning_rate": 4.980974757995236e-05,
      "loss": 4.2416,
      "step": 86300
    },
    {
      "epoch": 0.015238006551284622,
      "grad_norm": 5.183323860168457,
      "learning_rate": 4.9809527122681655e-05,
      "loss": 4.148,
      "step": 86400
    },
    {
      "epoch": 0.0152556431329412,
      "grad_norm": 6.941133975982666,
      "learning_rate": 4.9809306665410944e-05,
      "loss": 4.1028,
      "step": 86500
    },
    {
      "epoch": 0.015273279714597781,
      "grad_norm": 6.991862773895264,
      "learning_rate": 4.980908620814024e-05,
      "loss": 4.2508,
      "step": 86600
    },
    {
      "epoch": 0.015290916296254361,
      "grad_norm": 6.803560256958008,
      "learning_rate": 4.9808865750869535e-05,
      "loss": 4.2099,
      "step": 86700
    },
    {
      "epoch": 0.01530855287791094,
      "grad_norm": 7.100414276123047,
      "learning_rate": 4.9808645293598824e-05,
      "loss": 4.3085,
      "step": 86800
    },
    {
      "epoch": 0.01532618945956752,
      "grad_norm": 5.772853851318359,
      "learning_rate": 4.980842483632812e-05,
      "loss": 4.2494,
      "step": 86900
    },
    {
      "epoch": 0.015343826041224098,
      "grad_norm": 6.377017974853516,
      "learning_rate": 4.980820437905741e-05,
      "loss": 4.2832,
      "step": 87000
    },
    {
      "epoch": 0.015361462622880679,
      "grad_norm": 7.042937755584717,
      "learning_rate": 4.9807983921786697e-05,
      "loss": 4.2369,
      "step": 87100
    },
    {
      "epoch": 0.015379099204537257,
      "grad_norm": 6.719748020172119,
      "learning_rate": 4.980776346451599e-05,
      "loss": 4.2899,
      "step": 87200
    },
    {
      "epoch": 0.015396735786193837,
      "grad_norm": 6.332753658294678,
      "learning_rate": 4.980754300724529e-05,
      "loss": 4.2244,
      "step": 87300
    },
    {
      "epoch": 0.015414372367850416,
      "grad_norm": 9.16073989868164,
      "learning_rate": 4.9807322549974576e-05,
      "loss": 4.1363,
      "step": 87400
    },
    {
      "epoch": 0.015432008949506996,
      "grad_norm": 5.850334167480469,
      "learning_rate": 4.980710209270387e-05,
      "loss": 4.1383,
      "step": 87500
    },
    {
      "epoch": 0.015449645531163575,
      "grad_norm": 7.303922653198242,
      "learning_rate": 4.980688163543317e-05,
      "loss": 4.268,
      "step": 87600
    },
    {
      "epoch": 0.015467282112820155,
      "grad_norm": 7.5365681648254395,
      "learning_rate": 4.9806661178162456e-05,
      "loss": 4.1833,
      "step": 87700
    },
    {
      "epoch": 0.015484918694476733,
      "grad_norm": 8.251529693603516,
      "learning_rate": 4.980644072089175e-05,
      "loss": 4.0714,
      "step": 87800
    },
    {
      "epoch": 0.015502555276133314,
      "grad_norm": 8.265453338623047,
      "learning_rate": 4.980622026362105e-05,
      "loss": 4.2488,
      "step": 87900
    },
    {
      "epoch": 0.015520191857789892,
      "grad_norm": 4.500360012054443,
      "learning_rate": 4.9805999806350335e-05,
      "loss": 4.1802,
      "step": 88000
    },
    {
      "epoch": 0.015537828439446472,
      "grad_norm": 6.564836025238037,
      "learning_rate": 4.980577934907963e-05,
      "loss": 4.1943,
      "step": 88100
    },
    {
      "epoch": 0.015555465021103053,
      "grad_norm": 5.46412467956543,
      "learning_rate": 4.9805558891808926e-05,
      "loss": 4.2699,
      "step": 88200
    },
    {
      "epoch": 0.015573101602759631,
      "grad_norm": 7.228821754455566,
      "learning_rate": 4.9805338434538215e-05,
      "loss": 4.0857,
      "step": 88300
    },
    {
      "epoch": 0.015590738184416211,
      "grad_norm": 6.5515851974487305,
      "learning_rate": 4.9805117977267504e-05,
      "loss": 4.3142,
      "step": 88400
    },
    {
      "epoch": 0.01560837476607279,
      "grad_norm": 8.403645515441895,
      "learning_rate": 4.98048975199968e-05,
      "loss": 4.2424,
      "step": 88500
    },
    {
      "epoch": 0.01562601134772937,
      "grad_norm": 8.939428329467773,
      "learning_rate": 4.980467706272609e-05,
      "loss": 4.1402,
      "step": 88600
    },
    {
      "epoch": 0.01564364792938595,
      "grad_norm": 6.501457691192627,
      "learning_rate": 4.980445660545538e-05,
      "loss": 4.2034,
      "step": 88700
    },
    {
      "epoch": 0.01566128451104253,
      "grad_norm": 9.035466194152832,
      "learning_rate": 4.980423614818468e-05,
      "loss": 4.1967,
      "step": 88800
    },
    {
      "epoch": 0.015678921092699107,
      "grad_norm": 7.692783355712891,
      "learning_rate": 4.980401569091397e-05,
      "loss": 4.2542,
      "step": 88900
    },
    {
      "epoch": 0.015696557674355686,
      "grad_norm": 7.697030544281006,
      "learning_rate": 4.980379523364326e-05,
      "loss": 4.206,
      "step": 89000
    },
    {
      "epoch": 0.015714194256012268,
      "grad_norm": 8.65722942352295,
      "learning_rate": 4.980357477637256e-05,
      "loss": 4.2021,
      "step": 89100
    },
    {
      "epoch": 0.015731830837668846,
      "grad_norm": 6.463964939117432,
      "learning_rate": 4.980335431910185e-05,
      "loss": 4.2742,
      "step": 89200
    },
    {
      "epoch": 0.015749467419325425,
      "grad_norm": 6.705415725708008,
      "learning_rate": 4.980313386183114e-05,
      "loss": 4.3062,
      "step": 89300
    },
    {
      "epoch": 0.015767104000982003,
      "grad_norm": 7.646500587463379,
      "learning_rate": 4.980291340456044e-05,
      "loss": 4.1736,
      "step": 89400
    },
    {
      "epoch": 0.015784740582638585,
      "grad_norm": 4.951512336730957,
      "learning_rate": 4.980269294728973e-05,
      "loss": 4.159,
      "step": 89500
    },
    {
      "epoch": 0.015802377164295164,
      "grad_norm": 7.863715171813965,
      "learning_rate": 4.980247249001902e-05,
      "loss": 4.1848,
      "step": 89600
    },
    {
      "epoch": 0.015820013745951742,
      "grad_norm": 9.743597030639648,
      "learning_rate": 4.980225203274832e-05,
      "loss": 4.1715,
      "step": 89700
    },
    {
      "epoch": 0.015837650327608324,
      "grad_norm": 6.049289226531982,
      "learning_rate": 4.9802031575477606e-05,
      "loss": 4.1474,
      "step": 89800
    },
    {
      "epoch": 0.015855286909264903,
      "grad_norm": 5.362893581390381,
      "learning_rate": 4.9801811118206895e-05,
      "loss": 4.2617,
      "step": 89900
    },
    {
      "epoch": 0.01587292349092148,
      "grad_norm": 8.340738296508789,
      "learning_rate": 4.980159066093619e-05,
      "loss": 4.2315,
      "step": 90000
    },
    {
      "epoch": 0.01589056007257806,
      "grad_norm": 7.166862964630127,
      "learning_rate": 4.980137020366548e-05,
      "loss": 4.1562,
      "step": 90100
    },
    {
      "epoch": 0.01590819665423464,
      "grad_norm": 6.3424458503723145,
      "learning_rate": 4.9801149746394775e-05,
      "loss": 4.2451,
      "step": 90200
    },
    {
      "epoch": 0.01592583323589122,
      "grad_norm": 5.886220455169678,
      "learning_rate": 4.980092928912407e-05,
      "loss": 4.2803,
      "step": 90300
    },
    {
      "epoch": 0.0159434698175478,
      "grad_norm": 6.900235652923584,
      "learning_rate": 4.980070883185336e-05,
      "loss": 4.188,
      "step": 90400
    },
    {
      "epoch": 0.015961106399204377,
      "grad_norm": 8.930835723876953,
      "learning_rate": 4.9800488374582654e-05,
      "loss": 4.2155,
      "step": 90500
    },
    {
      "epoch": 0.01597874298086096,
      "grad_norm": 11.002732276916504,
      "learning_rate": 4.980026791731195e-05,
      "loss": 4.1656,
      "step": 90600
    },
    {
      "epoch": 0.015996379562517538,
      "grad_norm": 6.973154544830322,
      "learning_rate": 4.980004746004124e-05,
      "loss": 4.3175,
      "step": 90700
    },
    {
      "epoch": 0.016014016144174116,
      "grad_norm": 8.358388900756836,
      "learning_rate": 4.9799827002770534e-05,
      "loss": 4.2096,
      "step": 90800
    },
    {
      "epoch": 0.016031652725830695,
      "grad_norm": 9.738155364990234,
      "learning_rate": 4.979960654549983e-05,
      "loss": 4.2392,
      "step": 90900
    },
    {
      "epoch": 0.016049289307487277,
      "grad_norm": 5.913871765136719,
      "learning_rate": 4.979938608822912e-05,
      "loss": 4.1638,
      "step": 91000
    },
    {
      "epoch": 0.016066925889143855,
      "grad_norm": 6.9941511154174805,
      "learning_rate": 4.9799165630958414e-05,
      "loss": 4.184,
      "step": 91100
    },
    {
      "epoch": 0.016084562470800434,
      "grad_norm": 5.7461981773376465,
      "learning_rate": 4.97989451736877e-05,
      "loss": 4.1361,
      "step": 91200
    },
    {
      "epoch": 0.016102199052457015,
      "grad_norm": 11.392669677734375,
      "learning_rate": 4.9798724716417e-05,
      "loss": 4.2464,
      "step": 91300
    },
    {
      "epoch": 0.016119835634113594,
      "grad_norm": 7.539524555206299,
      "learning_rate": 4.9798504259146286e-05,
      "loss": 4.1747,
      "step": 91400
    },
    {
      "epoch": 0.016137472215770172,
      "grad_norm": 12.602445602416992,
      "learning_rate": 4.979828380187558e-05,
      "loss": 4.164,
      "step": 91500
    },
    {
      "epoch": 0.01615510879742675,
      "grad_norm": 6.058858394622803,
      "learning_rate": 4.979806334460487e-05,
      "loss": 4.159,
      "step": 91600
    },
    {
      "epoch": 0.016172745379083333,
      "grad_norm": 5.608648300170898,
      "learning_rate": 4.9797842887334166e-05,
      "loss": 4.2434,
      "step": 91700
    },
    {
      "epoch": 0.01619038196073991,
      "grad_norm": 10.425902366638184,
      "learning_rate": 4.979762243006346e-05,
      "loss": 4.2482,
      "step": 91800
    },
    {
      "epoch": 0.01620801854239649,
      "grad_norm": 10.155667304992676,
      "learning_rate": 4.979740197279275e-05,
      "loss": 4.0095,
      "step": 91900
    },
    {
      "epoch": 0.01622565512405307,
      "grad_norm": 16.668737411499023,
      "learning_rate": 4.9797181515522046e-05,
      "loss": 4.1949,
      "step": 92000
    },
    {
      "epoch": 0.01624329170570965,
      "grad_norm": 7.284942626953125,
      "learning_rate": 4.979696105825134e-05,
      "loss": 4.0879,
      "step": 92100
    },
    {
      "epoch": 0.01626092828736623,
      "grad_norm": 7.274195194244385,
      "learning_rate": 4.979674060098063e-05,
      "loss": 4.2422,
      "step": 92200
    },
    {
      "epoch": 0.016278564869022807,
      "grad_norm": 9.159951210021973,
      "learning_rate": 4.9796520143709925e-05,
      "loss": 4.2368,
      "step": 92300
    },
    {
      "epoch": 0.016296201450679386,
      "grad_norm": 6.52421236038208,
      "learning_rate": 4.979629968643922e-05,
      "loss": 4.2115,
      "step": 92400
    },
    {
      "epoch": 0.016313838032335968,
      "grad_norm": 5.42572546005249,
      "learning_rate": 4.979607922916851e-05,
      "loss": 4.1387,
      "step": 92500
    },
    {
      "epoch": 0.016331474613992546,
      "grad_norm": 5.451174736022949,
      "learning_rate": 4.9795858771897805e-05,
      "loss": 4.2252,
      "step": 92600
    },
    {
      "epoch": 0.016349111195649125,
      "grad_norm": 7.761170864105225,
      "learning_rate": 4.9795638314627094e-05,
      "loss": 4.1612,
      "step": 92700
    },
    {
      "epoch": 0.016366747777305707,
      "grad_norm": 7.824829578399658,
      "learning_rate": 4.979541785735638e-05,
      "loss": 4.256,
      "step": 92800
    },
    {
      "epoch": 0.016384384358962285,
      "grad_norm": 5.405402660369873,
      "learning_rate": 4.979519740008568e-05,
      "loss": 3.9802,
      "step": 92900
    },
    {
      "epoch": 0.016402020940618864,
      "grad_norm": 7.099165439605713,
      "learning_rate": 4.979497694281497e-05,
      "loss": 4.1299,
      "step": 93000
    },
    {
      "epoch": 0.016419657522275442,
      "grad_norm": 11.978035926818848,
      "learning_rate": 4.979475648554426e-05,
      "loss": 4.1922,
      "step": 93100
    },
    {
      "epoch": 0.016437294103932024,
      "grad_norm": 5.815700054168701,
      "learning_rate": 4.979453602827356e-05,
      "loss": 4.1454,
      "step": 93200
    },
    {
      "epoch": 0.016454930685588603,
      "grad_norm": 10.1237211227417,
      "learning_rate": 4.979431557100285e-05,
      "loss": 4.1884,
      "step": 93300
    },
    {
      "epoch": 0.01647256726724518,
      "grad_norm": 6.50931453704834,
      "learning_rate": 4.979409511373214e-05,
      "loss": 4.1565,
      "step": 93400
    },
    {
      "epoch": 0.01649020384890176,
      "grad_norm": 5.684497833251953,
      "learning_rate": 4.979387465646144e-05,
      "loss": 4.1472,
      "step": 93500
    },
    {
      "epoch": 0.016507840430558342,
      "grad_norm": 7.750527381896973,
      "learning_rate": 4.979365419919073e-05,
      "loss": 4.2002,
      "step": 93600
    },
    {
      "epoch": 0.01652547701221492,
      "grad_norm": 6.552631855010986,
      "learning_rate": 4.979343374192002e-05,
      "loss": 4.1672,
      "step": 93700
    },
    {
      "epoch": 0.0165431135938715,
      "grad_norm": 7.610915184020996,
      "learning_rate": 4.979321328464932e-05,
      "loss": 4.2042,
      "step": 93800
    },
    {
      "epoch": 0.016560750175528077,
      "grad_norm": 6.71970796585083,
      "learning_rate": 4.979299282737861e-05,
      "loss": 4.1687,
      "step": 93900
    },
    {
      "epoch": 0.01657838675718466,
      "grad_norm": 9.100445747375488,
      "learning_rate": 4.97927723701079e-05,
      "loss": 4.157,
      "step": 94000
    },
    {
      "epoch": 0.016596023338841238,
      "grad_norm": 6.552329063415527,
      "learning_rate": 4.9792551912837196e-05,
      "loss": 4.0461,
      "step": 94100
    },
    {
      "epoch": 0.016613659920497816,
      "grad_norm": 7.872473239898682,
      "learning_rate": 4.9792331455566485e-05,
      "loss": 4.2819,
      "step": 94200
    },
    {
      "epoch": 0.016631296502154398,
      "grad_norm": 6.063905715942383,
      "learning_rate": 4.9792110998295774e-05,
      "loss": 4.246,
      "step": 94300
    },
    {
      "epoch": 0.016648933083810977,
      "grad_norm": 8.680536270141602,
      "learning_rate": 4.979189054102507e-05,
      "loss": 4.1749,
      "step": 94400
    },
    {
      "epoch": 0.016666569665467555,
      "grad_norm": 5.607302188873291,
      "learning_rate": 4.9791670083754365e-05,
      "loss": 4.1759,
      "step": 94500
    },
    {
      "epoch": 0.016684206247124134,
      "grad_norm": 5.724308967590332,
      "learning_rate": 4.979144962648366e-05,
      "loss": 4.2331,
      "step": 94600
    },
    {
      "epoch": 0.016701842828780716,
      "grad_norm": 7.044544696807861,
      "learning_rate": 4.979122916921295e-05,
      "loss": 4.11,
      "step": 94700
    },
    {
      "epoch": 0.016719479410437294,
      "grad_norm": 6.4144158363342285,
      "learning_rate": 4.9791008711942244e-05,
      "loss": 4.3012,
      "step": 94800
    },
    {
      "epoch": 0.016737115992093873,
      "grad_norm": 5.964237689971924,
      "learning_rate": 4.979078825467154e-05,
      "loss": 4.2541,
      "step": 94900
    },
    {
      "epoch": 0.01675475257375045,
      "grad_norm": 6.5616350173950195,
      "learning_rate": 4.979056779740083e-05,
      "loss": 4.1855,
      "step": 95000
    },
    {
      "epoch": 0.016772389155407033,
      "grad_norm": 6.836639881134033,
      "learning_rate": 4.9790347340130124e-05,
      "loss": 4.2322,
      "step": 95100
    },
    {
      "epoch": 0.01679002573706361,
      "grad_norm": 5.757740020751953,
      "learning_rate": 4.979012688285942e-05,
      "loss": 4.1216,
      "step": 95200
    },
    {
      "epoch": 0.01680766231872019,
      "grad_norm": 6.71119499206543,
      "learning_rate": 4.978990642558871e-05,
      "loss": 4.1451,
      "step": 95300
    },
    {
      "epoch": 0.01682529890037677,
      "grad_norm": 7.046156406402588,
      "learning_rate": 4.9789685968318004e-05,
      "loss": 4.2498,
      "step": 95400
    },
    {
      "epoch": 0.01684293548203335,
      "grad_norm": 6.966878890991211,
      "learning_rate": 4.978946551104729e-05,
      "loss": 4.061,
      "step": 95500
    },
    {
      "epoch": 0.01686057206368993,
      "grad_norm": 6.977431297302246,
      "learning_rate": 4.978924505377658e-05,
      "loss": 4.2018,
      "step": 95600
    },
    {
      "epoch": 0.016878208645346508,
      "grad_norm": 7.626182556152344,
      "learning_rate": 4.9789024596505876e-05,
      "loss": 4.2126,
      "step": 95700
    },
    {
      "epoch": 0.01689584522700309,
      "grad_norm": 6.5285515785217285,
      "learning_rate": 4.978880413923517e-05,
      "loss": 4.1432,
      "step": 95800
    },
    {
      "epoch": 0.016913481808659668,
      "grad_norm": 5.38038969039917,
      "learning_rate": 4.978858368196446e-05,
      "loss": 4.0587,
      "step": 95900
    },
    {
      "epoch": 0.016931118390316247,
      "grad_norm": 6.806650161743164,
      "learning_rate": 4.9788363224693756e-05,
      "loss": 4.1487,
      "step": 96000
    },
    {
      "epoch": 0.016948754971972825,
      "grad_norm": 6.156496047973633,
      "learning_rate": 4.978814276742305e-05,
      "loss": 4.1645,
      "step": 96100
    },
    {
      "epoch": 0.016966391553629407,
      "grad_norm": 7.295240879058838,
      "learning_rate": 4.978792231015234e-05,
      "loss": 4.1817,
      "step": 96200
    },
    {
      "epoch": 0.016984028135285986,
      "grad_norm": 14.713037490844727,
      "learning_rate": 4.9787701852881636e-05,
      "loss": 4.1813,
      "step": 96300
    },
    {
      "epoch": 0.017001664716942564,
      "grad_norm": 6.214339256286621,
      "learning_rate": 4.978748139561093e-05,
      "loss": 4.1408,
      "step": 96400
    },
    {
      "epoch": 0.017019301298599143,
      "grad_norm": 8.050678253173828,
      "learning_rate": 4.978726093834022e-05,
      "loss": 4.0946,
      "step": 96500
    },
    {
      "epoch": 0.017036937880255724,
      "grad_norm": 8.249312400817871,
      "learning_rate": 4.9787040481069515e-05,
      "loss": 4.1344,
      "step": 96600
    },
    {
      "epoch": 0.017054574461912303,
      "grad_norm": 6.616001129150391,
      "learning_rate": 4.978682002379881e-05,
      "loss": 4.1083,
      "step": 96700
    },
    {
      "epoch": 0.01707221104356888,
      "grad_norm": 5.71678352355957,
      "learning_rate": 4.97865995665281e-05,
      "loss": 4.2112,
      "step": 96800
    },
    {
      "epoch": 0.01708984762522546,
      "grad_norm": 11.877423286437988,
      "learning_rate": 4.9786379109257395e-05,
      "loss": 4.1757,
      "step": 96900
    },
    {
      "epoch": 0.017107484206882042,
      "grad_norm": 6.692354679107666,
      "learning_rate": 4.9786158651986684e-05,
      "loss": 4.1174,
      "step": 97000
    },
    {
      "epoch": 0.01712512078853862,
      "grad_norm": 6.058107852935791,
      "learning_rate": 4.978593819471597e-05,
      "loss": 4.1546,
      "step": 97100
    },
    {
      "epoch": 0.0171427573701952,
      "grad_norm": 6.800384521484375,
      "learning_rate": 4.978571773744527e-05,
      "loss": 4.1789,
      "step": 97200
    },
    {
      "epoch": 0.01716039395185178,
      "grad_norm": 5.286583423614502,
      "learning_rate": 4.978549728017456e-05,
      "loss": 4.1238,
      "step": 97300
    },
    {
      "epoch": 0.01717803053350836,
      "grad_norm": 7.2722673416137695,
      "learning_rate": 4.978527682290385e-05,
      "loss": 4.2369,
      "step": 97400
    },
    {
      "epoch": 0.017195667115164938,
      "grad_norm": 6.561347007751465,
      "learning_rate": 4.978505636563315e-05,
      "loss": 4.0442,
      "step": 97500
    },
    {
      "epoch": 0.017213303696821516,
      "grad_norm": 6.105751037597656,
      "learning_rate": 4.978483590836244e-05,
      "loss": 4.2347,
      "step": 97600
    },
    {
      "epoch": 0.0172309402784781,
      "grad_norm": 7.357702732086182,
      "learning_rate": 4.978461545109173e-05,
      "loss": 4.1509,
      "step": 97700
    },
    {
      "epoch": 0.017248576860134677,
      "grad_norm": 5.92031192779541,
      "learning_rate": 4.978439499382103e-05,
      "loss": 4.1176,
      "step": 97800
    },
    {
      "epoch": 0.017266213441791255,
      "grad_norm": 7.302209854125977,
      "learning_rate": 4.978417453655032e-05,
      "loss": 4.1083,
      "step": 97900
    },
    {
      "epoch": 0.017283850023447834,
      "grad_norm": 8.795595169067383,
      "learning_rate": 4.978395407927961e-05,
      "loss": 4.1439,
      "step": 98000
    },
    {
      "epoch": 0.017301486605104416,
      "grad_norm": 8.187496185302734,
      "learning_rate": 4.978373362200891e-05,
      "loss": 4.1716,
      "step": 98100
    },
    {
      "epoch": 0.017319123186760994,
      "grad_norm": 4.25250768661499,
      "learning_rate": 4.97835131647382e-05,
      "loss": 4.2452,
      "step": 98200
    },
    {
      "epoch": 0.017336759768417573,
      "grad_norm": 6.988112449645996,
      "learning_rate": 4.978329270746749e-05,
      "loss": 4.0845,
      "step": 98300
    },
    {
      "epoch": 0.01735439635007415,
      "grad_norm": 6.913187503814697,
      "learning_rate": 4.978307225019678e-05,
      "loss": 4.1582,
      "step": 98400
    },
    {
      "epoch": 0.017372032931730733,
      "grad_norm": 8.928215980529785,
      "learning_rate": 4.9782851792926075e-05,
      "loss": 4.1589,
      "step": 98500
    },
    {
      "epoch": 0.017389669513387312,
      "grad_norm": 5.445764541625977,
      "learning_rate": 4.9782631335655364e-05,
      "loss": 4.1518,
      "step": 98600
    },
    {
      "epoch": 0.01740730609504389,
      "grad_norm": 6.468540191650391,
      "learning_rate": 4.978241087838466e-05,
      "loss": 4.1511,
      "step": 98700
    },
    {
      "epoch": 0.017424942676700472,
      "grad_norm": 6.225820541381836,
      "learning_rate": 4.9782190421113955e-05,
      "loss": 4.1526,
      "step": 98800
    },
    {
      "epoch": 0.01744257925835705,
      "grad_norm": 6.428521156311035,
      "learning_rate": 4.978196996384324e-05,
      "loss": 4.1407,
      "step": 98900
    },
    {
      "epoch": 0.01746021584001363,
      "grad_norm": 6.823538780212402,
      "learning_rate": 4.978174950657254e-05,
      "loss": 4.1047,
      "step": 99000
    },
    {
      "epoch": 0.017477852421670208,
      "grad_norm": 5.129122734069824,
      "learning_rate": 4.9781529049301834e-05,
      "loss": 4.2188,
      "step": 99100
    },
    {
      "epoch": 0.01749548900332679,
      "grad_norm": 6.054465293884277,
      "learning_rate": 4.978130859203112e-05,
      "loss": 4.1883,
      "step": 99200
    },
    {
      "epoch": 0.017513125584983368,
      "grad_norm": 4.701778888702393,
      "learning_rate": 4.978108813476042e-05,
      "loss": 4.1526,
      "step": 99300
    },
    {
      "epoch": 0.017530762166639947,
      "grad_norm": 6.308530330657959,
      "learning_rate": 4.9780867677489714e-05,
      "loss": 4.149,
      "step": 99400
    },
    {
      "epoch": 0.017548398748296525,
      "grad_norm": 7.307627201080322,
      "learning_rate": 4.9780647220219e-05,
      "loss": 4.1051,
      "step": 99500
    },
    {
      "epoch": 0.017566035329953107,
      "grad_norm": 5.321896553039551,
      "learning_rate": 4.97804267629483e-05,
      "loss": 4.1769,
      "step": 99600
    },
    {
      "epoch": 0.017583671911609686,
      "grad_norm": 12.977255821228027,
      "learning_rate": 4.9780206305677594e-05,
      "loss": 4.159,
      "step": 99700
    },
    {
      "epoch": 0.017601308493266264,
      "grad_norm": 6.886948585510254,
      "learning_rate": 4.977998584840688e-05,
      "loss": 4.274,
      "step": 99800
    },
    {
      "epoch": 0.017618945074922843,
      "grad_norm": 6.3806538581848145,
      "learning_rate": 4.977976539113617e-05,
      "loss": 4.1857,
      "step": 99900
    },
    {
      "epoch": 0.017636581656579425,
      "grad_norm": 6.032115459442139,
      "learning_rate": 4.9779544933865466e-05,
      "loss": 4.1832,
      "step": 100000
    },
    {
      "epoch": 0.017654218238236003,
      "grad_norm": 6.627918720245361,
      "learning_rate": 4.9779324476594755e-05,
      "loss": 4.1752,
      "step": 100100
    },
    {
      "epoch": 0.01767185481989258,
      "grad_norm": 8.603306770324707,
      "learning_rate": 4.977910401932405e-05,
      "loss": 4.1658,
      "step": 100200
    },
    {
      "epoch": 0.017689491401549164,
      "grad_norm": 8.629486083984375,
      "learning_rate": 4.9778883562053346e-05,
      "loss": 4.1357,
      "step": 100300
    },
    {
      "epoch": 0.017707127983205742,
      "grad_norm": 7.935814380645752,
      "learning_rate": 4.9778663104782635e-05,
      "loss": 4.201,
      "step": 100400
    },
    {
      "epoch": 0.01772476456486232,
      "grad_norm": 6.7911200523376465,
      "learning_rate": 4.977844264751193e-05,
      "loss": 4.0909,
      "step": 100500
    },
    {
      "epoch": 0.0177424011465189,
      "grad_norm": 6.399227619171143,
      "learning_rate": 4.9778222190241226e-05,
      "loss": 4.1197,
      "step": 100600
    },
    {
      "epoch": 0.01776003772817548,
      "grad_norm": 5.976515293121338,
      "learning_rate": 4.9778001732970514e-05,
      "loss": 4.2001,
      "step": 100700
    },
    {
      "epoch": 0.01777767430983206,
      "grad_norm": 6.254382610321045,
      "learning_rate": 4.977778127569981e-05,
      "loss": 4.1405,
      "step": 100800
    },
    {
      "epoch": 0.017795310891488638,
      "grad_norm": 5.154393196105957,
      "learning_rate": 4.9777560818429105e-05,
      "loss": 4.1148,
      "step": 100900
    },
    {
      "epoch": 0.017812947473145217,
      "grad_norm": 6.760119438171387,
      "learning_rate": 4.9777340361158394e-05,
      "loss": 4.1634,
      "step": 101000
    },
    {
      "epoch": 0.0178305840548018,
      "grad_norm": 5.522965431213379,
      "learning_rate": 4.977711990388769e-05,
      "loss": 4.051,
      "step": 101100
    },
    {
      "epoch": 0.017848220636458377,
      "grad_norm": 8.71080493927002,
      "learning_rate": 4.977689944661698e-05,
      "loss": 4.1033,
      "step": 101200
    },
    {
      "epoch": 0.017865857218114956,
      "grad_norm": 6.594156742095947,
      "learning_rate": 4.9776678989346274e-05,
      "loss": 4.1261,
      "step": 101300
    },
    {
      "epoch": 0.017883493799771534,
      "grad_norm": 6.4377641677856445,
      "learning_rate": 4.977645853207556e-05,
      "loss": 4.1303,
      "step": 101400
    },
    {
      "epoch": 0.017901130381428116,
      "grad_norm": 9.478860855102539,
      "learning_rate": 4.977623807480486e-05,
      "loss": 4.0529,
      "step": 101500
    },
    {
      "epoch": 0.017918766963084694,
      "grad_norm": 6.285197734832764,
      "learning_rate": 4.9776017617534146e-05,
      "loss": 4.1261,
      "step": 101600
    },
    {
      "epoch": 0.017936403544741273,
      "grad_norm": 9.271807670593262,
      "learning_rate": 4.977579716026344e-05,
      "loss": 4.0827,
      "step": 101700
    },
    {
      "epoch": 0.017954040126397855,
      "grad_norm": 6.492455005645752,
      "learning_rate": 4.977557670299274e-05,
      "loss": 4.1153,
      "step": 101800
    },
    {
      "epoch": 0.017971676708054433,
      "grad_norm": 6.659708023071289,
      "learning_rate": 4.9775356245722026e-05,
      "loss": 4.1926,
      "step": 101900
    },
    {
      "epoch": 0.017989313289711012,
      "grad_norm": 8.258461952209473,
      "learning_rate": 4.977513578845132e-05,
      "loss": 4.0486,
      "step": 102000
    },
    {
      "epoch": 0.01800694987136759,
      "grad_norm": 6.904146671295166,
      "learning_rate": 4.977491533118062e-05,
      "loss": 4.1729,
      "step": 102100
    },
    {
      "epoch": 0.018024586453024172,
      "grad_norm": 8.149131774902344,
      "learning_rate": 4.9774694873909906e-05,
      "loss": 4.0436,
      "step": 102200
    },
    {
      "epoch": 0.01804222303468075,
      "grad_norm": 6.256854057312012,
      "learning_rate": 4.97744744166392e-05,
      "loss": 4.094,
      "step": 102300
    },
    {
      "epoch": 0.01805985961633733,
      "grad_norm": 6.1555585861206055,
      "learning_rate": 4.9774253959368497e-05,
      "loss": 4.2382,
      "step": 102400
    },
    {
      "epoch": 0.018077496197993908,
      "grad_norm": 6.563257694244385,
      "learning_rate": 4.9774033502097785e-05,
      "loss": 4.1643,
      "step": 102500
    },
    {
      "epoch": 0.01809513277965049,
      "grad_norm": 6.787959575653076,
      "learning_rate": 4.977381304482708e-05,
      "loss": 4.0627,
      "step": 102600
    },
    {
      "epoch": 0.01811276936130707,
      "grad_norm": 5.937346458435059,
      "learning_rate": 4.977359258755637e-05,
      "loss": 4.1221,
      "step": 102700
    },
    {
      "epoch": 0.018130405942963647,
      "grad_norm": 7.273428440093994,
      "learning_rate": 4.977337213028566e-05,
      "loss": 4.1276,
      "step": 102800
    },
    {
      "epoch": 0.018148042524620225,
      "grad_norm": 6.451190948486328,
      "learning_rate": 4.9773151673014954e-05,
      "loss": 4.1476,
      "step": 102900
    },
    {
      "epoch": 0.018165679106276807,
      "grad_norm": 9.119743347167969,
      "learning_rate": 4.977293121574425e-05,
      "loss": 4.0848,
      "step": 103000
    },
    {
      "epoch": 0.018183315687933386,
      "grad_norm": 4.034359455108643,
      "learning_rate": 4.977271075847354e-05,
      "loss": 4.0338,
      "step": 103100
    },
    {
      "epoch": 0.018200952269589964,
      "grad_norm": 12.117490768432617,
      "learning_rate": 4.977249030120283e-05,
      "loss": 4.1787,
      "step": 103200
    },
    {
      "epoch": 0.018218588851246546,
      "grad_norm": 7.421409606933594,
      "learning_rate": 4.977226984393213e-05,
      "loss": 4.1168,
      "step": 103300
    },
    {
      "epoch": 0.018236225432903125,
      "grad_norm": 12.635032653808594,
      "learning_rate": 4.977204938666142e-05,
      "loss": 4.1924,
      "step": 103400
    },
    {
      "epoch": 0.018253862014559703,
      "grad_norm": 7.688989639282227,
      "learning_rate": 4.977182892939071e-05,
      "loss": 4.2122,
      "step": 103500
    },
    {
      "epoch": 0.018271498596216282,
      "grad_norm": 5.575625896453857,
      "learning_rate": 4.977160847212001e-05,
      "loss": 4.1635,
      "step": 103600
    },
    {
      "epoch": 0.018289135177872864,
      "grad_norm": 6.109974384307861,
      "learning_rate": 4.97713880148493e-05,
      "loss": 4.1151,
      "step": 103700
    },
    {
      "epoch": 0.018306771759529442,
      "grad_norm": 8.11794662475586,
      "learning_rate": 4.977116755757859e-05,
      "loss": 4.0832,
      "step": 103800
    },
    {
      "epoch": 0.01832440834118602,
      "grad_norm": 7.918439865112305,
      "learning_rate": 4.977094710030789e-05,
      "loss": 4.2317,
      "step": 103900
    },
    {
      "epoch": 0.0183420449228426,
      "grad_norm": 7.149223327636719,
      "learning_rate": 4.977072664303718e-05,
      "loss": 4.1244,
      "step": 104000
    },
    {
      "epoch": 0.01835968150449918,
      "grad_norm": 10.415358543395996,
      "learning_rate": 4.977050618576647e-05,
      "loss": 4.1156,
      "step": 104100
    },
    {
      "epoch": 0.01837731808615576,
      "grad_norm": 8.224957466125488,
      "learning_rate": 4.977028572849576e-05,
      "loss": 4.1394,
      "step": 104200
    },
    {
      "epoch": 0.018394954667812338,
      "grad_norm": 8.390499114990234,
      "learning_rate": 4.977006527122505e-05,
      "loss": 4.2527,
      "step": 104300
    },
    {
      "epoch": 0.018412591249468917,
      "grad_norm": 6.229876518249512,
      "learning_rate": 4.9769844813954345e-05,
      "loss": 4.2849,
      "step": 104400
    },
    {
      "epoch": 0.0184302278311255,
      "grad_norm": 6.090096950531006,
      "learning_rate": 4.976962435668364e-05,
      "loss": 4.0731,
      "step": 104500
    },
    {
      "epoch": 0.018447864412782077,
      "grad_norm": 7.112229347229004,
      "learning_rate": 4.976940389941293e-05,
      "loss": 4.178,
      "step": 104600
    },
    {
      "epoch": 0.018465500994438656,
      "grad_norm": 8.756340980529785,
      "learning_rate": 4.9769183442142225e-05,
      "loss": 4.2016,
      "step": 104700
    },
    {
      "epoch": 0.018483137576095238,
      "grad_norm": 8.605826377868652,
      "learning_rate": 4.976896298487152e-05,
      "loss": 4.1052,
      "step": 104800
    },
    {
      "epoch": 0.018500774157751816,
      "grad_norm": 7.217924118041992,
      "learning_rate": 4.976874252760081e-05,
      "loss": 4.0508,
      "step": 104900
    },
    {
      "epoch": 0.018518410739408395,
      "grad_norm": 10.62226390838623,
      "learning_rate": 4.9768522070330104e-05,
      "loss": 4.0569,
      "step": 105000
    },
    {
      "epoch": 0.018536047321064973,
      "grad_norm": 6.1050333976745605,
      "learning_rate": 4.97683016130594e-05,
      "loss": 4.1329,
      "step": 105100
    },
    {
      "epoch": 0.018553683902721555,
      "grad_norm": 8.836712837219238,
      "learning_rate": 4.9768081155788695e-05,
      "loss": 4.2196,
      "step": 105200
    },
    {
      "epoch": 0.018571320484378134,
      "grad_norm": 9.66949462890625,
      "learning_rate": 4.9767860698517984e-05,
      "loss": 4.0622,
      "step": 105300
    },
    {
      "epoch": 0.018588957066034712,
      "grad_norm": 9.106865882873535,
      "learning_rate": 4.976764024124728e-05,
      "loss": 4.0994,
      "step": 105400
    },
    {
      "epoch": 0.01860659364769129,
      "grad_norm": 5.185985088348389,
      "learning_rate": 4.976741978397657e-05,
      "loss": 4.1599,
      "step": 105500
    },
    {
      "epoch": 0.018624230229347873,
      "grad_norm": 4.933216571807861,
      "learning_rate": 4.976719932670586e-05,
      "loss": 4.1396,
      "step": 105600
    },
    {
      "epoch": 0.01864186681100445,
      "grad_norm": 6.453482151031494,
      "learning_rate": 4.976697886943515e-05,
      "loss": 4.1886,
      "step": 105700
    },
    {
      "epoch": 0.01865950339266103,
      "grad_norm": 6.455549240112305,
      "learning_rate": 4.976675841216445e-05,
      "loss": 4.117,
      "step": 105800
    },
    {
      "epoch": 0.018677139974317608,
      "grad_norm": 6.289504528045654,
      "learning_rate": 4.9766537954893736e-05,
      "loss": 4.1439,
      "step": 105900
    },
    {
      "epoch": 0.01869477655597419,
      "grad_norm": 9.025287628173828,
      "learning_rate": 4.976631749762303e-05,
      "loss": 4.0112,
      "step": 106000
    },
    {
      "epoch": 0.01871241313763077,
      "grad_norm": 5.598341464996338,
      "learning_rate": 4.976609704035233e-05,
      "loss": 4.0982,
      "step": 106100
    },
    {
      "epoch": 0.018730049719287347,
      "grad_norm": 6.871098518371582,
      "learning_rate": 4.9765876583081616e-05,
      "loss": 4.1319,
      "step": 106200
    },
    {
      "epoch": 0.01874768630094393,
      "grad_norm": 7.074423789978027,
      "learning_rate": 4.976565612581091e-05,
      "loss": 4.0567,
      "step": 106300
    },
    {
      "epoch": 0.018765322882600508,
      "grad_norm": 7.738760471343994,
      "learning_rate": 4.976543566854021e-05,
      "loss": 4.1289,
      "step": 106400
    },
    {
      "epoch": 0.018782959464257086,
      "grad_norm": 9.192200660705566,
      "learning_rate": 4.9765215211269496e-05,
      "loss": 4.1804,
      "step": 106500
    },
    {
      "epoch": 0.018800596045913665,
      "grad_norm": 9.569090843200684,
      "learning_rate": 4.976499475399879e-05,
      "loss": 4.1055,
      "step": 106600
    },
    {
      "epoch": 0.018818232627570246,
      "grad_norm": 8.109313011169434,
      "learning_rate": 4.9764774296728087e-05,
      "loss": 4.0961,
      "step": 106700
    },
    {
      "epoch": 0.018835869209226825,
      "grad_norm": 5.280881404876709,
      "learning_rate": 4.9764553839457375e-05,
      "loss": 4.0538,
      "step": 106800
    },
    {
      "epoch": 0.018853505790883403,
      "grad_norm": 6.988755702972412,
      "learning_rate": 4.976433338218667e-05,
      "loss": 4.145,
      "step": 106900
    },
    {
      "epoch": 0.018871142372539982,
      "grad_norm": 7.434787750244141,
      "learning_rate": 4.976411292491596e-05,
      "loss": 4.1478,
      "step": 107000
    },
    {
      "epoch": 0.018888778954196564,
      "grad_norm": 7.56599235534668,
      "learning_rate": 4.976389246764525e-05,
      "loss": 4.115,
      "step": 107100
    },
    {
      "epoch": 0.018906415535853142,
      "grad_norm": 10.651004791259766,
      "learning_rate": 4.9763672010374544e-05,
      "loss": 4.1639,
      "step": 107200
    },
    {
      "epoch": 0.01892405211750972,
      "grad_norm": 9.6407470703125,
      "learning_rate": 4.976345155310384e-05,
      "loss": 4.0939,
      "step": 107300
    },
    {
      "epoch": 0.0189416886991663,
      "grad_norm": 9.160717964172363,
      "learning_rate": 4.976323109583313e-05,
      "loss": 4.0598,
      "step": 107400
    },
    {
      "epoch": 0.01895932528082288,
      "grad_norm": 6.585011959075928,
      "learning_rate": 4.976301063856242e-05,
      "loss": 4.0328,
      "step": 107500
    },
    {
      "epoch": 0.01897696186247946,
      "grad_norm": 6.77360200881958,
      "learning_rate": 4.976279018129172e-05,
      "loss": 4.1308,
      "step": 107600
    },
    {
      "epoch": 0.01899459844413604,
      "grad_norm": 7.668215274810791,
      "learning_rate": 4.976256972402101e-05,
      "loss": 4.0941,
      "step": 107700
    },
    {
      "epoch": 0.01901223502579262,
      "grad_norm": 7.703432083129883,
      "learning_rate": 4.97623492667503e-05,
      "loss": 4.0725,
      "step": 107800
    },
    {
      "epoch": 0.0190298716074492,
      "grad_norm": 6.299343585968018,
      "learning_rate": 4.97621288094796e-05,
      "loss": 4.0481,
      "step": 107900
    },
    {
      "epoch": 0.019047508189105777,
      "grad_norm": 7.925664901733398,
      "learning_rate": 4.976190835220889e-05,
      "loss": 4.0305,
      "step": 108000
    },
    {
      "epoch": 0.019065144770762356,
      "grad_norm": 7.62339448928833,
      "learning_rate": 4.976168789493818e-05,
      "loss": 4.0285,
      "step": 108100
    },
    {
      "epoch": 0.019082781352418938,
      "grad_norm": 4.996388912200928,
      "learning_rate": 4.976146743766748e-05,
      "loss": 4.1251,
      "step": 108200
    },
    {
      "epoch": 0.019100417934075516,
      "grad_norm": 12.043789863586426,
      "learning_rate": 4.976124698039677e-05,
      "loss": 4.0837,
      "step": 108300
    },
    {
      "epoch": 0.019118054515732095,
      "grad_norm": 9.417216300964355,
      "learning_rate": 4.9761026523126055e-05,
      "loss": 4.1416,
      "step": 108400
    },
    {
      "epoch": 0.019135691097388673,
      "grad_norm": 6.997842788696289,
      "learning_rate": 4.976080606585535e-05,
      "loss": 4.113,
      "step": 108500
    },
    {
      "epoch": 0.019153327679045255,
      "grad_norm": 8.113518714904785,
      "learning_rate": 4.976058560858464e-05,
      "loss": 4.0198,
      "step": 108600
    },
    {
      "epoch": 0.019170964260701834,
      "grad_norm": 7.859006881713867,
      "learning_rate": 4.9760365151313935e-05,
      "loss": 4.0975,
      "step": 108700
    },
    {
      "epoch": 0.019188600842358412,
      "grad_norm": 6.6329450607299805,
      "learning_rate": 4.976014469404323e-05,
      "loss": 4.1461,
      "step": 108800
    },
    {
      "epoch": 0.01920623742401499,
      "grad_norm": 7.049435138702393,
      "learning_rate": 4.975992423677252e-05,
      "loss": 4.1514,
      "step": 108900
    },
    {
      "epoch": 0.019223874005671573,
      "grad_norm": 8.244935035705566,
      "learning_rate": 4.9759703779501815e-05,
      "loss": 4.1242,
      "step": 109000
    },
    {
      "epoch": 0.01924151058732815,
      "grad_norm": 6.472760200500488,
      "learning_rate": 4.975948332223111e-05,
      "loss": 4.1282,
      "step": 109100
    },
    {
      "epoch": 0.01925914716898473,
      "grad_norm": 8.199501991271973,
      "learning_rate": 4.97592628649604e-05,
      "loss": 4.1144,
      "step": 109200
    },
    {
      "epoch": 0.01927678375064131,
      "grad_norm": 7.8987202644348145,
      "learning_rate": 4.9759042407689694e-05,
      "loss": 4.1819,
      "step": 109300
    },
    {
      "epoch": 0.01929442033229789,
      "grad_norm": 6.294332027435303,
      "learning_rate": 4.975882195041899e-05,
      "loss": 4.1797,
      "step": 109400
    },
    {
      "epoch": 0.01931205691395447,
      "grad_norm": 8.19675350189209,
      "learning_rate": 4.975860149314828e-05,
      "loss": 4.2106,
      "step": 109500
    },
    {
      "epoch": 0.019329693495611047,
      "grad_norm": 5.6053361892700195,
      "learning_rate": 4.9758381035877574e-05,
      "loss": 4.1223,
      "step": 109600
    },
    {
      "epoch": 0.01934733007726763,
      "grad_norm": 7.730648517608643,
      "learning_rate": 4.975816057860687e-05,
      "loss": 4.0345,
      "step": 109700
    },
    {
      "epoch": 0.019364966658924208,
      "grad_norm": 6.8173909187316895,
      "learning_rate": 4.975794012133616e-05,
      "loss": 4.094,
      "step": 109800
    },
    {
      "epoch": 0.019382603240580786,
      "grad_norm": 6.394141674041748,
      "learning_rate": 4.975771966406545e-05,
      "loss": 4.2219,
      "step": 109900
    },
    {
      "epoch": 0.019400239822237365,
      "grad_norm": 6.999342441558838,
      "learning_rate": 4.975749920679474e-05,
      "loss": 4.1602,
      "step": 110000
    },
    {
      "epoch": 0.019417876403893947,
      "grad_norm": 9.587745666503906,
      "learning_rate": 4.975727874952403e-05,
      "loss": 4.1508,
      "step": 110100
    },
    {
      "epoch": 0.019435512985550525,
      "grad_norm": 9.984968185424805,
      "learning_rate": 4.9757058292253326e-05,
      "loss": 4.1587,
      "step": 110200
    },
    {
      "epoch": 0.019453149567207104,
      "grad_norm": 6.831272125244141,
      "learning_rate": 4.975683783498262e-05,
      "loss": 4.16,
      "step": 110300
    },
    {
      "epoch": 0.019470786148863682,
      "grad_norm": 6.62962532043457,
      "learning_rate": 4.975661737771191e-05,
      "loss": 4.1009,
      "step": 110400
    },
    {
      "epoch": 0.019488422730520264,
      "grad_norm": 6.831207275390625,
      "learning_rate": 4.9756396920441206e-05,
      "loss": 4.1194,
      "step": 110500
    },
    {
      "epoch": 0.019506059312176843,
      "grad_norm": 7.51404333114624,
      "learning_rate": 4.97561764631705e-05,
      "loss": 4.1295,
      "step": 110600
    },
    {
      "epoch": 0.01952369589383342,
      "grad_norm": 6.520881175994873,
      "learning_rate": 4.975595600589979e-05,
      "loss": 4.1623,
      "step": 110700
    },
    {
      "epoch": 0.019541332475490003,
      "grad_norm": 7.265395641326904,
      "learning_rate": 4.9755735548629086e-05,
      "loss": 4.2276,
      "step": 110800
    },
    {
      "epoch": 0.01955896905714658,
      "grad_norm": 6.3783698081970215,
      "learning_rate": 4.975551509135838e-05,
      "loss": 4.208,
      "step": 110900
    },
    {
      "epoch": 0.01957660563880316,
      "grad_norm": 10.54831600189209,
      "learning_rate": 4.975529463408767e-05,
      "loss": 4.1516,
      "step": 111000
    },
    {
      "epoch": 0.01959424222045974,
      "grad_norm": 9.194750785827637,
      "learning_rate": 4.9755074176816965e-05,
      "loss": 4.0826,
      "step": 111100
    },
    {
      "epoch": 0.01961187880211632,
      "grad_norm": 4.762484550476074,
      "learning_rate": 4.9754853719546254e-05,
      "loss": 4.1256,
      "step": 111200
    },
    {
      "epoch": 0.0196295153837729,
      "grad_norm": 6.235752582550049,
      "learning_rate": 4.975463326227555e-05,
      "loss": 4.1838,
      "step": 111300
    },
    {
      "epoch": 0.019647151965429478,
      "grad_norm": 5.871755123138428,
      "learning_rate": 4.975441280500484e-05,
      "loss": 4.1288,
      "step": 111400
    },
    {
      "epoch": 0.019664788547086056,
      "grad_norm": 7.012057781219482,
      "learning_rate": 4.9754192347734134e-05,
      "loss": 4.0906,
      "step": 111500
    },
    {
      "epoch": 0.019682425128742638,
      "grad_norm": 6.4419355392456055,
      "learning_rate": 4.975397189046342e-05,
      "loss": 4.1849,
      "step": 111600
    },
    {
      "epoch": 0.019700061710399216,
      "grad_norm": 6.075728893280029,
      "learning_rate": 4.975375143319272e-05,
      "loss": 4.1906,
      "step": 111700
    },
    {
      "epoch": 0.019717698292055795,
      "grad_norm": 5.825372219085693,
      "learning_rate": 4.975353097592201e-05,
      "loss": 4.1495,
      "step": 111800
    },
    {
      "epoch": 0.019735334873712373,
      "grad_norm": 8.325251579284668,
      "learning_rate": 4.97533105186513e-05,
      "loss": 4.1455,
      "step": 111900
    },
    {
      "epoch": 0.019752971455368955,
      "grad_norm": 6.5256733894348145,
      "learning_rate": 4.97530900613806e-05,
      "loss": 4.0032,
      "step": 112000
    },
    {
      "epoch": 0.019770608037025534,
      "grad_norm": 6.412446022033691,
      "learning_rate": 4.975286960410989e-05,
      "loss": 4.0661,
      "step": 112100
    },
    {
      "epoch": 0.019788244618682112,
      "grad_norm": 8.487954139709473,
      "learning_rate": 4.975264914683918e-05,
      "loss": 4.1597,
      "step": 112200
    },
    {
      "epoch": 0.019805881200338694,
      "grad_norm": 6.138504981994629,
      "learning_rate": 4.975242868956848e-05,
      "loss": 4.2708,
      "step": 112300
    },
    {
      "epoch": 0.019823517781995273,
      "grad_norm": 5.61401891708374,
      "learning_rate": 4.975220823229777e-05,
      "loss": 4.1666,
      "step": 112400
    },
    {
      "epoch": 0.01984115436365185,
      "grad_norm": 8.914305686950684,
      "learning_rate": 4.975198777502706e-05,
      "loss": 4.0381,
      "step": 112500
    },
    {
      "epoch": 0.01985879094530843,
      "grad_norm": 5.864469528198242,
      "learning_rate": 4.9751767317756357e-05,
      "loss": 4.0723,
      "step": 112600
    },
    {
      "epoch": 0.019876427526965012,
      "grad_norm": 6.499421119689941,
      "learning_rate": 4.9751546860485645e-05,
      "loss": 4.0139,
      "step": 112700
    },
    {
      "epoch": 0.01989406410862159,
      "grad_norm": 7.9614763259887695,
      "learning_rate": 4.9751326403214934e-05,
      "loss": 4.0592,
      "step": 112800
    },
    {
      "epoch": 0.01991170069027817,
      "grad_norm": 5.706823825836182,
      "learning_rate": 4.975110594594423e-05,
      "loss": 3.9752,
      "step": 112900
    },
    {
      "epoch": 0.019929337271934747,
      "grad_norm": 8.083373069763184,
      "learning_rate": 4.9750885488673525e-05,
      "loss": 4.1313,
      "step": 113000
    },
    {
      "epoch": 0.01994697385359133,
      "grad_norm": 7.112678527832031,
      "learning_rate": 4.9750665031402814e-05,
      "loss": 4.1084,
      "step": 113100
    },
    {
      "epoch": 0.019964610435247908,
      "grad_norm": 4.721975803375244,
      "learning_rate": 4.975044457413211e-05,
      "loss": 4.0334,
      "step": 113200
    },
    {
      "epoch": 0.019982247016904486,
      "grad_norm": 6.318990230560303,
      "learning_rate": 4.9750224116861405e-05,
      "loss": 4.1254,
      "step": 113300
    },
    {
      "epoch": 0.019999883598561065,
      "grad_norm": 13.527190208435059,
      "learning_rate": 4.975000365959069e-05,
      "loss": 4.0932,
      "step": 113400
    },
    {
      "epoch": 0.020017520180217647,
      "grad_norm": 8.283498764038086,
      "learning_rate": 4.974978320231999e-05,
      "loss": 4.093,
      "step": 113500
    },
    {
      "epoch": 0.020035156761874225,
      "grad_norm": 6.677966594696045,
      "learning_rate": 4.9749562745049284e-05,
      "loss": 4.0466,
      "step": 113600
    },
    {
      "epoch": 0.020052793343530804,
      "grad_norm": 6.807382583618164,
      "learning_rate": 4.974934228777857e-05,
      "loss": 4.1406,
      "step": 113700
    },
    {
      "epoch": 0.020070429925187386,
      "grad_norm": 7.5961689949035645,
      "learning_rate": 4.974912183050787e-05,
      "loss": 4.1029,
      "step": 113800
    },
    {
      "epoch": 0.020088066506843964,
      "grad_norm": 6.753057956695557,
      "learning_rate": 4.9748901373237164e-05,
      "loss": 4.1005,
      "step": 113900
    },
    {
      "epoch": 0.020105703088500543,
      "grad_norm": 6.5031304359436035,
      "learning_rate": 4.974868091596645e-05,
      "loss": 3.9565,
      "step": 114000
    },
    {
      "epoch": 0.02012333967015712,
      "grad_norm": 6.989290714263916,
      "learning_rate": 4.974846045869575e-05,
      "loss": 4.1623,
      "step": 114100
    },
    {
      "epoch": 0.020140976251813703,
      "grad_norm": 8.060647010803223,
      "learning_rate": 4.974824000142504e-05,
      "loss": 4.1468,
      "step": 114200
    },
    {
      "epoch": 0.020158612833470282,
      "grad_norm": 8.171664237976074,
      "learning_rate": 4.9748019544154325e-05,
      "loss": 4.0699,
      "step": 114300
    },
    {
      "epoch": 0.02017624941512686,
      "grad_norm": 7.526660442352295,
      "learning_rate": 4.974779908688362e-05,
      "loss": 4.0532,
      "step": 114400
    },
    {
      "epoch": 0.02019388599678344,
      "grad_norm": 6.869924545288086,
      "learning_rate": 4.9747578629612916e-05,
      "loss": 4.1273,
      "step": 114500
    },
    {
      "epoch": 0.02021152257844002,
      "grad_norm": 12.311529159545898,
      "learning_rate": 4.9747358172342205e-05,
      "loss": 4.0796,
      "step": 114600
    },
    {
      "epoch": 0.0202291591600966,
      "grad_norm": 6.121920585632324,
      "learning_rate": 4.97471377150715e-05,
      "loss": 4.139,
      "step": 114700
    },
    {
      "epoch": 0.020246795741753178,
      "grad_norm": 5.468155860900879,
      "learning_rate": 4.9746917257800796e-05,
      "loss": 4.0725,
      "step": 114800
    },
    {
      "epoch": 0.020264432323409756,
      "grad_norm": 7.624846935272217,
      "learning_rate": 4.9746696800530085e-05,
      "loss": 4.0531,
      "step": 114900
    },
    {
      "epoch": 0.020282068905066338,
      "grad_norm": 8.382224082946777,
      "learning_rate": 4.974647634325938e-05,
      "loss": 4.007,
      "step": 115000
    },
    {
      "epoch": 0.020299705486722917,
      "grad_norm": 8.113442420959473,
      "learning_rate": 4.9746255885988676e-05,
      "loss": 4.1792,
      "step": 115100
    },
    {
      "epoch": 0.020317342068379495,
      "grad_norm": 11.66505241394043,
      "learning_rate": 4.9746035428717964e-05,
      "loss": 4.1591,
      "step": 115200
    },
    {
      "epoch": 0.020334978650036077,
      "grad_norm": 8.638320922851562,
      "learning_rate": 4.974581497144726e-05,
      "loss": 4.0318,
      "step": 115300
    },
    {
      "epoch": 0.020352615231692656,
      "grad_norm": 6.272963047027588,
      "learning_rate": 4.9745594514176555e-05,
      "loss": 4.1292,
      "step": 115400
    },
    {
      "epoch": 0.020370251813349234,
      "grad_norm": 6.041863441467285,
      "learning_rate": 4.9745374056905844e-05,
      "loss": 4.0803,
      "step": 115500
    },
    {
      "epoch": 0.020387888395005813,
      "grad_norm": 8.397299766540527,
      "learning_rate": 4.974515359963513e-05,
      "loss": 4.0947,
      "step": 115600
    },
    {
      "epoch": 0.020405524976662395,
      "grad_norm": 8.616117477416992,
      "learning_rate": 4.974493314236443e-05,
      "loss": 4.1847,
      "step": 115700
    },
    {
      "epoch": 0.020423161558318973,
      "grad_norm": 5.839334011077881,
      "learning_rate": 4.9744712685093723e-05,
      "loss": 4.1557,
      "step": 115800
    },
    {
      "epoch": 0.02044079813997555,
      "grad_norm": 8.214933395385742,
      "learning_rate": 4.974449222782301e-05,
      "loss": 4.0729,
      "step": 115900
    },
    {
      "epoch": 0.02045843472163213,
      "grad_norm": 9.098386764526367,
      "learning_rate": 4.974427177055231e-05,
      "loss": 4.1703,
      "step": 116000
    },
    {
      "epoch": 0.020476071303288712,
      "grad_norm": 6.939682960510254,
      "learning_rate": 4.97440513132816e-05,
      "loss": 4.0702,
      "step": 116100
    },
    {
      "epoch": 0.02049370788494529,
      "grad_norm": 10.200809478759766,
      "learning_rate": 4.974383085601089e-05,
      "loss": 4.0652,
      "step": 116200
    },
    {
      "epoch": 0.02051134446660187,
      "grad_norm": 6.554250717163086,
      "learning_rate": 4.974361039874019e-05,
      "loss": 4.0963,
      "step": 116300
    },
    {
      "epoch": 0.02052898104825845,
      "grad_norm": 6.898882865905762,
      "learning_rate": 4.974338994146948e-05,
      "loss": 4.1045,
      "step": 116400
    },
    {
      "epoch": 0.02054661762991503,
      "grad_norm": 6.2796950340271,
      "learning_rate": 4.974316948419877e-05,
      "loss": 4.142,
      "step": 116500
    },
    {
      "epoch": 0.020564254211571608,
      "grad_norm": 9.668146133422852,
      "learning_rate": 4.974294902692807e-05,
      "loss": 4.0039,
      "step": 116600
    },
    {
      "epoch": 0.020581890793228187,
      "grad_norm": 8.333430290222168,
      "learning_rate": 4.974272856965736e-05,
      "loss": 4.0564,
      "step": 116700
    },
    {
      "epoch": 0.02059952737488477,
      "grad_norm": 7.587309837341309,
      "learning_rate": 4.974250811238665e-05,
      "loss": 4.115,
      "step": 116800
    },
    {
      "epoch": 0.020617163956541347,
      "grad_norm": 9.845820426940918,
      "learning_rate": 4.9742287655115947e-05,
      "loss": 4.0229,
      "step": 116900
    },
    {
      "epoch": 0.020634800538197925,
      "grad_norm": 7.016519546508789,
      "learning_rate": 4.9742067197845235e-05,
      "loss": 4.0567,
      "step": 117000
    },
    {
      "epoch": 0.020652437119854504,
      "grad_norm": 12.781620979309082,
      "learning_rate": 4.9741846740574524e-05,
      "loss": 4.102,
      "step": 117100
    },
    {
      "epoch": 0.020670073701511086,
      "grad_norm": 7.050835132598877,
      "learning_rate": 4.974162628330382e-05,
      "loss": 4.0549,
      "step": 117200
    },
    {
      "epoch": 0.020687710283167664,
      "grad_norm": 8.134394645690918,
      "learning_rate": 4.9741405826033115e-05,
      "loss": 4.0178,
      "step": 117300
    },
    {
      "epoch": 0.020705346864824243,
      "grad_norm": 6.595921039581299,
      "learning_rate": 4.9741185368762404e-05,
      "loss": 4.1083,
      "step": 117400
    },
    {
      "epoch": 0.02072298344648082,
      "grad_norm": 7.9850335121154785,
      "learning_rate": 4.97409649114917e-05,
      "loss": 4.0904,
      "step": 117500
    },
    {
      "epoch": 0.020740620028137403,
      "grad_norm": 6.209157466888428,
      "learning_rate": 4.9740744454220994e-05,
      "loss": 4.1073,
      "step": 117600
    },
    {
      "epoch": 0.020758256609793982,
      "grad_norm": 6.8676981925964355,
      "learning_rate": 4.974052399695028e-05,
      "loss": 4.1209,
      "step": 117700
    },
    {
      "epoch": 0.02077589319145056,
      "grad_norm": 10.045058250427246,
      "learning_rate": 4.974030353967958e-05,
      "loss": 4.0785,
      "step": 117800
    },
    {
      "epoch": 0.020793529773107142,
      "grad_norm": 12.529272079467773,
      "learning_rate": 4.9740083082408874e-05,
      "loss": 4.1042,
      "step": 117900
    },
    {
      "epoch": 0.02081116635476372,
      "grad_norm": 6.245138645172119,
      "learning_rate": 4.973986262513816e-05,
      "loss": 4.0381,
      "step": 118000
    },
    {
      "epoch": 0.0208288029364203,
      "grad_norm": 5.117504596710205,
      "learning_rate": 4.973964216786746e-05,
      "loss": 4.1393,
      "step": 118100
    },
    {
      "epoch": 0.020846439518076878,
      "grad_norm": 6.817309856414795,
      "learning_rate": 4.9739421710596754e-05,
      "loss": 4.0959,
      "step": 118200
    },
    {
      "epoch": 0.02086407609973346,
      "grad_norm": 9.212298393249512,
      "learning_rate": 4.973920125332604e-05,
      "loss": 4.0847,
      "step": 118300
    },
    {
      "epoch": 0.02088171268139004,
      "grad_norm": 8.100728034973145,
      "learning_rate": 4.973898079605533e-05,
      "loss": 4.0162,
      "step": 118400
    },
    {
      "epoch": 0.020899349263046617,
      "grad_norm": 7.114537239074707,
      "learning_rate": 4.9738760338784627e-05,
      "loss": 4.1307,
      "step": 118500
    },
    {
      "epoch": 0.020916985844703195,
      "grad_norm": 7.075119495391846,
      "learning_rate": 4.9738539881513915e-05,
      "loss": 4.0233,
      "step": 118600
    },
    {
      "epoch": 0.020934622426359777,
      "grad_norm": 5.555380821228027,
      "learning_rate": 4.973831942424321e-05,
      "loss": 4.0565,
      "step": 118700
    },
    {
      "epoch": 0.020952259008016356,
      "grad_norm": 8.084697723388672,
      "learning_rate": 4.9738098966972506e-05,
      "loss": 4.022,
      "step": 118800
    },
    {
      "epoch": 0.020969895589672934,
      "grad_norm": 11.276395797729492,
      "learning_rate": 4.9737878509701795e-05,
      "loss": 4.094,
      "step": 118900
    },
    {
      "epoch": 0.020987532171329513,
      "grad_norm": 6.768502712249756,
      "learning_rate": 4.973765805243109e-05,
      "loss": 4.0748,
      "step": 119000
    },
    {
      "epoch": 0.021005168752986095,
      "grad_norm": 6.754488468170166,
      "learning_rate": 4.9737437595160386e-05,
      "loss": 4.096,
      "step": 119100
    },
    {
      "epoch": 0.021022805334642673,
      "grad_norm": 6.286369800567627,
      "learning_rate": 4.9737217137889675e-05,
      "loss": 4.076,
      "step": 119200
    },
    {
      "epoch": 0.021040441916299252,
      "grad_norm": 6.988655090332031,
      "learning_rate": 4.973699668061897e-05,
      "loss": 4.1126,
      "step": 119300
    },
    {
      "epoch": 0.021058078497955834,
      "grad_norm": 6.099969387054443,
      "learning_rate": 4.9736776223348265e-05,
      "loss": 3.975,
      "step": 119400
    },
    {
      "epoch": 0.021075715079612412,
      "grad_norm": 6.774655342102051,
      "learning_rate": 4.9736555766077554e-05,
      "loss": 4.141,
      "step": 119500
    },
    {
      "epoch": 0.02109335166126899,
      "grad_norm": 6.886048316955566,
      "learning_rate": 4.973633530880685e-05,
      "loss": 4.0249,
      "step": 119600
    },
    {
      "epoch": 0.02111098824292557,
      "grad_norm": 7.677066802978516,
      "learning_rate": 4.9736114851536145e-05,
      "loss": 4.0683,
      "step": 119700
    },
    {
      "epoch": 0.02112862482458215,
      "grad_norm": 6.589161396026611,
      "learning_rate": 4.9735894394265434e-05,
      "loss": 4.1323,
      "step": 119800
    },
    {
      "epoch": 0.02114626140623873,
      "grad_norm": 6.706538200378418,
      "learning_rate": 4.973567393699472e-05,
      "loss": 4.0869,
      "step": 119900
    },
    {
      "epoch": 0.021163897987895308,
      "grad_norm": 4.120406627655029,
      "learning_rate": 4.973545347972402e-05,
      "loss": 4.029,
      "step": 120000
    },
    {
      "epoch": 0.021181534569551887,
      "grad_norm": 7.9398193359375,
      "learning_rate": 4.973523302245331e-05,
      "loss": 4.0638,
      "step": 120100
    },
    {
      "epoch": 0.02119917115120847,
      "grad_norm": 6.526096343994141,
      "learning_rate": 4.97350125651826e-05,
      "loss": 4.0596,
      "step": 120200
    },
    {
      "epoch": 0.021216807732865047,
      "grad_norm": 8.903746604919434,
      "learning_rate": 4.97347921079119e-05,
      "loss": 4.129,
      "step": 120300
    },
    {
      "epoch": 0.021234444314521626,
      "grad_norm": 5.720481872558594,
      "learning_rate": 4.9734571650641186e-05,
      "loss": 4.0106,
      "step": 120400
    },
    {
      "epoch": 0.021252080896178204,
      "grad_norm": 8.059653282165527,
      "learning_rate": 4.973435119337048e-05,
      "loss": 4.0722,
      "step": 120500
    },
    {
      "epoch": 0.021269717477834786,
      "grad_norm": 7.6387410163879395,
      "learning_rate": 4.973413073609978e-05,
      "loss": 4.1138,
      "step": 120600
    },
    {
      "epoch": 0.021287354059491365,
      "grad_norm": 5.726522445678711,
      "learning_rate": 4.9733910278829066e-05,
      "loss": 4.1403,
      "step": 120700
    },
    {
      "epoch": 0.021304990641147943,
      "grad_norm": 7.546392917633057,
      "learning_rate": 4.973368982155836e-05,
      "loss": 4.0958,
      "step": 120800
    },
    {
      "epoch": 0.021322627222804525,
      "grad_norm": 6.978457450866699,
      "learning_rate": 4.973346936428766e-05,
      "loss": 4.0687,
      "step": 120900
    },
    {
      "epoch": 0.021340263804461104,
      "grad_norm": 6.656416893005371,
      "learning_rate": 4.9733248907016946e-05,
      "loss": 4.155,
      "step": 121000
    },
    {
      "epoch": 0.021357900386117682,
      "grad_norm": 9.18391227722168,
      "learning_rate": 4.973302844974624e-05,
      "loss": 4.0249,
      "step": 121100
    },
    {
      "epoch": 0.02137553696777426,
      "grad_norm": 6.1015400886535645,
      "learning_rate": 4.973280799247553e-05,
      "loss": 4.1299,
      "step": 121200
    },
    {
      "epoch": 0.021393173549430843,
      "grad_norm": 5.865787982940674,
      "learning_rate": 4.9732587535204825e-05,
      "loss": 4.0306,
      "step": 121300
    },
    {
      "epoch": 0.02141081013108742,
      "grad_norm": 9.165671348571777,
      "learning_rate": 4.9732367077934114e-05,
      "loss": 4.0414,
      "step": 121400
    },
    {
      "epoch": 0.021428446712744,
      "grad_norm": 5.3153204917907715,
      "learning_rate": 4.973214662066341e-05,
      "loss": 4.0856,
      "step": 121500
    },
    {
      "epoch": 0.021446083294400578,
      "grad_norm": 6.4093403816223145,
      "learning_rate": 4.97319261633927e-05,
      "loss": 4.0818,
      "step": 121600
    },
    {
      "epoch": 0.02146371987605716,
      "grad_norm": 7.377500534057617,
      "learning_rate": 4.9731705706121993e-05,
      "loss": 4.1077,
      "step": 121700
    },
    {
      "epoch": 0.02148135645771374,
      "grad_norm": 7.467153549194336,
      "learning_rate": 4.973148524885129e-05,
      "loss": 3.9952,
      "step": 121800
    },
    {
      "epoch": 0.021498993039370317,
      "grad_norm": 6.27755880355835,
      "learning_rate": 4.973126479158058e-05,
      "loss": 3.9865,
      "step": 121900
    },
    {
      "epoch": 0.021516629621026895,
      "grad_norm": 5.587318420410156,
      "learning_rate": 4.973104433430987e-05,
      "loss": 4.1106,
      "step": 122000
    },
    {
      "epoch": 0.021534266202683477,
      "grad_norm": 7.4851813316345215,
      "learning_rate": 4.973082387703917e-05,
      "loss": 4.0773,
      "step": 122100
    },
    {
      "epoch": 0.021551902784340056,
      "grad_norm": 7.166053771972656,
      "learning_rate": 4.973060341976846e-05,
      "loss": 4.1264,
      "step": 122200
    },
    {
      "epoch": 0.021569539365996634,
      "grad_norm": 8.152054786682129,
      "learning_rate": 4.973038296249775e-05,
      "loss": 4.1262,
      "step": 122300
    },
    {
      "epoch": 0.021587175947653216,
      "grad_norm": 9.51248836517334,
      "learning_rate": 4.973016250522705e-05,
      "loss": 4.1786,
      "step": 122400
    },
    {
      "epoch": 0.021604812529309795,
      "grad_norm": 6.934228420257568,
      "learning_rate": 4.972994204795634e-05,
      "loss": 4.2538,
      "step": 122500
    },
    {
      "epoch": 0.021622449110966373,
      "grad_norm": 7.316615104675293,
      "learning_rate": 4.972972159068563e-05,
      "loss": 4.0313,
      "step": 122600
    },
    {
      "epoch": 0.021640085692622952,
      "grad_norm": 6.409769058227539,
      "learning_rate": 4.972950113341492e-05,
      "loss": 4.0053,
      "step": 122700
    },
    {
      "epoch": 0.021657722274279534,
      "grad_norm": 6.298953056335449,
      "learning_rate": 4.9729280676144217e-05,
      "loss": 4.024,
      "step": 122800
    },
    {
      "epoch": 0.021675358855936112,
      "grad_norm": 7.818673610687256,
      "learning_rate": 4.9729060218873505e-05,
      "loss": 4.0014,
      "step": 122900
    },
    {
      "epoch": 0.02169299543759269,
      "grad_norm": 9.27508544921875,
      "learning_rate": 4.97288397616028e-05,
      "loss": 4.1378,
      "step": 123000
    },
    {
      "epoch": 0.02171063201924927,
      "grad_norm": 6.564505577087402,
      "learning_rate": 4.972861930433209e-05,
      "loss": 4.1647,
      "step": 123100
    },
    {
      "epoch": 0.02172826860090585,
      "grad_norm": 7.670507431030273,
      "learning_rate": 4.9728398847061385e-05,
      "loss": 4.1433,
      "step": 123200
    },
    {
      "epoch": 0.02174590518256243,
      "grad_norm": 5.7110514640808105,
      "learning_rate": 4.972817838979068e-05,
      "loss": 4.0208,
      "step": 123300
    },
    {
      "epoch": 0.02176354176421901,
      "grad_norm": 10.098721504211426,
      "learning_rate": 4.972795793251997e-05,
      "loss": 4.1212,
      "step": 123400
    },
    {
      "epoch": 0.021781178345875587,
      "grad_norm": 8.420089721679688,
      "learning_rate": 4.9727737475249264e-05,
      "loss": 4.0811,
      "step": 123500
    },
    {
      "epoch": 0.02179881492753217,
      "grad_norm": 6.769477844238281,
      "learning_rate": 4.972751701797856e-05,
      "loss": 4.0507,
      "step": 123600
    },
    {
      "epoch": 0.021816451509188747,
      "grad_norm": 8.70280647277832,
      "learning_rate": 4.972729656070785e-05,
      "loss": 4.1077,
      "step": 123700
    },
    {
      "epoch": 0.021834088090845326,
      "grad_norm": 6.090972900390625,
      "learning_rate": 4.9727076103437144e-05,
      "loss": 4.0202,
      "step": 123800
    },
    {
      "epoch": 0.021851724672501908,
      "grad_norm": 7.226374626159668,
      "learning_rate": 4.972685564616644e-05,
      "loss": 4.141,
      "step": 123900
    },
    {
      "epoch": 0.021869361254158486,
      "grad_norm": 8.230069160461426,
      "learning_rate": 4.972663518889573e-05,
      "loss": 4.0653,
      "step": 124000
    },
    {
      "epoch": 0.021886997835815065,
      "grad_norm": 6.511599063873291,
      "learning_rate": 4.9726414731625024e-05,
      "loss": 4.0714,
      "step": 124100
    },
    {
      "epoch": 0.021904634417471643,
      "grad_norm": 7.664409637451172,
      "learning_rate": 4.972619427435431e-05,
      "loss": 4.012,
      "step": 124200
    },
    {
      "epoch": 0.021922270999128225,
      "grad_norm": 7.125474452972412,
      "learning_rate": 4.97259738170836e-05,
      "loss": 4.0686,
      "step": 124300
    },
    {
      "epoch": 0.021939907580784804,
      "grad_norm": 5.967776775360107,
      "learning_rate": 4.9725753359812897e-05,
      "loss": 4.0535,
      "step": 124400
    },
    {
      "epoch": 0.021957544162441382,
      "grad_norm": 7.045738697052002,
      "learning_rate": 4.972553290254219e-05,
      "loss": 4.0185,
      "step": 124500
    },
    {
      "epoch": 0.02197518074409796,
      "grad_norm": 8.682268142700195,
      "learning_rate": 4.972531244527148e-05,
      "loss": 4.0395,
      "step": 124600
    },
    {
      "epoch": 0.021992817325754543,
      "grad_norm": 5.962413787841797,
      "learning_rate": 4.9725091988000776e-05,
      "loss": 4.0711,
      "step": 124700
    },
    {
      "epoch": 0.02201045390741112,
      "grad_norm": 5.331303596496582,
      "learning_rate": 4.972487153073007e-05,
      "loss": 3.9114,
      "step": 124800
    },
    {
      "epoch": 0.0220280904890677,
      "grad_norm": 6.879775524139404,
      "learning_rate": 4.972465107345936e-05,
      "loss": 4.0766,
      "step": 124900
    },
    {
      "epoch": 0.022045727070724278,
      "grad_norm": 7.167799949645996,
      "learning_rate": 4.9724430616188656e-05,
      "loss": 4.0687,
      "step": 125000
    },
    {
      "epoch": 0.02206336365238086,
      "grad_norm": 8.614131927490234,
      "learning_rate": 4.972421015891795e-05,
      "loss": 4.0731,
      "step": 125100
    },
    {
      "epoch": 0.02208100023403744,
      "grad_norm": 7.055258274078369,
      "learning_rate": 4.972398970164724e-05,
      "loss": 4.0975,
      "step": 125200
    },
    {
      "epoch": 0.022098636815694017,
      "grad_norm": 6.968653202056885,
      "learning_rate": 4.9723769244376535e-05,
      "loss": 4.1048,
      "step": 125300
    },
    {
      "epoch": 0.0221162733973506,
      "grad_norm": 8.226826667785645,
      "learning_rate": 4.972354878710583e-05,
      "loss": 4.0687,
      "step": 125400
    },
    {
      "epoch": 0.022133909979007178,
      "grad_norm": 9.297346115112305,
      "learning_rate": 4.972332832983512e-05,
      "loss": 4.0286,
      "step": 125500
    },
    {
      "epoch": 0.022151546560663756,
      "grad_norm": 10.46959400177002,
      "learning_rate": 4.9723107872564415e-05,
      "loss": 4.1045,
      "step": 125600
    },
    {
      "epoch": 0.022169183142320335,
      "grad_norm": 8.27273178100586,
      "learning_rate": 4.9722887415293704e-05,
      "loss": 4.0015,
      "step": 125700
    },
    {
      "epoch": 0.022186819723976917,
      "grad_norm": 5.264815330505371,
      "learning_rate": 4.972266695802299e-05,
      "loss": 3.9987,
      "step": 125800
    },
    {
      "epoch": 0.022204456305633495,
      "grad_norm": 8.566837310791016,
      "learning_rate": 4.972244650075229e-05,
      "loss": 4.1274,
      "step": 125900
    },
    {
      "epoch": 0.022222092887290074,
      "grad_norm": 7.225116729736328,
      "learning_rate": 4.9722226043481583e-05,
      "loss": 4.1058,
      "step": 126000
    },
    {
      "epoch": 0.022239729468946652,
      "grad_norm": 6.865287780761719,
      "learning_rate": 4.972200558621087e-05,
      "loss": 4.0501,
      "step": 126100
    },
    {
      "epoch": 0.022257366050603234,
      "grad_norm": 5.385522842407227,
      "learning_rate": 4.972178512894017e-05,
      "loss": 4.069,
      "step": 126200
    },
    {
      "epoch": 0.022275002632259813,
      "grad_norm": 7.594875335693359,
      "learning_rate": 4.972156467166946e-05,
      "loss": 3.9968,
      "step": 126300
    },
    {
      "epoch": 0.02229263921391639,
      "grad_norm": 7.621737003326416,
      "learning_rate": 4.972134421439875e-05,
      "loss": 4.0258,
      "step": 126400
    },
    {
      "epoch": 0.02231027579557297,
      "grad_norm": 8.87176513671875,
      "learning_rate": 4.972112375712805e-05,
      "loss": 4.039,
      "step": 126500
    },
    {
      "epoch": 0.02232791237722955,
      "grad_norm": 8.2808256149292,
      "learning_rate": 4.972090329985734e-05,
      "loss": 3.9801,
      "step": 126600
    },
    {
      "epoch": 0.02234554895888613,
      "grad_norm": 7.497721195220947,
      "learning_rate": 4.972068284258664e-05,
      "loss": 4.0673,
      "step": 126700
    },
    {
      "epoch": 0.02236318554054271,
      "grad_norm": 8.47370719909668,
      "learning_rate": 4.972046238531593e-05,
      "loss": 3.9042,
      "step": 126800
    },
    {
      "epoch": 0.02238082212219929,
      "grad_norm": 8.776632308959961,
      "learning_rate": 4.972024192804522e-05,
      "loss": 4.0866,
      "step": 126900
    },
    {
      "epoch": 0.02239845870385587,
      "grad_norm": 10.514214515686035,
      "learning_rate": 4.972002147077451e-05,
      "loss": 4.1398,
      "step": 127000
    },
    {
      "epoch": 0.022416095285512447,
      "grad_norm": 7.6966094970703125,
      "learning_rate": 4.97198010135038e-05,
      "loss": 4.0535,
      "step": 127100
    },
    {
      "epoch": 0.022433731867169026,
      "grad_norm": 16.39314079284668,
      "learning_rate": 4.9719580556233095e-05,
      "loss": 4.0766,
      "step": 127200
    },
    {
      "epoch": 0.022451368448825608,
      "grad_norm": 6.401749610900879,
      "learning_rate": 4.971936009896239e-05,
      "loss": 3.9143,
      "step": 127300
    },
    {
      "epoch": 0.022469005030482186,
      "grad_norm": 6.730916976928711,
      "learning_rate": 4.971913964169168e-05,
      "loss": 4.0861,
      "step": 127400
    },
    {
      "epoch": 0.022486641612138765,
      "grad_norm": 12.097131729125977,
      "learning_rate": 4.9718919184420975e-05,
      "loss": 4.0871,
      "step": 127500
    },
    {
      "epoch": 0.022504278193795343,
      "grad_norm": 6.218404769897461,
      "learning_rate": 4.971869872715027e-05,
      "loss": 4.1474,
      "step": 127600
    },
    {
      "epoch": 0.022521914775451925,
      "grad_norm": 6.785645008087158,
      "learning_rate": 4.971847826987956e-05,
      "loss": 4.1079,
      "step": 127700
    },
    {
      "epoch": 0.022539551357108504,
      "grad_norm": 6.371240615844727,
      "learning_rate": 4.9718257812608854e-05,
      "loss": 4.0091,
      "step": 127800
    },
    {
      "epoch": 0.022557187938765082,
      "grad_norm": 6.977118015289307,
      "learning_rate": 4.971803735533815e-05,
      "loss": 4.0413,
      "step": 127900
    },
    {
      "epoch": 0.02257482452042166,
      "grad_norm": 6.361294269561768,
      "learning_rate": 4.971781689806744e-05,
      "loss": 4.0568,
      "step": 128000
    },
    {
      "epoch": 0.022592461102078243,
      "grad_norm": 6.356100559234619,
      "learning_rate": 4.9717596440796734e-05,
      "loss": 3.9944,
      "step": 128100
    },
    {
      "epoch": 0.02261009768373482,
      "grad_norm": 6.232942581176758,
      "learning_rate": 4.971737598352603e-05,
      "loss": 4.086,
      "step": 128200
    },
    {
      "epoch": 0.0226277342653914,
      "grad_norm": 6.5354156494140625,
      "learning_rate": 4.971715552625532e-05,
      "loss": 3.9211,
      "step": 128300
    },
    {
      "epoch": 0.022645370847047982,
      "grad_norm": 10.423972129821777,
      "learning_rate": 4.9716935068984614e-05,
      "loss": 4.1448,
      "step": 128400
    },
    {
      "epoch": 0.02266300742870456,
      "grad_norm": 8.495993614196777,
      "learning_rate": 4.97167146117139e-05,
      "loss": 4.0385,
      "step": 128500
    },
    {
      "epoch": 0.02268064401036114,
      "grad_norm": 4.010089874267578,
      "learning_rate": 4.971649415444319e-05,
      "loss": 3.9881,
      "step": 128600
    },
    {
      "epoch": 0.022698280592017717,
      "grad_norm": 6.188802719116211,
      "learning_rate": 4.9716273697172487e-05,
      "loss": 4.1137,
      "step": 128700
    },
    {
      "epoch": 0.0227159171736743,
      "grad_norm": 8.675615310668945,
      "learning_rate": 4.971605323990178e-05,
      "loss": 4.075,
      "step": 128800
    },
    {
      "epoch": 0.022733553755330878,
      "grad_norm": 6.4278154373168945,
      "learning_rate": 4.971583278263107e-05,
      "loss": 4.0206,
      "step": 128900
    },
    {
      "epoch": 0.022751190336987456,
      "grad_norm": 8.281290054321289,
      "learning_rate": 4.9715612325360366e-05,
      "loss": 3.9823,
      "step": 129000
    },
    {
      "epoch": 0.022768826918644035,
      "grad_norm": 7.974095344543457,
      "learning_rate": 4.971539186808966e-05,
      "loss": 4.0219,
      "step": 129100
    },
    {
      "epoch": 0.022786463500300617,
      "grad_norm": 7.662581443786621,
      "learning_rate": 4.971517141081895e-05,
      "loss": 4.1029,
      "step": 129200
    },
    {
      "epoch": 0.022804100081957195,
      "grad_norm": 7.9884490966796875,
      "learning_rate": 4.9714950953548246e-05,
      "loss": 3.9799,
      "step": 129300
    },
    {
      "epoch": 0.022821736663613774,
      "grad_norm": 7.43548059463501,
      "learning_rate": 4.971473049627754e-05,
      "loss": 4.0644,
      "step": 129400
    },
    {
      "epoch": 0.022839373245270352,
      "grad_norm": 5.864931106567383,
      "learning_rate": 4.971451003900683e-05,
      "loss": 4.0624,
      "step": 129500
    },
    {
      "epoch": 0.022857009826926934,
      "grad_norm": 6.700328350067139,
      "learning_rate": 4.9714289581736125e-05,
      "loss": 4.0537,
      "step": 129600
    },
    {
      "epoch": 0.022874646408583513,
      "grad_norm": 5.878274440765381,
      "learning_rate": 4.971406912446542e-05,
      "loss": 4.0963,
      "step": 129700
    },
    {
      "epoch": 0.02289228299024009,
      "grad_norm": 5.94580602645874,
      "learning_rate": 4.971384866719471e-05,
      "loss": 4.0291,
      "step": 129800
    },
    {
      "epoch": 0.022909919571896673,
      "grad_norm": 5.383919715881348,
      "learning_rate": 4.9713628209924e-05,
      "loss": 3.9657,
      "step": 129900
    },
    {
      "epoch": 0.02292755615355325,
      "grad_norm": 6.534219741821289,
      "learning_rate": 4.9713407752653294e-05,
      "loss": 4.1675,
      "step": 130000
    },
    {
      "epoch": 0.02294519273520983,
      "grad_norm": 6.244427680969238,
      "learning_rate": 4.971318729538258e-05,
      "loss": 4.0148,
      "step": 130100
    },
    {
      "epoch": 0.02296282931686641,
      "grad_norm": 7.639453887939453,
      "learning_rate": 4.971296683811188e-05,
      "loss": 4.1079,
      "step": 130200
    },
    {
      "epoch": 0.02298046589852299,
      "grad_norm": 7.714693546295166,
      "learning_rate": 4.971274638084117e-05,
      "loss": 3.9793,
      "step": 130300
    },
    {
      "epoch": 0.02299810248017957,
      "grad_norm": 7.420655250549316,
      "learning_rate": 4.971252592357046e-05,
      "loss": 4.0343,
      "step": 130400
    },
    {
      "epoch": 0.023015739061836148,
      "grad_norm": 6.205324172973633,
      "learning_rate": 4.971230546629976e-05,
      "loss": 4.0445,
      "step": 130500
    },
    {
      "epoch": 0.023033375643492726,
      "grad_norm": 7.518370151519775,
      "learning_rate": 4.971208500902905e-05,
      "loss": 4.0962,
      "step": 130600
    },
    {
      "epoch": 0.023051012225149308,
      "grad_norm": 5.559493064880371,
      "learning_rate": 4.971186455175834e-05,
      "loss": 4.1068,
      "step": 130700
    },
    {
      "epoch": 0.023068648806805887,
      "grad_norm": 7.949943542480469,
      "learning_rate": 4.971164409448764e-05,
      "loss": 4.0941,
      "step": 130800
    },
    {
      "epoch": 0.023086285388462465,
      "grad_norm": 6.6404500007629395,
      "learning_rate": 4.971142363721693e-05,
      "loss": 4.0362,
      "step": 130900
    },
    {
      "epoch": 0.023103921970119044,
      "grad_norm": 9.24171257019043,
      "learning_rate": 4.971120317994622e-05,
      "loss": 4.05,
      "step": 131000
    },
    {
      "epoch": 0.023121558551775626,
      "grad_norm": 8.412989616394043,
      "learning_rate": 4.971098272267552e-05,
      "loss": 3.9474,
      "step": 131100
    },
    {
      "epoch": 0.023139195133432204,
      "grad_norm": 7.1614484786987305,
      "learning_rate": 4.9710762265404805e-05,
      "loss": 4.0034,
      "step": 131200
    },
    {
      "epoch": 0.023156831715088783,
      "grad_norm": 6.187598705291748,
      "learning_rate": 4.97105418081341e-05,
      "loss": 4.0313,
      "step": 131300
    },
    {
      "epoch": 0.023174468296745365,
      "grad_norm": 7.072935104370117,
      "learning_rate": 4.971032135086339e-05,
      "loss": 4.1221,
      "step": 131400
    },
    {
      "epoch": 0.023192104878401943,
      "grad_norm": 9.48204517364502,
      "learning_rate": 4.9710100893592685e-05,
      "loss": 3.9033,
      "step": 131500
    },
    {
      "epoch": 0.02320974146005852,
      "grad_norm": 5.711884021759033,
      "learning_rate": 4.9709880436321974e-05,
      "loss": 4.0705,
      "step": 131600
    },
    {
      "epoch": 0.0232273780417151,
      "grad_norm": 7.01347017288208,
      "learning_rate": 4.970965997905127e-05,
      "loss": 4.0509,
      "step": 131700
    },
    {
      "epoch": 0.023245014623371682,
      "grad_norm": 6.402686595916748,
      "learning_rate": 4.9709439521780565e-05,
      "loss": 4.0158,
      "step": 131800
    },
    {
      "epoch": 0.02326265120502826,
      "grad_norm": 5.67176628112793,
      "learning_rate": 4.9709219064509853e-05,
      "loss": 3.988,
      "step": 131900
    },
    {
      "epoch": 0.02328028778668484,
      "grad_norm": 17.977142333984375,
      "learning_rate": 4.970899860723915e-05,
      "loss": 3.9834,
      "step": 132000
    },
    {
      "epoch": 0.023297924368341418,
      "grad_norm": 6.9951491355896,
      "learning_rate": 4.9708778149968444e-05,
      "loss": 3.9553,
      "step": 132100
    },
    {
      "epoch": 0.023315560949998,
      "grad_norm": 7.895097255706787,
      "learning_rate": 4.970855769269773e-05,
      "loss": 4.0105,
      "step": 132200
    },
    {
      "epoch": 0.023333197531654578,
      "grad_norm": 7.7798566818237305,
      "learning_rate": 4.970833723542703e-05,
      "loss": 4.0255,
      "step": 132300
    },
    {
      "epoch": 0.023350834113311156,
      "grad_norm": 5.561779499053955,
      "learning_rate": 4.9708116778156324e-05,
      "loss": 4.0603,
      "step": 132400
    },
    {
      "epoch": 0.023368470694967735,
      "grad_norm": 6.765684127807617,
      "learning_rate": 4.970789632088561e-05,
      "loss": 4.0412,
      "step": 132500
    },
    {
      "epoch": 0.023386107276624317,
      "grad_norm": 5.530495643615723,
      "learning_rate": 4.970767586361491e-05,
      "loss": 4.2504,
      "step": 132600
    },
    {
      "epoch": 0.023403743858280895,
      "grad_norm": 8.398914337158203,
      "learning_rate": 4.97074554063442e-05,
      "loss": 3.9359,
      "step": 132700
    },
    {
      "epoch": 0.023421380439937474,
      "grad_norm": 7.282561302185059,
      "learning_rate": 4.970723494907349e-05,
      "loss": 4.0221,
      "step": 132800
    },
    {
      "epoch": 0.023439017021594056,
      "grad_norm": 6.715332508087158,
      "learning_rate": 4.970701449180278e-05,
      "loss": 4.1194,
      "step": 132900
    },
    {
      "epoch": 0.023456653603250634,
      "grad_norm": 5.588011741638184,
      "learning_rate": 4.9706794034532076e-05,
      "loss": 4.0699,
      "step": 133000
    },
    {
      "epoch": 0.023474290184907213,
      "grad_norm": 7.528113842010498,
      "learning_rate": 4.9706573577261365e-05,
      "loss": 4.1194,
      "step": 133100
    },
    {
      "epoch": 0.02349192676656379,
      "grad_norm": 7.380304336547852,
      "learning_rate": 4.970635311999066e-05,
      "loss": 4.0404,
      "step": 133200
    },
    {
      "epoch": 0.023509563348220373,
      "grad_norm": 6.455844402313232,
      "learning_rate": 4.9706132662719956e-05,
      "loss": 4.0632,
      "step": 133300
    },
    {
      "epoch": 0.023527199929876952,
      "grad_norm": 6.801522731781006,
      "learning_rate": 4.9705912205449245e-05,
      "loss": 3.8659,
      "step": 133400
    },
    {
      "epoch": 0.02354483651153353,
      "grad_norm": 7.69761848449707,
      "learning_rate": 4.970569174817854e-05,
      "loss": 3.9814,
      "step": 133500
    },
    {
      "epoch": 0.02356247309319011,
      "grad_norm": 7.042780876159668,
      "learning_rate": 4.9705471290907836e-05,
      "loss": 3.963,
      "step": 133600
    },
    {
      "epoch": 0.02358010967484669,
      "grad_norm": 6.66668701171875,
      "learning_rate": 4.9705250833637124e-05,
      "loss": 3.917,
      "step": 133700
    },
    {
      "epoch": 0.02359774625650327,
      "grad_norm": 9.109663009643555,
      "learning_rate": 4.970503037636642e-05,
      "loss": 4.109,
      "step": 133800
    },
    {
      "epoch": 0.023615382838159848,
      "grad_norm": 7.957479000091553,
      "learning_rate": 4.9704809919095715e-05,
      "loss": 4.1177,
      "step": 133900
    },
    {
      "epoch": 0.023633019419816426,
      "grad_norm": 7.641194820404053,
      "learning_rate": 4.9704589461825004e-05,
      "loss": 4.0581,
      "step": 134000
    },
    {
      "epoch": 0.02365065600147301,
      "grad_norm": 5.825985908508301,
      "learning_rate": 4.97043690045543e-05,
      "loss": 4.0145,
      "step": 134100
    },
    {
      "epoch": 0.023668292583129587,
      "grad_norm": 6.691442966461182,
      "learning_rate": 4.970414854728359e-05,
      "loss": 4.046,
      "step": 134200
    },
    {
      "epoch": 0.023685929164786165,
      "grad_norm": 5.918811321258545,
      "learning_rate": 4.970392809001288e-05,
      "loss": 4.0921,
      "step": 134300
    },
    {
      "epoch": 0.023703565746442747,
      "grad_norm": 9.765911102294922,
      "learning_rate": 4.970370763274217e-05,
      "loss": 3.987,
      "step": 134400
    },
    {
      "epoch": 0.023721202328099326,
      "grad_norm": 6.695079803466797,
      "learning_rate": 4.970348717547147e-05,
      "loss": 3.9013,
      "step": 134500
    },
    {
      "epoch": 0.023738838909755904,
      "grad_norm": 6.24791145324707,
      "learning_rate": 4.9703266718200757e-05,
      "loss": 3.9982,
      "step": 134600
    },
    {
      "epoch": 0.023756475491412483,
      "grad_norm": 8.165007591247559,
      "learning_rate": 4.970304626093005e-05,
      "loss": 3.9724,
      "step": 134700
    },
    {
      "epoch": 0.023774112073069065,
      "grad_norm": 5.985430717468262,
      "learning_rate": 4.970282580365935e-05,
      "loss": 3.941,
      "step": 134800
    },
    {
      "epoch": 0.023791748654725643,
      "grad_norm": 7.696625709533691,
      "learning_rate": 4.9702605346388636e-05,
      "loss": 4.0628,
      "step": 134900
    },
    {
      "epoch": 0.02380938523638222,
      "grad_norm": 7.25829553604126,
      "learning_rate": 4.970238488911793e-05,
      "loss": 4.1436,
      "step": 135000
    },
    {
      "epoch": 0.0238270218180388,
      "grad_norm": 5.441198348999023,
      "learning_rate": 4.970216443184723e-05,
      "loss": 4.0581,
      "step": 135100
    },
    {
      "epoch": 0.023844658399695382,
      "grad_norm": 6.9517645835876465,
      "learning_rate": 4.9701943974576516e-05,
      "loss": 4.0581,
      "step": 135200
    },
    {
      "epoch": 0.02386229498135196,
      "grad_norm": 9.424290657043457,
      "learning_rate": 4.970172351730581e-05,
      "loss": 4.0199,
      "step": 135300
    },
    {
      "epoch": 0.02387993156300854,
      "grad_norm": 6.378131866455078,
      "learning_rate": 4.970150306003511e-05,
      "loss": 4.0088,
      "step": 135400
    },
    {
      "epoch": 0.023897568144665118,
      "grad_norm": 5.570781707763672,
      "learning_rate": 4.9701282602764395e-05,
      "loss": 3.9316,
      "step": 135500
    },
    {
      "epoch": 0.0239152047263217,
      "grad_norm": 9.707118034362793,
      "learning_rate": 4.970106214549369e-05,
      "loss": 4.0986,
      "step": 135600
    },
    {
      "epoch": 0.023932841307978278,
      "grad_norm": 6.868252277374268,
      "learning_rate": 4.970084168822298e-05,
      "loss": 4.0529,
      "step": 135700
    },
    {
      "epoch": 0.023950477889634857,
      "grad_norm": 7.526180267333984,
      "learning_rate": 4.970062123095227e-05,
      "loss": 4.0874,
      "step": 135800
    },
    {
      "epoch": 0.02396811447129144,
      "grad_norm": 6.771136283874512,
      "learning_rate": 4.9700400773681564e-05,
      "loss": 3.9874,
      "step": 135900
    },
    {
      "epoch": 0.023985751052948017,
      "grad_norm": 6.466000556945801,
      "learning_rate": 4.970018031641086e-05,
      "loss": 3.9892,
      "step": 136000
    },
    {
      "epoch": 0.024003387634604596,
      "grad_norm": 7.218388557434082,
      "learning_rate": 4.969995985914015e-05,
      "loss": 4.0443,
      "step": 136100
    },
    {
      "epoch": 0.024021024216261174,
      "grad_norm": 5.1247029304504395,
      "learning_rate": 4.969973940186944e-05,
      "loss": 3.9182,
      "step": 136200
    },
    {
      "epoch": 0.024038660797917756,
      "grad_norm": 7.542787075042725,
      "learning_rate": 4.969951894459874e-05,
      "loss": 4.0086,
      "step": 136300
    },
    {
      "epoch": 0.024056297379574335,
      "grad_norm": 5.019976615905762,
      "learning_rate": 4.969929848732803e-05,
      "loss": 4.0256,
      "step": 136400
    },
    {
      "epoch": 0.024073933961230913,
      "grad_norm": 6.371925354003906,
      "learning_rate": 4.969907803005732e-05,
      "loss": 4.0551,
      "step": 136500
    },
    {
      "epoch": 0.02409157054288749,
      "grad_norm": 6.775601387023926,
      "learning_rate": 4.969885757278662e-05,
      "loss": 3.9796,
      "step": 136600
    },
    {
      "epoch": 0.024109207124544074,
      "grad_norm": 8.969521522521973,
      "learning_rate": 4.969863711551591e-05,
      "loss": 3.9481,
      "step": 136700
    },
    {
      "epoch": 0.024126843706200652,
      "grad_norm": 6.07460355758667,
      "learning_rate": 4.96984166582452e-05,
      "loss": 3.9688,
      "step": 136800
    },
    {
      "epoch": 0.02414448028785723,
      "grad_norm": 6.205623626708984,
      "learning_rate": 4.96981962009745e-05,
      "loss": 4.0879,
      "step": 136900
    },
    {
      "epoch": 0.02416211686951381,
      "grad_norm": 5.8332366943359375,
      "learning_rate": 4.969797574370379e-05,
      "loss": 4.0424,
      "step": 137000
    },
    {
      "epoch": 0.02417975345117039,
      "grad_norm": 6.444250106811523,
      "learning_rate": 4.9697755286433075e-05,
      "loss": 3.9683,
      "step": 137100
    },
    {
      "epoch": 0.02419739003282697,
      "grad_norm": 5.354803562164307,
      "learning_rate": 4.969753482916237e-05,
      "loss": 4.0138,
      "step": 137200
    },
    {
      "epoch": 0.024215026614483548,
      "grad_norm": 7.5819501876831055,
      "learning_rate": 4.9697314371891666e-05,
      "loss": 4.0186,
      "step": 137300
    },
    {
      "epoch": 0.02423266319614013,
      "grad_norm": 9.504605293273926,
      "learning_rate": 4.9697093914620955e-05,
      "loss": 3.9883,
      "step": 137400
    },
    {
      "epoch": 0.02425029977779671,
      "grad_norm": 8.17206859588623,
      "learning_rate": 4.969687345735025e-05,
      "loss": 4.1921,
      "step": 137500
    },
    {
      "epoch": 0.024267936359453287,
      "grad_norm": 7.344610214233398,
      "learning_rate": 4.9696653000079546e-05,
      "loss": 4.0716,
      "step": 137600
    },
    {
      "epoch": 0.024285572941109865,
      "grad_norm": 6.562244892120361,
      "learning_rate": 4.9696432542808835e-05,
      "loss": 3.9988,
      "step": 137700
    },
    {
      "epoch": 0.024303209522766447,
      "grad_norm": 6.053778171539307,
      "learning_rate": 4.969621208553813e-05,
      "loss": 4.039,
      "step": 137800
    },
    {
      "epoch": 0.024320846104423026,
      "grad_norm": 6.495398044586182,
      "learning_rate": 4.9695991628267426e-05,
      "loss": 4.0146,
      "step": 137900
    },
    {
      "epoch": 0.024338482686079604,
      "grad_norm": 8.353937149047852,
      "learning_rate": 4.9695771170996714e-05,
      "loss": 4.0557,
      "step": 138000
    },
    {
      "epoch": 0.024356119267736183,
      "grad_norm": 6.934501647949219,
      "learning_rate": 4.969555071372601e-05,
      "loss": 4.0565,
      "step": 138100
    },
    {
      "epoch": 0.024373755849392765,
      "grad_norm": 8.975749015808105,
      "learning_rate": 4.9695330256455305e-05,
      "loss": 3.9384,
      "step": 138200
    },
    {
      "epoch": 0.024391392431049343,
      "grad_norm": 6.573286056518555,
      "learning_rate": 4.9695109799184594e-05,
      "loss": 4.012,
      "step": 138300
    },
    {
      "epoch": 0.024409029012705922,
      "grad_norm": 6.0637946128845215,
      "learning_rate": 4.969488934191389e-05,
      "loss": 4.0582,
      "step": 138400
    },
    {
      "epoch": 0.0244266655943625,
      "grad_norm": 5.7585039138793945,
      "learning_rate": 4.969466888464318e-05,
      "loss": 4.0877,
      "step": 138500
    },
    {
      "epoch": 0.024444302176019082,
      "grad_norm": 5.463169097900391,
      "learning_rate": 4.969444842737247e-05,
      "loss": 4.0277,
      "step": 138600
    },
    {
      "epoch": 0.02446193875767566,
      "grad_norm": 6.441878318786621,
      "learning_rate": 4.969422797010176e-05,
      "loss": 3.9999,
      "step": 138700
    },
    {
      "epoch": 0.02447957533933224,
      "grad_norm": 7.605706691741943,
      "learning_rate": 4.969400751283106e-05,
      "loss": 3.9696,
      "step": 138800
    },
    {
      "epoch": 0.02449721192098882,
      "grad_norm": 10.313117027282715,
      "learning_rate": 4.9693787055560346e-05,
      "loss": 4.1317,
      "step": 138900
    },
    {
      "epoch": 0.0245148485026454,
      "grad_norm": 9.486960411071777,
      "learning_rate": 4.969356659828964e-05,
      "loss": 4.0429,
      "step": 139000
    },
    {
      "epoch": 0.02453248508430198,
      "grad_norm": 7.997204303741455,
      "learning_rate": 4.969334614101894e-05,
      "loss": 3.9942,
      "step": 139100
    },
    {
      "epoch": 0.024550121665958557,
      "grad_norm": 7.2518744468688965,
      "learning_rate": 4.9693125683748226e-05,
      "loss": 4.0608,
      "step": 139200
    },
    {
      "epoch": 0.02456775824761514,
      "grad_norm": 5.943393230438232,
      "learning_rate": 4.969290522647752e-05,
      "loss": 4.0531,
      "step": 139300
    },
    {
      "epoch": 0.024585394829271717,
      "grad_norm": 7.107843399047852,
      "learning_rate": 4.969268476920682e-05,
      "loss": 3.9478,
      "step": 139400
    },
    {
      "epoch": 0.024603031410928296,
      "grad_norm": 9.013851165771484,
      "learning_rate": 4.9692464311936106e-05,
      "loss": 3.9658,
      "step": 139500
    },
    {
      "epoch": 0.024620667992584874,
      "grad_norm": 7.152223110198975,
      "learning_rate": 4.96922438546654e-05,
      "loss": 3.9933,
      "step": 139600
    },
    {
      "epoch": 0.024638304574241456,
      "grad_norm": 11.617059707641602,
      "learning_rate": 4.96920233973947e-05,
      "loss": 3.9874,
      "step": 139700
    },
    {
      "epoch": 0.024655941155898035,
      "grad_norm": 6.462819576263428,
      "learning_rate": 4.9691802940123985e-05,
      "loss": 4.0599,
      "step": 139800
    },
    {
      "epoch": 0.024673577737554613,
      "grad_norm": 7.345839500427246,
      "learning_rate": 4.9691582482853274e-05,
      "loss": 4.0863,
      "step": 139900
    },
    {
      "epoch": 0.02469121431921119,
      "grad_norm": 6.627739429473877,
      "learning_rate": 4.969136202558257e-05,
      "loss": 3.8736,
      "step": 140000
    },
    {
      "epoch": 0.024708850900867774,
      "grad_norm": 7.411902904510498,
      "learning_rate": 4.969114156831186e-05,
      "loss": 3.9901,
      "step": 140100
    },
    {
      "epoch": 0.024726487482524352,
      "grad_norm": 6.412590503692627,
      "learning_rate": 4.9690921111041154e-05,
      "loss": 4.0206,
      "step": 140200
    },
    {
      "epoch": 0.02474412406418093,
      "grad_norm": 5.118391513824463,
      "learning_rate": 4.969070065377045e-05,
      "loss": 4.0225,
      "step": 140300
    },
    {
      "epoch": 0.024761760645837513,
      "grad_norm": 5.631199359893799,
      "learning_rate": 4.969048019649974e-05,
      "loss": 4.0287,
      "step": 140400
    },
    {
      "epoch": 0.02477939722749409,
      "grad_norm": 8.795008659362793,
      "learning_rate": 4.969025973922903e-05,
      "loss": 4.0212,
      "step": 140500
    },
    {
      "epoch": 0.02479703380915067,
      "grad_norm": 6.847253799438477,
      "learning_rate": 4.969003928195833e-05,
      "loss": 4.0968,
      "step": 140600
    },
    {
      "epoch": 0.024814670390807248,
      "grad_norm": 5.467113018035889,
      "learning_rate": 4.968981882468762e-05,
      "loss": 4.0905,
      "step": 140700
    },
    {
      "epoch": 0.02483230697246383,
      "grad_norm": 5.239996433258057,
      "learning_rate": 4.968959836741691e-05,
      "loss": 4.0943,
      "step": 140800
    },
    {
      "epoch": 0.02484994355412041,
      "grad_norm": 10.681255340576172,
      "learning_rate": 4.968937791014621e-05,
      "loss": 4.0372,
      "step": 140900
    },
    {
      "epoch": 0.024867580135776987,
      "grad_norm": 5.628304958343506,
      "learning_rate": 4.96891574528755e-05,
      "loss": 4.0319,
      "step": 141000
    },
    {
      "epoch": 0.024885216717433566,
      "grad_norm": 7.192749977111816,
      "learning_rate": 4.968893699560479e-05,
      "loss": 4.0009,
      "step": 141100
    },
    {
      "epoch": 0.024902853299090148,
      "grad_norm": 8.8938570022583,
      "learning_rate": 4.968871653833409e-05,
      "loss": 4.052,
      "step": 141200
    },
    {
      "epoch": 0.024920489880746726,
      "grad_norm": 8.541935920715332,
      "learning_rate": 4.968849608106338e-05,
      "loss": 3.9523,
      "step": 141300
    },
    {
      "epoch": 0.024938126462403305,
      "grad_norm": 4.599579811096191,
      "learning_rate": 4.9688275623792665e-05,
      "loss": 3.9791,
      "step": 141400
    },
    {
      "epoch": 0.024955763044059883,
      "grad_norm": 5.647940635681152,
      "learning_rate": 4.968805516652196e-05,
      "loss": 4.1121,
      "step": 141500
    },
    {
      "epoch": 0.024973399625716465,
      "grad_norm": 7.239049911499023,
      "learning_rate": 4.968783470925125e-05,
      "loss": 4.046,
      "step": 141600
    },
    {
      "epoch": 0.024991036207373044,
      "grad_norm": 13.86254596710205,
      "learning_rate": 4.9687614251980545e-05,
      "loss": 3.956,
      "step": 141700
    },
    {
      "epoch": 0.025008672789029622,
      "grad_norm": 4.358945846557617,
      "learning_rate": 4.968739379470984e-05,
      "loss": 4.0345,
      "step": 141800
    },
    {
      "epoch": 0.025026309370686204,
      "grad_norm": 7.682425498962402,
      "learning_rate": 4.968717333743913e-05,
      "loss": 4.078,
      "step": 141900
    },
    {
      "epoch": 0.025043945952342783,
      "grad_norm": 7.055861949920654,
      "learning_rate": 4.9686952880168425e-05,
      "loss": 3.9874,
      "step": 142000
    },
    {
      "epoch": 0.02506158253399936,
      "grad_norm": 8.283699035644531,
      "learning_rate": 4.968673242289772e-05,
      "loss": 3.9489,
      "step": 142100
    },
    {
      "epoch": 0.02507921911565594,
      "grad_norm": 8.813244819641113,
      "learning_rate": 4.968651196562701e-05,
      "loss": 3.8973,
      "step": 142200
    },
    {
      "epoch": 0.02509685569731252,
      "grad_norm": 7.620509147644043,
      "learning_rate": 4.9686291508356304e-05,
      "loss": 4.1231,
      "step": 142300
    },
    {
      "epoch": 0.0251144922789691,
      "grad_norm": 8.970914840698242,
      "learning_rate": 4.96860710510856e-05,
      "loss": 4.0179,
      "step": 142400
    },
    {
      "epoch": 0.02513212886062568,
      "grad_norm": 6.521573543548584,
      "learning_rate": 4.968585059381489e-05,
      "loss": 3.9261,
      "step": 142500
    },
    {
      "epoch": 0.025149765442282257,
      "grad_norm": 7.8286004066467285,
      "learning_rate": 4.9685630136544184e-05,
      "loss": 4.111,
      "step": 142600
    },
    {
      "epoch": 0.02516740202393884,
      "grad_norm": 3.8360681533813477,
      "learning_rate": 4.968540967927347e-05,
      "loss": 3.9836,
      "step": 142700
    },
    {
      "epoch": 0.025185038605595417,
      "grad_norm": 9.417126655578613,
      "learning_rate": 4.968518922200277e-05,
      "loss": 4.0757,
      "step": 142800
    },
    {
      "epoch": 0.025202675187251996,
      "grad_norm": 8.595770835876465,
      "learning_rate": 4.968496876473206e-05,
      "loss": 3.926,
      "step": 142900
    },
    {
      "epoch": 0.025220311768908574,
      "grad_norm": 5.421635627746582,
      "learning_rate": 4.968474830746135e-05,
      "loss": 3.9548,
      "step": 143000
    },
    {
      "epoch": 0.025237948350565156,
      "grad_norm": 7.438559055328369,
      "learning_rate": 4.968452785019064e-05,
      "loss": 3.9619,
      "step": 143100
    },
    {
      "epoch": 0.025255584932221735,
      "grad_norm": 6.8583784103393555,
      "learning_rate": 4.9684307392919936e-05,
      "loss": 3.9647,
      "step": 143200
    },
    {
      "epoch": 0.025273221513878313,
      "grad_norm": 6.247711181640625,
      "learning_rate": 4.968408693564923e-05,
      "loss": 4.0624,
      "step": 143300
    },
    {
      "epoch": 0.025290858095534895,
      "grad_norm": 7.857727527618408,
      "learning_rate": 4.968386647837852e-05,
      "loss": 3.9614,
      "step": 143400
    },
    {
      "epoch": 0.025308494677191474,
      "grad_norm": 7.4477715492248535,
      "learning_rate": 4.9683646021107816e-05,
      "loss": 4.06,
      "step": 143500
    },
    {
      "epoch": 0.025326131258848052,
      "grad_norm": 7.467872142791748,
      "learning_rate": 4.968342556383711e-05,
      "loss": 4.0387,
      "step": 143600
    },
    {
      "epoch": 0.02534376784050463,
      "grad_norm": 5.948078632354736,
      "learning_rate": 4.96832051065664e-05,
      "loss": 3.9923,
      "step": 143700
    },
    {
      "epoch": 0.025361404422161213,
      "grad_norm": 11.300790786743164,
      "learning_rate": 4.9682984649295696e-05,
      "loss": 4.0924,
      "step": 143800
    },
    {
      "epoch": 0.02537904100381779,
      "grad_norm": 5.5174078941345215,
      "learning_rate": 4.968276419202499e-05,
      "loss": 3.9139,
      "step": 143900
    },
    {
      "epoch": 0.02539667758547437,
      "grad_norm": 5.312154293060303,
      "learning_rate": 4.968254373475428e-05,
      "loss": 3.9557,
      "step": 144000
    },
    {
      "epoch": 0.02541431416713095,
      "grad_norm": 6.367043495178223,
      "learning_rate": 4.9682323277483575e-05,
      "loss": 4.0425,
      "step": 144100
    },
    {
      "epoch": 0.02543195074878753,
      "grad_norm": 5.6045026779174805,
      "learning_rate": 4.9682102820212864e-05,
      "loss": 3.9185,
      "step": 144200
    },
    {
      "epoch": 0.02544958733044411,
      "grad_norm": 5.645009517669678,
      "learning_rate": 4.968188236294215e-05,
      "loss": 3.8912,
      "step": 144300
    },
    {
      "epoch": 0.025467223912100687,
      "grad_norm": 7.411245822906494,
      "learning_rate": 4.968166190567145e-05,
      "loss": 3.9387,
      "step": 144400
    },
    {
      "epoch": 0.025484860493757266,
      "grad_norm": 8.027422904968262,
      "learning_rate": 4.9681441448400744e-05,
      "loss": 4.0982,
      "step": 144500
    },
    {
      "epoch": 0.025502497075413848,
      "grad_norm": 10.745973587036133,
      "learning_rate": 4.968122099113003e-05,
      "loss": 4.0155,
      "step": 144600
    },
    {
      "epoch": 0.025520133657070426,
      "grad_norm": 8.74436092376709,
      "learning_rate": 4.968100053385933e-05,
      "loss": 3.9764,
      "step": 144700
    },
    {
      "epoch": 0.025537770238727005,
      "grad_norm": 10.12285041809082,
      "learning_rate": 4.968078007658862e-05,
      "loss": 4.0336,
      "step": 144800
    },
    {
      "epoch": 0.025555406820383587,
      "grad_norm": 6.8894805908203125,
      "learning_rate": 4.968055961931791e-05,
      "loss": 4.0113,
      "step": 144900
    },
    {
      "epoch": 0.025573043402040165,
      "grad_norm": 6.924065589904785,
      "learning_rate": 4.968033916204721e-05,
      "loss": 3.9719,
      "step": 145000
    },
    {
      "epoch": 0.025590679983696744,
      "grad_norm": 6.393929481506348,
      "learning_rate": 4.96801187047765e-05,
      "loss": 3.9758,
      "step": 145100
    },
    {
      "epoch": 0.025608316565353322,
      "grad_norm": 5.146887302398682,
      "learning_rate": 4.967989824750579e-05,
      "loss": 3.9882,
      "step": 145200
    },
    {
      "epoch": 0.025625953147009904,
      "grad_norm": 5.566965103149414,
      "learning_rate": 4.967967779023509e-05,
      "loss": 3.983,
      "step": 145300
    },
    {
      "epoch": 0.025643589728666483,
      "grad_norm": 6.270830154418945,
      "learning_rate": 4.967945733296438e-05,
      "loss": 3.9122,
      "step": 145400
    },
    {
      "epoch": 0.02566122631032306,
      "grad_norm": 5.814508438110352,
      "learning_rate": 4.967923687569367e-05,
      "loss": 4.0709,
      "step": 145500
    },
    {
      "epoch": 0.02567886289197964,
      "grad_norm": 7.164484024047852,
      "learning_rate": 4.967901641842297e-05,
      "loss": 3.9946,
      "step": 145600
    },
    {
      "epoch": 0.02569649947363622,
      "grad_norm": 5.784485816955566,
      "learning_rate": 4.9678795961152255e-05,
      "loss": 3.9924,
      "step": 145700
    },
    {
      "epoch": 0.0257141360552928,
      "grad_norm": 6.561059951782227,
      "learning_rate": 4.9678575503881544e-05,
      "loss": 3.9534,
      "step": 145800
    },
    {
      "epoch": 0.02573177263694938,
      "grad_norm": 7.677093982696533,
      "learning_rate": 4.967835504661084e-05,
      "loss": 4.0258,
      "step": 145900
    },
    {
      "epoch": 0.025749409218605957,
      "grad_norm": 5.896761894226074,
      "learning_rate": 4.9678134589340135e-05,
      "loss": 3.9451,
      "step": 146000
    },
    {
      "epoch": 0.02576704580026254,
      "grad_norm": 5.826980113983154,
      "learning_rate": 4.9677914132069424e-05,
      "loss": 3.9866,
      "step": 146100
    },
    {
      "epoch": 0.025784682381919118,
      "grad_norm": 11.668745994567871,
      "learning_rate": 4.967769367479872e-05,
      "loss": 4.078,
      "step": 146200
    },
    {
      "epoch": 0.025802318963575696,
      "grad_norm": 5.8782267570495605,
      "learning_rate": 4.9677473217528015e-05,
      "loss": 4.0686,
      "step": 146300
    },
    {
      "epoch": 0.025819955545232278,
      "grad_norm": 8.388303756713867,
      "learning_rate": 4.96772527602573e-05,
      "loss": 3.9365,
      "step": 146400
    },
    {
      "epoch": 0.025837592126888857,
      "grad_norm": 7.755557060241699,
      "learning_rate": 4.96770323029866e-05,
      "loss": 3.8503,
      "step": 146500
    },
    {
      "epoch": 0.025855228708545435,
      "grad_norm": 12.262781143188477,
      "learning_rate": 4.9676811845715894e-05,
      "loss": 4.023,
      "step": 146600
    },
    {
      "epoch": 0.025872865290202014,
      "grad_norm": 7.0426201820373535,
      "learning_rate": 4.967659138844518e-05,
      "loss": 4.0567,
      "step": 146700
    },
    {
      "epoch": 0.025890501871858596,
      "grad_norm": 7.175382137298584,
      "learning_rate": 4.967637093117448e-05,
      "loss": 4.0683,
      "step": 146800
    },
    {
      "epoch": 0.025908138453515174,
      "grad_norm": 5.771072864532471,
      "learning_rate": 4.9676150473903774e-05,
      "loss": 4.1114,
      "step": 146900
    },
    {
      "epoch": 0.025925775035171753,
      "grad_norm": 7.754361629486084,
      "learning_rate": 4.967593001663306e-05,
      "loss": 4.0094,
      "step": 147000
    },
    {
      "epoch": 0.02594341161682833,
      "grad_norm": 9.004331588745117,
      "learning_rate": 4.967570955936235e-05,
      "loss": 3.9316,
      "step": 147100
    },
    {
      "epoch": 0.025961048198484913,
      "grad_norm": 7.234943866729736,
      "learning_rate": 4.967548910209165e-05,
      "loss": 3.9574,
      "step": 147200
    },
    {
      "epoch": 0.02597868478014149,
      "grad_norm": 6.202936172485352,
      "learning_rate": 4.9675268644820935e-05,
      "loss": 3.9567,
      "step": 147300
    },
    {
      "epoch": 0.02599632136179807,
      "grad_norm": 6.792755603790283,
      "learning_rate": 4.967504818755023e-05,
      "loss": 3.9787,
      "step": 147400
    },
    {
      "epoch": 0.02601395794345465,
      "grad_norm": 6.330587863922119,
      "learning_rate": 4.9674827730279526e-05,
      "loss": 3.9845,
      "step": 147500
    },
    {
      "epoch": 0.02603159452511123,
      "grad_norm": 9.237180709838867,
      "learning_rate": 4.9674607273008815e-05,
      "loss": 4.0144,
      "step": 147600
    },
    {
      "epoch": 0.02604923110676781,
      "grad_norm": 6.102980136871338,
      "learning_rate": 4.967438681573811e-05,
      "loss": 4.0488,
      "step": 147700
    },
    {
      "epoch": 0.026066867688424387,
      "grad_norm": 7.802971839904785,
      "learning_rate": 4.9674166358467406e-05,
      "loss": 4.0041,
      "step": 147800
    },
    {
      "epoch": 0.02608450427008097,
      "grad_norm": 6.321959018707275,
      "learning_rate": 4.96739459011967e-05,
      "loss": 4.0595,
      "step": 147900
    },
    {
      "epoch": 0.026102140851737548,
      "grad_norm": 6.645859241485596,
      "learning_rate": 4.967372544392599e-05,
      "loss": 3.9607,
      "step": 148000
    },
    {
      "epoch": 0.026119777433394126,
      "grad_norm": 7.348761558532715,
      "learning_rate": 4.9673504986655286e-05,
      "loss": 3.9977,
      "step": 148100
    },
    {
      "epoch": 0.026137414015050705,
      "grad_norm": 6.31704044342041,
      "learning_rate": 4.967328452938458e-05,
      "loss": 3.959,
      "step": 148200
    },
    {
      "epoch": 0.026155050596707287,
      "grad_norm": 8.979790687561035,
      "learning_rate": 4.967306407211387e-05,
      "loss": 4.0558,
      "step": 148300
    },
    {
      "epoch": 0.026172687178363865,
      "grad_norm": 7.896569728851318,
      "learning_rate": 4.9672843614843165e-05,
      "loss": 3.9341,
      "step": 148400
    },
    {
      "epoch": 0.026190323760020444,
      "grad_norm": 8.187484741210938,
      "learning_rate": 4.9672623157572454e-05,
      "loss": 3.9882,
      "step": 148500
    },
    {
      "epoch": 0.026207960341677022,
      "grad_norm": 7.011857032775879,
      "learning_rate": 4.967240270030174e-05,
      "loss": 3.99,
      "step": 148600
    },
    {
      "epoch": 0.026225596923333604,
      "grad_norm": 9.141195297241211,
      "learning_rate": 4.967218224303104e-05,
      "loss": 3.9518,
      "step": 148700
    },
    {
      "epoch": 0.026243233504990183,
      "grad_norm": 10.292717933654785,
      "learning_rate": 4.9671961785760334e-05,
      "loss": 4.0466,
      "step": 148800
    },
    {
      "epoch": 0.02626087008664676,
      "grad_norm": 7.974295139312744,
      "learning_rate": 4.967174132848962e-05,
      "loss": 4.0245,
      "step": 148900
    },
    {
      "epoch": 0.02627850666830334,
      "grad_norm": 6.385110855102539,
      "learning_rate": 4.967152087121892e-05,
      "loss": 4.1406,
      "step": 149000
    },
    {
      "epoch": 0.026296143249959922,
      "grad_norm": 6.031630992889404,
      "learning_rate": 4.967130041394821e-05,
      "loss": 4.061,
      "step": 149100
    },
    {
      "epoch": 0.0263137798316165,
      "grad_norm": 12.172094345092773,
      "learning_rate": 4.96710799566775e-05,
      "loss": 4.0648,
      "step": 149200
    },
    {
      "epoch": 0.02633141641327308,
      "grad_norm": 6.163558006286621,
      "learning_rate": 4.96708594994068e-05,
      "loss": 3.9986,
      "step": 149300
    },
    {
      "epoch": 0.02634905299492966,
      "grad_norm": 7.652120113372803,
      "learning_rate": 4.967063904213609e-05,
      "loss": 3.9334,
      "step": 149400
    },
    {
      "epoch": 0.02636668957658624,
      "grad_norm": 5.681727409362793,
      "learning_rate": 4.967041858486538e-05,
      "loss": 4.0181,
      "step": 149500
    },
    {
      "epoch": 0.026384326158242818,
      "grad_norm": 9.691338539123535,
      "learning_rate": 4.967019812759468e-05,
      "loss": 3.9343,
      "step": 149600
    },
    {
      "epoch": 0.026401962739899396,
      "grad_norm": 7.496194839477539,
      "learning_rate": 4.966997767032397e-05,
      "loss": 3.9713,
      "step": 149700
    },
    {
      "epoch": 0.026419599321555978,
      "grad_norm": 5.779184818267822,
      "learning_rate": 4.966975721305326e-05,
      "loss": 3.9534,
      "step": 149800
    },
    {
      "epoch": 0.026437235903212557,
      "grad_norm": 5.655357360839844,
      "learning_rate": 4.966953675578255e-05,
      "loss": 4.0131,
      "step": 149900
    },
    {
      "epoch": 0.026454872484869135,
      "grad_norm": 6.109676361083984,
      "learning_rate": 4.9669316298511845e-05,
      "loss": 4.0268,
      "step": 150000
    },
    {
      "epoch": 0.026472509066525714,
      "grad_norm": 5.656920909881592,
      "learning_rate": 4.9669095841241134e-05,
      "loss": 3.925,
      "step": 150100
    },
    {
      "epoch": 0.026490145648182296,
      "grad_norm": 5.649477481842041,
      "learning_rate": 4.966887538397043e-05,
      "loss": 4.1203,
      "step": 150200
    },
    {
      "epoch": 0.026507782229838874,
      "grad_norm": 7.486598968505859,
      "learning_rate": 4.9668654926699725e-05,
      "loss": 4.0084,
      "step": 150300
    },
    {
      "epoch": 0.026525418811495453,
      "grad_norm": 8.25161075592041,
      "learning_rate": 4.9668434469429014e-05,
      "loss": 4.0395,
      "step": 150400
    },
    {
      "epoch": 0.02654305539315203,
      "grad_norm": 6.68951940536499,
      "learning_rate": 4.966821401215831e-05,
      "loss": 3.9426,
      "step": 150500
    },
    {
      "epoch": 0.026560691974808613,
      "grad_norm": 8.655784606933594,
      "learning_rate": 4.9667993554887605e-05,
      "loss": 4.0696,
      "step": 150600
    },
    {
      "epoch": 0.02657832855646519,
      "grad_norm": 6.8373236656188965,
      "learning_rate": 4.966777309761689e-05,
      "loss": 3.9632,
      "step": 150700
    },
    {
      "epoch": 0.02659596513812177,
      "grad_norm": 6.739715576171875,
      "learning_rate": 4.966755264034619e-05,
      "loss": 3.9848,
      "step": 150800
    },
    {
      "epoch": 0.026613601719778352,
      "grad_norm": 7.389422416687012,
      "learning_rate": 4.9667332183075484e-05,
      "loss": 3.9371,
      "step": 150900
    },
    {
      "epoch": 0.02663123830143493,
      "grad_norm": 4.908675193786621,
      "learning_rate": 4.966711172580477e-05,
      "loss": 4.0938,
      "step": 151000
    },
    {
      "epoch": 0.02664887488309151,
      "grad_norm": 12.981832504272461,
      "learning_rate": 4.966689126853407e-05,
      "loss": 3.9254,
      "step": 151100
    },
    {
      "epoch": 0.026666511464748088,
      "grad_norm": 8.461431503295898,
      "learning_rate": 4.9666670811263364e-05,
      "loss": 3.9848,
      "step": 151200
    },
    {
      "epoch": 0.02668414804640467,
      "grad_norm": 6.647451877593994,
      "learning_rate": 4.966645035399265e-05,
      "loss": 3.9828,
      "step": 151300
    },
    {
      "epoch": 0.026701784628061248,
      "grad_norm": 4.871875286102295,
      "learning_rate": 4.966622989672194e-05,
      "loss": 4.0174,
      "step": 151400
    },
    {
      "epoch": 0.026719421209717827,
      "grad_norm": 6.078271865844727,
      "learning_rate": 4.966600943945124e-05,
      "loss": 4.0622,
      "step": 151500
    },
    {
      "epoch": 0.026737057791374405,
      "grad_norm": 5.913357734680176,
      "learning_rate": 4.9665788982180525e-05,
      "loss": 3.9302,
      "step": 151600
    },
    {
      "epoch": 0.026754694373030987,
      "grad_norm": 7.067214488983154,
      "learning_rate": 4.966556852490982e-05,
      "loss": 4.0219,
      "step": 151700
    },
    {
      "epoch": 0.026772330954687566,
      "grad_norm": 7.219533443450928,
      "learning_rate": 4.9665348067639116e-05,
      "loss": 4.0162,
      "step": 151800
    },
    {
      "epoch": 0.026789967536344144,
      "grad_norm": 5.726380825042725,
      "learning_rate": 4.9665127610368405e-05,
      "loss": 3.9815,
      "step": 151900
    },
    {
      "epoch": 0.026807604118000723,
      "grad_norm": 5.6216254234313965,
      "learning_rate": 4.96649071530977e-05,
      "loss": 4.0023,
      "step": 152000
    },
    {
      "epoch": 0.026825240699657305,
      "grad_norm": 6.872640609741211,
      "learning_rate": 4.9664686695826996e-05,
      "loss": 4.0452,
      "step": 152100
    },
    {
      "epoch": 0.026842877281313883,
      "grad_norm": 7.69218111038208,
      "learning_rate": 4.9664466238556285e-05,
      "loss": 4.0026,
      "step": 152200
    },
    {
      "epoch": 0.02686051386297046,
      "grad_norm": 7.557948112487793,
      "learning_rate": 4.966424578128558e-05,
      "loss": 4.0344,
      "step": 152300
    },
    {
      "epoch": 0.026878150444627043,
      "grad_norm": 7.8473992347717285,
      "learning_rate": 4.9664025324014876e-05,
      "loss": 4.0495,
      "step": 152400
    },
    {
      "epoch": 0.026895787026283622,
      "grad_norm": 5.144616603851318,
      "learning_rate": 4.9663804866744164e-05,
      "loss": 3.9962,
      "step": 152500
    },
    {
      "epoch": 0.0269134236079402,
      "grad_norm": 7.8782734870910645,
      "learning_rate": 4.966358440947346e-05,
      "loss": 4.0332,
      "step": 152600
    },
    {
      "epoch": 0.02693106018959678,
      "grad_norm": 7.870491981506348,
      "learning_rate": 4.966336395220275e-05,
      "loss": 4.0757,
      "step": 152700
    },
    {
      "epoch": 0.02694869677125336,
      "grad_norm": 8.590414047241211,
      "learning_rate": 4.9663143494932044e-05,
      "loss": 3.8887,
      "step": 152800
    },
    {
      "epoch": 0.02696633335290994,
      "grad_norm": 7.660171985626221,
      "learning_rate": 4.966292303766133e-05,
      "loss": 4.0646,
      "step": 152900
    },
    {
      "epoch": 0.026983969934566518,
      "grad_norm": 6.745847702026367,
      "learning_rate": 4.966270258039063e-05,
      "loss": 3.9875,
      "step": 153000
    },
    {
      "epoch": 0.027001606516223096,
      "grad_norm": 6.558876991271973,
      "learning_rate": 4.966248212311992e-05,
      "loss": 4.0182,
      "step": 153100
    },
    {
      "epoch": 0.02701924309787968,
      "grad_norm": 8.205655097961426,
      "learning_rate": 4.966226166584921e-05,
      "loss": 4.0522,
      "step": 153200
    },
    {
      "epoch": 0.027036879679536257,
      "grad_norm": 6.030440807342529,
      "learning_rate": 4.966204120857851e-05,
      "loss": 3.8728,
      "step": 153300
    },
    {
      "epoch": 0.027054516261192835,
      "grad_norm": 6.527510166168213,
      "learning_rate": 4.9661820751307796e-05,
      "loss": 4.0243,
      "step": 153400
    },
    {
      "epoch": 0.027072152842849414,
      "grad_norm": 7.786890029907227,
      "learning_rate": 4.966160029403709e-05,
      "loss": 4.0385,
      "step": 153500
    },
    {
      "epoch": 0.027089789424505996,
      "grad_norm": 14.00351333618164,
      "learning_rate": 4.966137983676639e-05,
      "loss": 4.0345,
      "step": 153600
    },
    {
      "epoch": 0.027107426006162574,
      "grad_norm": 7.4350433349609375,
      "learning_rate": 4.9661159379495676e-05,
      "loss": 3.995,
      "step": 153700
    },
    {
      "epoch": 0.027125062587819153,
      "grad_norm": 8.116083145141602,
      "learning_rate": 4.966093892222497e-05,
      "loss": 4.0181,
      "step": 153800
    },
    {
      "epoch": 0.027142699169475735,
      "grad_norm": 5.193649768829346,
      "learning_rate": 4.966071846495427e-05,
      "loss": 4.0129,
      "step": 153900
    },
    {
      "epoch": 0.027160335751132313,
      "grad_norm": 5.250637054443359,
      "learning_rate": 4.9660498007683556e-05,
      "loss": 4.0468,
      "step": 154000
    },
    {
      "epoch": 0.027177972332788892,
      "grad_norm": 6.046564102172852,
      "learning_rate": 4.966027755041285e-05,
      "loss": 3.9255,
      "step": 154100
    },
    {
      "epoch": 0.02719560891444547,
      "grad_norm": 12.270405769348145,
      "learning_rate": 4.966005709314214e-05,
      "loss": 3.9485,
      "step": 154200
    },
    {
      "epoch": 0.027213245496102052,
      "grad_norm": 7.862817287445068,
      "learning_rate": 4.965983663587143e-05,
      "loss": 3.9659,
      "step": 154300
    },
    {
      "epoch": 0.02723088207775863,
      "grad_norm": 5.458335876464844,
      "learning_rate": 4.9659616178600724e-05,
      "loss": 4.0398,
      "step": 154400
    },
    {
      "epoch": 0.02724851865941521,
      "grad_norm": 6.5829386711120605,
      "learning_rate": 4.965939572133002e-05,
      "loss": 3.9851,
      "step": 154500
    },
    {
      "epoch": 0.027266155241071788,
      "grad_norm": 7.1166510581970215,
      "learning_rate": 4.965917526405931e-05,
      "loss": 3.8865,
      "step": 154600
    },
    {
      "epoch": 0.02728379182272837,
      "grad_norm": 6.6843390464782715,
      "learning_rate": 4.9658954806788604e-05,
      "loss": 3.922,
      "step": 154700
    },
    {
      "epoch": 0.027301428404384948,
      "grad_norm": 6.506069660186768,
      "learning_rate": 4.96587343495179e-05,
      "loss": 4.0469,
      "step": 154800
    },
    {
      "epoch": 0.027319064986041527,
      "grad_norm": 5.695384979248047,
      "learning_rate": 4.965851389224719e-05,
      "loss": 3.9751,
      "step": 154900
    },
    {
      "epoch": 0.027336701567698105,
      "grad_norm": 6.121545791625977,
      "learning_rate": 4.965829343497648e-05,
      "loss": 3.9251,
      "step": 155000
    },
    {
      "epoch": 0.027354338149354687,
      "grad_norm": 6.427226543426514,
      "learning_rate": 4.965807297770578e-05,
      "loss": 4.0047,
      "step": 155100
    },
    {
      "epoch": 0.027371974731011266,
      "grad_norm": 7.0701775550842285,
      "learning_rate": 4.965785252043507e-05,
      "loss": 4.042,
      "step": 155200
    },
    {
      "epoch": 0.027389611312667844,
      "grad_norm": 11.852726936340332,
      "learning_rate": 4.965763206316436e-05,
      "loss": 3.9482,
      "step": 155300
    },
    {
      "epoch": 0.027407247894324426,
      "grad_norm": 5.788224697113037,
      "learning_rate": 4.965741160589366e-05,
      "loss": 4.0871,
      "step": 155400
    },
    {
      "epoch": 0.027424884475981005,
      "grad_norm": 9.58438777923584,
      "learning_rate": 4.965719114862295e-05,
      "loss": 3.9513,
      "step": 155500
    },
    {
      "epoch": 0.027442521057637583,
      "grad_norm": 7.777733325958252,
      "learning_rate": 4.965697069135224e-05,
      "loss": 4.0165,
      "step": 155600
    },
    {
      "epoch": 0.02746015763929416,
      "grad_norm": 5.97398567199707,
      "learning_rate": 4.965675023408153e-05,
      "loss": 3.9573,
      "step": 155700
    },
    {
      "epoch": 0.027477794220950744,
      "grad_norm": 6.055402755737305,
      "learning_rate": 4.965652977681082e-05,
      "loss": 4.0717,
      "step": 155800
    },
    {
      "epoch": 0.027495430802607322,
      "grad_norm": 5.853491306304932,
      "learning_rate": 4.9656309319540115e-05,
      "loss": 3.9491,
      "step": 155900
    },
    {
      "epoch": 0.0275130673842639,
      "grad_norm": 4.966978549957275,
      "learning_rate": 4.965608886226941e-05,
      "loss": 4.0057,
      "step": 156000
    },
    {
      "epoch": 0.02753070396592048,
      "grad_norm": 6.862979888916016,
      "learning_rate": 4.96558684049987e-05,
      "loss": 3.9833,
      "step": 156100
    },
    {
      "epoch": 0.02754834054757706,
      "grad_norm": 7.3137125968933105,
      "learning_rate": 4.9655647947727995e-05,
      "loss": 4.0745,
      "step": 156200
    },
    {
      "epoch": 0.02756597712923364,
      "grad_norm": 6.277409076690674,
      "learning_rate": 4.965542749045729e-05,
      "loss": 4.0038,
      "step": 156300
    },
    {
      "epoch": 0.027583613710890218,
      "grad_norm": 6.835943698883057,
      "learning_rate": 4.965520703318658e-05,
      "loss": 3.9412,
      "step": 156400
    },
    {
      "epoch": 0.027601250292546797,
      "grad_norm": 6.606168270111084,
      "learning_rate": 4.9654986575915875e-05,
      "loss": 3.9606,
      "step": 156500
    },
    {
      "epoch": 0.02761888687420338,
      "grad_norm": 5.646399021148682,
      "learning_rate": 4.965476611864517e-05,
      "loss": 3.9359,
      "step": 156600
    },
    {
      "epoch": 0.027636523455859957,
      "grad_norm": 6.628448486328125,
      "learning_rate": 4.965454566137446e-05,
      "loss": 3.8985,
      "step": 156700
    },
    {
      "epoch": 0.027654160037516536,
      "grad_norm": 5.4586405754089355,
      "learning_rate": 4.9654325204103754e-05,
      "loss": 4.0531,
      "step": 156800
    },
    {
      "epoch": 0.027671796619173118,
      "grad_norm": 6.235082149505615,
      "learning_rate": 4.965410474683305e-05,
      "loss": 3.8867,
      "step": 156900
    },
    {
      "epoch": 0.027689433200829696,
      "grad_norm": 6.00203275680542,
      "learning_rate": 4.965388428956234e-05,
      "loss": 4.0227,
      "step": 157000
    },
    {
      "epoch": 0.027707069782486275,
      "grad_norm": 10.1784086227417,
      "learning_rate": 4.965366383229163e-05,
      "loss": 3.9306,
      "step": 157100
    },
    {
      "epoch": 0.027724706364142853,
      "grad_norm": 6.1385369300842285,
      "learning_rate": 4.965344337502092e-05,
      "loss": 3.9764,
      "step": 157200
    },
    {
      "epoch": 0.027742342945799435,
      "grad_norm": 6.7318010330200195,
      "learning_rate": 4.965322291775021e-05,
      "loss": 4.0295,
      "step": 157300
    },
    {
      "epoch": 0.027759979527456013,
      "grad_norm": 8.135452270507812,
      "learning_rate": 4.965300246047951e-05,
      "loss": 4.0053,
      "step": 157400
    },
    {
      "epoch": 0.027777616109112592,
      "grad_norm": 5.332188606262207,
      "learning_rate": 4.96527820032088e-05,
      "loss": 4.0221,
      "step": 157500
    },
    {
      "epoch": 0.02779525269076917,
      "grad_norm": 5.928988933563232,
      "learning_rate": 4.965256154593809e-05,
      "loss": 3.9063,
      "step": 157600
    },
    {
      "epoch": 0.027812889272425752,
      "grad_norm": 9.89107608795166,
      "learning_rate": 4.9652341088667386e-05,
      "loss": 3.8921,
      "step": 157700
    },
    {
      "epoch": 0.02783052585408233,
      "grad_norm": 5.84255313873291,
      "learning_rate": 4.965212063139668e-05,
      "loss": 4.0092,
      "step": 157800
    },
    {
      "epoch": 0.02784816243573891,
      "grad_norm": 8.286211967468262,
      "learning_rate": 4.965190017412597e-05,
      "loss": 4.0957,
      "step": 157900
    },
    {
      "epoch": 0.027865799017395488,
      "grad_norm": 7.440908432006836,
      "learning_rate": 4.9651679716855266e-05,
      "loss": 4.0347,
      "step": 158000
    },
    {
      "epoch": 0.02788343559905207,
      "grad_norm": 7.39103889465332,
      "learning_rate": 4.965145925958456e-05,
      "loss": 3.9984,
      "step": 158100
    },
    {
      "epoch": 0.02790107218070865,
      "grad_norm": 7.978909492492676,
      "learning_rate": 4.965123880231385e-05,
      "loss": 3.9493,
      "step": 158200
    },
    {
      "epoch": 0.027918708762365227,
      "grad_norm": 10.24013614654541,
      "learning_rate": 4.9651018345043146e-05,
      "loss": 3.9782,
      "step": 158300
    },
    {
      "epoch": 0.02793634534402181,
      "grad_norm": 5.89267635345459,
      "learning_rate": 4.965079788777244e-05,
      "loss": 4.0694,
      "step": 158400
    },
    {
      "epoch": 0.027953981925678387,
      "grad_norm": 6.3447265625,
      "learning_rate": 4.965057743050173e-05,
      "loss": 4.003,
      "step": 158500
    },
    {
      "epoch": 0.027971618507334966,
      "grad_norm": 6.161655902862549,
      "learning_rate": 4.965035697323102e-05,
      "loss": 3.9638,
      "step": 158600
    },
    {
      "epoch": 0.027989255088991544,
      "grad_norm": 4.804558277130127,
      "learning_rate": 4.9650136515960314e-05,
      "loss": 3.9203,
      "step": 158700
    },
    {
      "epoch": 0.028006891670648126,
      "grad_norm": 7.882320880889893,
      "learning_rate": 4.964991605868961e-05,
      "loss": 4.0956,
      "step": 158800
    },
    {
      "epoch": 0.028024528252304705,
      "grad_norm": 6.633650779724121,
      "learning_rate": 4.96496956014189e-05,
      "loss": 3.874,
      "step": 158900
    },
    {
      "epoch": 0.028042164833961283,
      "grad_norm": 7.612485885620117,
      "learning_rate": 4.9649475144148194e-05,
      "loss": 3.9463,
      "step": 159000
    },
    {
      "epoch": 0.028059801415617862,
      "grad_norm": 6.731201648712158,
      "learning_rate": 4.964925468687749e-05,
      "loss": 4.0187,
      "step": 159100
    },
    {
      "epoch": 0.028077437997274444,
      "grad_norm": 5.211479187011719,
      "learning_rate": 4.964903422960678e-05,
      "loss": 3.9786,
      "step": 159200
    },
    {
      "epoch": 0.028095074578931022,
      "grad_norm": 6.606971263885498,
      "learning_rate": 4.964881377233607e-05,
      "loss": 4.0394,
      "step": 159300
    },
    {
      "epoch": 0.0281127111605876,
      "grad_norm": 4.535529613494873,
      "learning_rate": 4.964859331506537e-05,
      "loss": 3.8298,
      "step": 159400
    },
    {
      "epoch": 0.02813034774224418,
      "grad_norm": 7.5320329666137695,
      "learning_rate": 4.964837285779466e-05,
      "loss": 4.0578,
      "step": 159500
    },
    {
      "epoch": 0.02814798432390076,
      "grad_norm": 10.896334648132324,
      "learning_rate": 4.964815240052395e-05,
      "loss": 3.959,
      "step": 159600
    },
    {
      "epoch": 0.02816562090555734,
      "grad_norm": 4.342674255371094,
      "learning_rate": 4.964793194325325e-05,
      "loss": 3.9202,
      "step": 159700
    },
    {
      "epoch": 0.02818325748721392,
      "grad_norm": 7.097509384155273,
      "learning_rate": 4.964771148598254e-05,
      "loss": 4.0095,
      "step": 159800
    },
    {
      "epoch": 0.0282008940688705,
      "grad_norm": 6.591973304748535,
      "learning_rate": 4.9647491028711826e-05,
      "loss": 3.7906,
      "step": 159900
    },
    {
      "epoch": 0.02821853065052708,
      "grad_norm": 7.869134902954102,
      "learning_rate": 4.964727057144112e-05,
      "loss": 3.9866,
      "step": 160000
    },
    {
      "epoch": 0.028236167232183657,
      "grad_norm": 14.71194839477539,
      "learning_rate": 4.964705011417041e-05,
      "loss": 3.9468,
      "step": 160100
    },
    {
      "epoch": 0.028253803813840236,
      "grad_norm": 11.482035636901855,
      "learning_rate": 4.9646829656899705e-05,
      "loss": 3.9271,
      "step": 160200
    },
    {
      "epoch": 0.028271440395496818,
      "grad_norm": 8.443450927734375,
      "learning_rate": 4.9646609199629e-05,
      "loss": 3.9451,
      "step": 160300
    },
    {
      "epoch": 0.028289076977153396,
      "grad_norm": 6.569412708282471,
      "learning_rate": 4.964638874235829e-05,
      "loss": 3.8776,
      "step": 160400
    },
    {
      "epoch": 0.028306713558809975,
      "grad_norm": 7.167259216308594,
      "learning_rate": 4.9646168285087585e-05,
      "loss": 4.1109,
      "step": 160500
    },
    {
      "epoch": 0.028324350140466553,
      "grad_norm": 6.7271904945373535,
      "learning_rate": 4.964594782781688e-05,
      "loss": 4.0273,
      "step": 160600
    },
    {
      "epoch": 0.028341986722123135,
      "grad_norm": 7.866915225982666,
      "learning_rate": 4.964572737054617e-05,
      "loss": 3.9497,
      "step": 160700
    },
    {
      "epoch": 0.028359623303779714,
      "grad_norm": 4.916324615478516,
      "learning_rate": 4.9645506913275465e-05,
      "loss": 3.9326,
      "step": 160800
    },
    {
      "epoch": 0.028377259885436292,
      "grad_norm": 6.9451189041137695,
      "learning_rate": 4.964528645600476e-05,
      "loss": 4.1102,
      "step": 160900
    },
    {
      "epoch": 0.02839489646709287,
      "grad_norm": 10.987838745117188,
      "learning_rate": 4.964506599873405e-05,
      "loss": 3.9581,
      "step": 161000
    },
    {
      "epoch": 0.028412533048749453,
      "grad_norm": 6.595731258392334,
      "learning_rate": 4.9644845541463344e-05,
      "loss": 3.9072,
      "step": 161100
    },
    {
      "epoch": 0.02843016963040603,
      "grad_norm": 6.41585111618042,
      "learning_rate": 4.964462508419264e-05,
      "loss": 4.0862,
      "step": 161200
    },
    {
      "epoch": 0.02844780621206261,
      "grad_norm": 8.837479591369629,
      "learning_rate": 4.964440462692193e-05,
      "loss": 3.9897,
      "step": 161300
    },
    {
      "epoch": 0.02846544279371919,
      "grad_norm": 13.529092788696289,
      "learning_rate": 4.964418416965122e-05,
      "loss": 3.988,
      "step": 161400
    },
    {
      "epoch": 0.02848307937537577,
      "grad_norm": 6.583240985870361,
      "learning_rate": 4.964396371238051e-05,
      "loss": 3.8796,
      "step": 161500
    },
    {
      "epoch": 0.02850071595703235,
      "grad_norm": 8.5767183303833,
      "learning_rate": 4.96437432551098e-05,
      "loss": 3.9588,
      "step": 161600
    },
    {
      "epoch": 0.028518352538688927,
      "grad_norm": 6.808130264282227,
      "learning_rate": 4.96435227978391e-05,
      "loss": 4.0527,
      "step": 161700
    },
    {
      "epoch": 0.02853598912034551,
      "grad_norm": 7.301492691040039,
      "learning_rate": 4.964330234056839e-05,
      "loss": 3.9086,
      "step": 161800
    },
    {
      "epoch": 0.028553625702002088,
      "grad_norm": 7.701179027557373,
      "learning_rate": 4.964308188329768e-05,
      "loss": 4.0059,
      "step": 161900
    },
    {
      "epoch": 0.028571262283658666,
      "grad_norm": 5.878876686096191,
      "learning_rate": 4.9642861426026976e-05,
      "loss": 4.1304,
      "step": 162000
    },
    {
      "epoch": 0.028588898865315245,
      "grad_norm": 6.9226226806640625,
      "learning_rate": 4.964264096875627e-05,
      "loss": 3.9795,
      "step": 162100
    },
    {
      "epoch": 0.028606535446971827,
      "grad_norm": 7.404670715332031,
      "learning_rate": 4.964242051148556e-05,
      "loss": 3.9215,
      "step": 162200
    },
    {
      "epoch": 0.028624172028628405,
      "grad_norm": 5.631311893463135,
      "learning_rate": 4.9642200054214856e-05,
      "loss": 3.986,
      "step": 162300
    },
    {
      "epoch": 0.028641808610284984,
      "grad_norm": 7.63995361328125,
      "learning_rate": 4.964197959694415e-05,
      "loss": 4.0496,
      "step": 162400
    },
    {
      "epoch": 0.028659445191941562,
      "grad_norm": 6.851282119750977,
      "learning_rate": 4.964175913967344e-05,
      "loss": 3.9124,
      "step": 162500
    },
    {
      "epoch": 0.028677081773598144,
      "grad_norm": 9.013340950012207,
      "learning_rate": 4.9641538682402736e-05,
      "loss": 3.8965,
      "step": 162600
    },
    {
      "epoch": 0.028694718355254722,
      "grad_norm": 9.169560432434082,
      "learning_rate": 4.9641318225132024e-05,
      "loss": 3.9759,
      "step": 162700
    },
    {
      "epoch": 0.0287123549369113,
      "grad_norm": 7.131947994232178,
      "learning_rate": 4.964109776786132e-05,
      "loss": 3.9912,
      "step": 162800
    },
    {
      "epoch": 0.028729991518567883,
      "grad_norm": 5.988112926483154,
      "learning_rate": 4.964087731059061e-05,
      "loss": 3.9021,
      "step": 162900
    },
    {
      "epoch": 0.02874762810022446,
      "grad_norm": 9.763683319091797,
      "learning_rate": 4.9640656853319904e-05,
      "loss": 3.982,
      "step": 163000
    },
    {
      "epoch": 0.02876526468188104,
      "grad_norm": 5.366955757141113,
      "learning_rate": 4.964043639604919e-05,
      "loss": 3.9481,
      "step": 163100
    },
    {
      "epoch": 0.02878290126353762,
      "grad_norm": 6.2937397956848145,
      "learning_rate": 4.964021593877849e-05,
      "loss": 3.9699,
      "step": 163200
    },
    {
      "epoch": 0.0288005378451942,
      "grad_norm": 8.151801109313965,
      "learning_rate": 4.9639995481507783e-05,
      "loss": 3.9472,
      "step": 163300
    },
    {
      "epoch": 0.02881817442685078,
      "grad_norm": 9.270998001098633,
      "learning_rate": 4.963977502423707e-05,
      "loss": 3.9938,
      "step": 163400
    },
    {
      "epoch": 0.028835811008507357,
      "grad_norm": 8.023876190185547,
      "learning_rate": 4.963955456696637e-05,
      "loss": 3.9848,
      "step": 163500
    },
    {
      "epoch": 0.028853447590163936,
      "grad_norm": 5.658787250518799,
      "learning_rate": 4.963933410969566e-05,
      "loss": 3.9285,
      "step": 163600
    },
    {
      "epoch": 0.028871084171820518,
      "grad_norm": 9.075058937072754,
      "learning_rate": 4.963911365242495e-05,
      "loss": 4.023,
      "step": 163700
    },
    {
      "epoch": 0.028888720753477096,
      "grad_norm": 6.985743999481201,
      "learning_rate": 4.963889319515425e-05,
      "loss": 3.9445,
      "step": 163800
    },
    {
      "epoch": 0.028906357335133675,
      "grad_norm": 5.069480895996094,
      "learning_rate": 4.963867273788354e-05,
      "loss": 3.9618,
      "step": 163900
    },
    {
      "epoch": 0.028923993916790253,
      "grad_norm": 9.455215454101562,
      "learning_rate": 4.963845228061283e-05,
      "loss": 3.9954,
      "step": 164000
    },
    {
      "epoch": 0.028941630498446835,
      "grad_norm": 8.433144569396973,
      "learning_rate": 4.963823182334213e-05,
      "loss": 4.0023,
      "step": 164100
    },
    {
      "epoch": 0.028959267080103414,
      "grad_norm": 7.645687580108643,
      "learning_rate": 4.9638011366071416e-05,
      "loss": 3.9746,
      "step": 164200
    },
    {
      "epoch": 0.028976903661759992,
      "grad_norm": 7.783392906188965,
      "learning_rate": 4.9637790908800704e-05,
      "loss": 3.9721,
      "step": 164300
    },
    {
      "epoch": 0.028994540243416574,
      "grad_norm": 5.785364151000977,
      "learning_rate": 4.963757045153e-05,
      "loss": 4.0289,
      "step": 164400
    },
    {
      "epoch": 0.029012176825073153,
      "grad_norm": 8.066031455993652,
      "learning_rate": 4.9637349994259295e-05,
      "loss": 3.8886,
      "step": 164500
    },
    {
      "epoch": 0.02902981340672973,
      "grad_norm": 11.737370491027832,
      "learning_rate": 4.9637129536988584e-05,
      "loss": 3.9227,
      "step": 164600
    },
    {
      "epoch": 0.02904744998838631,
      "grad_norm": 7.616678714752197,
      "learning_rate": 4.963690907971788e-05,
      "loss": 3.9054,
      "step": 164700
    },
    {
      "epoch": 0.029065086570042892,
      "grad_norm": 9.021932601928711,
      "learning_rate": 4.9636688622447175e-05,
      "loss": 3.9028,
      "step": 164800
    },
    {
      "epoch": 0.02908272315169947,
      "grad_norm": 6.394311904907227,
      "learning_rate": 4.9636468165176464e-05,
      "loss": 3.9436,
      "step": 164900
    },
    {
      "epoch": 0.02910035973335605,
      "grad_norm": 9.05533504486084,
      "learning_rate": 4.963624770790576e-05,
      "loss": 3.9952,
      "step": 165000
    },
    {
      "epoch": 0.029117996315012627,
      "grad_norm": 14.092299461364746,
      "learning_rate": 4.9636027250635054e-05,
      "loss": 3.9824,
      "step": 165100
    },
    {
      "epoch": 0.02913563289666921,
      "grad_norm": 5.314957141876221,
      "learning_rate": 4.963580679336434e-05,
      "loss": 4.0203,
      "step": 165200
    },
    {
      "epoch": 0.029153269478325788,
      "grad_norm": 6.908047199249268,
      "learning_rate": 4.963558633609364e-05,
      "loss": 4.0005,
      "step": 165300
    },
    {
      "epoch": 0.029170906059982366,
      "grad_norm": 8.299139022827148,
      "learning_rate": 4.9635365878822934e-05,
      "loss": 4.0065,
      "step": 165400
    },
    {
      "epoch": 0.029188542641638945,
      "grad_norm": 6.313272476196289,
      "learning_rate": 4.963514542155222e-05,
      "loss": 3.9674,
      "step": 165500
    },
    {
      "epoch": 0.029206179223295527,
      "grad_norm": 6.899411678314209,
      "learning_rate": 4.963492496428152e-05,
      "loss": 3.9551,
      "step": 165600
    },
    {
      "epoch": 0.029223815804952105,
      "grad_norm": 7.440223693847656,
      "learning_rate": 4.963470450701081e-05,
      "loss": 3.9652,
      "step": 165700
    },
    {
      "epoch": 0.029241452386608684,
      "grad_norm": 9.815351486206055,
      "learning_rate": 4.9634484049740096e-05,
      "loss": 3.9907,
      "step": 165800
    },
    {
      "epoch": 0.029259088968265266,
      "grad_norm": 6.865332126617432,
      "learning_rate": 4.963426359246939e-05,
      "loss": 4.0771,
      "step": 165900
    },
    {
      "epoch": 0.029276725549921844,
      "grad_norm": 7.5044660568237305,
      "learning_rate": 4.9634043135198687e-05,
      "loss": 3.9713,
      "step": 166000
    },
    {
      "epoch": 0.029294362131578423,
      "grad_norm": 7.057561874389648,
      "learning_rate": 4.9633822677927975e-05,
      "loss": 4.0609,
      "step": 166100
    },
    {
      "epoch": 0.029311998713235,
      "grad_norm": 7.3285698890686035,
      "learning_rate": 4.963360222065727e-05,
      "loss": 3.9557,
      "step": 166200
    },
    {
      "epoch": 0.029329635294891583,
      "grad_norm": 6.661628723144531,
      "learning_rate": 4.9633381763386566e-05,
      "loss": 3.961,
      "step": 166300
    },
    {
      "epoch": 0.02934727187654816,
      "grad_norm": 11.545860290527344,
      "learning_rate": 4.9633161306115855e-05,
      "loss": 3.9295,
      "step": 166400
    },
    {
      "epoch": 0.02936490845820474,
      "grad_norm": 6.0715765953063965,
      "learning_rate": 4.963294084884515e-05,
      "loss": 3.9272,
      "step": 166500
    },
    {
      "epoch": 0.02938254503986132,
      "grad_norm": 5.397994041442871,
      "learning_rate": 4.9632720391574446e-05,
      "loss": 3.9234,
      "step": 166600
    },
    {
      "epoch": 0.0294001816215179,
      "grad_norm": 9.668142318725586,
      "learning_rate": 4.9632499934303735e-05,
      "loss": 3.854,
      "step": 166700
    },
    {
      "epoch": 0.02941781820317448,
      "grad_norm": 6.627072811126709,
      "learning_rate": 4.963227947703303e-05,
      "loss": 4.0344,
      "step": 166800
    },
    {
      "epoch": 0.029435454784831058,
      "grad_norm": 7.353672981262207,
      "learning_rate": 4.9632059019762325e-05,
      "loss": 4.0144,
      "step": 166900
    },
    {
      "epoch": 0.029453091366487636,
      "grad_norm": 6.574998378753662,
      "learning_rate": 4.9631838562491614e-05,
      "loss": 3.9659,
      "step": 167000
    },
    {
      "epoch": 0.029470727948144218,
      "grad_norm": 6.748349189758301,
      "learning_rate": 4.96316181052209e-05,
      "loss": 4.1219,
      "step": 167100
    },
    {
      "epoch": 0.029488364529800797,
      "grad_norm": 7.837881088256836,
      "learning_rate": 4.96313976479502e-05,
      "loss": 3.967,
      "step": 167200
    },
    {
      "epoch": 0.029506001111457375,
      "grad_norm": 5.795040130615234,
      "learning_rate": 4.963117719067949e-05,
      "loss": 3.9444,
      "step": 167300
    },
    {
      "epoch": 0.029523637693113957,
      "grad_norm": 8.480310440063477,
      "learning_rate": 4.963095673340878e-05,
      "loss": 3.8732,
      "step": 167400
    },
    {
      "epoch": 0.029541274274770535,
      "grad_norm": 6.19185209274292,
      "learning_rate": 4.963073627613808e-05,
      "loss": 3.8718,
      "step": 167500
    },
    {
      "epoch": 0.029558910856427114,
      "grad_norm": 7.739235877990723,
      "learning_rate": 4.963051581886737e-05,
      "loss": 3.9541,
      "step": 167600
    },
    {
      "epoch": 0.029576547438083692,
      "grad_norm": 8.838993072509766,
      "learning_rate": 4.963029536159666e-05,
      "loss": 3.9294,
      "step": 167700
    },
    {
      "epoch": 0.029594184019740274,
      "grad_norm": 6.482025146484375,
      "learning_rate": 4.963007490432596e-05,
      "loss": 3.9296,
      "step": 167800
    },
    {
      "epoch": 0.029611820601396853,
      "grad_norm": 7.351596355438232,
      "learning_rate": 4.9629854447055246e-05,
      "loss": 3.9929,
      "step": 167900
    },
    {
      "epoch": 0.02962945718305343,
      "grad_norm": 9.687460899353027,
      "learning_rate": 4.962963398978454e-05,
      "loss": 3.9704,
      "step": 168000
    },
    {
      "epoch": 0.02964709376471001,
      "grad_norm": 8.166297912597656,
      "learning_rate": 4.962941353251384e-05,
      "loss": 3.9257,
      "step": 168100
    },
    {
      "epoch": 0.029664730346366592,
      "grad_norm": 6.047059535980225,
      "learning_rate": 4.9629193075243126e-05,
      "loss": 3.8881,
      "step": 168200
    },
    {
      "epoch": 0.02968236692802317,
      "grad_norm": 9.553839683532715,
      "learning_rate": 4.962897261797242e-05,
      "loss": 3.9789,
      "step": 168300
    },
    {
      "epoch": 0.02970000350967975,
      "grad_norm": 7.137696743011475,
      "learning_rate": 4.962875216070172e-05,
      "loss": 3.9567,
      "step": 168400
    },
    {
      "epoch": 0.029717640091336327,
      "grad_norm": 5.041121006011963,
      "learning_rate": 4.9628531703431006e-05,
      "loss": 4.0206,
      "step": 168500
    },
    {
      "epoch": 0.02973527667299291,
      "grad_norm": 6.092220306396484,
      "learning_rate": 4.9628311246160294e-05,
      "loss": 3.9242,
      "step": 168600
    },
    {
      "epoch": 0.029752913254649488,
      "grad_norm": 6.249416351318359,
      "learning_rate": 4.962809078888959e-05,
      "loss": 3.9523,
      "step": 168700
    },
    {
      "epoch": 0.029770549836306066,
      "grad_norm": 7.9477996826171875,
      "learning_rate": 4.962787033161888e-05,
      "loss": 3.9774,
      "step": 168800
    },
    {
      "epoch": 0.02978818641796265,
      "grad_norm": 7.48763370513916,
      "learning_rate": 4.9627649874348174e-05,
      "loss": 3.9302,
      "step": 168900
    },
    {
      "epoch": 0.029805822999619227,
      "grad_norm": 6.749074935913086,
      "learning_rate": 4.962742941707747e-05,
      "loss": 3.9274,
      "step": 169000
    },
    {
      "epoch": 0.029823459581275805,
      "grad_norm": 6.3567633628845215,
      "learning_rate": 4.962720895980676e-05,
      "loss": 3.941,
      "step": 169100
    },
    {
      "epoch": 0.029841096162932384,
      "grad_norm": 7.686967372894287,
      "learning_rate": 4.9626988502536053e-05,
      "loss": 3.8937,
      "step": 169200
    },
    {
      "epoch": 0.029858732744588966,
      "grad_norm": 7.335594654083252,
      "learning_rate": 4.962676804526535e-05,
      "loss": 3.9005,
      "step": 169300
    },
    {
      "epoch": 0.029876369326245544,
      "grad_norm": 6.480386734008789,
      "learning_rate": 4.9626547587994644e-05,
      "loss": 4.0168,
      "step": 169400
    },
    {
      "epoch": 0.029894005907902123,
      "grad_norm": 6.160390853881836,
      "learning_rate": 4.962632713072393e-05,
      "loss": 4.0431,
      "step": 169500
    },
    {
      "epoch": 0.0299116424895587,
      "grad_norm": 5.34321403503418,
      "learning_rate": 4.962610667345323e-05,
      "loss": 3.9116,
      "step": 169600
    },
    {
      "epoch": 0.029929279071215283,
      "grad_norm": 6.159492492675781,
      "learning_rate": 4.9625886216182524e-05,
      "loss": 3.8922,
      "step": 169700
    },
    {
      "epoch": 0.029946915652871862,
      "grad_norm": 6.137758255004883,
      "learning_rate": 4.962566575891181e-05,
      "loss": 3.8627,
      "step": 169800
    },
    {
      "epoch": 0.02996455223452844,
      "grad_norm": 6.652703762054443,
      "learning_rate": 4.96254453016411e-05,
      "loss": 4.0079,
      "step": 169900
    },
    {
      "epoch": 0.02998218881618502,
      "grad_norm": 7.240493297576904,
      "learning_rate": 4.96252248443704e-05,
      "loss": 3.8848,
      "step": 170000
    },
    {
      "epoch": 0.0299998253978416,
      "grad_norm": 8.35506820678711,
      "learning_rate": 4.9625004387099686e-05,
      "loss": 3.9822,
      "step": 170100
    },
    {
      "epoch": 0.03001746197949818,
      "grad_norm": 12.910651206970215,
      "learning_rate": 4.962478392982898e-05,
      "loss": 3.8939,
      "step": 170200
    },
    {
      "epoch": 0.030035098561154758,
      "grad_norm": 9.806007385253906,
      "learning_rate": 4.9624563472558277e-05,
      "loss": 3.8438,
      "step": 170300
    },
    {
      "epoch": 0.03005273514281134,
      "grad_norm": 9.868682861328125,
      "learning_rate": 4.9624343015287565e-05,
      "loss": 3.9801,
      "step": 170400
    },
    {
      "epoch": 0.030070371724467918,
      "grad_norm": 9.405933380126953,
      "learning_rate": 4.962412255801686e-05,
      "loss": 4.1327,
      "step": 170500
    },
    {
      "epoch": 0.030088008306124497,
      "grad_norm": 7.102090358734131,
      "learning_rate": 4.9623902100746156e-05,
      "loss": 3.9888,
      "step": 170600
    },
    {
      "epoch": 0.030105644887781075,
      "grad_norm": 5.96371603012085,
      "learning_rate": 4.9623681643475445e-05,
      "loss": 3.9186,
      "step": 170700
    },
    {
      "epoch": 0.030123281469437657,
      "grad_norm": 8.463380813598633,
      "learning_rate": 4.962346118620474e-05,
      "loss": 4.0299,
      "step": 170800
    },
    {
      "epoch": 0.030140918051094236,
      "grad_norm": 9.931685447692871,
      "learning_rate": 4.9623240728934036e-05,
      "loss": 3.8795,
      "step": 170900
    },
    {
      "epoch": 0.030158554632750814,
      "grad_norm": 6.925073623657227,
      "learning_rate": 4.9623020271663324e-05,
      "loss": 3.8807,
      "step": 171000
    },
    {
      "epoch": 0.030176191214407393,
      "grad_norm": 7.318554878234863,
      "learning_rate": 4.962279981439262e-05,
      "loss": 3.8732,
      "step": 171100
    },
    {
      "epoch": 0.030193827796063975,
      "grad_norm": 6.133088111877441,
      "learning_rate": 4.9622579357121915e-05,
      "loss": 3.8903,
      "step": 171200
    },
    {
      "epoch": 0.030211464377720553,
      "grad_norm": 6.855798244476318,
      "learning_rate": 4.9622358899851204e-05,
      "loss": 3.9208,
      "step": 171300
    },
    {
      "epoch": 0.03022910095937713,
      "grad_norm": 6.947573184967041,
      "learning_rate": 4.962213844258049e-05,
      "loss": 3.9105,
      "step": 171400
    },
    {
      "epoch": 0.03024673754103371,
      "grad_norm": 6.583323955535889,
      "learning_rate": 4.962191798530979e-05,
      "loss": 3.9312,
      "step": 171500
    },
    {
      "epoch": 0.030264374122690292,
      "grad_norm": 6.2382893562316895,
      "learning_rate": 4.962169752803908e-05,
      "loss": 3.8897,
      "step": 171600
    },
    {
      "epoch": 0.03028201070434687,
      "grad_norm": 8.409725189208984,
      "learning_rate": 4.962147707076837e-05,
      "loss": 4.0921,
      "step": 171700
    },
    {
      "epoch": 0.03029964728600345,
      "grad_norm": 7.930353164672852,
      "learning_rate": 4.962125661349767e-05,
      "loss": 4.0152,
      "step": 171800
    },
    {
      "epoch": 0.03031728386766003,
      "grad_norm": 6.1314826011657715,
      "learning_rate": 4.9621036156226957e-05,
      "loss": 3.9621,
      "step": 171900
    },
    {
      "epoch": 0.03033492044931661,
      "grad_norm": 6.109037399291992,
      "learning_rate": 4.962081569895625e-05,
      "loss": 3.8964,
      "step": 172000
    },
    {
      "epoch": 0.030352557030973188,
      "grad_norm": 7.3947882652282715,
      "learning_rate": 4.962059524168555e-05,
      "loss": 4.0018,
      "step": 172100
    },
    {
      "epoch": 0.030370193612629767,
      "grad_norm": 7.686550140380859,
      "learning_rate": 4.9620374784414836e-05,
      "loss": 3.9382,
      "step": 172200
    },
    {
      "epoch": 0.03038783019428635,
      "grad_norm": 7.597422122955322,
      "learning_rate": 4.962015432714413e-05,
      "loss": 3.9193,
      "step": 172300
    },
    {
      "epoch": 0.030405466775942927,
      "grad_norm": 6.2252631187438965,
      "learning_rate": 4.961993386987343e-05,
      "loss": 4.0545,
      "step": 172400
    },
    {
      "epoch": 0.030423103357599506,
      "grad_norm": 8.952924728393555,
      "learning_rate": 4.9619713412602716e-05,
      "loss": 3.9677,
      "step": 172500
    },
    {
      "epoch": 0.030440739939256084,
      "grad_norm": 6.643424987792969,
      "learning_rate": 4.961949295533201e-05,
      "loss": 3.9942,
      "step": 172600
    },
    {
      "epoch": 0.030458376520912666,
      "grad_norm": 7.532771587371826,
      "learning_rate": 4.96192724980613e-05,
      "loss": 3.9485,
      "step": 172700
    },
    {
      "epoch": 0.030476013102569244,
      "grad_norm": 13.034988403320312,
      "learning_rate": 4.9619052040790595e-05,
      "loss": 3.9472,
      "step": 172800
    },
    {
      "epoch": 0.030493649684225823,
      "grad_norm": 7.740664482116699,
      "learning_rate": 4.9618831583519884e-05,
      "loss": 3.9343,
      "step": 172900
    },
    {
      "epoch": 0.0305112862658824,
      "grad_norm": 6.441495418548584,
      "learning_rate": 4.961861112624918e-05,
      "loss": 3.9635,
      "step": 173000
    },
    {
      "epoch": 0.030528922847538983,
      "grad_norm": 5.491728782653809,
      "learning_rate": 4.961839066897847e-05,
      "loss": 3.9703,
      "step": 173100
    },
    {
      "epoch": 0.030546559429195562,
      "grad_norm": 7.81932258605957,
      "learning_rate": 4.9618170211707764e-05,
      "loss": 3.8781,
      "step": 173200
    },
    {
      "epoch": 0.03056419601085214,
      "grad_norm": 5.677675724029541,
      "learning_rate": 4.961794975443706e-05,
      "loss": 3.9621,
      "step": 173300
    },
    {
      "epoch": 0.030581832592508722,
      "grad_norm": 6.5669684410095215,
      "learning_rate": 4.961772929716635e-05,
      "loss": 3.988,
      "step": 173400
    },
    {
      "epoch": 0.0305994691741653,
      "grad_norm": 5.015180587768555,
      "learning_rate": 4.9617508839895643e-05,
      "loss": 3.966,
      "step": 173500
    },
    {
      "epoch": 0.03061710575582188,
      "grad_norm": 12.305704116821289,
      "learning_rate": 4.961728838262494e-05,
      "loss": 3.9225,
      "step": 173600
    },
    {
      "epoch": 0.030634742337478458,
      "grad_norm": 7.188404083251953,
      "learning_rate": 4.961706792535423e-05,
      "loss": 3.9391,
      "step": 173700
    },
    {
      "epoch": 0.03065237891913504,
      "grad_norm": 4.835137844085693,
      "learning_rate": 4.961684746808352e-05,
      "loss": 3.9528,
      "step": 173800
    },
    {
      "epoch": 0.03067001550079162,
      "grad_norm": 8.992964744567871,
      "learning_rate": 4.961662701081282e-05,
      "loss": 3.9223,
      "step": 173900
    },
    {
      "epoch": 0.030687652082448197,
      "grad_norm": 8.129263877868652,
      "learning_rate": 4.961640655354211e-05,
      "loss": 3.9838,
      "step": 174000
    },
    {
      "epoch": 0.030705288664104775,
      "grad_norm": 8.527737617492676,
      "learning_rate": 4.96161860962714e-05,
      "loss": 3.9138,
      "step": 174100
    },
    {
      "epoch": 0.030722925245761357,
      "grad_norm": 6.646540641784668,
      "learning_rate": 4.961596563900069e-05,
      "loss": 3.968,
      "step": 174200
    },
    {
      "epoch": 0.030740561827417936,
      "grad_norm": 5.348471641540527,
      "learning_rate": 4.961574518172998e-05,
      "loss": 3.9902,
      "step": 174300
    },
    {
      "epoch": 0.030758198409074514,
      "grad_norm": 10.10473918914795,
      "learning_rate": 4.9615524724459276e-05,
      "loss": 3.9194,
      "step": 174400
    },
    {
      "epoch": 0.030775834990731093,
      "grad_norm": 8.366740226745605,
      "learning_rate": 4.961530426718857e-05,
      "loss": 3.869,
      "step": 174500
    },
    {
      "epoch": 0.030793471572387675,
      "grad_norm": 7.8758087158203125,
      "learning_rate": 4.961508380991786e-05,
      "loss": 4.0323,
      "step": 174600
    },
    {
      "epoch": 0.030811108154044253,
      "grad_norm": 6.319733142852783,
      "learning_rate": 4.9614863352647155e-05,
      "loss": 3.8923,
      "step": 174700
    },
    {
      "epoch": 0.030828744735700832,
      "grad_norm": 9.49543571472168,
      "learning_rate": 4.961464289537645e-05,
      "loss": 4.0401,
      "step": 174800
    },
    {
      "epoch": 0.030846381317357414,
      "grad_norm": 9.600188255310059,
      "learning_rate": 4.961442243810574e-05,
      "loss": 3.9939,
      "step": 174900
    },
    {
      "epoch": 0.030864017899013992,
      "grad_norm": 11.503644943237305,
      "learning_rate": 4.9614201980835035e-05,
      "loss": 4.0044,
      "step": 175000
    },
    {
      "epoch": 0.03088165448067057,
      "grad_norm": 7.3089165687561035,
      "learning_rate": 4.961398152356433e-05,
      "loss": 3.954,
      "step": 175100
    },
    {
      "epoch": 0.03089929106232715,
      "grad_norm": 6.377825736999512,
      "learning_rate": 4.961376106629362e-05,
      "loss": 4.0711,
      "step": 175200
    },
    {
      "epoch": 0.03091692764398373,
      "grad_norm": 7.483861923217773,
      "learning_rate": 4.9613540609022914e-05,
      "loss": 3.886,
      "step": 175300
    },
    {
      "epoch": 0.03093456422564031,
      "grad_norm": 6.6068034172058105,
      "learning_rate": 4.961332015175221e-05,
      "loss": 3.9295,
      "step": 175400
    },
    {
      "epoch": 0.030952200807296888,
      "grad_norm": 7.85221004486084,
      "learning_rate": 4.96130996944815e-05,
      "loss": 4.0259,
      "step": 175500
    },
    {
      "epoch": 0.030969837388953467,
      "grad_norm": 7.318794250488281,
      "learning_rate": 4.9612879237210794e-05,
      "loss": 3.9669,
      "step": 175600
    },
    {
      "epoch": 0.03098747397061005,
      "grad_norm": 6.763332366943359,
      "learning_rate": 4.961265877994008e-05,
      "loss": 3.8826,
      "step": 175700
    },
    {
      "epoch": 0.031005110552266627,
      "grad_norm": 5.93750524520874,
      "learning_rate": 4.961243832266937e-05,
      "loss": 3.8292,
      "step": 175800
    },
    {
      "epoch": 0.031022747133923206,
      "grad_norm": 7.3075690269470215,
      "learning_rate": 4.961221786539867e-05,
      "loss": 3.9499,
      "step": 175900
    },
    {
      "epoch": 0.031040383715579784,
      "grad_norm": 5.138243675231934,
      "learning_rate": 4.961199740812796e-05,
      "loss": 3.8899,
      "step": 176000
    },
    {
      "epoch": 0.031058020297236366,
      "grad_norm": 6.94546365737915,
      "learning_rate": 4.961177695085725e-05,
      "loss": 3.9498,
      "step": 176100
    },
    {
      "epoch": 0.031075656878892945,
      "grad_norm": 5.176919460296631,
      "learning_rate": 4.9611556493586547e-05,
      "loss": 3.963,
      "step": 176200
    },
    {
      "epoch": 0.031093293460549523,
      "grad_norm": 7.058077812194824,
      "learning_rate": 4.961133603631584e-05,
      "loss": 3.9337,
      "step": 176300
    },
    {
      "epoch": 0.031110930042206105,
      "grad_norm": 7.38777494430542,
      "learning_rate": 4.961111557904513e-05,
      "loss": 3.9669,
      "step": 176400
    },
    {
      "epoch": 0.031128566623862684,
      "grad_norm": 8.162251472473145,
      "learning_rate": 4.9610895121774426e-05,
      "loss": 3.8499,
      "step": 176500
    },
    {
      "epoch": 0.031146203205519262,
      "grad_norm": 5.504064083099365,
      "learning_rate": 4.961067466450372e-05,
      "loss": 4.0328,
      "step": 176600
    },
    {
      "epoch": 0.03116383978717584,
      "grad_norm": 9.856101036071777,
      "learning_rate": 4.961045420723301e-05,
      "loss": 3.9053,
      "step": 176700
    },
    {
      "epoch": 0.031181476368832423,
      "grad_norm": 6.200411319732666,
      "learning_rate": 4.9610233749962306e-05,
      "loss": 3.8844,
      "step": 176800
    },
    {
      "epoch": 0.031199112950489,
      "grad_norm": 8.085051536560059,
      "learning_rate": 4.96100132926916e-05,
      "loss": 3.9642,
      "step": 176900
    },
    {
      "epoch": 0.03121674953214558,
      "grad_norm": 6.007898330688477,
      "learning_rate": 4.960979283542089e-05,
      "loss": 4.0242,
      "step": 177000
    },
    {
      "epoch": 0.031234386113802158,
      "grad_norm": 9.144604682922363,
      "learning_rate": 4.960957237815018e-05,
      "loss": 3.9174,
      "step": 177100
    },
    {
      "epoch": 0.03125202269545874,
      "grad_norm": 7.696727752685547,
      "learning_rate": 4.9609351920879474e-05,
      "loss": 3.8983,
      "step": 177200
    },
    {
      "epoch": 0.031269659277115315,
      "grad_norm": 7.352122783660889,
      "learning_rate": 4.960913146360876e-05,
      "loss": 4.029,
      "step": 177300
    },
    {
      "epoch": 0.0312872958587719,
      "grad_norm": 11.32556438446045,
      "learning_rate": 4.960891100633806e-05,
      "loss": 4.0088,
      "step": 177400
    },
    {
      "epoch": 0.03130493244042848,
      "grad_norm": 5.025181770324707,
      "learning_rate": 4.9608690549067354e-05,
      "loss": 3.8857,
      "step": 177500
    },
    {
      "epoch": 0.03132256902208506,
      "grad_norm": 6.337418556213379,
      "learning_rate": 4.960847009179664e-05,
      "loss": 3.8984,
      "step": 177600
    },
    {
      "epoch": 0.031340205603741636,
      "grad_norm": 7.067330837249756,
      "learning_rate": 4.960824963452594e-05,
      "loss": 3.978,
      "step": 177700
    },
    {
      "epoch": 0.031357842185398214,
      "grad_norm": 6.392729759216309,
      "learning_rate": 4.960802917725523e-05,
      "loss": 3.9378,
      "step": 177800
    },
    {
      "epoch": 0.03137547876705479,
      "grad_norm": 8.405278205871582,
      "learning_rate": 4.960780871998452e-05,
      "loss": 3.907,
      "step": 177900
    },
    {
      "epoch": 0.03139311534871137,
      "grad_norm": 6.593796253204346,
      "learning_rate": 4.960758826271382e-05,
      "loss": 3.9181,
      "step": 178000
    },
    {
      "epoch": 0.03141075193036796,
      "grad_norm": 6.540466785430908,
      "learning_rate": 4.960736780544311e-05,
      "loss": 4.0005,
      "step": 178100
    },
    {
      "epoch": 0.031428388512024535,
      "grad_norm": 6.098276615142822,
      "learning_rate": 4.96071473481724e-05,
      "loss": 4.0247,
      "step": 178200
    },
    {
      "epoch": 0.031446025093681114,
      "grad_norm": 8.70187759399414,
      "learning_rate": 4.96069268909017e-05,
      "loss": 3.83,
      "step": 178300
    },
    {
      "epoch": 0.03146366167533769,
      "grad_norm": 6.5472564697265625,
      "learning_rate": 4.960670643363099e-05,
      "loss": 3.9683,
      "step": 178400
    },
    {
      "epoch": 0.03148129825699427,
      "grad_norm": 7.322556972503662,
      "learning_rate": 4.960648597636028e-05,
      "loss": 3.9642,
      "step": 178500
    },
    {
      "epoch": 0.03149893483865085,
      "grad_norm": 6.803463935852051,
      "learning_rate": 4.960626551908957e-05,
      "loss": 3.9284,
      "step": 178600
    },
    {
      "epoch": 0.03151657142030743,
      "grad_norm": 7.978791236877441,
      "learning_rate": 4.9606045061818865e-05,
      "loss": 3.9682,
      "step": 178700
    },
    {
      "epoch": 0.031534208001964006,
      "grad_norm": 6.6071295738220215,
      "learning_rate": 4.9605824604548154e-05,
      "loss": 3.966,
      "step": 178800
    },
    {
      "epoch": 0.03155184458362059,
      "grad_norm": 6.10175895690918,
      "learning_rate": 4.960560414727745e-05,
      "loss": 3.8814,
      "step": 178900
    },
    {
      "epoch": 0.03156948116527717,
      "grad_norm": 6.5187249183654785,
      "learning_rate": 4.9605383690006745e-05,
      "loss": 3.8877,
      "step": 179000
    },
    {
      "epoch": 0.03158711774693375,
      "grad_norm": 6.069039821624756,
      "learning_rate": 4.9605163232736034e-05,
      "loss": 3.8068,
      "step": 179100
    },
    {
      "epoch": 0.03160475432859033,
      "grad_norm": 6.909053802490234,
      "learning_rate": 4.960494277546533e-05,
      "loss": 3.9295,
      "step": 179200
    },
    {
      "epoch": 0.031622390910246906,
      "grad_norm": 6.933252334594727,
      "learning_rate": 4.9604722318194625e-05,
      "loss": 3.9128,
      "step": 179300
    },
    {
      "epoch": 0.031640027491903484,
      "grad_norm": 7.518777370452881,
      "learning_rate": 4.9604501860923913e-05,
      "loss": 3.9336,
      "step": 179400
    },
    {
      "epoch": 0.03165766407356006,
      "grad_norm": 6.569334506988525,
      "learning_rate": 4.960428140365321e-05,
      "loss": 3.977,
      "step": 179500
    },
    {
      "epoch": 0.03167530065521665,
      "grad_norm": 5.935787200927734,
      "learning_rate": 4.9604060946382504e-05,
      "loss": 4.0033,
      "step": 179600
    },
    {
      "epoch": 0.03169293723687323,
      "grad_norm": 6.477328777313232,
      "learning_rate": 4.960384048911179e-05,
      "loss": 3.9771,
      "step": 179700
    },
    {
      "epoch": 0.031710573818529805,
      "grad_norm": 7.969303131103516,
      "learning_rate": 4.960362003184109e-05,
      "loss": 3.8781,
      "step": 179800
    },
    {
      "epoch": 0.031728210400186384,
      "grad_norm": 6.820152759552002,
      "learning_rate": 4.960339957457038e-05,
      "loss": 4.0192,
      "step": 179900
    },
    {
      "epoch": 0.03174584698184296,
      "grad_norm": 10.496442794799805,
      "learning_rate": 4.960317911729967e-05,
      "loss": 3.911,
      "step": 180000
    },
    {
      "epoch": 0.03176348356349954,
      "grad_norm": 6.038681507110596,
      "learning_rate": 4.960295866002896e-05,
      "loss": 3.9967,
      "step": 180100
    },
    {
      "epoch": 0.03178112014515612,
      "grad_norm": 8.359509468078613,
      "learning_rate": 4.960273820275826e-05,
      "loss": 3.9875,
      "step": 180200
    },
    {
      "epoch": 0.0317987567268127,
      "grad_norm": 6.604253768920898,
      "learning_rate": 4.960251774548755e-05,
      "loss": 3.8794,
      "step": 180300
    },
    {
      "epoch": 0.03181639330846928,
      "grad_norm": 8.462589263916016,
      "learning_rate": 4.960229728821684e-05,
      "loss": 3.8519,
      "step": 180400
    },
    {
      "epoch": 0.03183402989012586,
      "grad_norm": 6.249289035797119,
      "learning_rate": 4.9602076830946136e-05,
      "loss": 4.0082,
      "step": 180500
    },
    {
      "epoch": 0.03185166647178244,
      "grad_norm": 6.717724800109863,
      "learning_rate": 4.960185637367543e-05,
      "loss": 3.9485,
      "step": 180600
    },
    {
      "epoch": 0.03186930305343902,
      "grad_norm": 8.775023460388184,
      "learning_rate": 4.960163591640472e-05,
      "loss": 3.9373,
      "step": 180700
    },
    {
      "epoch": 0.0318869396350956,
      "grad_norm": 5.746724605560303,
      "learning_rate": 4.9601415459134016e-05,
      "loss": 4.0264,
      "step": 180800
    },
    {
      "epoch": 0.031904576216752176,
      "grad_norm": 7.157637596130371,
      "learning_rate": 4.960119500186331e-05,
      "loss": 3.9734,
      "step": 180900
    },
    {
      "epoch": 0.031922212798408754,
      "grad_norm": 8.414556503295898,
      "learning_rate": 4.96009745445926e-05,
      "loss": 4.0538,
      "step": 181000
    },
    {
      "epoch": 0.03193984938006534,
      "grad_norm": 9.658294677734375,
      "learning_rate": 4.9600754087321896e-05,
      "loss": 3.8159,
      "step": 181100
    },
    {
      "epoch": 0.03195748596172192,
      "grad_norm": 7.629756450653076,
      "learning_rate": 4.960053363005119e-05,
      "loss": 3.9124,
      "step": 181200
    },
    {
      "epoch": 0.0319751225433785,
      "grad_norm": 5.991476058959961,
      "learning_rate": 4.960031317278048e-05,
      "loss": 3.912,
      "step": 181300
    },
    {
      "epoch": 0.031992759125035075,
      "grad_norm": 6.346750736236572,
      "learning_rate": 4.960009271550977e-05,
      "loss": 3.9374,
      "step": 181400
    },
    {
      "epoch": 0.032010395706691654,
      "grad_norm": 5.692108154296875,
      "learning_rate": 4.9599872258239064e-05,
      "loss": 3.9628,
      "step": 181500
    },
    {
      "epoch": 0.03202803228834823,
      "grad_norm": 8.89373779296875,
      "learning_rate": 4.959965180096835e-05,
      "loss": 3.9167,
      "step": 181600
    },
    {
      "epoch": 0.03204566887000481,
      "grad_norm": 7.705410957336426,
      "learning_rate": 4.959943134369765e-05,
      "loss": 3.951,
      "step": 181700
    },
    {
      "epoch": 0.03206330545166139,
      "grad_norm": 8.35572338104248,
      "learning_rate": 4.9599210886426944e-05,
      "loss": 3.832,
      "step": 181800
    },
    {
      "epoch": 0.032080942033317975,
      "grad_norm": 7.791440010070801,
      "learning_rate": 4.959899042915623e-05,
      "loss": 3.9184,
      "step": 181900
    },
    {
      "epoch": 0.03209857861497455,
      "grad_norm": 6.424297332763672,
      "learning_rate": 4.959876997188553e-05,
      "loss": 3.9862,
      "step": 182000
    },
    {
      "epoch": 0.03211621519663113,
      "grad_norm": 6.587876796722412,
      "learning_rate": 4.959854951461482e-05,
      "loss": 3.9792,
      "step": 182100
    },
    {
      "epoch": 0.03213385177828771,
      "grad_norm": 13.20748519897461,
      "learning_rate": 4.959832905734411e-05,
      "loss": 3.9251,
      "step": 182200
    },
    {
      "epoch": 0.03215148835994429,
      "grad_norm": 5.2400994300842285,
      "learning_rate": 4.959810860007341e-05,
      "loss": 3.9813,
      "step": 182300
    },
    {
      "epoch": 0.03216912494160087,
      "grad_norm": 7.588478088378906,
      "learning_rate": 4.95978881428027e-05,
      "loss": 3.9912,
      "step": 182400
    },
    {
      "epoch": 0.032186761523257446,
      "grad_norm": 6.692082405090332,
      "learning_rate": 4.959766768553199e-05,
      "loss": 3.9998,
      "step": 182500
    },
    {
      "epoch": 0.03220439810491403,
      "grad_norm": 5.945059776306152,
      "learning_rate": 4.959744722826129e-05,
      "loss": 4.0002,
      "step": 182600
    },
    {
      "epoch": 0.03222203468657061,
      "grad_norm": 4.99980354309082,
      "learning_rate": 4.9597226770990576e-05,
      "loss": 3.9465,
      "step": 182700
    },
    {
      "epoch": 0.03223967126822719,
      "grad_norm": 13.40560245513916,
      "learning_rate": 4.959700631371987e-05,
      "loss": 4.0374,
      "step": 182800
    },
    {
      "epoch": 0.032257307849883766,
      "grad_norm": 12.040749549865723,
      "learning_rate": 4.959678585644916e-05,
      "loss": 3.8983,
      "step": 182900
    },
    {
      "epoch": 0.032274944431540345,
      "grad_norm": 9.794920921325684,
      "learning_rate": 4.9596565399178455e-05,
      "loss": 3.8462,
      "step": 183000
    },
    {
      "epoch": 0.032292581013196923,
      "grad_norm": 5.965604305267334,
      "learning_rate": 4.9596344941907744e-05,
      "loss": 3.967,
      "step": 183100
    },
    {
      "epoch": 0.0323102175948535,
      "grad_norm": 7.0513596534729,
      "learning_rate": 4.959612448463704e-05,
      "loss": 3.896,
      "step": 183200
    },
    {
      "epoch": 0.03232785417651008,
      "grad_norm": 5.169890403747559,
      "learning_rate": 4.9595904027366335e-05,
      "loss": 3.9574,
      "step": 183300
    },
    {
      "epoch": 0.032345490758166666,
      "grad_norm": 8.763935089111328,
      "learning_rate": 4.9595683570095624e-05,
      "loss": 3.997,
      "step": 183400
    },
    {
      "epoch": 0.032363127339823244,
      "grad_norm": 8.65095043182373,
      "learning_rate": 4.959546311282492e-05,
      "loss": 3.9608,
      "step": 183500
    },
    {
      "epoch": 0.03238076392147982,
      "grad_norm": 6.311750411987305,
      "learning_rate": 4.9595242655554215e-05,
      "loss": 3.9584,
      "step": 183600
    },
    {
      "epoch": 0.0323984005031364,
      "grad_norm": 6.9722700119018555,
      "learning_rate": 4.95950221982835e-05,
      "loss": 3.987,
      "step": 183700
    },
    {
      "epoch": 0.03241603708479298,
      "grad_norm": 5.854153633117676,
      "learning_rate": 4.95948017410128e-05,
      "loss": 3.9084,
      "step": 183800
    },
    {
      "epoch": 0.03243367366644956,
      "grad_norm": 7.391134738922119,
      "learning_rate": 4.9594581283742094e-05,
      "loss": 3.7908,
      "step": 183900
    },
    {
      "epoch": 0.03245131024810614,
      "grad_norm": 5.922350883483887,
      "learning_rate": 4.959436082647138e-05,
      "loss": 4.0092,
      "step": 184000
    },
    {
      "epoch": 0.03246894682976272,
      "grad_norm": 5.112847805023193,
      "learning_rate": 4.959414036920068e-05,
      "loss": 3.9119,
      "step": 184100
    },
    {
      "epoch": 0.0324865834114193,
      "grad_norm": 6.583773136138916,
      "learning_rate": 4.959391991192997e-05,
      "loss": 3.9113,
      "step": 184200
    },
    {
      "epoch": 0.03250421999307588,
      "grad_norm": 6.08730411529541,
      "learning_rate": 4.959369945465926e-05,
      "loss": 3.8198,
      "step": 184300
    },
    {
      "epoch": 0.03252185657473246,
      "grad_norm": 10.305461883544922,
      "learning_rate": 4.959347899738855e-05,
      "loss": 3.9616,
      "step": 184400
    },
    {
      "epoch": 0.032539493156389036,
      "grad_norm": 9.783568382263184,
      "learning_rate": 4.959325854011785e-05,
      "loss": 3.886,
      "step": 184500
    },
    {
      "epoch": 0.032557129738045615,
      "grad_norm": 6.778839588165283,
      "learning_rate": 4.9593038082847135e-05,
      "loss": 3.9254,
      "step": 184600
    },
    {
      "epoch": 0.03257476631970219,
      "grad_norm": 6.5265092849731445,
      "learning_rate": 4.959281762557643e-05,
      "loss": 3.9062,
      "step": 184700
    },
    {
      "epoch": 0.03259240290135877,
      "grad_norm": 8.218462944030762,
      "learning_rate": 4.9592597168305726e-05,
      "loss": 4.0209,
      "step": 184800
    },
    {
      "epoch": 0.03261003948301536,
      "grad_norm": 8.328315734863281,
      "learning_rate": 4.9592376711035015e-05,
      "loss": 3.9123,
      "step": 184900
    },
    {
      "epoch": 0.032627676064671936,
      "grad_norm": 6.320935249328613,
      "learning_rate": 4.959215625376431e-05,
      "loss": 3.889,
      "step": 185000
    },
    {
      "epoch": 0.032645312646328514,
      "grad_norm": 8.435016632080078,
      "learning_rate": 4.9591935796493606e-05,
      "loss": 3.9017,
      "step": 185100
    },
    {
      "epoch": 0.03266294922798509,
      "grad_norm": 7.12525749206543,
      "learning_rate": 4.9591715339222895e-05,
      "loss": 4.0512,
      "step": 185200
    },
    {
      "epoch": 0.03268058580964167,
      "grad_norm": 10.894319534301758,
      "learning_rate": 4.959149488195219e-05,
      "loss": 3.9581,
      "step": 185300
    },
    {
      "epoch": 0.03269822239129825,
      "grad_norm": 6.5522847175598145,
      "learning_rate": 4.9591274424681486e-05,
      "loss": 3.9517,
      "step": 185400
    },
    {
      "epoch": 0.03271585897295483,
      "grad_norm": 7.771719932556152,
      "learning_rate": 4.9591053967410774e-05,
      "loss": 3.9305,
      "step": 185500
    },
    {
      "epoch": 0.032733495554611414,
      "grad_norm": 7.486474990844727,
      "learning_rate": 4.959083351014007e-05,
      "loss": 3.8934,
      "step": 185600
    },
    {
      "epoch": 0.03275113213626799,
      "grad_norm": 6.750190734863281,
      "learning_rate": 4.959061305286936e-05,
      "loss": 3.8359,
      "step": 185700
    },
    {
      "epoch": 0.03276876871792457,
      "grad_norm": 12.066115379333496,
      "learning_rate": 4.959039259559865e-05,
      "loss": 3.9496,
      "step": 185800
    },
    {
      "epoch": 0.03278640529958115,
      "grad_norm": 7.3950276374816895,
      "learning_rate": 4.959017213832794e-05,
      "loss": 3.9435,
      "step": 185900
    },
    {
      "epoch": 0.03280404188123773,
      "grad_norm": 9.14885139465332,
      "learning_rate": 4.958995168105724e-05,
      "loss": 4.0776,
      "step": 186000
    },
    {
      "epoch": 0.032821678462894306,
      "grad_norm": 5.301156520843506,
      "learning_rate": 4.958973122378653e-05,
      "loss": 3.7922,
      "step": 186100
    },
    {
      "epoch": 0.032839315044550885,
      "grad_norm": 5.772351264953613,
      "learning_rate": 4.958951076651582e-05,
      "loss": 3.9557,
      "step": 186200
    },
    {
      "epoch": 0.03285695162620746,
      "grad_norm": 7.4074578285217285,
      "learning_rate": 4.958929030924512e-05,
      "loss": 3.9513,
      "step": 186300
    },
    {
      "epoch": 0.03287458820786405,
      "grad_norm": 7.495373249053955,
      "learning_rate": 4.9589069851974406e-05,
      "loss": 3.8978,
      "step": 186400
    },
    {
      "epoch": 0.03289222478952063,
      "grad_norm": 10.133176803588867,
      "learning_rate": 4.95888493947037e-05,
      "loss": 3.9172,
      "step": 186500
    },
    {
      "epoch": 0.032909861371177206,
      "grad_norm": 9.13936710357666,
      "learning_rate": 4.9588628937433e-05,
      "loss": 3.9958,
      "step": 186600
    },
    {
      "epoch": 0.032927497952833784,
      "grad_norm": 5.585074424743652,
      "learning_rate": 4.9588408480162286e-05,
      "loss": 3.9507,
      "step": 186700
    },
    {
      "epoch": 0.03294513453449036,
      "grad_norm": 7.04925537109375,
      "learning_rate": 4.958818802289158e-05,
      "loss": 3.8544,
      "step": 186800
    },
    {
      "epoch": 0.03296277111614694,
      "grad_norm": 8.386614799499512,
      "learning_rate": 4.958796756562088e-05,
      "loss": 3.8431,
      "step": 186900
    },
    {
      "epoch": 0.03298040769780352,
      "grad_norm": 5.963678359985352,
      "learning_rate": 4.9587747108350166e-05,
      "loss": 3.9572,
      "step": 187000
    },
    {
      "epoch": 0.032998044279460105,
      "grad_norm": 7.957641124725342,
      "learning_rate": 4.9587526651079454e-05,
      "loss": 3.8948,
      "step": 187100
    },
    {
      "epoch": 0.033015680861116684,
      "grad_norm": 7.091065883636475,
      "learning_rate": 4.958730619380875e-05,
      "loss": 3.9041,
      "step": 187200
    },
    {
      "epoch": 0.03303331744277326,
      "grad_norm": 7.093544960021973,
      "learning_rate": 4.958708573653804e-05,
      "loss": 3.8239,
      "step": 187300
    },
    {
      "epoch": 0.03305095402442984,
      "grad_norm": 5.417814254760742,
      "learning_rate": 4.9586865279267334e-05,
      "loss": 3.9717,
      "step": 187400
    },
    {
      "epoch": 0.03306859060608642,
      "grad_norm": 7.65657901763916,
      "learning_rate": 4.958664482199663e-05,
      "loss": 3.8199,
      "step": 187500
    },
    {
      "epoch": 0.033086227187743,
      "grad_norm": 7.351376056671143,
      "learning_rate": 4.958642436472592e-05,
      "loss": 3.9188,
      "step": 187600
    },
    {
      "epoch": 0.033103863769399576,
      "grad_norm": 5.951053142547607,
      "learning_rate": 4.9586203907455214e-05,
      "loss": 3.9309,
      "step": 187700
    },
    {
      "epoch": 0.033121500351056155,
      "grad_norm": 5.634037017822266,
      "learning_rate": 4.958598345018451e-05,
      "loss": 4.0181,
      "step": 187800
    },
    {
      "epoch": 0.03313913693271274,
      "grad_norm": 8.528566360473633,
      "learning_rate": 4.95857629929138e-05,
      "loss": 3.9342,
      "step": 187900
    },
    {
      "epoch": 0.03315677351436932,
      "grad_norm": 5.716965198516846,
      "learning_rate": 4.958554253564309e-05,
      "loss": 3.9487,
      "step": 188000
    },
    {
      "epoch": 0.0331744100960259,
      "grad_norm": 6.171939373016357,
      "learning_rate": 4.958532207837239e-05,
      "loss": 3.8554,
      "step": 188100
    },
    {
      "epoch": 0.033192046677682475,
      "grad_norm": 7.4436798095703125,
      "learning_rate": 4.958510162110168e-05,
      "loss": 3.8744,
      "step": 188200
    },
    {
      "epoch": 0.033209683259339054,
      "grad_norm": 9.4290132522583,
      "learning_rate": 4.958488116383097e-05,
      "loss": 4.0215,
      "step": 188300
    },
    {
      "epoch": 0.03322731984099563,
      "grad_norm": 5.865484714508057,
      "learning_rate": 4.958466070656027e-05,
      "loss": 3.8217,
      "step": 188400
    },
    {
      "epoch": 0.03324495642265221,
      "grad_norm": 7.725407123565674,
      "learning_rate": 4.958444024928956e-05,
      "loss": 4.0051,
      "step": 188500
    },
    {
      "epoch": 0.033262593004308796,
      "grad_norm": 8.431459426879883,
      "learning_rate": 4.9584219792018846e-05,
      "loss": 4.0348,
      "step": 188600
    },
    {
      "epoch": 0.033280229585965375,
      "grad_norm": 5.0283002853393555,
      "learning_rate": 4.958399933474814e-05,
      "loss": 3.9851,
      "step": 188700
    },
    {
      "epoch": 0.03329786616762195,
      "grad_norm": 11.843937873840332,
      "learning_rate": 4.958377887747743e-05,
      "loss": 3.8567,
      "step": 188800
    },
    {
      "epoch": 0.03331550274927853,
      "grad_norm": 8.575806617736816,
      "learning_rate": 4.9583558420206725e-05,
      "loss": 3.9091,
      "step": 188900
    },
    {
      "epoch": 0.03333313933093511,
      "grad_norm": 5.55142068862915,
      "learning_rate": 4.958333796293602e-05,
      "loss": 3.9522,
      "step": 189000
    },
    {
      "epoch": 0.03335077591259169,
      "grad_norm": 7.652029037475586,
      "learning_rate": 4.958311750566531e-05,
      "loss": 3.9531,
      "step": 189100
    },
    {
      "epoch": 0.03336841249424827,
      "grad_norm": 7.899833679199219,
      "learning_rate": 4.9582897048394605e-05,
      "loss": 3.9158,
      "step": 189200
    },
    {
      "epoch": 0.033386049075904846,
      "grad_norm": 5.992010116577148,
      "learning_rate": 4.95826765911239e-05,
      "loss": 3.8084,
      "step": 189300
    },
    {
      "epoch": 0.03340368565756143,
      "grad_norm": 8.056012153625488,
      "learning_rate": 4.958245613385319e-05,
      "loss": 3.7575,
      "step": 189400
    },
    {
      "epoch": 0.03342132223921801,
      "grad_norm": 5.0757036209106445,
      "learning_rate": 4.9582235676582485e-05,
      "loss": 3.8249,
      "step": 189500
    },
    {
      "epoch": 0.03343895882087459,
      "grad_norm": 7.271453857421875,
      "learning_rate": 4.958201521931178e-05,
      "loss": 3.9851,
      "step": 189600
    },
    {
      "epoch": 0.03345659540253117,
      "grad_norm": 8.259807586669922,
      "learning_rate": 4.958179476204107e-05,
      "loss": 3.9463,
      "step": 189700
    },
    {
      "epoch": 0.033474231984187745,
      "grad_norm": 5.805665969848633,
      "learning_rate": 4.9581574304770364e-05,
      "loss": 3.945,
      "step": 189800
    },
    {
      "epoch": 0.033491868565844324,
      "grad_norm": 8.853670120239258,
      "learning_rate": 4.958135384749965e-05,
      "loss": 3.8956,
      "step": 189900
    },
    {
      "epoch": 0.0335095051475009,
      "grad_norm": 7.889161586761475,
      "learning_rate": 4.958113339022895e-05,
      "loss": 3.9502,
      "step": 190000
    },
    {
      "epoch": 0.03352714172915749,
      "grad_norm": 7.460117816925049,
      "learning_rate": 4.958091293295824e-05,
      "loss": 3.9198,
      "step": 190100
    },
    {
      "epoch": 0.033544778310814066,
      "grad_norm": 8.115530014038086,
      "learning_rate": 4.958069247568753e-05,
      "loss": 3.9497,
      "step": 190200
    },
    {
      "epoch": 0.033562414892470645,
      "grad_norm": 5.916452407836914,
      "learning_rate": 4.958047201841682e-05,
      "loss": 3.8423,
      "step": 190300
    },
    {
      "epoch": 0.03358005147412722,
      "grad_norm": 5.260281085968018,
      "learning_rate": 4.958025156114612e-05,
      "loss": 3.917,
      "step": 190400
    },
    {
      "epoch": 0.0335976880557838,
      "grad_norm": 7.72741174697876,
      "learning_rate": 4.958003110387541e-05,
      "loss": 3.9233,
      "step": 190500
    },
    {
      "epoch": 0.03361532463744038,
      "grad_norm": 6.790989875793457,
      "learning_rate": 4.957981064660471e-05,
      "loss": 3.8971,
      "step": 190600
    },
    {
      "epoch": 0.03363296121909696,
      "grad_norm": 6.981840133666992,
      "learning_rate": 4.9579590189333996e-05,
      "loss": 3.8929,
      "step": 190700
    },
    {
      "epoch": 0.03365059780075354,
      "grad_norm": 6.655432224273682,
      "learning_rate": 4.957936973206329e-05,
      "loss": 3.9232,
      "step": 190800
    },
    {
      "epoch": 0.03366823438241012,
      "grad_norm": 20.605958938598633,
      "learning_rate": 4.957914927479259e-05,
      "loss": 3.8198,
      "step": 190900
    },
    {
      "epoch": 0.0336858709640667,
      "grad_norm": 7.267073631286621,
      "learning_rate": 4.9578928817521876e-05,
      "loss": 3.9308,
      "step": 191000
    },
    {
      "epoch": 0.03370350754572328,
      "grad_norm": 5.17501974105835,
      "learning_rate": 4.957870836025117e-05,
      "loss": 3.9618,
      "step": 191100
    },
    {
      "epoch": 0.03372114412737986,
      "grad_norm": 6.552369117736816,
      "learning_rate": 4.957848790298047e-05,
      "loss": 3.8702,
      "step": 191200
    },
    {
      "epoch": 0.03373878070903644,
      "grad_norm": 6.35638952255249,
      "learning_rate": 4.9578267445709756e-05,
      "loss": 3.9291,
      "step": 191300
    },
    {
      "epoch": 0.033756417290693015,
      "grad_norm": 6.129046440124512,
      "learning_rate": 4.9578046988439044e-05,
      "loss": 4.0166,
      "step": 191400
    },
    {
      "epoch": 0.033774053872349594,
      "grad_norm": 6.220322608947754,
      "learning_rate": 4.957782653116834e-05,
      "loss": 3.8506,
      "step": 191500
    },
    {
      "epoch": 0.03379169045400618,
      "grad_norm": 6.7659993171691895,
      "learning_rate": 4.957760607389763e-05,
      "loss": 3.9148,
      "step": 191600
    },
    {
      "epoch": 0.03380932703566276,
      "grad_norm": 7.2462358474731445,
      "learning_rate": 4.9577385616626924e-05,
      "loss": 3.9536,
      "step": 191700
    },
    {
      "epoch": 0.033826963617319336,
      "grad_norm": 6.022522449493408,
      "learning_rate": 4.957716515935622e-05,
      "loss": 3.9436,
      "step": 191800
    },
    {
      "epoch": 0.033844600198975915,
      "grad_norm": 5.178598880767822,
      "learning_rate": 4.957694470208551e-05,
      "loss": 3.9006,
      "step": 191900
    },
    {
      "epoch": 0.03386223678063249,
      "grad_norm": 11.386247634887695,
      "learning_rate": 4.9576724244814804e-05,
      "loss": 3.8979,
      "step": 192000
    },
    {
      "epoch": 0.03387987336228907,
      "grad_norm": 10.164113998413086,
      "learning_rate": 4.95765037875441e-05,
      "loss": 3.9242,
      "step": 192100
    },
    {
      "epoch": 0.03389750994394565,
      "grad_norm": 11.599005699157715,
      "learning_rate": 4.957628333027339e-05,
      "loss": 3.8721,
      "step": 192200
    },
    {
      "epoch": 0.03391514652560223,
      "grad_norm": 8.958449363708496,
      "learning_rate": 4.957606287300268e-05,
      "loss": 3.899,
      "step": 192300
    },
    {
      "epoch": 0.033932783107258814,
      "grad_norm": 10.656225204467773,
      "learning_rate": 4.957584241573198e-05,
      "loss": 3.8982,
      "step": 192400
    },
    {
      "epoch": 0.03395041968891539,
      "grad_norm": 8.947905540466309,
      "learning_rate": 4.957562195846127e-05,
      "loss": 3.9364,
      "step": 192500
    },
    {
      "epoch": 0.03396805627057197,
      "grad_norm": 6.338267803192139,
      "learning_rate": 4.957540150119056e-05,
      "loss": 3.8369,
      "step": 192600
    },
    {
      "epoch": 0.03398569285222855,
      "grad_norm": 8.555251121520996,
      "learning_rate": 4.957518104391985e-05,
      "loss": 3.9524,
      "step": 192700
    },
    {
      "epoch": 0.03400332943388513,
      "grad_norm": 6.940639972686768,
      "learning_rate": 4.957496058664915e-05,
      "loss": 3.8846,
      "step": 192800
    },
    {
      "epoch": 0.034020966015541707,
      "grad_norm": 6.885018825531006,
      "learning_rate": 4.9574740129378436e-05,
      "loss": 3.9004,
      "step": 192900
    },
    {
      "epoch": 0.034038602597198285,
      "grad_norm": 7.506309986114502,
      "learning_rate": 4.957451967210773e-05,
      "loss": 3.8782,
      "step": 193000
    },
    {
      "epoch": 0.03405623917885487,
      "grad_norm": 10.816181182861328,
      "learning_rate": 4.957429921483702e-05,
      "loss": 3.9596,
      "step": 193100
    },
    {
      "epoch": 0.03407387576051145,
      "grad_norm": 8.88599681854248,
      "learning_rate": 4.9574078757566315e-05,
      "loss": 4.0036,
      "step": 193200
    },
    {
      "epoch": 0.03409151234216803,
      "grad_norm": 9.27186393737793,
      "learning_rate": 4.957385830029561e-05,
      "loss": 3.8464,
      "step": 193300
    },
    {
      "epoch": 0.034109148923824606,
      "grad_norm": 9.363621711730957,
      "learning_rate": 4.95736378430249e-05,
      "loss": 3.951,
      "step": 193400
    },
    {
      "epoch": 0.034126785505481184,
      "grad_norm": 7.2165141105651855,
      "learning_rate": 4.9573417385754195e-05,
      "loss": 3.921,
      "step": 193500
    },
    {
      "epoch": 0.03414442208713776,
      "grad_norm": 8.09129524230957,
      "learning_rate": 4.957319692848349e-05,
      "loss": 3.8783,
      "step": 193600
    },
    {
      "epoch": 0.03416205866879434,
      "grad_norm": 8.05928897857666,
      "learning_rate": 4.957297647121278e-05,
      "loss": 3.8796,
      "step": 193700
    },
    {
      "epoch": 0.03417969525045092,
      "grad_norm": 4.753965854644775,
      "learning_rate": 4.9572756013942075e-05,
      "loss": 3.8427,
      "step": 193800
    },
    {
      "epoch": 0.034197331832107505,
      "grad_norm": 10.66873550415039,
      "learning_rate": 4.957253555667137e-05,
      "loss": 3.8843,
      "step": 193900
    },
    {
      "epoch": 0.034214968413764084,
      "grad_norm": 6.036839485168457,
      "learning_rate": 4.957231509940066e-05,
      "loss": 3.8651,
      "step": 194000
    },
    {
      "epoch": 0.03423260499542066,
      "grad_norm": 6.636466026306152,
      "learning_rate": 4.9572094642129954e-05,
      "loss": 4.0394,
      "step": 194100
    },
    {
      "epoch": 0.03425024157707724,
      "grad_norm": 8.537510871887207,
      "learning_rate": 4.957187418485924e-05,
      "loss": 3.7424,
      "step": 194200
    },
    {
      "epoch": 0.03426787815873382,
      "grad_norm": 5.756537437438965,
      "learning_rate": 4.957165372758854e-05,
      "loss": 3.8372,
      "step": 194300
    },
    {
      "epoch": 0.0342855147403904,
      "grad_norm": 6.569467067718506,
      "learning_rate": 4.957143327031783e-05,
      "loss": 3.9432,
      "step": 194400
    },
    {
      "epoch": 0.034303151322046976,
      "grad_norm": 5.03179931640625,
      "learning_rate": 4.957121281304712e-05,
      "loss": 3.8173,
      "step": 194500
    },
    {
      "epoch": 0.03432078790370356,
      "grad_norm": 7.9674482345581055,
      "learning_rate": 4.957099235577641e-05,
      "loss": 3.8417,
      "step": 194600
    },
    {
      "epoch": 0.03433842448536014,
      "grad_norm": 7.248775005340576,
      "learning_rate": 4.957077189850571e-05,
      "loss": 3.9405,
      "step": 194700
    },
    {
      "epoch": 0.03435606106701672,
      "grad_norm": 11.182845115661621,
      "learning_rate": 4.9570551441235e-05,
      "loss": 3.8461,
      "step": 194800
    },
    {
      "epoch": 0.0343736976486733,
      "grad_norm": 5.801526069641113,
      "learning_rate": 4.957033098396429e-05,
      "loss": 3.835,
      "step": 194900
    },
    {
      "epoch": 0.034391334230329876,
      "grad_norm": 4.869809150695801,
      "learning_rate": 4.9570110526693586e-05,
      "loss": 4.0369,
      "step": 195000
    },
    {
      "epoch": 0.034408970811986454,
      "grad_norm": 6.664838790893555,
      "learning_rate": 4.956989006942288e-05,
      "loss": 3.8865,
      "step": 195100
    },
    {
      "epoch": 0.03442660739364303,
      "grad_norm": 6.277443885803223,
      "learning_rate": 4.956966961215217e-05,
      "loss": 3.956,
      "step": 195200
    },
    {
      "epoch": 0.03444424397529961,
      "grad_norm": 7.103852272033691,
      "learning_rate": 4.9569449154881466e-05,
      "loss": 3.8978,
      "step": 195300
    },
    {
      "epoch": 0.0344618805569562,
      "grad_norm": 8.1453857421875,
      "learning_rate": 4.956922869761076e-05,
      "loss": 4.0373,
      "step": 195400
    },
    {
      "epoch": 0.034479517138612775,
      "grad_norm": 6.703872203826904,
      "learning_rate": 4.956900824034005e-05,
      "loss": 3.9052,
      "step": 195500
    },
    {
      "epoch": 0.034497153720269354,
      "grad_norm": 7.17078971862793,
      "learning_rate": 4.9568787783069346e-05,
      "loss": 3.841,
      "step": 195600
    },
    {
      "epoch": 0.03451479030192593,
      "grad_norm": 6.7360148429870605,
      "learning_rate": 4.9568567325798634e-05,
      "loss": 3.9405,
      "step": 195700
    },
    {
      "epoch": 0.03453242688358251,
      "grad_norm": 6.058888912200928,
      "learning_rate": 4.956834686852792e-05,
      "loss": 3.9035,
      "step": 195800
    },
    {
      "epoch": 0.03455006346523909,
      "grad_norm": 6.896694660186768,
      "learning_rate": 4.956812641125722e-05,
      "loss": 3.8997,
      "step": 195900
    },
    {
      "epoch": 0.03456770004689567,
      "grad_norm": 6.591447353363037,
      "learning_rate": 4.9567905953986514e-05,
      "loss": 3.8582,
      "step": 196000
    },
    {
      "epoch": 0.03458533662855225,
      "grad_norm": 5.330419540405273,
      "learning_rate": 4.95676854967158e-05,
      "loss": 3.8259,
      "step": 196100
    },
    {
      "epoch": 0.03460297321020883,
      "grad_norm": 8.881048202514648,
      "learning_rate": 4.95674650394451e-05,
      "loss": 3.8504,
      "step": 196200
    },
    {
      "epoch": 0.03462060979186541,
      "grad_norm": 7.230775833129883,
      "learning_rate": 4.9567244582174394e-05,
      "loss": 3.9282,
      "step": 196300
    },
    {
      "epoch": 0.03463824637352199,
      "grad_norm": 5.6939826011657715,
      "learning_rate": 4.956702412490368e-05,
      "loss": 3.9103,
      "step": 196400
    },
    {
      "epoch": 0.03465588295517857,
      "grad_norm": 5.474081993103027,
      "learning_rate": 4.956680366763298e-05,
      "loss": 3.854,
      "step": 196500
    },
    {
      "epoch": 0.034673519536835146,
      "grad_norm": 7.1067070960998535,
      "learning_rate": 4.956658321036227e-05,
      "loss": 3.8043,
      "step": 196600
    },
    {
      "epoch": 0.034691156118491724,
      "grad_norm": 4.865123271942139,
      "learning_rate": 4.956636275309156e-05,
      "loss": 3.868,
      "step": 196700
    },
    {
      "epoch": 0.0347087927001483,
      "grad_norm": 6.480312824249268,
      "learning_rate": 4.956614229582086e-05,
      "loss": 3.9424,
      "step": 196800
    },
    {
      "epoch": 0.03472642928180489,
      "grad_norm": 6.990014553070068,
      "learning_rate": 4.956592183855015e-05,
      "loss": 3.8318,
      "step": 196900
    },
    {
      "epoch": 0.03474406586346147,
      "grad_norm": 11.962754249572754,
      "learning_rate": 4.956570138127944e-05,
      "loss": 3.9254,
      "step": 197000
    },
    {
      "epoch": 0.034761702445118045,
      "grad_norm": 8.024184226989746,
      "learning_rate": 4.956548092400874e-05,
      "loss": 3.8964,
      "step": 197100
    },
    {
      "epoch": 0.034779339026774624,
      "grad_norm": 8.990874290466309,
      "learning_rate": 4.9565260466738026e-05,
      "loss": 3.8455,
      "step": 197200
    },
    {
      "epoch": 0.0347969756084312,
      "grad_norm": 6.9786272048950195,
      "learning_rate": 4.9565040009467314e-05,
      "loss": 3.8492,
      "step": 197300
    },
    {
      "epoch": 0.03481461219008778,
      "grad_norm": 8.807854652404785,
      "learning_rate": 4.956481955219661e-05,
      "loss": 3.8981,
      "step": 197400
    },
    {
      "epoch": 0.03483224877174436,
      "grad_norm": 6.844719886779785,
      "learning_rate": 4.9564599094925905e-05,
      "loss": 3.8551,
      "step": 197500
    },
    {
      "epoch": 0.034849885353400945,
      "grad_norm": 7.517312526702881,
      "learning_rate": 4.9564378637655194e-05,
      "loss": 3.8981,
      "step": 197600
    },
    {
      "epoch": 0.03486752193505752,
      "grad_norm": 7.138086318969727,
      "learning_rate": 4.956415818038449e-05,
      "loss": 3.9337,
      "step": 197700
    },
    {
      "epoch": 0.0348851585167141,
      "grad_norm": 6.60403299331665,
      "learning_rate": 4.9563937723113785e-05,
      "loss": 3.9132,
      "step": 197800
    },
    {
      "epoch": 0.03490279509837068,
      "grad_norm": 9.334168434143066,
      "learning_rate": 4.9563717265843074e-05,
      "loss": 4.0211,
      "step": 197900
    },
    {
      "epoch": 0.03492043168002726,
      "grad_norm": 7.020024299621582,
      "learning_rate": 4.956349680857237e-05,
      "loss": 3.9052,
      "step": 198000
    },
    {
      "epoch": 0.03493806826168384,
      "grad_norm": 7.348874092102051,
      "learning_rate": 4.9563276351301665e-05,
      "loss": 3.8387,
      "step": 198100
    },
    {
      "epoch": 0.034955704843340415,
      "grad_norm": 9.208968162536621,
      "learning_rate": 4.956305589403095e-05,
      "loss": 3.8946,
      "step": 198200
    },
    {
      "epoch": 0.034973341424996994,
      "grad_norm": 5.6167168617248535,
      "learning_rate": 4.956283543676025e-05,
      "loss": 3.8289,
      "step": 198300
    },
    {
      "epoch": 0.03499097800665358,
      "grad_norm": 6.363797664642334,
      "learning_rate": 4.9562614979489544e-05,
      "loss": 3.9019,
      "step": 198400
    },
    {
      "epoch": 0.03500861458831016,
      "grad_norm": 7.513065814971924,
      "learning_rate": 4.956239452221883e-05,
      "loss": 3.9296,
      "step": 198500
    },
    {
      "epoch": 0.035026251169966736,
      "grad_norm": 6.4893975257873535,
      "learning_rate": 4.956217406494812e-05,
      "loss": 4.0158,
      "step": 198600
    },
    {
      "epoch": 0.035043887751623315,
      "grad_norm": 7.029702186584473,
      "learning_rate": 4.956195360767742e-05,
      "loss": 4.0383,
      "step": 198700
    },
    {
      "epoch": 0.03506152433327989,
      "grad_norm": 8.07345199584961,
      "learning_rate": 4.9561733150406706e-05,
      "loss": 3.8754,
      "step": 198800
    },
    {
      "epoch": 0.03507916091493647,
      "grad_norm": 6.164796829223633,
      "learning_rate": 4.9561512693136e-05,
      "loss": 3.867,
      "step": 198900
    },
    {
      "epoch": 0.03509679749659305,
      "grad_norm": 8.664162635803223,
      "learning_rate": 4.95612922358653e-05,
      "loss": 3.8911,
      "step": 199000
    },
    {
      "epoch": 0.035114434078249636,
      "grad_norm": 9.063847541809082,
      "learning_rate": 4.9561071778594585e-05,
      "loss": 3.8255,
      "step": 199100
    },
    {
      "epoch": 0.035132070659906214,
      "grad_norm": 7.445128440856934,
      "learning_rate": 4.956085132132388e-05,
      "loss": 3.941,
      "step": 199200
    },
    {
      "epoch": 0.03514970724156279,
      "grad_norm": 8.339613914489746,
      "learning_rate": 4.9560630864053176e-05,
      "loss": 3.8494,
      "step": 199300
    },
    {
      "epoch": 0.03516734382321937,
      "grad_norm": 7.498711109161377,
      "learning_rate": 4.9560410406782465e-05,
      "loss": 3.8878,
      "step": 199400
    },
    {
      "epoch": 0.03518498040487595,
      "grad_norm": 10.31351089477539,
      "learning_rate": 4.956018994951176e-05,
      "loss": 3.8784,
      "step": 199500
    },
    {
      "epoch": 0.03520261698653253,
      "grad_norm": 11.466147422790527,
      "learning_rate": 4.9559969492241056e-05,
      "loss": 3.8757,
      "step": 199600
    },
    {
      "epoch": 0.03522025356818911,
      "grad_norm": 10.126832008361816,
      "learning_rate": 4.9559749034970345e-05,
      "loss": 3.9348,
      "step": 199700
    },
    {
      "epoch": 0.035237890149845685,
      "grad_norm": 9.229350090026855,
      "learning_rate": 4.955952857769964e-05,
      "loss": 3.949,
      "step": 199800
    },
    {
      "epoch": 0.03525552673150227,
      "grad_norm": 12.254093170166016,
      "learning_rate": 4.9559308120428936e-05,
      "loss": 3.8074,
      "step": 199900
    },
    {
      "epoch": 0.03527316331315885,
      "grad_norm": 7.533566474914551,
      "learning_rate": 4.9559087663158224e-05,
      "loss": 3.8642,
      "step": 200000
    },
    {
      "epoch": 0.03529079989481543,
      "grad_norm": 6.738366603851318,
      "learning_rate": 4.955886720588751e-05,
      "loss": 3.9324,
      "step": 200100
    },
    {
      "epoch": 0.035308436476472006,
      "grad_norm": 13.901065826416016,
      "learning_rate": 4.955864674861681e-05,
      "loss": 3.925,
      "step": 200200
    },
    {
      "epoch": 0.035326073058128585,
      "grad_norm": 7.403789043426514,
      "learning_rate": 4.95584262913461e-05,
      "loss": 3.8086,
      "step": 200300
    },
    {
      "epoch": 0.03534370963978516,
      "grad_norm": 5.472123146057129,
      "learning_rate": 4.955820583407539e-05,
      "loss": 3.8575,
      "step": 200400
    },
    {
      "epoch": 0.03536134622144174,
      "grad_norm": 5.773478031158447,
      "learning_rate": 4.955798537680469e-05,
      "loss": 3.9279,
      "step": 200500
    },
    {
      "epoch": 0.03537898280309833,
      "grad_norm": 7.441827774047852,
      "learning_rate": 4.955776491953398e-05,
      "loss": 3.9045,
      "step": 200600
    },
    {
      "epoch": 0.035396619384754906,
      "grad_norm": 10.349115371704102,
      "learning_rate": 4.955754446226327e-05,
      "loss": 3.923,
      "step": 200700
    },
    {
      "epoch": 0.035414255966411484,
      "grad_norm": 10.163052558898926,
      "learning_rate": 4.955732400499257e-05,
      "loss": 3.8796,
      "step": 200800
    },
    {
      "epoch": 0.03543189254806806,
      "grad_norm": 7.790917873382568,
      "learning_rate": 4.9557103547721856e-05,
      "loss": 3.8931,
      "step": 200900
    },
    {
      "epoch": 0.03544952912972464,
      "grad_norm": 8.850875854492188,
      "learning_rate": 4.955688309045115e-05,
      "loss": 3.8729,
      "step": 201000
    },
    {
      "epoch": 0.03546716571138122,
      "grad_norm": 5.873142719268799,
      "learning_rate": 4.955666263318045e-05,
      "loss": 3.8594,
      "step": 201100
    },
    {
      "epoch": 0.0354848022930378,
      "grad_norm": 7.844121932983398,
      "learning_rate": 4.955644217590974e-05,
      "loss": 3.9509,
      "step": 201200
    },
    {
      "epoch": 0.03550243887469438,
      "grad_norm": 10.841071128845215,
      "learning_rate": 4.955622171863903e-05,
      "loss": 3.866,
      "step": 201300
    },
    {
      "epoch": 0.03552007545635096,
      "grad_norm": 6.172073841094971,
      "learning_rate": 4.955600126136832e-05,
      "loss": 3.8136,
      "step": 201400
    },
    {
      "epoch": 0.03553771203800754,
      "grad_norm": 4.654813289642334,
      "learning_rate": 4.9555780804097616e-05,
      "loss": 3.7918,
      "step": 201500
    },
    {
      "epoch": 0.03555534861966412,
      "grad_norm": 7.917187690734863,
      "learning_rate": 4.9555560346826904e-05,
      "loss": 3.983,
      "step": 201600
    },
    {
      "epoch": 0.0355729852013207,
      "grad_norm": 5.668092250823975,
      "learning_rate": 4.95553398895562e-05,
      "loss": 3.9357,
      "step": 201700
    },
    {
      "epoch": 0.035590621782977276,
      "grad_norm": 8.055498123168945,
      "learning_rate": 4.9555119432285495e-05,
      "loss": 3.942,
      "step": 201800
    },
    {
      "epoch": 0.035608258364633855,
      "grad_norm": 8.643187522888184,
      "learning_rate": 4.9554898975014784e-05,
      "loss": 3.8991,
      "step": 201900
    },
    {
      "epoch": 0.03562589494629043,
      "grad_norm": 5.156257629394531,
      "learning_rate": 4.955467851774408e-05,
      "loss": 3.9509,
      "step": 202000
    },
    {
      "epoch": 0.03564353152794702,
      "grad_norm": 5.890399932861328,
      "learning_rate": 4.9554458060473375e-05,
      "loss": 3.9253,
      "step": 202100
    },
    {
      "epoch": 0.0356611681096036,
      "grad_norm": 9.130926132202148,
      "learning_rate": 4.9554237603202664e-05,
      "loss": 3.9662,
      "step": 202200
    },
    {
      "epoch": 0.035678804691260176,
      "grad_norm": 7.65815544128418,
      "learning_rate": 4.955401714593196e-05,
      "loss": 3.9017,
      "step": 202300
    },
    {
      "epoch": 0.035696441272916754,
      "grad_norm": 8.839824676513672,
      "learning_rate": 4.9553796688661255e-05,
      "loss": 3.9028,
      "step": 202400
    },
    {
      "epoch": 0.03571407785457333,
      "grad_norm": 4.86443567276001,
      "learning_rate": 4.955357623139054e-05,
      "loss": 3.8893,
      "step": 202500
    },
    {
      "epoch": 0.03573171443622991,
      "grad_norm": 7.846296310424805,
      "learning_rate": 4.955335577411984e-05,
      "loss": 3.9484,
      "step": 202600
    },
    {
      "epoch": 0.03574935101788649,
      "grad_norm": 6.284113883972168,
      "learning_rate": 4.9553135316849134e-05,
      "loss": 3.8404,
      "step": 202700
    },
    {
      "epoch": 0.03576698759954307,
      "grad_norm": 6.433334827423096,
      "learning_rate": 4.955291485957842e-05,
      "loss": 3.8754,
      "step": 202800
    },
    {
      "epoch": 0.035784624181199653,
      "grad_norm": 5.761466026306152,
      "learning_rate": 4.955269440230771e-05,
      "loss": 3.9191,
      "step": 202900
    },
    {
      "epoch": 0.03580226076285623,
      "grad_norm": 7.978728771209717,
      "learning_rate": 4.955247394503701e-05,
      "loss": 3.8477,
      "step": 203000
    },
    {
      "epoch": 0.03581989734451281,
      "grad_norm": 12.579093933105469,
      "learning_rate": 4.9552253487766296e-05,
      "loss": 3.8828,
      "step": 203100
    },
    {
      "epoch": 0.03583753392616939,
      "grad_norm": 8.904532432556152,
      "learning_rate": 4.955203303049559e-05,
      "loss": 3.7937,
      "step": 203200
    },
    {
      "epoch": 0.03585517050782597,
      "grad_norm": 6.99296760559082,
      "learning_rate": 4.955181257322489e-05,
      "loss": 3.9962,
      "step": 203300
    },
    {
      "epoch": 0.035872807089482546,
      "grad_norm": 6.59491491317749,
      "learning_rate": 4.9551592115954175e-05,
      "loss": 3.9702,
      "step": 203400
    },
    {
      "epoch": 0.035890443671139124,
      "grad_norm": 8.121475219726562,
      "learning_rate": 4.955137165868347e-05,
      "loss": 3.8098,
      "step": 203500
    },
    {
      "epoch": 0.03590808025279571,
      "grad_norm": 7.282920837402344,
      "learning_rate": 4.9551151201412766e-05,
      "loss": 3.9194,
      "step": 203600
    },
    {
      "epoch": 0.03592571683445229,
      "grad_norm": 6.036340236663818,
      "learning_rate": 4.9550930744142055e-05,
      "loss": 3.8105,
      "step": 203700
    },
    {
      "epoch": 0.03594335341610887,
      "grad_norm": 5.076497554779053,
      "learning_rate": 4.955071028687135e-05,
      "loss": 3.9888,
      "step": 203800
    },
    {
      "epoch": 0.035960989997765445,
      "grad_norm": 8.234028816223145,
      "learning_rate": 4.9550489829600646e-05,
      "loss": 3.9014,
      "step": 203900
    },
    {
      "epoch": 0.035978626579422024,
      "grad_norm": 5.525613307952881,
      "learning_rate": 4.9550269372329935e-05,
      "loss": 3.855,
      "step": 204000
    },
    {
      "epoch": 0.0359962631610786,
      "grad_norm": 5.918509006500244,
      "learning_rate": 4.955004891505923e-05,
      "loss": 3.8682,
      "step": 204100
    },
    {
      "epoch": 0.03601389974273518,
      "grad_norm": 9.542983055114746,
      "learning_rate": 4.954982845778852e-05,
      "loss": 3.8384,
      "step": 204200
    },
    {
      "epoch": 0.03603153632439176,
      "grad_norm": 10.833667755126953,
      "learning_rate": 4.9549608000517814e-05,
      "loss": 3.9584,
      "step": 204300
    },
    {
      "epoch": 0.036049172906048345,
      "grad_norm": 7.3966193199157715,
      "learning_rate": 4.95493875432471e-05,
      "loss": 3.7854,
      "step": 204400
    },
    {
      "epoch": 0.03606680948770492,
      "grad_norm": 11.621007919311523,
      "learning_rate": 4.95491670859764e-05,
      "loss": 3.9273,
      "step": 204500
    },
    {
      "epoch": 0.0360844460693615,
      "grad_norm": 13.911678314208984,
      "learning_rate": 4.954894662870569e-05,
      "loss": 3.9076,
      "step": 204600
    },
    {
      "epoch": 0.03610208265101808,
      "grad_norm": 7.073949813842773,
      "learning_rate": 4.954872617143498e-05,
      "loss": 3.8974,
      "step": 204700
    },
    {
      "epoch": 0.03611971923267466,
      "grad_norm": 5.815494060516357,
      "learning_rate": 4.954850571416428e-05,
      "loss": 3.9273,
      "step": 204800
    },
    {
      "epoch": 0.03613735581433124,
      "grad_norm": 6.942405700683594,
      "learning_rate": 4.954828525689357e-05,
      "loss": 3.9269,
      "step": 204900
    },
    {
      "epoch": 0.036154992395987816,
      "grad_norm": 16.21009635925293,
      "learning_rate": 4.954806479962286e-05,
      "loss": 3.9365,
      "step": 205000
    },
    {
      "epoch": 0.0361726289776444,
      "grad_norm": 6.582633018493652,
      "learning_rate": 4.954784434235216e-05,
      "loss": 3.8394,
      "step": 205100
    },
    {
      "epoch": 0.03619026555930098,
      "grad_norm": 8.050395965576172,
      "learning_rate": 4.9547623885081446e-05,
      "loss": 3.7454,
      "step": 205200
    },
    {
      "epoch": 0.03620790214095756,
      "grad_norm": 7.400535583496094,
      "learning_rate": 4.954740342781074e-05,
      "loss": 3.9379,
      "step": 205300
    },
    {
      "epoch": 0.03622553872261414,
      "grad_norm": 10.207662582397461,
      "learning_rate": 4.954718297054004e-05,
      "loss": 3.9543,
      "step": 205400
    },
    {
      "epoch": 0.036243175304270715,
      "grad_norm": 8.126660346984863,
      "learning_rate": 4.9546962513269326e-05,
      "loss": 3.8227,
      "step": 205500
    },
    {
      "epoch": 0.036260811885927294,
      "grad_norm": 9.007780075073242,
      "learning_rate": 4.954674205599862e-05,
      "loss": 3.8364,
      "step": 205600
    },
    {
      "epoch": 0.03627844846758387,
      "grad_norm": 9.511798858642578,
      "learning_rate": 4.954652159872791e-05,
      "loss": 3.8747,
      "step": 205700
    },
    {
      "epoch": 0.03629608504924045,
      "grad_norm": 9.991012573242188,
      "learning_rate": 4.95463011414572e-05,
      "loss": 3.9197,
      "step": 205800
    },
    {
      "epoch": 0.036313721630897036,
      "grad_norm": 5.420840740203857,
      "learning_rate": 4.9546080684186494e-05,
      "loss": 3.8527,
      "step": 205900
    },
    {
      "epoch": 0.036331358212553615,
      "grad_norm": 5.723538398742676,
      "learning_rate": 4.954586022691579e-05,
      "loss": 3.8877,
      "step": 206000
    },
    {
      "epoch": 0.03634899479421019,
      "grad_norm": 8.982623100280762,
      "learning_rate": 4.954563976964508e-05,
      "loss": 3.882,
      "step": 206100
    },
    {
      "epoch": 0.03636663137586677,
      "grad_norm": 7.5039896965026855,
      "learning_rate": 4.9545419312374374e-05,
      "loss": 3.895,
      "step": 206200
    },
    {
      "epoch": 0.03638426795752335,
      "grad_norm": 4.774082183837891,
      "learning_rate": 4.954519885510367e-05,
      "loss": 3.9911,
      "step": 206300
    },
    {
      "epoch": 0.03640190453917993,
      "grad_norm": 8.143362998962402,
      "learning_rate": 4.954497839783296e-05,
      "loss": 3.9401,
      "step": 206400
    },
    {
      "epoch": 0.03641954112083651,
      "grad_norm": 8.607250213623047,
      "learning_rate": 4.9544757940562254e-05,
      "loss": 3.9137,
      "step": 206500
    },
    {
      "epoch": 0.03643717770249309,
      "grad_norm": 6.108802795410156,
      "learning_rate": 4.954453748329155e-05,
      "loss": 3.8488,
      "step": 206600
    },
    {
      "epoch": 0.03645481428414967,
      "grad_norm": 6.653571605682373,
      "learning_rate": 4.954431702602084e-05,
      "loss": 3.8482,
      "step": 206700
    },
    {
      "epoch": 0.03647245086580625,
      "grad_norm": 6.17507266998291,
      "learning_rate": 4.954409656875013e-05,
      "loss": 3.9587,
      "step": 206800
    },
    {
      "epoch": 0.03649008744746283,
      "grad_norm": 5.924243927001953,
      "learning_rate": 4.954387611147943e-05,
      "loss": 3.9942,
      "step": 206900
    },
    {
      "epoch": 0.03650772402911941,
      "grad_norm": 7.587080478668213,
      "learning_rate": 4.954365565420872e-05,
      "loss": 3.9966,
      "step": 207000
    },
    {
      "epoch": 0.036525360610775985,
      "grad_norm": 7.924434661865234,
      "learning_rate": 4.954343519693801e-05,
      "loss": 3.9333,
      "step": 207100
    },
    {
      "epoch": 0.036542997192432564,
      "grad_norm": 8.892093658447266,
      "learning_rate": 4.95432147396673e-05,
      "loss": 3.8742,
      "step": 207200
    },
    {
      "epoch": 0.03656063377408914,
      "grad_norm": 7.505297660827637,
      "learning_rate": 4.954299428239659e-05,
      "loss": 3.8128,
      "step": 207300
    },
    {
      "epoch": 0.03657827035574573,
      "grad_norm": 5.37882137298584,
      "learning_rate": 4.9542773825125886e-05,
      "loss": 3.8178,
      "step": 207400
    },
    {
      "epoch": 0.036595906937402306,
      "grad_norm": 8.160862922668457,
      "learning_rate": 4.954255336785518e-05,
      "loss": 3.8279,
      "step": 207500
    },
    {
      "epoch": 0.036613543519058885,
      "grad_norm": 7.346384048461914,
      "learning_rate": 4.954233291058447e-05,
      "loss": 3.9473,
      "step": 207600
    },
    {
      "epoch": 0.03663118010071546,
      "grad_norm": 8.219346046447754,
      "learning_rate": 4.9542112453313765e-05,
      "loss": 3.7934,
      "step": 207700
    },
    {
      "epoch": 0.03664881668237204,
      "grad_norm": 8.950958251953125,
      "learning_rate": 4.954189199604306e-05,
      "loss": 3.951,
      "step": 207800
    },
    {
      "epoch": 0.03666645326402862,
      "grad_norm": 5.526369571685791,
      "learning_rate": 4.954167153877235e-05,
      "loss": 3.8171,
      "step": 207900
    },
    {
      "epoch": 0.0366840898456852,
      "grad_norm": 5.9535651206970215,
      "learning_rate": 4.9541451081501645e-05,
      "loss": 3.8817,
      "step": 208000
    },
    {
      "epoch": 0.036701726427341784,
      "grad_norm": 5.796947002410889,
      "learning_rate": 4.954123062423094e-05,
      "loss": 3.7753,
      "step": 208100
    },
    {
      "epoch": 0.03671936300899836,
      "grad_norm": 7.698080062866211,
      "learning_rate": 4.954101016696023e-05,
      "loss": 3.8176,
      "step": 208200
    },
    {
      "epoch": 0.03673699959065494,
      "grad_norm": 9.351568222045898,
      "learning_rate": 4.9540789709689525e-05,
      "loss": 3.9215,
      "step": 208300
    },
    {
      "epoch": 0.03675463617231152,
      "grad_norm": 7.393157005310059,
      "learning_rate": 4.954056925241882e-05,
      "loss": 3.9003,
      "step": 208400
    },
    {
      "epoch": 0.0367722727539681,
      "grad_norm": 5.634836673736572,
      "learning_rate": 4.954034879514811e-05,
      "loss": 3.8324,
      "step": 208500
    },
    {
      "epoch": 0.036789909335624676,
      "grad_norm": 7.128263473510742,
      "learning_rate": 4.95401283378774e-05,
      "loss": 3.7773,
      "step": 208600
    },
    {
      "epoch": 0.036807545917281255,
      "grad_norm": 8.011957168579102,
      "learning_rate": 4.953990788060669e-05,
      "loss": 3.8903,
      "step": 208700
    },
    {
      "epoch": 0.03682518249893783,
      "grad_norm": 5.512641906738281,
      "learning_rate": 4.953968742333598e-05,
      "loss": 3.8643,
      "step": 208800
    },
    {
      "epoch": 0.03684281908059442,
      "grad_norm": 7.213906288146973,
      "learning_rate": 4.953946696606528e-05,
      "loss": 3.9387,
      "step": 208900
    },
    {
      "epoch": 0.036860455662251,
      "grad_norm": 9.35192584991455,
      "learning_rate": 4.953924650879457e-05,
      "loss": 3.9021,
      "step": 209000
    },
    {
      "epoch": 0.036878092243907576,
      "grad_norm": 6.4395060539245605,
      "learning_rate": 4.953902605152386e-05,
      "loss": 3.8671,
      "step": 209100
    },
    {
      "epoch": 0.036895728825564154,
      "grad_norm": 7.571158409118652,
      "learning_rate": 4.953880559425316e-05,
      "loss": 3.8905,
      "step": 209200
    },
    {
      "epoch": 0.03691336540722073,
      "grad_norm": 5.859551429748535,
      "learning_rate": 4.953858513698245e-05,
      "loss": 3.8779,
      "step": 209300
    },
    {
      "epoch": 0.03693100198887731,
      "grad_norm": 6.72170352935791,
      "learning_rate": 4.953836467971174e-05,
      "loss": 3.8699,
      "step": 209400
    },
    {
      "epoch": 0.03694863857053389,
      "grad_norm": 5.988353252410889,
      "learning_rate": 4.9538144222441036e-05,
      "loss": 3.8681,
      "step": 209500
    },
    {
      "epoch": 0.036966275152190475,
      "grad_norm": 6.033074855804443,
      "learning_rate": 4.953792376517033e-05,
      "loss": 3.8527,
      "step": 209600
    },
    {
      "epoch": 0.036983911733847054,
      "grad_norm": 9.400349617004395,
      "learning_rate": 4.953770330789962e-05,
      "loss": 3.9682,
      "step": 209700
    },
    {
      "epoch": 0.03700154831550363,
      "grad_norm": 9.145713806152344,
      "learning_rate": 4.9537482850628916e-05,
      "loss": 3.7393,
      "step": 209800
    },
    {
      "epoch": 0.03701918489716021,
      "grad_norm": 6.883082866668701,
      "learning_rate": 4.953726239335821e-05,
      "loss": 3.8197,
      "step": 209900
    },
    {
      "epoch": 0.03703682147881679,
      "grad_norm": 5.146891117095947,
      "learning_rate": 4.95370419360875e-05,
      "loss": 3.8685,
      "step": 210000
    },
    {
      "epoch": 0.03705445806047337,
      "grad_norm": 6.143849849700928,
      "learning_rate": 4.953682147881679e-05,
      "loss": 3.8956,
      "step": 210100
    },
    {
      "epoch": 0.037072094642129946,
      "grad_norm": 9.823972702026367,
      "learning_rate": 4.9536601021546084e-05,
      "loss": 3.8703,
      "step": 210200
    },
    {
      "epoch": 0.037089731223786525,
      "grad_norm": 5.413976669311523,
      "learning_rate": 4.953638056427537e-05,
      "loss": 3.8946,
      "step": 210300
    },
    {
      "epoch": 0.03710736780544311,
      "grad_norm": 6.75929069519043,
      "learning_rate": 4.953616010700467e-05,
      "loss": 3.8214,
      "step": 210400
    },
    {
      "epoch": 0.03712500438709969,
      "grad_norm": 6.467700958251953,
      "learning_rate": 4.9535939649733964e-05,
      "loss": 3.8687,
      "step": 210500
    },
    {
      "epoch": 0.03714264096875627,
      "grad_norm": 6.79014253616333,
      "learning_rate": 4.953571919246325e-05,
      "loss": 3.9842,
      "step": 210600
    },
    {
      "epoch": 0.037160277550412846,
      "grad_norm": 6.7757110595703125,
      "learning_rate": 4.953549873519255e-05,
      "loss": 3.9066,
      "step": 210700
    },
    {
      "epoch": 0.037177914132069424,
      "grad_norm": 6.031835079193115,
      "learning_rate": 4.9535278277921843e-05,
      "loss": 3.7983,
      "step": 210800
    },
    {
      "epoch": 0.037195550713726,
      "grad_norm": 10.030887603759766,
      "learning_rate": 4.953505782065113e-05,
      "loss": 3.9715,
      "step": 210900
    },
    {
      "epoch": 0.03721318729538258,
      "grad_norm": 8.370870590209961,
      "learning_rate": 4.953483736338043e-05,
      "loss": 3.8087,
      "step": 211000
    },
    {
      "epoch": 0.03723082387703917,
      "grad_norm": 6.777022838592529,
      "learning_rate": 4.953461690610972e-05,
      "loss": 3.8487,
      "step": 211100
    },
    {
      "epoch": 0.037248460458695745,
      "grad_norm": 6.524583339691162,
      "learning_rate": 4.953439644883901e-05,
      "loss": 3.9337,
      "step": 211200
    },
    {
      "epoch": 0.037266097040352324,
      "grad_norm": 6.34177303314209,
      "learning_rate": 4.953417599156831e-05,
      "loss": 3.9023,
      "step": 211300
    },
    {
      "epoch": 0.0372837336220089,
      "grad_norm": 7.577175140380859,
      "learning_rate": 4.9533955534297596e-05,
      "loss": 3.8308,
      "step": 211400
    },
    {
      "epoch": 0.03730137020366548,
      "grad_norm": 10.624886512756348,
      "learning_rate": 4.953373507702689e-05,
      "loss": 3.9275,
      "step": 211500
    },
    {
      "epoch": 0.03731900678532206,
      "grad_norm": 10.297964096069336,
      "learning_rate": 4.953351461975618e-05,
      "loss": 3.8981,
      "step": 211600
    },
    {
      "epoch": 0.03733664336697864,
      "grad_norm": 9.003543853759766,
      "learning_rate": 4.9533294162485476e-05,
      "loss": 3.8921,
      "step": 211700
    },
    {
      "epoch": 0.037354279948635216,
      "grad_norm": 6.041595458984375,
      "learning_rate": 4.953307370521477e-05,
      "loss": 3.8171,
      "step": 211800
    },
    {
      "epoch": 0.0373719165302918,
      "grad_norm": 5.928317546844482,
      "learning_rate": 4.953285324794406e-05,
      "loss": 3.8511,
      "step": 211900
    },
    {
      "epoch": 0.03738955311194838,
      "grad_norm": 5.3542351722717285,
      "learning_rate": 4.9532632790673355e-05,
      "loss": 3.8471,
      "step": 212000
    },
    {
      "epoch": 0.03740718969360496,
      "grad_norm": 10.992380142211914,
      "learning_rate": 4.953241233340265e-05,
      "loss": 3.7858,
      "step": 212100
    },
    {
      "epoch": 0.03742482627526154,
      "grad_norm": 8.188756942749023,
      "learning_rate": 4.953219187613194e-05,
      "loss": 3.8973,
      "step": 212200
    },
    {
      "epoch": 0.037442462856918116,
      "grad_norm": 11.507745742797852,
      "learning_rate": 4.9531971418861235e-05,
      "loss": 3.9069,
      "step": 212300
    },
    {
      "epoch": 0.037460099438574694,
      "grad_norm": 7.983794212341309,
      "learning_rate": 4.953175096159053e-05,
      "loss": 3.9189,
      "step": 212400
    },
    {
      "epoch": 0.03747773602023127,
      "grad_norm": 6.961728572845459,
      "learning_rate": 4.953153050431982e-05,
      "loss": 3.9164,
      "step": 212500
    },
    {
      "epoch": 0.03749537260188786,
      "grad_norm": 6.344881534576416,
      "learning_rate": 4.9531310047049114e-05,
      "loss": 3.9427,
      "step": 212600
    },
    {
      "epoch": 0.03751300918354444,
      "grad_norm": 11.616089820861816,
      "learning_rate": 4.953108958977841e-05,
      "loss": 3.9283,
      "step": 212700
    },
    {
      "epoch": 0.037530645765201015,
      "grad_norm": 6.25382137298584,
      "learning_rate": 4.95308691325077e-05,
      "loss": 3.8771,
      "step": 212800
    },
    {
      "epoch": 0.037548282346857594,
      "grad_norm": 9.848411560058594,
      "learning_rate": 4.953064867523699e-05,
      "loss": 3.8352,
      "step": 212900
    },
    {
      "epoch": 0.03756591892851417,
      "grad_norm": 6.768625736236572,
      "learning_rate": 4.953042821796628e-05,
      "loss": 3.8424,
      "step": 213000
    },
    {
      "epoch": 0.03758355551017075,
      "grad_norm": 6.323286056518555,
      "learning_rate": 4.953020776069557e-05,
      "loss": 3.8864,
      "step": 213100
    },
    {
      "epoch": 0.03760119209182733,
      "grad_norm": 6.744988918304443,
      "learning_rate": 4.952998730342487e-05,
      "loss": 4.0076,
      "step": 213200
    },
    {
      "epoch": 0.03761882867348391,
      "grad_norm": 5.643828392028809,
      "learning_rate": 4.952976684615416e-05,
      "loss": 3.9483,
      "step": 213300
    },
    {
      "epoch": 0.03763646525514049,
      "grad_norm": 6.56209135055542,
      "learning_rate": 4.952954638888345e-05,
      "loss": 3.9239,
      "step": 213400
    },
    {
      "epoch": 0.03765410183679707,
      "grad_norm": 6.1548380851745605,
      "learning_rate": 4.9529325931612747e-05,
      "loss": 3.944,
      "step": 213500
    },
    {
      "epoch": 0.03767173841845365,
      "grad_norm": 7.290313243865967,
      "learning_rate": 4.952910547434204e-05,
      "loss": 3.9234,
      "step": 213600
    },
    {
      "epoch": 0.03768937500011023,
      "grad_norm": 8.077037811279297,
      "learning_rate": 4.952888501707133e-05,
      "loss": 3.8555,
      "step": 213700
    },
    {
      "epoch": 0.03770701158176681,
      "grad_norm": 6.9267168045043945,
      "learning_rate": 4.9528664559800626e-05,
      "loss": 3.9935,
      "step": 213800
    },
    {
      "epoch": 0.037724648163423385,
      "grad_norm": 7.6672043800354,
      "learning_rate": 4.952844410252992e-05,
      "loss": 3.8358,
      "step": 213900
    },
    {
      "epoch": 0.037742284745079964,
      "grad_norm": 8.221244812011719,
      "learning_rate": 4.952822364525921e-05,
      "loss": 3.8815,
      "step": 214000
    },
    {
      "epoch": 0.03775992132673655,
      "grad_norm": 7.72647762298584,
      "learning_rate": 4.9528003187988506e-05,
      "loss": 3.8391,
      "step": 214100
    },
    {
      "epoch": 0.03777755790839313,
      "grad_norm": 7.020346641540527,
      "learning_rate": 4.9527782730717795e-05,
      "loss": 3.841,
      "step": 214200
    },
    {
      "epoch": 0.037795194490049706,
      "grad_norm": 8.638374328613281,
      "learning_rate": 4.952756227344709e-05,
      "loss": 3.8517,
      "step": 214300
    },
    {
      "epoch": 0.037812831071706285,
      "grad_norm": 5.760591506958008,
      "learning_rate": 4.952734181617638e-05,
      "loss": 3.8207,
      "step": 214400
    },
    {
      "epoch": 0.03783046765336286,
      "grad_norm": 11.636270523071289,
      "learning_rate": 4.9527121358905674e-05,
      "loss": 3.8872,
      "step": 214500
    },
    {
      "epoch": 0.03784810423501944,
      "grad_norm": 6.785744667053223,
      "learning_rate": 4.952690090163496e-05,
      "loss": 3.8716,
      "step": 214600
    },
    {
      "epoch": 0.03786574081667602,
      "grad_norm": 8.023397445678711,
      "learning_rate": 4.952668044436426e-05,
      "loss": 3.9034,
      "step": 214700
    },
    {
      "epoch": 0.0378833773983326,
      "grad_norm": 7.5468549728393555,
      "learning_rate": 4.9526459987093554e-05,
      "loss": 3.8285,
      "step": 214800
    },
    {
      "epoch": 0.037901013979989184,
      "grad_norm": 9.348997116088867,
      "learning_rate": 4.952623952982284e-05,
      "loss": 3.9607,
      "step": 214900
    },
    {
      "epoch": 0.03791865056164576,
      "grad_norm": 10.94981861114502,
      "learning_rate": 4.952601907255214e-05,
      "loss": 3.8809,
      "step": 215000
    },
    {
      "epoch": 0.03793628714330234,
      "grad_norm": 5.880869388580322,
      "learning_rate": 4.9525798615281433e-05,
      "loss": 3.8914,
      "step": 215100
    },
    {
      "epoch": 0.03795392372495892,
      "grad_norm": 6.422975540161133,
      "learning_rate": 4.952557815801072e-05,
      "loss": 3.8838,
      "step": 215200
    },
    {
      "epoch": 0.0379715603066155,
      "grad_norm": 7.689650058746338,
      "learning_rate": 4.952535770074002e-05,
      "loss": 3.8594,
      "step": 215300
    },
    {
      "epoch": 0.03798919688827208,
      "grad_norm": 6.524754524230957,
      "learning_rate": 4.952513724346931e-05,
      "loss": 3.8964,
      "step": 215400
    },
    {
      "epoch": 0.038006833469928655,
      "grad_norm": 9.563997268676758,
      "learning_rate": 4.95249167861986e-05,
      "loss": 3.8659,
      "step": 215500
    },
    {
      "epoch": 0.03802447005158524,
      "grad_norm": 6.484688758850098,
      "learning_rate": 4.95246963289279e-05,
      "loss": 3.8424,
      "step": 215600
    },
    {
      "epoch": 0.03804210663324182,
      "grad_norm": 6.080249786376953,
      "learning_rate": 4.9524475871657186e-05,
      "loss": 3.903,
      "step": 215700
    },
    {
      "epoch": 0.0380597432148984,
      "grad_norm": 5.570042610168457,
      "learning_rate": 4.9524255414386475e-05,
      "loss": 3.9201,
      "step": 215800
    },
    {
      "epoch": 0.038077379796554976,
      "grad_norm": 6.168600559234619,
      "learning_rate": 4.952403495711577e-05,
      "loss": 3.7908,
      "step": 215900
    },
    {
      "epoch": 0.038095016378211555,
      "grad_norm": 9.007262229919434,
      "learning_rate": 4.9523814499845066e-05,
      "loss": 3.8548,
      "step": 216000
    },
    {
      "epoch": 0.03811265295986813,
      "grad_norm": 7.947815418243408,
      "learning_rate": 4.9523594042574354e-05,
      "loss": 3.963,
      "step": 216100
    },
    {
      "epoch": 0.03813028954152471,
      "grad_norm": 12.719133377075195,
      "learning_rate": 4.952337358530365e-05,
      "loss": 3.883,
      "step": 216200
    },
    {
      "epoch": 0.03814792612318129,
      "grad_norm": 7.812444686889648,
      "learning_rate": 4.9523153128032945e-05,
      "loss": 3.6456,
      "step": 216300
    },
    {
      "epoch": 0.038165562704837876,
      "grad_norm": 7.9849042892456055,
      "learning_rate": 4.9522932670762234e-05,
      "loss": 3.8916,
      "step": 216400
    },
    {
      "epoch": 0.038183199286494454,
      "grad_norm": 7.347374439239502,
      "learning_rate": 4.952271221349153e-05,
      "loss": 3.8529,
      "step": 216500
    },
    {
      "epoch": 0.03820083586815103,
      "grad_norm": 6.366581916809082,
      "learning_rate": 4.9522491756220825e-05,
      "loss": 3.8439,
      "step": 216600
    },
    {
      "epoch": 0.03821847244980761,
      "grad_norm": 10.143885612487793,
      "learning_rate": 4.9522271298950114e-05,
      "loss": 3.8793,
      "step": 216700
    },
    {
      "epoch": 0.03823610903146419,
      "grad_norm": 7.540766716003418,
      "learning_rate": 4.952205084167941e-05,
      "loss": 3.8986,
      "step": 216800
    },
    {
      "epoch": 0.03825374561312077,
      "grad_norm": 6.677589416503906,
      "learning_rate": 4.9521830384408704e-05,
      "loss": 3.8808,
      "step": 216900
    },
    {
      "epoch": 0.03827138219477735,
      "grad_norm": 5.760190486907959,
      "learning_rate": 4.952160992713799e-05,
      "loss": 3.7635,
      "step": 217000
    },
    {
      "epoch": 0.03828901877643393,
      "grad_norm": 8.010945320129395,
      "learning_rate": 4.952138946986729e-05,
      "loss": 3.8605,
      "step": 217100
    },
    {
      "epoch": 0.03830665535809051,
      "grad_norm": 8.632818222045898,
      "learning_rate": 4.952116901259658e-05,
      "loss": 3.8834,
      "step": 217200
    },
    {
      "epoch": 0.03832429193974709,
      "grad_norm": 7.3031134605407715,
      "learning_rate": 4.9520948555325866e-05,
      "loss": 3.8323,
      "step": 217300
    },
    {
      "epoch": 0.03834192852140367,
      "grad_norm": 5.620641231536865,
      "learning_rate": 4.952072809805516e-05,
      "loss": 3.8713,
      "step": 217400
    },
    {
      "epoch": 0.038359565103060246,
      "grad_norm": 9.312651634216309,
      "learning_rate": 4.952050764078446e-05,
      "loss": 3.8149,
      "step": 217500
    },
    {
      "epoch": 0.038377201684716825,
      "grad_norm": 7.070362567901611,
      "learning_rate": 4.9520287183513746e-05,
      "loss": 3.8248,
      "step": 217600
    },
    {
      "epoch": 0.0383948382663734,
      "grad_norm": 6.542611122131348,
      "learning_rate": 4.952006672624304e-05,
      "loss": 3.8099,
      "step": 217700
    },
    {
      "epoch": 0.03841247484802998,
      "grad_norm": 7.6581573486328125,
      "learning_rate": 4.9519846268972337e-05,
      "loss": 3.8197,
      "step": 217800
    },
    {
      "epoch": 0.03843011142968657,
      "grad_norm": 6.2209062576293945,
      "learning_rate": 4.9519625811701625e-05,
      "loss": 3.9578,
      "step": 217900
    },
    {
      "epoch": 0.038447748011343146,
      "grad_norm": 5.027271747589111,
      "learning_rate": 4.951940535443092e-05,
      "loss": 3.8507,
      "step": 218000
    },
    {
      "epoch": 0.038465384592999724,
      "grad_norm": 9.804651260375977,
      "learning_rate": 4.9519184897160216e-05,
      "loss": 3.8171,
      "step": 218100
    },
    {
      "epoch": 0.0384830211746563,
      "grad_norm": 7.278557777404785,
      "learning_rate": 4.9518964439889505e-05,
      "loss": 3.9056,
      "step": 218200
    },
    {
      "epoch": 0.03850065775631288,
      "grad_norm": 11.091024398803711,
      "learning_rate": 4.95187439826188e-05,
      "loss": 3.871,
      "step": 218300
    },
    {
      "epoch": 0.03851829433796946,
      "grad_norm": 6.1771087646484375,
      "learning_rate": 4.9518523525348096e-05,
      "loss": 3.8809,
      "step": 218400
    },
    {
      "epoch": 0.03853593091962604,
      "grad_norm": 7.850979328155518,
      "learning_rate": 4.9518303068077385e-05,
      "loss": 3.8151,
      "step": 218500
    },
    {
      "epoch": 0.03855356750128262,
      "grad_norm": 7.927789211273193,
      "learning_rate": 4.951808261080667e-05,
      "loss": 3.971,
      "step": 218600
    },
    {
      "epoch": 0.0385712040829392,
      "grad_norm": 8.581428527832031,
      "learning_rate": 4.951786215353597e-05,
      "loss": 3.9444,
      "step": 218700
    },
    {
      "epoch": 0.03858884066459578,
      "grad_norm": 5.56629753112793,
      "learning_rate": 4.951764169626526e-05,
      "loss": 3.8232,
      "step": 218800
    },
    {
      "epoch": 0.03860647724625236,
      "grad_norm": 8.13283920288086,
      "learning_rate": 4.951742123899455e-05,
      "loss": 3.931,
      "step": 218900
    },
    {
      "epoch": 0.03862411382790894,
      "grad_norm": 4.750533580780029,
      "learning_rate": 4.951720078172385e-05,
      "loss": 3.9055,
      "step": 219000
    },
    {
      "epoch": 0.038641750409565516,
      "grad_norm": 9.090982437133789,
      "learning_rate": 4.951698032445314e-05,
      "loss": 3.7777,
      "step": 219100
    },
    {
      "epoch": 0.038659386991222094,
      "grad_norm": 6.19020938873291,
      "learning_rate": 4.951675986718243e-05,
      "loss": 3.8162,
      "step": 219200
    },
    {
      "epoch": 0.03867702357287867,
      "grad_norm": 8.46958065032959,
      "learning_rate": 4.951653940991173e-05,
      "loss": 3.8942,
      "step": 219300
    },
    {
      "epoch": 0.03869466015453526,
      "grad_norm": 7.305179119110107,
      "learning_rate": 4.9516318952641017e-05,
      "loss": 3.8614,
      "step": 219400
    },
    {
      "epoch": 0.03871229673619184,
      "grad_norm": 10.538644790649414,
      "learning_rate": 4.951609849537031e-05,
      "loss": 3.8849,
      "step": 219500
    },
    {
      "epoch": 0.038729933317848415,
      "grad_norm": 8.504528045654297,
      "learning_rate": 4.951587803809961e-05,
      "loss": 3.8763,
      "step": 219600
    },
    {
      "epoch": 0.038747569899504994,
      "grad_norm": 5.36759614944458,
      "learning_rate": 4.9515657580828896e-05,
      "loss": 3.7439,
      "step": 219700
    },
    {
      "epoch": 0.03876520648116157,
      "grad_norm": 6.285497188568115,
      "learning_rate": 4.951543712355819e-05,
      "loss": 3.956,
      "step": 219800
    },
    {
      "epoch": 0.03878284306281815,
      "grad_norm": 6.394875526428223,
      "learning_rate": 4.951521666628749e-05,
      "loss": 3.8969,
      "step": 219900
    },
    {
      "epoch": 0.03880047964447473,
      "grad_norm": 8.701827049255371,
      "learning_rate": 4.9514996209016776e-05,
      "loss": 3.8771,
      "step": 220000
    },
    {
      "epoch": 0.038818116226131315,
      "grad_norm": 8.111438751220703,
      "learning_rate": 4.9514775751746065e-05,
      "loss": 3.8048,
      "step": 220100
    },
    {
      "epoch": 0.03883575280778789,
      "grad_norm": 6.641472816467285,
      "learning_rate": 4.951455529447536e-05,
      "loss": 3.9051,
      "step": 220200
    },
    {
      "epoch": 0.03885338938944447,
      "grad_norm": 6.918719291687012,
      "learning_rate": 4.951433483720465e-05,
      "loss": 3.918,
      "step": 220300
    },
    {
      "epoch": 0.03887102597110105,
      "grad_norm": 6.596826553344727,
      "learning_rate": 4.9514114379933944e-05,
      "loss": 3.8928,
      "step": 220400
    },
    {
      "epoch": 0.03888866255275763,
      "grad_norm": 7.601325035095215,
      "learning_rate": 4.951389392266324e-05,
      "loss": 3.89,
      "step": 220500
    },
    {
      "epoch": 0.03890629913441421,
      "grad_norm": 6.56550931930542,
      "learning_rate": 4.951367346539253e-05,
      "loss": 3.8772,
      "step": 220600
    },
    {
      "epoch": 0.038923935716070786,
      "grad_norm": 8.143733978271484,
      "learning_rate": 4.9513453008121824e-05,
      "loss": 3.8149,
      "step": 220700
    },
    {
      "epoch": 0.038941572297727364,
      "grad_norm": 9.077051162719727,
      "learning_rate": 4.951323255085112e-05,
      "loss": 3.9279,
      "step": 220800
    },
    {
      "epoch": 0.03895920887938395,
      "grad_norm": 7.612215042114258,
      "learning_rate": 4.951301209358041e-05,
      "loss": 3.8421,
      "step": 220900
    },
    {
      "epoch": 0.03897684546104053,
      "grad_norm": 10.013446807861328,
      "learning_rate": 4.9512791636309703e-05,
      "loss": 3.7895,
      "step": 221000
    },
    {
      "epoch": 0.03899448204269711,
      "grad_norm": 8.571430206298828,
      "learning_rate": 4.9512571179039e-05,
      "loss": 3.8746,
      "step": 221100
    },
    {
      "epoch": 0.039012118624353685,
      "grad_norm": 8.555463790893555,
      "learning_rate": 4.951235072176829e-05,
      "loss": 3.9145,
      "step": 221200
    },
    {
      "epoch": 0.039029755206010264,
      "grad_norm": 6.117182731628418,
      "learning_rate": 4.951213026449758e-05,
      "loss": 3.9511,
      "step": 221300
    },
    {
      "epoch": 0.03904739178766684,
      "grad_norm": 6.078074932098389,
      "learning_rate": 4.951190980722687e-05,
      "loss": 3.8658,
      "step": 221400
    },
    {
      "epoch": 0.03906502836932342,
      "grad_norm": 7.454602241516113,
      "learning_rate": 4.951168934995617e-05,
      "loss": 3.7296,
      "step": 221500
    },
    {
      "epoch": 0.039082664950980006,
      "grad_norm": 7.17549991607666,
      "learning_rate": 4.9511468892685456e-05,
      "loss": 3.9602,
      "step": 221600
    },
    {
      "epoch": 0.039100301532636585,
      "grad_norm": 5.524033069610596,
      "learning_rate": 4.951124843541475e-05,
      "loss": 3.9423,
      "step": 221700
    },
    {
      "epoch": 0.03911793811429316,
      "grad_norm": 5.894602298736572,
      "learning_rate": 4.951102797814404e-05,
      "loss": 3.9159,
      "step": 221800
    },
    {
      "epoch": 0.03913557469594974,
      "grad_norm": 6.33438777923584,
      "learning_rate": 4.9510807520873336e-05,
      "loss": 3.8826,
      "step": 221900
    },
    {
      "epoch": 0.03915321127760632,
      "grad_norm": 6.917542934417725,
      "learning_rate": 4.951058706360263e-05,
      "loss": 3.9301,
      "step": 222000
    },
    {
      "epoch": 0.0391708478592629,
      "grad_norm": 7.5219855308532715,
      "learning_rate": 4.951036660633192e-05,
      "loss": 3.8053,
      "step": 222100
    },
    {
      "epoch": 0.03918848444091948,
      "grad_norm": 8.300829887390137,
      "learning_rate": 4.9510146149061215e-05,
      "loss": 3.8033,
      "step": 222200
    },
    {
      "epoch": 0.039206121022576056,
      "grad_norm": 5.272517681121826,
      "learning_rate": 4.950992569179051e-05,
      "loss": 3.8654,
      "step": 222300
    },
    {
      "epoch": 0.03922375760423264,
      "grad_norm": 7.245931148529053,
      "learning_rate": 4.95097052345198e-05,
      "loss": 4.0002,
      "step": 222400
    },
    {
      "epoch": 0.03924139418588922,
      "grad_norm": 8.842514038085938,
      "learning_rate": 4.9509484777249095e-05,
      "loss": 3.8228,
      "step": 222500
    },
    {
      "epoch": 0.0392590307675458,
      "grad_norm": 11.499300956726074,
      "learning_rate": 4.950926431997839e-05,
      "loss": 3.8555,
      "step": 222600
    },
    {
      "epoch": 0.03927666734920238,
      "grad_norm": 7.775376796722412,
      "learning_rate": 4.9509043862707686e-05,
      "loss": 3.919,
      "step": 222700
    },
    {
      "epoch": 0.039294303930858955,
      "grad_norm": 10.368630409240723,
      "learning_rate": 4.9508823405436974e-05,
      "loss": 3.9819,
      "step": 222800
    },
    {
      "epoch": 0.039311940512515534,
      "grad_norm": 16.061681747436523,
      "learning_rate": 4.950860294816626e-05,
      "loss": 3.7901,
      "step": 222900
    },
    {
      "epoch": 0.03932957709417211,
      "grad_norm": 10.18539810180664,
      "learning_rate": 4.950838249089556e-05,
      "loss": 3.8443,
      "step": 223000
    },
    {
      "epoch": 0.0393472136758287,
      "grad_norm": 6.439243316650391,
      "learning_rate": 4.950816203362485e-05,
      "loss": 3.8813,
      "step": 223100
    },
    {
      "epoch": 0.039364850257485276,
      "grad_norm": 5.3206377029418945,
      "learning_rate": 4.950794157635414e-05,
      "loss": 3.9081,
      "step": 223200
    },
    {
      "epoch": 0.039382486839141854,
      "grad_norm": 7.336560249328613,
      "learning_rate": 4.950772111908344e-05,
      "loss": 3.9302,
      "step": 223300
    },
    {
      "epoch": 0.03940012342079843,
      "grad_norm": 6.09922456741333,
      "learning_rate": 4.950750066181273e-05,
      "loss": 3.7128,
      "step": 223400
    },
    {
      "epoch": 0.03941776000245501,
      "grad_norm": 8.907036781311035,
      "learning_rate": 4.950728020454202e-05,
      "loss": 3.8126,
      "step": 223500
    },
    {
      "epoch": 0.03943539658411159,
      "grad_norm": 8.122859954833984,
      "learning_rate": 4.950705974727132e-05,
      "loss": 3.8533,
      "step": 223600
    },
    {
      "epoch": 0.03945303316576817,
      "grad_norm": 10.288387298583984,
      "learning_rate": 4.9506839290000607e-05,
      "loss": 3.8683,
      "step": 223700
    },
    {
      "epoch": 0.03947066974742475,
      "grad_norm": 6.959540367126465,
      "learning_rate": 4.95066188327299e-05,
      "loss": 3.7419,
      "step": 223800
    },
    {
      "epoch": 0.03948830632908133,
      "grad_norm": 6.981386661529541,
      "learning_rate": 4.95063983754592e-05,
      "loss": 3.8486,
      "step": 223900
    },
    {
      "epoch": 0.03950594291073791,
      "grad_norm": 6.762815475463867,
      "learning_rate": 4.9506177918188486e-05,
      "loss": 3.9148,
      "step": 224000
    },
    {
      "epoch": 0.03952357949239449,
      "grad_norm": 7.273760795593262,
      "learning_rate": 4.950595746091778e-05,
      "loss": 3.9246,
      "step": 224100
    },
    {
      "epoch": 0.03954121607405107,
      "grad_norm": 7.259675979614258,
      "learning_rate": 4.950573700364707e-05,
      "loss": 3.8387,
      "step": 224200
    },
    {
      "epoch": 0.039558852655707646,
      "grad_norm": 8.27284049987793,
      "learning_rate": 4.9505516546376366e-05,
      "loss": 3.8386,
      "step": 224300
    },
    {
      "epoch": 0.039576489237364225,
      "grad_norm": 12.670536994934082,
      "learning_rate": 4.9505296089105655e-05,
      "loss": 3.9271,
      "step": 224400
    },
    {
      "epoch": 0.0395941258190208,
      "grad_norm": 7.270350933074951,
      "learning_rate": 4.950507563183495e-05,
      "loss": 3.8924,
      "step": 224500
    },
    {
      "epoch": 0.03961176240067739,
      "grad_norm": 8.19092845916748,
      "learning_rate": 4.950485517456424e-05,
      "loss": 3.8729,
      "step": 224600
    },
    {
      "epoch": 0.03962939898233397,
      "grad_norm": 10.169038772583008,
      "learning_rate": 4.9504634717293534e-05,
      "loss": 3.8047,
      "step": 224700
    },
    {
      "epoch": 0.039647035563990546,
      "grad_norm": 7.895853042602539,
      "learning_rate": 4.950441426002283e-05,
      "loss": 3.837,
      "step": 224800
    },
    {
      "epoch": 0.039664672145647124,
      "grad_norm": 8.094527244567871,
      "learning_rate": 4.950419380275212e-05,
      "loss": 3.8114,
      "step": 224900
    },
    {
      "epoch": 0.0396823087273037,
      "grad_norm": 7.186868667602539,
      "learning_rate": 4.9503973345481414e-05,
      "loss": 3.8328,
      "step": 225000
    },
    {
      "epoch": 0.03969994530896028,
      "grad_norm": 5.863954544067383,
      "learning_rate": 4.950375288821071e-05,
      "loss": 3.8723,
      "step": 225100
    },
    {
      "epoch": 0.03971758189061686,
      "grad_norm": 6.7062859535217285,
      "learning_rate": 4.950353243094e-05,
      "loss": 3.9301,
      "step": 225200
    },
    {
      "epoch": 0.03973521847227344,
      "grad_norm": 6.794949531555176,
      "learning_rate": 4.9503311973669293e-05,
      "loss": 3.8867,
      "step": 225300
    },
    {
      "epoch": 0.039752855053930024,
      "grad_norm": 6.292199611663818,
      "learning_rate": 4.950309151639859e-05,
      "loss": 3.846,
      "step": 225400
    },
    {
      "epoch": 0.0397704916355866,
      "grad_norm": 5.800449371337891,
      "learning_rate": 4.950287105912788e-05,
      "loss": 3.9079,
      "step": 225500
    },
    {
      "epoch": 0.03978812821724318,
      "grad_norm": 6.719967842102051,
      "learning_rate": 4.950265060185717e-05,
      "loss": 3.8574,
      "step": 225600
    },
    {
      "epoch": 0.03980576479889976,
      "grad_norm": 11.190139770507812,
      "learning_rate": 4.950243014458646e-05,
      "loss": 3.9399,
      "step": 225700
    },
    {
      "epoch": 0.03982340138055634,
      "grad_norm": 5.870033264160156,
      "learning_rate": 4.950220968731575e-05,
      "loss": 3.7896,
      "step": 225800
    },
    {
      "epoch": 0.039841037962212916,
      "grad_norm": 5.647708415985107,
      "learning_rate": 4.9501989230045046e-05,
      "loss": 3.8435,
      "step": 225900
    },
    {
      "epoch": 0.039858674543869495,
      "grad_norm": 6.3802056312561035,
      "learning_rate": 4.950176877277434e-05,
      "loss": 3.7601,
      "step": 226000
    },
    {
      "epoch": 0.03987631112552608,
      "grad_norm": 9.611762046813965,
      "learning_rate": 4.950154831550363e-05,
      "loss": 3.8664,
      "step": 226100
    },
    {
      "epoch": 0.03989394770718266,
      "grad_norm": 5.774247646331787,
      "learning_rate": 4.9501327858232926e-05,
      "loss": 3.729,
      "step": 226200
    },
    {
      "epoch": 0.03991158428883924,
      "grad_norm": 5.743983745574951,
      "learning_rate": 4.950110740096222e-05,
      "loss": 3.8961,
      "step": 226300
    },
    {
      "epoch": 0.039929220870495816,
      "grad_norm": 7.647593975067139,
      "learning_rate": 4.950088694369151e-05,
      "loss": 3.8859,
      "step": 226400
    },
    {
      "epoch": 0.039946857452152394,
      "grad_norm": 7.316656112670898,
      "learning_rate": 4.9500666486420805e-05,
      "loss": 3.7561,
      "step": 226500
    },
    {
      "epoch": 0.03996449403380897,
      "grad_norm": 7.957100868225098,
      "learning_rate": 4.95004460291501e-05,
      "loss": 3.7564,
      "step": 226600
    },
    {
      "epoch": 0.03998213061546555,
      "grad_norm": 5.432332515716553,
      "learning_rate": 4.950022557187939e-05,
      "loss": 3.7899,
      "step": 226700
    },
    {
      "epoch": 0.03999976719712213,
      "grad_norm": 7.711762428283691,
      "learning_rate": 4.9500005114608685e-05,
      "loss": 3.861,
      "step": 226800
    },
    {
      "epoch": 0.040017403778778715,
      "grad_norm": 11.859756469726562,
      "learning_rate": 4.949978465733798e-05,
      "loss": 3.9446,
      "step": 226900
    },
    {
      "epoch": 0.040035040360435294,
      "grad_norm": 7.144359111785889,
      "learning_rate": 4.949956420006727e-05,
      "loss": 3.8436,
      "step": 227000
    },
    {
      "epoch": 0.04005267694209187,
      "grad_norm": 8.293025970458984,
      "learning_rate": 4.9499343742796564e-05,
      "loss": 3.9081,
      "step": 227100
    },
    {
      "epoch": 0.04007031352374845,
      "grad_norm": 7.460678577423096,
      "learning_rate": 4.949912328552585e-05,
      "loss": 3.835,
      "step": 227200
    },
    {
      "epoch": 0.04008795010540503,
      "grad_norm": 5.869134902954102,
      "learning_rate": 4.949890282825514e-05,
      "loss": 3.8365,
      "step": 227300
    },
    {
      "epoch": 0.04010558668706161,
      "grad_norm": 5.657440662384033,
      "learning_rate": 4.949868237098444e-05,
      "loss": 3.8469,
      "step": 227400
    },
    {
      "epoch": 0.040123223268718186,
      "grad_norm": 10.205865859985352,
      "learning_rate": 4.949846191371373e-05,
      "loss": 3.8463,
      "step": 227500
    },
    {
      "epoch": 0.04014085985037477,
      "grad_norm": 7.2376933097839355,
      "learning_rate": 4.949824145644302e-05,
      "loss": 3.9413,
      "step": 227600
    },
    {
      "epoch": 0.04015849643203135,
      "grad_norm": 5.621310234069824,
      "learning_rate": 4.949802099917232e-05,
      "loss": 3.7912,
      "step": 227700
    },
    {
      "epoch": 0.04017613301368793,
      "grad_norm": 5.976778030395508,
      "learning_rate": 4.949780054190161e-05,
      "loss": 3.8137,
      "step": 227800
    },
    {
      "epoch": 0.04019376959534451,
      "grad_norm": 7.232425212860107,
      "learning_rate": 4.94975800846309e-05,
      "loss": 3.7973,
      "step": 227900
    },
    {
      "epoch": 0.040211406177001086,
      "grad_norm": 8.257405281066895,
      "learning_rate": 4.9497359627360197e-05,
      "loss": 3.8273,
      "step": 228000
    },
    {
      "epoch": 0.040229042758657664,
      "grad_norm": 5.7938127517700195,
      "learning_rate": 4.949713917008949e-05,
      "loss": 3.7765,
      "step": 228100
    },
    {
      "epoch": 0.04024667934031424,
      "grad_norm": 5.583505153656006,
      "learning_rate": 4.949691871281878e-05,
      "loss": 3.8995,
      "step": 228200
    },
    {
      "epoch": 0.04026431592197082,
      "grad_norm": 6.463102340698242,
      "learning_rate": 4.9496698255548076e-05,
      "loss": 3.8469,
      "step": 228300
    },
    {
      "epoch": 0.040281952503627406,
      "grad_norm": 8.398839950561523,
      "learning_rate": 4.949647779827737e-05,
      "loss": 3.8772,
      "step": 228400
    },
    {
      "epoch": 0.040299589085283985,
      "grad_norm": 7.386801242828369,
      "learning_rate": 4.949625734100666e-05,
      "loss": 3.8582,
      "step": 228500
    },
    {
      "epoch": 0.040317225666940563,
      "grad_norm": 5.792060852050781,
      "learning_rate": 4.949603688373595e-05,
      "loss": 3.9228,
      "step": 228600
    },
    {
      "epoch": 0.04033486224859714,
      "grad_norm": 6.346574306488037,
      "learning_rate": 4.9495816426465244e-05,
      "loss": 3.9463,
      "step": 228700
    },
    {
      "epoch": 0.04035249883025372,
      "grad_norm": 7.462143421173096,
      "learning_rate": 4.949559596919453e-05,
      "loss": 3.9451,
      "step": 228800
    },
    {
      "epoch": 0.0403701354119103,
      "grad_norm": 6.785452365875244,
      "learning_rate": 4.949537551192383e-05,
      "loss": 3.8233,
      "step": 228900
    },
    {
      "epoch": 0.04038777199356688,
      "grad_norm": 10.09489917755127,
      "learning_rate": 4.9495155054653124e-05,
      "loss": 3.8937,
      "step": 229000
    },
    {
      "epoch": 0.04040540857522346,
      "grad_norm": 11.486510276794434,
      "learning_rate": 4.949493459738241e-05,
      "loss": 3.8013,
      "step": 229100
    },
    {
      "epoch": 0.04042304515688004,
      "grad_norm": 7.120514869689941,
      "learning_rate": 4.949471414011171e-05,
      "loss": 3.8713,
      "step": 229200
    },
    {
      "epoch": 0.04044068173853662,
      "grad_norm": 8.901256561279297,
      "learning_rate": 4.9494493682841004e-05,
      "loss": 3.8838,
      "step": 229300
    },
    {
      "epoch": 0.0404583183201932,
      "grad_norm": 5.885071754455566,
      "learning_rate": 4.949427322557029e-05,
      "loss": 3.8408,
      "step": 229400
    },
    {
      "epoch": 0.04047595490184978,
      "grad_norm": 9.259077072143555,
      "learning_rate": 4.949405276829959e-05,
      "loss": 3.7672,
      "step": 229500
    },
    {
      "epoch": 0.040493591483506355,
      "grad_norm": 7.156352519989014,
      "learning_rate": 4.949383231102888e-05,
      "loss": 3.8738,
      "step": 229600
    },
    {
      "epoch": 0.040511228065162934,
      "grad_norm": 7.846264839172363,
      "learning_rate": 4.949361185375817e-05,
      "loss": 3.9596,
      "step": 229700
    },
    {
      "epoch": 0.04052886464681951,
      "grad_norm": 13.82440185546875,
      "learning_rate": 4.949339139648747e-05,
      "loss": 3.8465,
      "step": 229800
    },
    {
      "epoch": 0.0405465012284761,
      "grad_norm": 7.45928955078125,
      "learning_rate": 4.949317093921676e-05,
      "loss": 3.7428,
      "step": 229900
    },
    {
      "epoch": 0.040564137810132676,
      "grad_norm": 6.4071946144104,
      "learning_rate": 4.949295048194605e-05,
      "loss": 3.7154,
      "step": 230000
    },
    {
      "epoch": 0.040581774391789255,
      "grad_norm": 7.579870223999023,
      "learning_rate": 4.949273002467534e-05,
      "loss": 3.8839,
      "step": 230100
    },
    {
      "epoch": 0.04059941097344583,
      "grad_norm": 8.053644180297852,
      "learning_rate": 4.9492509567404636e-05,
      "loss": 3.9551,
      "step": 230200
    },
    {
      "epoch": 0.04061704755510241,
      "grad_norm": 7.747986793518066,
      "learning_rate": 4.9492289110133925e-05,
      "loss": 3.8877,
      "step": 230300
    },
    {
      "epoch": 0.04063468413675899,
      "grad_norm": 6.182301998138428,
      "learning_rate": 4.949206865286322e-05,
      "loss": 3.8565,
      "step": 230400
    },
    {
      "epoch": 0.04065232071841557,
      "grad_norm": 7.071657657623291,
      "learning_rate": 4.9491848195592515e-05,
      "loss": 3.8833,
      "step": 230500
    },
    {
      "epoch": 0.040669957300072154,
      "grad_norm": 6.638195037841797,
      "learning_rate": 4.9491627738321804e-05,
      "loss": 3.9719,
      "step": 230600
    },
    {
      "epoch": 0.04068759388172873,
      "grad_norm": 7.037492275238037,
      "learning_rate": 4.94914072810511e-05,
      "loss": 3.8578,
      "step": 230700
    },
    {
      "epoch": 0.04070523046338531,
      "grad_norm": 6.5496134757995605,
      "learning_rate": 4.9491186823780395e-05,
      "loss": 3.9126,
      "step": 230800
    },
    {
      "epoch": 0.04072286704504189,
      "grad_norm": 6.01315975189209,
      "learning_rate": 4.9490966366509684e-05,
      "loss": 3.8281,
      "step": 230900
    },
    {
      "epoch": 0.04074050362669847,
      "grad_norm": 6.044812202453613,
      "learning_rate": 4.949074590923898e-05,
      "loss": 3.8621,
      "step": 231000
    },
    {
      "epoch": 0.04075814020835505,
      "grad_norm": 5.214873790740967,
      "learning_rate": 4.9490525451968275e-05,
      "loss": 3.9358,
      "step": 231100
    },
    {
      "epoch": 0.040775776790011625,
      "grad_norm": 14.122739791870117,
      "learning_rate": 4.9490304994697563e-05,
      "loss": 3.83,
      "step": 231200
    },
    {
      "epoch": 0.04079341337166821,
      "grad_norm": 9.908997535705566,
      "learning_rate": 4.949008453742686e-05,
      "loss": 3.8812,
      "step": 231300
    },
    {
      "epoch": 0.04081104995332479,
      "grad_norm": 6.634819030761719,
      "learning_rate": 4.948986408015615e-05,
      "loss": 3.9305,
      "step": 231400
    },
    {
      "epoch": 0.04082868653498137,
      "grad_norm": 6.714391231536865,
      "learning_rate": 4.948964362288544e-05,
      "loss": 3.9027,
      "step": 231500
    },
    {
      "epoch": 0.040846323116637946,
      "grad_norm": 5.968659400939941,
      "learning_rate": 4.948942316561473e-05,
      "loss": 3.9027,
      "step": 231600
    },
    {
      "epoch": 0.040863959698294525,
      "grad_norm": 9.857404708862305,
      "learning_rate": 4.948920270834403e-05,
      "loss": 3.7224,
      "step": 231700
    },
    {
      "epoch": 0.0408815962799511,
      "grad_norm": 9.347793579101562,
      "learning_rate": 4.9488982251073316e-05,
      "loss": 3.9168,
      "step": 231800
    },
    {
      "epoch": 0.04089923286160768,
      "grad_norm": 7.916151523590088,
      "learning_rate": 4.948876179380261e-05,
      "loss": 3.7873,
      "step": 231900
    },
    {
      "epoch": 0.04091686944326426,
      "grad_norm": 6.784323692321777,
      "learning_rate": 4.948854133653191e-05,
      "loss": 3.8576,
      "step": 232000
    },
    {
      "epoch": 0.040934506024920846,
      "grad_norm": 6.573288917541504,
      "learning_rate": 4.9488320879261196e-05,
      "loss": 3.8043,
      "step": 232100
    },
    {
      "epoch": 0.040952142606577424,
      "grad_norm": 7.90393590927124,
      "learning_rate": 4.948810042199049e-05,
      "loss": 3.9526,
      "step": 232200
    },
    {
      "epoch": 0.040969779188234,
      "grad_norm": 8.465079307556152,
      "learning_rate": 4.9487879964719786e-05,
      "loss": 3.8967,
      "step": 232300
    },
    {
      "epoch": 0.04098741576989058,
      "grad_norm": 7.982710838317871,
      "learning_rate": 4.9487659507449075e-05,
      "loss": 3.7899,
      "step": 232400
    },
    {
      "epoch": 0.04100505235154716,
      "grad_norm": 9.804253578186035,
      "learning_rate": 4.948743905017837e-05,
      "loss": 3.9255,
      "step": 232500
    },
    {
      "epoch": 0.04102268893320374,
      "grad_norm": 8.667923927307129,
      "learning_rate": 4.9487218592907666e-05,
      "loss": 3.841,
      "step": 232600
    },
    {
      "epoch": 0.04104032551486032,
      "grad_norm": 8.12881088256836,
      "learning_rate": 4.9486998135636955e-05,
      "loss": 3.9532,
      "step": 232700
    },
    {
      "epoch": 0.0410579620965169,
      "grad_norm": 7.093610763549805,
      "learning_rate": 4.948677767836625e-05,
      "loss": 3.8918,
      "step": 232800
    },
    {
      "epoch": 0.04107559867817348,
      "grad_norm": 9.98133659362793,
      "learning_rate": 4.948655722109554e-05,
      "loss": 3.9125,
      "step": 232900
    },
    {
      "epoch": 0.04109323525983006,
      "grad_norm": 7.515544414520264,
      "learning_rate": 4.948633676382483e-05,
      "loss": 3.8999,
      "step": 233000
    },
    {
      "epoch": 0.04111087184148664,
      "grad_norm": 8.132318496704102,
      "learning_rate": 4.948611630655412e-05,
      "loss": 3.9551,
      "step": 233100
    },
    {
      "epoch": 0.041128508423143216,
      "grad_norm": 7.374598979949951,
      "learning_rate": 4.948589584928342e-05,
      "loss": 3.8877,
      "step": 233200
    },
    {
      "epoch": 0.041146145004799795,
      "grad_norm": 6.930896759033203,
      "learning_rate": 4.9485675392012714e-05,
      "loss": 3.9302,
      "step": 233300
    },
    {
      "epoch": 0.04116378158645637,
      "grad_norm": 5.586990833282471,
      "learning_rate": 4.9485454934742e-05,
      "loss": 3.8029,
      "step": 233400
    },
    {
      "epoch": 0.04118141816811295,
      "grad_norm": 5.631594181060791,
      "learning_rate": 4.94852344774713e-05,
      "loss": 3.8315,
      "step": 233500
    },
    {
      "epoch": 0.04119905474976954,
      "grad_norm": 5.651763916015625,
      "learning_rate": 4.9485014020200594e-05,
      "loss": 3.9225,
      "step": 233600
    },
    {
      "epoch": 0.041216691331426115,
      "grad_norm": 9.343573570251465,
      "learning_rate": 4.948479356292988e-05,
      "loss": 3.8719,
      "step": 233700
    },
    {
      "epoch": 0.041234327913082694,
      "grad_norm": 9.351003646850586,
      "learning_rate": 4.948457310565918e-05,
      "loss": 3.7831,
      "step": 233800
    },
    {
      "epoch": 0.04125196449473927,
      "grad_norm": 6.0907063484191895,
      "learning_rate": 4.948435264838847e-05,
      "loss": 3.8182,
      "step": 233900
    },
    {
      "epoch": 0.04126960107639585,
      "grad_norm": 7.348832607269287,
      "learning_rate": 4.948413219111776e-05,
      "loss": 3.8694,
      "step": 234000
    },
    {
      "epoch": 0.04128723765805243,
      "grad_norm": 6.047553062438965,
      "learning_rate": 4.948391173384706e-05,
      "loss": 3.9337,
      "step": 234100
    },
    {
      "epoch": 0.04130487423970901,
      "grad_norm": 8.470458984375,
      "learning_rate": 4.9483691276576346e-05,
      "loss": 3.9083,
      "step": 234200
    },
    {
      "epoch": 0.04132251082136559,
      "grad_norm": 6.445287227630615,
      "learning_rate": 4.948347081930564e-05,
      "loss": 3.9101,
      "step": 234300
    },
    {
      "epoch": 0.04134014740302217,
      "grad_norm": 8.437616348266602,
      "learning_rate": 4.948325036203493e-05,
      "loss": 3.8823,
      "step": 234400
    },
    {
      "epoch": 0.04135778398467875,
      "grad_norm": 10.109858512878418,
      "learning_rate": 4.9483029904764226e-05,
      "loss": 3.7937,
      "step": 234500
    },
    {
      "epoch": 0.04137542056633533,
      "grad_norm": 6.202871799468994,
      "learning_rate": 4.9482809447493514e-05,
      "loss": 3.9448,
      "step": 234600
    },
    {
      "epoch": 0.04139305714799191,
      "grad_norm": 7.634294509887695,
      "learning_rate": 4.948258899022281e-05,
      "loss": 3.8056,
      "step": 234700
    },
    {
      "epoch": 0.041410693729648486,
      "grad_norm": 6.33739709854126,
      "learning_rate": 4.9482368532952105e-05,
      "loss": 3.8299,
      "step": 234800
    },
    {
      "epoch": 0.041428330311305064,
      "grad_norm": 8.730558395385742,
      "learning_rate": 4.9482148075681394e-05,
      "loss": 3.7883,
      "step": 234900
    },
    {
      "epoch": 0.04144596689296164,
      "grad_norm": 8.083151817321777,
      "learning_rate": 4.948192761841069e-05,
      "loss": 3.8919,
      "step": 235000
    },
    {
      "epoch": 0.04146360347461823,
      "grad_norm": 6.626102447509766,
      "learning_rate": 4.9481707161139985e-05,
      "loss": 3.8022,
      "step": 235100
    },
    {
      "epoch": 0.04148124005627481,
      "grad_norm": 7.91973352432251,
      "learning_rate": 4.9481486703869274e-05,
      "loss": 3.9333,
      "step": 235200
    },
    {
      "epoch": 0.041498876637931385,
      "grad_norm": 8.758185386657715,
      "learning_rate": 4.948126624659857e-05,
      "loss": 3.8982,
      "step": 235300
    },
    {
      "epoch": 0.041516513219587964,
      "grad_norm": 8.104521751403809,
      "learning_rate": 4.9481045789327865e-05,
      "loss": 3.8715,
      "step": 235400
    },
    {
      "epoch": 0.04153414980124454,
      "grad_norm": 5.4141130447387695,
      "learning_rate": 4.948082533205715e-05,
      "loss": 3.8159,
      "step": 235500
    },
    {
      "epoch": 0.04155178638290112,
      "grad_norm": 7.274033546447754,
      "learning_rate": 4.948060487478645e-05,
      "loss": 3.9166,
      "step": 235600
    },
    {
      "epoch": 0.0415694229645577,
      "grad_norm": 9.893945693969727,
      "learning_rate": 4.948038441751574e-05,
      "loss": 3.8756,
      "step": 235700
    },
    {
      "epoch": 0.041587059546214285,
      "grad_norm": 5.212488651275635,
      "learning_rate": 4.9480163960245026e-05,
      "loss": 3.8314,
      "step": 235800
    },
    {
      "epoch": 0.04160469612787086,
      "grad_norm": 7.484795093536377,
      "learning_rate": 4.947994350297432e-05,
      "loss": 3.9047,
      "step": 235900
    },
    {
      "epoch": 0.04162233270952744,
      "grad_norm": 7.884880542755127,
      "learning_rate": 4.947972304570362e-05,
      "loss": 3.882,
      "step": 236000
    },
    {
      "epoch": 0.04163996929118402,
      "grad_norm": 6.284732341766357,
      "learning_rate": 4.9479502588432906e-05,
      "loss": 3.8262,
      "step": 236100
    },
    {
      "epoch": 0.0416576058728406,
      "grad_norm": 5.404550075531006,
      "learning_rate": 4.94792821311622e-05,
      "loss": 3.8673,
      "step": 236200
    },
    {
      "epoch": 0.04167524245449718,
      "grad_norm": 7.420975685119629,
      "learning_rate": 4.94790616738915e-05,
      "loss": 3.8352,
      "step": 236300
    },
    {
      "epoch": 0.041692879036153756,
      "grad_norm": 7.134320259094238,
      "learning_rate": 4.9478841216620785e-05,
      "loss": 3.8477,
      "step": 236400
    },
    {
      "epoch": 0.041710515617810334,
      "grad_norm": 8.092740058898926,
      "learning_rate": 4.947862075935008e-05,
      "loss": 3.8619,
      "step": 236500
    },
    {
      "epoch": 0.04172815219946692,
      "grad_norm": 6.564854145050049,
      "learning_rate": 4.9478400302079376e-05,
      "loss": 3.8291,
      "step": 236600
    },
    {
      "epoch": 0.0417457887811235,
      "grad_norm": 7.145264625549316,
      "learning_rate": 4.9478179844808665e-05,
      "loss": 3.8326,
      "step": 236700
    },
    {
      "epoch": 0.04176342536278008,
      "grad_norm": 9.606525421142578,
      "learning_rate": 4.947795938753796e-05,
      "loss": 3.8387,
      "step": 236800
    },
    {
      "epoch": 0.041781061944436655,
      "grad_norm": 8.471068382263184,
      "learning_rate": 4.9477738930267256e-05,
      "loss": 3.8186,
      "step": 236900
    },
    {
      "epoch": 0.041798698526093234,
      "grad_norm": 8.335855484008789,
      "learning_rate": 4.9477518472996545e-05,
      "loss": 3.9296,
      "step": 237000
    },
    {
      "epoch": 0.04181633510774981,
      "grad_norm": 7.433775901794434,
      "learning_rate": 4.947729801572584e-05,
      "loss": 3.8527,
      "step": 237100
    },
    {
      "epoch": 0.04183397168940639,
      "grad_norm": 6.20345401763916,
      "learning_rate": 4.947707755845513e-05,
      "loss": 3.7896,
      "step": 237200
    },
    {
      "epoch": 0.041851608271062976,
      "grad_norm": 8.612194061279297,
      "learning_rate": 4.947685710118442e-05,
      "loss": 3.8598,
      "step": 237300
    },
    {
      "epoch": 0.041869244852719555,
      "grad_norm": 10.397383689880371,
      "learning_rate": 4.947663664391371e-05,
      "loss": 3.84,
      "step": 237400
    },
    {
      "epoch": 0.04188688143437613,
      "grad_norm": 7.929078578948975,
      "learning_rate": 4.947641618664301e-05,
      "loss": 3.8769,
      "step": 237500
    },
    {
      "epoch": 0.04190451801603271,
      "grad_norm": 6.305673122406006,
      "learning_rate": 4.94761957293723e-05,
      "loss": 3.8814,
      "step": 237600
    },
    {
      "epoch": 0.04192215459768929,
      "grad_norm": 4.886209964752197,
      "learning_rate": 4.947597527210159e-05,
      "loss": 3.8597,
      "step": 237700
    },
    {
      "epoch": 0.04193979117934587,
      "grad_norm": 6.5373311042785645,
      "learning_rate": 4.947575481483089e-05,
      "loss": 3.8896,
      "step": 237800
    },
    {
      "epoch": 0.04195742776100245,
      "grad_norm": 6.78648567199707,
      "learning_rate": 4.947553435756018e-05,
      "loss": 3.7968,
      "step": 237900
    },
    {
      "epoch": 0.041975064342659026,
      "grad_norm": 6.478000640869141,
      "learning_rate": 4.947531390028947e-05,
      "loss": 3.9245,
      "step": 238000
    },
    {
      "epoch": 0.04199270092431561,
      "grad_norm": 8.167932510375977,
      "learning_rate": 4.947509344301877e-05,
      "loss": 3.8861,
      "step": 238100
    },
    {
      "epoch": 0.04201033750597219,
      "grad_norm": 9.228395462036133,
      "learning_rate": 4.9474872985748056e-05,
      "loss": 3.7389,
      "step": 238200
    },
    {
      "epoch": 0.04202797408762877,
      "grad_norm": 5.637050628662109,
      "learning_rate": 4.947465252847735e-05,
      "loss": 4.0102,
      "step": 238300
    },
    {
      "epoch": 0.042045610669285347,
      "grad_norm": 6.744691848754883,
      "learning_rate": 4.947443207120665e-05,
      "loss": 3.8307,
      "step": 238400
    },
    {
      "epoch": 0.042063247250941925,
      "grad_norm": 6.16554069519043,
      "learning_rate": 4.9474211613935936e-05,
      "loss": 3.8667,
      "step": 238500
    },
    {
      "epoch": 0.042080883832598504,
      "grad_norm": 6.076657295227051,
      "learning_rate": 4.9473991156665225e-05,
      "loss": 3.8215,
      "step": 238600
    },
    {
      "epoch": 0.04209852041425508,
      "grad_norm": 7.6591105461120605,
      "learning_rate": 4.947377069939452e-05,
      "loss": 3.7521,
      "step": 238700
    },
    {
      "epoch": 0.04211615699591167,
      "grad_norm": 5.972475051879883,
      "learning_rate": 4.947355024212381e-05,
      "loss": 3.743,
      "step": 238800
    },
    {
      "epoch": 0.042133793577568246,
      "grad_norm": 10.117423057556152,
      "learning_rate": 4.9473329784853104e-05,
      "loss": 3.8953,
      "step": 238900
    },
    {
      "epoch": 0.042151430159224824,
      "grad_norm": 5.694798946380615,
      "learning_rate": 4.94731093275824e-05,
      "loss": 3.8768,
      "step": 239000
    },
    {
      "epoch": 0.0421690667408814,
      "grad_norm": 8.66193675994873,
      "learning_rate": 4.947288887031169e-05,
      "loss": 3.7851,
      "step": 239100
    },
    {
      "epoch": 0.04218670332253798,
      "grad_norm": 9.408234596252441,
      "learning_rate": 4.9472668413040984e-05,
      "loss": 3.8463,
      "step": 239200
    },
    {
      "epoch": 0.04220433990419456,
      "grad_norm": 6.3749260902404785,
      "learning_rate": 4.947244795577028e-05,
      "loss": 3.8081,
      "step": 239300
    },
    {
      "epoch": 0.04222197648585114,
      "grad_norm": 6.929262638092041,
      "learning_rate": 4.947222749849957e-05,
      "loss": 3.8724,
      "step": 239400
    },
    {
      "epoch": 0.04223961306750772,
      "grad_norm": 7.91071891784668,
      "learning_rate": 4.9472007041228864e-05,
      "loss": 3.8122,
      "step": 239500
    },
    {
      "epoch": 0.0422572496491643,
      "grad_norm": 8.022562980651855,
      "learning_rate": 4.947178658395816e-05,
      "loss": 3.882,
      "step": 239600
    },
    {
      "epoch": 0.04227488623082088,
      "grad_norm": 6.653841972351074,
      "learning_rate": 4.947156612668745e-05,
      "loss": 3.8366,
      "step": 239700
    },
    {
      "epoch": 0.04229252281247746,
      "grad_norm": 6.09058952331543,
      "learning_rate": 4.947134566941674e-05,
      "loss": 3.8294,
      "step": 239800
    },
    {
      "epoch": 0.04231015939413404,
      "grad_norm": 7.012394428253174,
      "learning_rate": 4.947112521214604e-05,
      "loss": 3.742,
      "step": 239900
    },
    {
      "epoch": 0.042327795975790616,
      "grad_norm": 7.693484783172607,
      "learning_rate": 4.947090475487533e-05,
      "loss": 3.8429,
      "step": 240000
    },
    {
      "epoch": 0.042345432557447195,
      "grad_norm": 6.600486755371094,
      "learning_rate": 4.9470684297604616e-05,
      "loss": 3.9032,
      "step": 240100
    },
    {
      "epoch": 0.04236306913910377,
      "grad_norm": 7.079655647277832,
      "learning_rate": 4.947046384033391e-05,
      "loss": 3.7963,
      "step": 240200
    },
    {
      "epoch": 0.04238070572076036,
      "grad_norm": 8.188892364501953,
      "learning_rate": 4.94702433830632e-05,
      "loss": 3.8233,
      "step": 240300
    },
    {
      "epoch": 0.04239834230241694,
      "grad_norm": 4.819633483886719,
      "learning_rate": 4.9470022925792496e-05,
      "loss": 3.6708,
      "step": 240400
    },
    {
      "epoch": 0.042415978884073516,
      "grad_norm": 10.856898307800293,
      "learning_rate": 4.946980246852179e-05,
      "loss": 3.8322,
      "step": 240500
    },
    {
      "epoch": 0.042433615465730094,
      "grad_norm": 9.824162483215332,
      "learning_rate": 4.946958201125108e-05,
      "loss": 3.9026,
      "step": 240600
    },
    {
      "epoch": 0.04245125204738667,
      "grad_norm": 5.727833271026611,
      "learning_rate": 4.9469361553980375e-05,
      "loss": 3.8772,
      "step": 240700
    },
    {
      "epoch": 0.04246888862904325,
      "grad_norm": 7.615981101989746,
      "learning_rate": 4.946914109670967e-05,
      "loss": 3.8505,
      "step": 240800
    },
    {
      "epoch": 0.04248652521069983,
      "grad_norm": 5.485071659088135,
      "learning_rate": 4.946892063943896e-05,
      "loss": 3.8583,
      "step": 240900
    },
    {
      "epoch": 0.04250416179235641,
      "grad_norm": 7.273532867431641,
      "learning_rate": 4.9468700182168255e-05,
      "loss": 3.817,
      "step": 241000
    },
    {
      "epoch": 0.042521798374012994,
      "grad_norm": 7.180037021636963,
      "learning_rate": 4.946847972489755e-05,
      "loss": 3.8909,
      "step": 241100
    },
    {
      "epoch": 0.04253943495566957,
      "grad_norm": 8.51363468170166,
      "learning_rate": 4.946825926762684e-05,
      "loss": 3.7705,
      "step": 241200
    },
    {
      "epoch": 0.04255707153732615,
      "grad_norm": 6.293954849243164,
      "learning_rate": 4.9468038810356135e-05,
      "loss": 3.9276,
      "step": 241300
    },
    {
      "epoch": 0.04257470811898273,
      "grad_norm": 6.704718589782715,
      "learning_rate": 4.946781835308542e-05,
      "loss": 3.8107,
      "step": 241400
    },
    {
      "epoch": 0.04259234470063931,
      "grad_norm": 7.432142734527588,
      "learning_rate": 4.946759789581472e-05,
      "loss": 3.8596,
      "step": 241500
    },
    {
      "epoch": 0.042609981282295886,
      "grad_norm": 7.898136138916016,
      "learning_rate": 4.946737743854401e-05,
      "loss": 3.785,
      "step": 241600
    },
    {
      "epoch": 0.042627617863952465,
      "grad_norm": 7.310635089874268,
      "learning_rate": 4.94671569812733e-05,
      "loss": 3.8912,
      "step": 241700
    },
    {
      "epoch": 0.04264525444560905,
      "grad_norm": 9.612503051757812,
      "learning_rate": 4.946693652400259e-05,
      "loss": 3.7198,
      "step": 241800
    },
    {
      "epoch": 0.04266289102726563,
      "grad_norm": 8.5060396194458,
      "learning_rate": 4.946671606673189e-05,
      "loss": 3.8168,
      "step": 241900
    },
    {
      "epoch": 0.04268052760892221,
      "grad_norm": 5.583823204040527,
      "learning_rate": 4.946649560946118e-05,
      "loss": 3.8142,
      "step": 242000
    },
    {
      "epoch": 0.042698164190578786,
      "grad_norm": 7.254022121429443,
      "learning_rate": 4.946627515219047e-05,
      "loss": 3.8474,
      "step": 242100
    },
    {
      "epoch": 0.042715800772235364,
      "grad_norm": 8.722508430480957,
      "learning_rate": 4.946605469491977e-05,
      "loss": 3.6616,
      "step": 242200
    },
    {
      "epoch": 0.04273343735389194,
      "grad_norm": 6.29990291595459,
      "learning_rate": 4.946583423764906e-05,
      "loss": 3.8976,
      "step": 242300
    },
    {
      "epoch": 0.04275107393554852,
      "grad_norm": 6.21170711517334,
      "learning_rate": 4.946561378037835e-05,
      "loss": 3.6906,
      "step": 242400
    },
    {
      "epoch": 0.0427687105172051,
      "grad_norm": 5.924248218536377,
      "learning_rate": 4.9465393323107646e-05,
      "loss": 3.8537,
      "step": 242500
    },
    {
      "epoch": 0.042786347098861685,
      "grad_norm": 6.730725288391113,
      "learning_rate": 4.946517286583694e-05,
      "loss": 3.847,
      "step": 242600
    },
    {
      "epoch": 0.042803983680518264,
      "grad_norm": 7.1267619132995605,
      "learning_rate": 4.946495240856623e-05,
      "loss": 3.871,
      "step": 242700
    },
    {
      "epoch": 0.04282162026217484,
      "grad_norm": 9.709996223449707,
      "learning_rate": 4.9464731951295526e-05,
      "loss": 3.8318,
      "step": 242800
    },
    {
      "epoch": 0.04283925684383142,
      "grad_norm": 6.927267551422119,
      "learning_rate": 4.9464511494024815e-05,
      "loss": 3.8834,
      "step": 242900
    },
    {
      "epoch": 0.042856893425488,
      "grad_norm": 7.283726215362549,
      "learning_rate": 4.946429103675411e-05,
      "loss": 3.8079,
      "step": 243000
    },
    {
      "epoch": 0.04287453000714458,
      "grad_norm": 11.404321670532227,
      "learning_rate": 4.94640705794834e-05,
      "loss": 3.9413,
      "step": 243100
    },
    {
      "epoch": 0.042892166588801156,
      "grad_norm": 7.464777946472168,
      "learning_rate": 4.9463850122212694e-05,
      "loss": 3.8254,
      "step": 243200
    },
    {
      "epoch": 0.04290980317045774,
      "grad_norm": 9.513833999633789,
      "learning_rate": 4.946362966494198e-05,
      "loss": 3.7583,
      "step": 243300
    },
    {
      "epoch": 0.04292743975211432,
      "grad_norm": 5.497415065765381,
      "learning_rate": 4.946340920767128e-05,
      "loss": 3.8343,
      "step": 243400
    },
    {
      "epoch": 0.0429450763337709,
      "grad_norm": 5.621771812438965,
      "learning_rate": 4.9463188750400574e-05,
      "loss": 3.7759,
      "step": 243500
    },
    {
      "epoch": 0.04296271291542748,
      "grad_norm": 6.8917341232299805,
      "learning_rate": 4.946296829312986e-05,
      "loss": 3.8115,
      "step": 243600
    },
    {
      "epoch": 0.042980349497084055,
      "grad_norm": 7.514074325561523,
      "learning_rate": 4.946274783585916e-05,
      "loss": 3.7833,
      "step": 243700
    },
    {
      "epoch": 0.042997986078740634,
      "grad_norm": 10.929071426391602,
      "learning_rate": 4.9462527378588454e-05,
      "loss": 3.8231,
      "step": 243800
    },
    {
      "epoch": 0.04301562266039721,
      "grad_norm": 6.769370079040527,
      "learning_rate": 4.946230692131775e-05,
      "loss": 3.8885,
      "step": 243900
    },
    {
      "epoch": 0.04303325924205379,
      "grad_norm": 4.7625732421875,
      "learning_rate": 4.946208646404704e-05,
      "loss": 3.77,
      "step": 244000
    },
    {
      "epoch": 0.043050895823710376,
      "grad_norm": 11.7127046585083,
      "learning_rate": 4.946186600677633e-05,
      "loss": 3.8138,
      "step": 244100
    },
    {
      "epoch": 0.043068532405366955,
      "grad_norm": 7.128421306610107,
      "learning_rate": 4.946164554950562e-05,
      "loss": 3.8676,
      "step": 244200
    },
    {
      "epoch": 0.04308616898702353,
      "grad_norm": 9.549148559570312,
      "learning_rate": 4.946142509223492e-05,
      "loss": 3.7836,
      "step": 244300
    },
    {
      "epoch": 0.04310380556868011,
      "grad_norm": 7.696489334106445,
      "learning_rate": 4.9461204634964206e-05,
      "loss": 3.8796,
      "step": 244400
    },
    {
      "epoch": 0.04312144215033669,
      "grad_norm": 8.542396545410156,
      "learning_rate": 4.94609841776935e-05,
      "loss": 3.8555,
      "step": 244500
    },
    {
      "epoch": 0.04313907873199327,
      "grad_norm": 15.471854209899902,
      "learning_rate": 4.946076372042279e-05,
      "loss": 3.8354,
      "step": 244600
    },
    {
      "epoch": 0.04315671531364985,
      "grad_norm": 12.61294174194336,
      "learning_rate": 4.9460543263152086e-05,
      "loss": 3.8808,
      "step": 244700
    },
    {
      "epoch": 0.04317435189530643,
      "grad_norm": 6.345701217651367,
      "learning_rate": 4.946032280588138e-05,
      "loss": 3.8153,
      "step": 244800
    },
    {
      "epoch": 0.04319198847696301,
      "grad_norm": 6.41356897354126,
      "learning_rate": 4.946010234861067e-05,
      "loss": 3.8202,
      "step": 244900
    },
    {
      "epoch": 0.04320962505861959,
      "grad_norm": 6.663005828857422,
      "learning_rate": 4.9459881891339965e-05,
      "loss": 3.769,
      "step": 245000
    },
    {
      "epoch": 0.04322726164027617,
      "grad_norm": 7.3999762535095215,
      "learning_rate": 4.945966143406926e-05,
      "loss": 3.8439,
      "step": 245100
    },
    {
      "epoch": 0.04324489822193275,
      "grad_norm": 6.75451135635376,
      "learning_rate": 4.945944097679855e-05,
      "loss": 3.8536,
      "step": 245200
    },
    {
      "epoch": 0.043262534803589325,
      "grad_norm": 8.70943546295166,
      "learning_rate": 4.9459220519527845e-05,
      "loss": 3.9305,
      "step": 245300
    },
    {
      "epoch": 0.043280171385245904,
      "grad_norm": 6.246466159820557,
      "learning_rate": 4.945900006225714e-05,
      "loss": 3.8266,
      "step": 245400
    },
    {
      "epoch": 0.04329780796690248,
      "grad_norm": 6.377016067504883,
      "learning_rate": 4.945877960498643e-05,
      "loss": 3.8022,
      "step": 245500
    },
    {
      "epoch": 0.04331544454855907,
      "grad_norm": 8.809142112731934,
      "learning_rate": 4.9458559147715725e-05,
      "loss": 3.8487,
      "step": 245600
    },
    {
      "epoch": 0.043333081130215646,
      "grad_norm": 10.10236644744873,
      "learning_rate": 4.945833869044501e-05,
      "loss": 3.7249,
      "step": 245700
    },
    {
      "epoch": 0.043350717711872225,
      "grad_norm": 7.628852844238281,
      "learning_rate": 4.94581182331743e-05,
      "loss": 3.8306,
      "step": 245800
    },
    {
      "epoch": 0.0433683542935288,
      "grad_norm": 10.444287300109863,
      "learning_rate": 4.94578977759036e-05,
      "loss": 3.8341,
      "step": 245900
    },
    {
      "epoch": 0.04338599087518538,
      "grad_norm": 6.049841403961182,
      "learning_rate": 4.945767731863289e-05,
      "loss": 3.8146,
      "step": 246000
    },
    {
      "epoch": 0.04340362745684196,
      "grad_norm": 8.027235984802246,
      "learning_rate": 4.945745686136218e-05,
      "loss": 3.9268,
      "step": 246100
    },
    {
      "epoch": 0.04342126403849854,
      "grad_norm": 7.811060428619385,
      "learning_rate": 4.945723640409148e-05,
      "loss": 3.7153,
      "step": 246200
    },
    {
      "epoch": 0.043438900620155124,
      "grad_norm": 6.999720096588135,
      "learning_rate": 4.945701594682077e-05,
      "loss": 3.8612,
      "step": 246300
    },
    {
      "epoch": 0.0434565372018117,
      "grad_norm": 7.120610237121582,
      "learning_rate": 4.945679548955006e-05,
      "loss": 3.7117,
      "step": 246400
    },
    {
      "epoch": 0.04347417378346828,
      "grad_norm": 13.494233131408691,
      "learning_rate": 4.945657503227936e-05,
      "loss": 3.8527,
      "step": 246500
    },
    {
      "epoch": 0.04349181036512486,
      "grad_norm": 7.845856189727783,
      "learning_rate": 4.945635457500865e-05,
      "loss": 3.7598,
      "step": 246600
    },
    {
      "epoch": 0.04350944694678144,
      "grad_norm": 7.178345680236816,
      "learning_rate": 4.945613411773794e-05,
      "loss": 3.8224,
      "step": 246700
    },
    {
      "epoch": 0.04352708352843802,
      "grad_norm": 7.575686931610107,
      "learning_rate": 4.9455913660467236e-05,
      "loss": 3.8308,
      "step": 246800
    },
    {
      "epoch": 0.043544720110094595,
      "grad_norm": 7.941793441772461,
      "learning_rate": 4.945569320319653e-05,
      "loss": 3.7557,
      "step": 246900
    },
    {
      "epoch": 0.043562356691751174,
      "grad_norm": 5.808213233947754,
      "learning_rate": 4.945547274592582e-05,
      "loss": 3.8255,
      "step": 247000
    },
    {
      "epoch": 0.04357999327340776,
      "grad_norm": 8.92593002319336,
      "learning_rate": 4.9455252288655116e-05,
      "loss": 3.789,
      "step": 247100
    },
    {
      "epoch": 0.04359762985506434,
      "grad_norm": 7.416061878204346,
      "learning_rate": 4.9455031831384405e-05,
      "loss": 3.8186,
      "step": 247200
    },
    {
      "epoch": 0.043615266436720916,
      "grad_norm": 5.92686653137207,
      "learning_rate": 4.945481137411369e-05,
      "loss": 3.749,
      "step": 247300
    },
    {
      "epoch": 0.043632903018377495,
      "grad_norm": 10.674943923950195,
      "learning_rate": 4.945459091684299e-05,
      "loss": 3.8628,
      "step": 247400
    },
    {
      "epoch": 0.04365053960003407,
      "grad_norm": 8.131049156188965,
      "learning_rate": 4.9454370459572284e-05,
      "loss": 3.8783,
      "step": 247500
    },
    {
      "epoch": 0.04366817618169065,
      "grad_norm": 7.059845924377441,
      "learning_rate": 4.945415000230157e-05,
      "loss": 3.8409,
      "step": 247600
    },
    {
      "epoch": 0.04368581276334723,
      "grad_norm": 5.791048049926758,
      "learning_rate": 4.945392954503087e-05,
      "loss": 3.8183,
      "step": 247700
    },
    {
      "epoch": 0.043703449345003816,
      "grad_norm": 6.859860897064209,
      "learning_rate": 4.9453709087760164e-05,
      "loss": 3.7578,
      "step": 247800
    },
    {
      "epoch": 0.043721085926660394,
      "grad_norm": 8.660868644714355,
      "learning_rate": 4.945348863048945e-05,
      "loss": 3.8167,
      "step": 247900
    },
    {
      "epoch": 0.04373872250831697,
      "grad_norm": 8.037739753723145,
      "learning_rate": 4.945326817321875e-05,
      "loss": 3.8083,
      "step": 248000
    },
    {
      "epoch": 0.04375635908997355,
      "grad_norm": 7.542544364929199,
      "learning_rate": 4.9453047715948044e-05,
      "loss": 3.8362,
      "step": 248100
    },
    {
      "epoch": 0.04377399567163013,
      "grad_norm": 7.198462963104248,
      "learning_rate": 4.945282725867733e-05,
      "loss": 3.8673,
      "step": 248200
    },
    {
      "epoch": 0.04379163225328671,
      "grad_norm": 6.831444263458252,
      "learning_rate": 4.945260680140663e-05,
      "loss": 3.8916,
      "step": 248300
    },
    {
      "epoch": 0.04380926883494329,
      "grad_norm": 4.84999418258667,
      "learning_rate": 4.945238634413592e-05,
      "loss": 3.7581,
      "step": 248400
    },
    {
      "epoch": 0.043826905416599865,
      "grad_norm": 7.5318989753723145,
      "learning_rate": 4.945216588686521e-05,
      "loss": 3.8683,
      "step": 248500
    },
    {
      "epoch": 0.04384454199825645,
      "grad_norm": 5.647970199584961,
      "learning_rate": 4.94519454295945e-05,
      "loss": 3.8471,
      "step": 248600
    },
    {
      "epoch": 0.04386217857991303,
      "grad_norm": 7.201041221618652,
      "learning_rate": 4.9451724972323796e-05,
      "loss": 3.8007,
      "step": 248700
    },
    {
      "epoch": 0.04387981516156961,
      "grad_norm": 6.668851375579834,
      "learning_rate": 4.9451504515053085e-05,
      "loss": 3.8397,
      "step": 248800
    },
    {
      "epoch": 0.043897451743226186,
      "grad_norm": 6.990477561950684,
      "learning_rate": 4.945128405778238e-05,
      "loss": 3.8698,
      "step": 248900
    },
    {
      "epoch": 0.043915088324882764,
      "grad_norm": 6.323400020599365,
      "learning_rate": 4.9451063600511676e-05,
      "loss": 3.9179,
      "step": 249000
    },
    {
      "epoch": 0.04393272490653934,
      "grad_norm": 7.170477390289307,
      "learning_rate": 4.9450843143240964e-05,
      "loss": 3.6994,
      "step": 249100
    },
    {
      "epoch": 0.04395036148819592,
      "grad_norm": 11.12645435333252,
      "learning_rate": 4.945062268597026e-05,
      "loss": 3.8586,
      "step": 249200
    },
    {
      "epoch": 0.04396799806985251,
      "grad_norm": 7.708863258361816,
      "learning_rate": 4.9450402228699555e-05,
      "loss": 3.7851,
      "step": 249300
    },
    {
      "epoch": 0.043985634651509085,
      "grad_norm": 14.32563304901123,
      "learning_rate": 4.9450181771428844e-05,
      "loss": 3.8359,
      "step": 249400
    },
    {
      "epoch": 0.044003271233165664,
      "grad_norm": 8.092644691467285,
      "learning_rate": 4.944996131415814e-05,
      "loss": 3.8234,
      "step": 249500
    },
    {
      "epoch": 0.04402090781482224,
      "grad_norm": 9.215295791625977,
      "learning_rate": 4.9449740856887435e-05,
      "loss": 3.8763,
      "step": 249600
    },
    {
      "epoch": 0.04403854439647882,
      "grad_norm": 7.441647052764893,
      "learning_rate": 4.9449520399616724e-05,
      "loss": 3.9772,
      "step": 249700
    },
    {
      "epoch": 0.0440561809781354,
      "grad_norm": 6.821290493011475,
      "learning_rate": 4.944929994234602e-05,
      "loss": 3.7909,
      "step": 249800
    },
    {
      "epoch": 0.04407381755979198,
      "grad_norm": 9.928399085998535,
      "learning_rate": 4.9449079485075315e-05,
      "loss": 3.7236,
      "step": 249900
    },
    {
      "epoch": 0.044091454141448556,
      "grad_norm": 5.562711715698242,
      "learning_rate": 4.94488590278046e-05,
      "loss": 3.7488,
      "step": 250000
    },
    {
      "epoch": 0.04410909072310514,
      "grad_norm": 8.382416725158691,
      "learning_rate": 4.944863857053389e-05,
      "loss": 3.8475,
      "step": 250100
    },
    {
      "epoch": 0.04412672730476172,
      "grad_norm": 6.085850715637207,
      "learning_rate": 4.944841811326319e-05,
      "loss": 3.8219,
      "step": 250200
    },
    {
      "epoch": 0.0441443638864183,
      "grad_norm": 4.954039573669434,
      "learning_rate": 4.9448197655992476e-05,
      "loss": 3.7898,
      "step": 250300
    },
    {
      "epoch": 0.04416200046807488,
      "grad_norm": 7.399110317230225,
      "learning_rate": 4.944797719872177e-05,
      "loss": 3.8247,
      "step": 250400
    },
    {
      "epoch": 0.044179637049731456,
      "grad_norm": 7.377388954162598,
      "learning_rate": 4.944775674145107e-05,
      "loss": 3.8352,
      "step": 250500
    },
    {
      "epoch": 0.044197273631388034,
      "grad_norm": 7.326559543609619,
      "learning_rate": 4.9447536284180356e-05,
      "loss": 3.9312,
      "step": 250600
    },
    {
      "epoch": 0.04421491021304461,
      "grad_norm": 9.614314079284668,
      "learning_rate": 4.944731582690965e-05,
      "loss": 3.8799,
      "step": 250700
    },
    {
      "epoch": 0.0442325467947012,
      "grad_norm": 7.093203544616699,
      "learning_rate": 4.944709536963895e-05,
      "loss": 3.8961,
      "step": 250800
    },
    {
      "epoch": 0.04425018337635778,
      "grad_norm": 5.903504848480225,
      "learning_rate": 4.9446874912368235e-05,
      "loss": 3.7729,
      "step": 250900
    },
    {
      "epoch": 0.044267819958014355,
      "grad_norm": 8.360556602478027,
      "learning_rate": 4.944665445509753e-05,
      "loss": 3.8437,
      "step": 251000
    },
    {
      "epoch": 0.044285456539670934,
      "grad_norm": 6.849443435668945,
      "learning_rate": 4.9446433997826826e-05,
      "loss": 3.8002,
      "step": 251100
    },
    {
      "epoch": 0.04430309312132751,
      "grad_norm": 6.359761714935303,
      "learning_rate": 4.9446213540556115e-05,
      "loss": 3.686,
      "step": 251200
    },
    {
      "epoch": 0.04432072970298409,
      "grad_norm": 7.468809604644775,
      "learning_rate": 4.944599308328541e-05,
      "loss": 3.779,
      "step": 251300
    },
    {
      "epoch": 0.04433836628464067,
      "grad_norm": 7.799619674682617,
      "learning_rate": 4.94457726260147e-05,
      "loss": 3.8363,
      "step": 251400
    },
    {
      "epoch": 0.04435600286629725,
      "grad_norm": 8.624122619628906,
      "learning_rate": 4.9445552168743995e-05,
      "loss": 3.83,
      "step": 251500
    },
    {
      "epoch": 0.04437363944795383,
      "grad_norm": 7.857778072357178,
      "learning_rate": 4.944533171147328e-05,
      "loss": 3.768,
      "step": 251600
    },
    {
      "epoch": 0.04439127602961041,
      "grad_norm": 7.040119171142578,
      "learning_rate": 4.944511125420258e-05,
      "loss": 3.7425,
      "step": 251700
    },
    {
      "epoch": 0.04440891261126699,
      "grad_norm": 5.585012435913086,
      "learning_rate": 4.944489079693187e-05,
      "loss": 3.8096,
      "step": 251800
    },
    {
      "epoch": 0.04442654919292357,
      "grad_norm": 7.331859111785889,
      "learning_rate": 4.944467033966116e-05,
      "loss": 3.7736,
      "step": 251900
    },
    {
      "epoch": 0.04444418577458015,
      "grad_norm": 7.03377103805542,
      "learning_rate": 4.944444988239046e-05,
      "loss": 3.7985,
      "step": 252000
    },
    {
      "epoch": 0.044461822356236726,
      "grad_norm": 6.132340908050537,
      "learning_rate": 4.944422942511975e-05,
      "loss": 3.7519,
      "step": 252100
    },
    {
      "epoch": 0.044479458937893304,
      "grad_norm": 8.823169708251953,
      "learning_rate": 4.944400896784904e-05,
      "loss": 3.7806,
      "step": 252200
    },
    {
      "epoch": 0.04449709551954989,
      "grad_norm": 8.583191871643066,
      "learning_rate": 4.944378851057834e-05,
      "loss": 3.8517,
      "step": 252300
    },
    {
      "epoch": 0.04451473210120647,
      "grad_norm": 6.035589694976807,
      "learning_rate": 4.944356805330763e-05,
      "loss": 3.7781,
      "step": 252400
    },
    {
      "epoch": 0.04453236868286305,
      "grad_norm": 7.618238925933838,
      "learning_rate": 4.944334759603692e-05,
      "loss": 3.6349,
      "step": 252500
    },
    {
      "epoch": 0.044550005264519625,
      "grad_norm": 6.669912338256836,
      "learning_rate": 4.944312713876622e-05,
      "loss": 3.8341,
      "step": 252600
    },
    {
      "epoch": 0.044567641846176204,
      "grad_norm": 7.1524553298950195,
      "learning_rate": 4.9442906681495506e-05,
      "loss": 3.9489,
      "step": 252700
    },
    {
      "epoch": 0.04458527842783278,
      "grad_norm": 8.895925521850586,
      "learning_rate": 4.94426862242248e-05,
      "loss": 3.7282,
      "step": 252800
    },
    {
      "epoch": 0.04460291500948936,
      "grad_norm": 7.715207099914551,
      "learning_rate": 4.944246576695409e-05,
      "loss": 3.7901,
      "step": 252900
    },
    {
      "epoch": 0.04462055159114594,
      "grad_norm": 6.8216071128845215,
      "learning_rate": 4.9442245309683386e-05,
      "loss": 3.8576,
      "step": 253000
    },
    {
      "epoch": 0.044638188172802525,
      "grad_norm": 10.020097732543945,
      "learning_rate": 4.9442024852412675e-05,
      "loss": 3.8067,
      "step": 253100
    },
    {
      "epoch": 0.0446558247544591,
      "grad_norm": 6.202690124511719,
      "learning_rate": 4.944180439514197e-05,
      "loss": 3.9357,
      "step": 253200
    },
    {
      "epoch": 0.04467346133611568,
      "grad_norm": 7.741339206695557,
      "learning_rate": 4.944158393787126e-05,
      "loss": 3.7467,
      "step": 253300
    },
    {
      "epoch": 0.04469109791777226,
      "grad_norm": 5.5352349281311035,
      "learning_rate": 4.9441363480600554e-05,
      "loss": 3.8705,
      "step": 253400
    },
    {
      "epoch": 0.04470873449942884,
      "grad_norm": 8.463988304138184,
      "learning_rate": 4.944114302332985e-05,
      "loss": 3.8866,
      "step": 253500
    },
    {
      "epoch": 0.04472637108108542,
      "grad_norm": 6.3836669921875,
      "learning_rate": 4.944092256605914e-05,
      "loss": 3.8876,
      "step": 253600
    },
    {
      "epoch": 0.044744007662741996,
      "grad_norm": 5.837744235992432,
      "learning_rate": 4.9440702108788434e-05,
      "loss": 3.8298,
      "step": 253700
    },
    {
      "epoch": 0.04476164424439858,
      "grad_norm": 9.231575965881348,
      "learning_rate": 4.944048165151773e-05,
      "loss": 3.7807,
      "step": 253800
    },
    {
      "epoch": 0.04477928082605516,
      "grad_norm": 8.509716987609863,
      "learning_rate": 4.944026119424702e-05,
      "loss": 3.8005,
      "step": 253900
    },
    {
      "epoch": 0.04479691740771174,
      "grad_norm": 10.021496772766113,
      "learning_rate": 4.9440040736976314e-05,
      "loss": 3.809,
      "step": 254000
    },
    {
      "epoch": 0.044814553989368316,
      "grad_norm": 5.654052257537842,
      "learning_rate": 4.943982027970561e-05,
      "loss": 3.9153,
      "step": 254100
    },
    {
      "epoch": 0.044832190571024895,
      "grad_norm": 5.994570255279541,
      "learning_rate": 4.94395998224349e-05,
      "loss": 3.8551,
      "step": 254200
    },
    {
      "epoch": 0.04484982715268147,
      "grad_norm": 7.2521867752075195,
      "learning_rate": 4.943937936516419e-05,
      "loss": 3.7504,
      "step": 254300
    },
    {
      "epoch": 0.04486746373433805,
      "grad_norm": 7.485377788543701,
      "learning_rate": 4.943915890789348e-05,
      "loss": 3.7762,
      "step": 254400
    },
    {
      "epoch": 0.04488510031599463,
      "grad_norm": 5.940744400024414,
      "learning_rate": 4.943893845062278e-05,
      "loss": 3.85,
      "step": 254500
    },
    {
      "epoch": 0.044902736897651216,
      "grad_norm": 6.369863986968994,
      "learning_rate": 4.9438717993352066e-05,
      "loss": 3.8887,
      "step": 254600
    },
    {
      "epoch": 0.044920373479307794,
      "grad_norm": 7.095177173614502,
      "learning_rate": 4.943849753608136e-05,
      "loss": 3.9205,
      "step": 254700
    },
    {
      "epoch": 0.04493801006096437,
      "grad_norm": 8.795123100280762,
      "learning_rate": 4.943827707881066e-05,
      "loss": 3.7299,
      "step": 254800
    },
    {
      "epoch": 0.04495564664262095,
      "grad_norm": 11.42387580871582,
      "learning_rate": 4.9438056621539946e-05,
      "loss": 3.9128,
      "step": 254900
    },
    {
      "epoch": 0.04497328322427753,
      "grad_norm": 6.854007720947266,
      "learning_rate": 4.943783616426924e-05,
      "loss": 3.8643,
      "step": 255000
    },
    {
      "epoch": 0.04499091980593411,
      "grad_norm": 6.597108364105225,
      "learning_rate": 4.943761570699854e-05,
      "loss": 3.8989,
      "step": 255100
    },
    {
      "epoch": 0.04500855638759069,
      "grad_norm": 6.628139972686768,
      "learning_rate": 4.9437395249727825e-05,
      "loss": 3.8575,
      "step": 255200
    },
    {
      "epoch": 0.04502619296924727,
      "grad_norm": 8.793006896972656,
      "learning_rate": 4.943717479245712e-05,
      "loss": 3.8244,
      "step": 255300
    },
    {
      "epoch": 0.04504382955090385,
      "grad_norm": 6.640819072723389,
      "learning_rate": 4.9436954335186416e-05,
      "loss": 3.7684,
      "step": 255400
    },
    {
      "epoch": 0.04506146613256043,
      "grad_norm": 7.39971923828125,
      "learning_rate": 4.9436733877915705e-05,
      "loss": 3.7957,
      "step": 255500
    },
    {
      "epoch": 0.04507910271421701,
      "grad_norm": 6.48841667175293,
      "learning_rate": 4.9436513420645e-05,
      "loss": 3.913,
      "step": 255600
    },
    {
      "epoch": 0.045096739295873586,
      "grad_norm": 7.4997429847717285,
      "learning_rate": 4.943629296337429e-05,
      "loss": 3.8668,
      "step": 255700
    },
    {
      "epoch": 0.045114375877530165,
      "grad_norm": 7.0678558349609375,
      "learning_rate": 4.9436072506103585e-05,
      "loss": 3.8536,
      "step": 255800
    },
    {
      "epoch": 0.04513201245918674,
      "grad_norm": 8.171721458435059,
      "learning_rate": 4.943585204883287e-05,
      "loss": 3.8424,
      "step": 255900
    },
    {
      "epoch": 0.04514964904084332,
      "grad_norm": 6.75439977645874,
      "learning_rate": 4.943563159156217e-05,
      "loss": 3.888,
      "step": 256000
    },
    {
      "epoch": 0.04516728562249991,
      "grad_norm": 7.800980091094971,
      "learning_rate": 4.943541113429146e-05,
      "loss": 3.813,
      "step": 256100
    },
    {
      "epoch": 0.045184922204156486,
      "grad_norm": 6.152505397796631,
      "learning_rate": 4.943519067702075e-05,
      "loss": 3.8023,
      "step": 256200
    },
    {
      "epoch": 0.045202558785813064,
      "grad_norm": 6.2564802169799805,
      "learning_rate": 4.943497021975005e-05,
      "loss": 3.7816,
      "step": 256300
    },
    {
      "epoch": 0.04522019536746964,
      "grad_norm": 5.356896877288818,
      "learning_rate": 4.943474976247934e-05,
      "loss": 3.6876,
      "step": 256400
    },
    {
      "epoch": 0.04523783194912622,
      "grad_norm": 6.1742401123046875,
      "learning_rate": 4.943452930520863e-05,
      "loss": 3.7181,
      "step": 256500
    },
    {
      "epoch": 0.0452554685307828,
      "grad_norm": 6.501958847045898,
      "learning_rate": 4.943430884793793e-05,
      "loss": 3.8808,
      "step": 256600
    },
    {
      "epoch": 0.04527310511243938,
      "grad_norm": 6.084076881408691,
      "learning_rate": 4.943408839066722e-05,
      "loss": 3.8396,
      "step": 256700
    },
    {
      "epoch": 0.045290741694095964,
      "grad_norm": 6.610673427581787,
      "learning_rate": 4.943386793339651e-05,
      "loss": 3.8146,
      "step": 256800
    },
    {
      "epoch": 0.04530837827575254,
      "grad_norm": 10.026655197143555,
      "learning_rate": 4.943364747612581e-05,
      "loss": 3.8966,
      "step": 256900
    },
    {
      "epoch": 0.04532601485740912,
      "grad_norm": 7.160613059997559,
      "learning_rate": 4.9433427018855096e-05,
      "loss": 3.8028,
      "step": 257000
    },
    {
      "epoch": 0.0453436514390657,
      "grad_norm": 7.822289943695068,
      "learning_rate": 4.943320656158439e-05,
      "loss": 3.7403,
      "step": 257100
    },
    {
      "epoch": 0.04536128802072228,
      "grad_norm": 5.927651882171631,
      "learning_rate": 4.943298610431368e-05,
      "loss": 3.9149,
      "step": 257200
    },
    {
      "epoch": 0.045378924602378856,
      "grad_norm": 7.716021537780762,
      "learning_rate": 4.943276564704297e-05,
      "loss": 3.7938,
      "step": 257300
    },
    {
      "epoch": 0.045396561184035435,
      "grad_norm": 10.802452087402344,
      "learning_rate": 4.9432545189772265e-05,
      "loss": 3.7409,
      "step": 257400
    },
    {
      "epoch": 0.04541419776569201,
      "grad_norm": 8.763031959533691,
      "learning_rate": 4.943232473250156e-05,
      "loss": 3.8442,
      "step": 257500
    },
    {
      "epoch": 0.0454318343473486,
      "grad_norm": 8.160768508911133,
      "learning_rate": 4.943210427523085e-05,
      "loss": 3.9709,
      "step": 257600
    },
    {
      "epoch": 0.04544947092900518,
      "grad_norm": 10.04321575164795,
      "learning_rate": 4.9431883817960144e-05,
      "loss": 3.8192,
      "step": 257700
    },
    {
      "epoch": 0.045467107510661756,
      "grad_norm": 9.46981430053711,
      "learning_rate": 4.943166336068944e-05,
      "loss": 3.7163,
      "step": 257800
    },
    {
      "epoch": 0.045484744092318334,
      "grad_norm": 6.78743839263916,
      "learning_rate": 4.943144290341873e-05,
      "loss": 3.7896,
      "step": 257900
    },
    {
      "epoch": 0.04550238067397491,
      "grad_norm": 8.015981674194336,
      "learning_rate": 4.9431222446148024e-05,
      "loss": 3.8187,
      "step": 258000
    },
    {
      "epoch": 0.04552001725563149,
      "grad_norm": 7.975720405578613,
      "learning_rate": 4.943100198887732e-05,
      "loss": 3.804,
      "step": 258100
    },
    {
      "epoch": 0.04553765383728807,
      "grad_norm": 7.568554878234863,
      "learning_rate": 4.943078153160661e-05,
      "loss": 3.8424,
      "step": 258200
    },
    {
      "epoch": 0.045555290418944655,
      "grad_norm": 7.021284103393555,
      "learning_rate": 4.9430561074335904e-05,
      "loss": 3.7318,
      "step": 258300
    },
    {
      "epoch": 0.045572927000601234,
      "grad_norm": 6.286202907562256,
      "learning_rate": 4.94303406170652e-05,
      "loss": 3.7792,
      "step": 258400
    },
    {
      "epoch": 0.04559056358225781,
      "grad_norm": 8.501029014587402,
      "learning_rate": 4.943012015979449e-05,
      "loss": 3.8761,
      "step": 258500
    },
    {
      "epoch": 0.04560820016391439,
      "grad_norm": 6.046494007110596,
      "learning_rate": 4.942989970252378e-05,
      "loss": 3.902,
      "step": 258600
    },
    {
      "epoch": 0.04562583674557097,
      "grad_norm": 6.43341064453125,
      "learning_rate": 4.942967924525307e-05,
      "loss": 3.851,
      "step": 258700
    },
    {
      "epoch": 0.04564347332722755,
      "grad_norm": 8.503103256225586,
      "learning_rate": 4.942945878798236e-05,
      "loss": 3.8847,
      "step": 258800
    },
    {
      "epoch": 0.045661109908884126,
      "grad_norm": 6.4453840255737305,
      "learning_rate": 4.9429238330711656e-05,
      "loss": 3.8702,
      "step": 258900
    },
    {
      "epoch": 0.045678746490540705,
      "grad_norm": 7.318309307098389,
      "learning_rate": 4.942901787344095e-05,
      "loss": 3.7519,
      "step": 259000
    },
    {
      "epoch": 0.04569638307219729,
      "grad_norm": 7.623176097869873,
      "learning_rate": 4.942879741617024e-05,
      "loss": 3.6927,
      "step": 259100
    },
    {
      "epoch": 0.04571401965385387,
      "grad_norm": 7.563483715057373,
      "learning_rate": 4.9428576958899536e-05,
      "loss": 3.9439,
      "step": 259200
    },
    {
      "epoch": 0.04573165623551045,
      "grad_norm": 8.79467487335205,
      "learning_rate": 4.942835650162883e-05,
      "loss": 3.9206,
      "step": 259300
    },
    {
      "epoch": 0.045749292817167025,
      "grad_norm": 11.602421760559082,
      "learning_rate": 4.942813604435812e-05,
      "loss": 3.7862,
      "step": 259400
    },
    {
      "epoch": 0.045766929398823604,
      "grad_norm": 6.369513034820557,
      "learning_rate": 4.9427915587087415e-05,
      "loss": 3.8589,
      "step": 259500
    },
    {
      "epoch": 0.04578456598048018,
      "grad_norm": 11.498160362243652,
      "learning_rate": 4.942769512981671e-05,
      "loss": 3.884,
      "step": 259600
    },
    {
      "epoch": 0.04580220256213676,
      "grad_norm": 7.936922073364258,
      "learning_rate": 4.9427474672546e-05,
      "loss": 3.8174,
      "step": 259700
    },
    {
      "epoch": 0.045819839143793346,
      "grad_norm": 8.177273750305176,
      "learning_rate": 4.9427254215275295e-05,
      "loss": 3.7819,
      "step": 259800
    },
    {
      "epoch": 0.045837475725449925,
      "grad_norm": 7.833712577819824,
      "learning_rate": 4.942703375800459e-05,
      "loss": 3.6791,
      "step": 259900
    },
    {
      "epoch": 0.0458551123071065,
      "grad_norm": 7.6334228515625,
      "learning_rate": 4.942681330073388e-05,
      "loss": 3.773,
      "step": 260000
    },
    {
      "epoch": 0.04587274888876308,
      "grad_norm": 7.5034332275390625,
      "learning_rate": 4.942659284346317e-05,
      "loss": 3.8833,
      "step": 260100
    },
    {
      "epoch": 0.04589038547041966,
      "grad_norm": 9.270105361938477,
      "learning_rate": 4.942637238619246e-05,
      "loss": 3.8008,
      "step": 260200
    },
    {
      "epoch": 0.04590802205207624,
      "grad_norm": 8.009225845336914,
      "learning_rate": 4.942615192892175e-05,
      "loss": 3.7501,
      "step": 260300
    },
    {
      "epoch": 0.04592565863373282,
      "grad_norm": 6.924901962280273,
      "learning_rate": 4.942593147165105e-05,
      "loss": 3.775,
      "step": 260400
    },
    {
      "epoch": 0.045943295215389396,
      "grad_norm": 10.571914672851562,
      "learning_rate": 4.942571101438034e-05,
      "loss": 3.7818,
      "step": 260500
    },
    {
      "epoch": 0.04596093179704598,
      "grad_norm": 7.370654106140137,
      "learning_rate": 4.942549055710963e-05,
      "loss": 3.7099,
      "step": 260600
    },
    {
      "epoch": 0.04597856837870256,
      "grad_norm": 9.766551971435547,
      "learning_rate": 4.942527009983893e-05,
      "loss": 3.8633,
      "step": 260700
    },
    {
      "epoch": 0.04599620496035914,
      "grad_norm": 5.694925308227539,
      "learning_rate": 4.942504964256822e-05,
      "loss": 3.8689,
      "step": 260800
    },
    {
      "epoch": 0.04601384154201572,
      "grad_norm": 10.689553260803223,
      "learning_rate": 4.942482918529751e-05,
      "loss": 3.7559,
      "step": 260900
    },
    {
      "epoch": 0.046031478123672295,
      "grad_norm": 6.181005477905273,
      "learning_rate": 4.942460872802681e-05,
      "loss": 3.7795,
      "step": 261000
    },
    {
      "epoch": 0.046049114705328874,
      "grad_norm": 6.73354434967041,
      "learning_rate": 4.94243882707561e-05,
      "loss": 3.6619,
      "step": 261100
    },
    {
      "epoch": 0.04606675128698545,
      "grad_norm": 5.8214616775512695,
      "learning_rate": 4.942416781348539e-05,
      "loss": 3.8524,
      "step": 261200
    },
    {
      "epoch": 0.04608438786864204,
      "grad_norm": 6.241106986999512,
      "learning_rate": 4.9423947356214686e-05,
      "loss": 3.8476,
      "step": 261300
    },
    {
      "epoch": 0.046102024450298616,
      "grad_norm": 8.294571876525879,
      "learning_rate": 4.942372689894398e-05,
      "loss": 3.8622,
      "step": 261400
    },
    {
      "epoch": 0.046119661031955195,
      "grad_norm": 9.596540451049805,
      "learning_rate": 4.942350644167327e-05,
      "loss": 3.9383,
      "step": 261500
    },
    {
      "epoch": 0.04613729761361177,
      "grad_norm": 7.04266881942749,
      "learning_rate": 4.942328598440256e-05,
      "loss": 3.7822,
      "step": 261600
    },
    {
      "epoch": 0.04615493419526835,
      "grad_norm": 8.73695182800293,
      "learning_rate": 4.9423065527131855e-05,
      "loss": 3.6469,
      "step": 261700
    },
    {
      "epoch": 0.04617257077692493,
      "grad_norm": 8.459980964660645,
      "learning_rate": 4.942284506986114e-05,
      "loss": 3.824,
      "step": 261800
    },
    {
      "epoch": 0.04619020735858151,
      "grad_norm": 6.554434299468994,
      "learning_rate": 4.942262461259044e-05,
      "loss": 3.756,
      "step": 261900
    },
    {
      "epoch": 0.04620784394023809,
      "grad_norm": 8.007390975952148,
      "learning_rate": 4.9422404155319734e-05,
      "loss": 3.7281,
      "step": 262000
    },
    {
      "epoch": 0.04622548052189467,
      "grad_norm": 7.25400447845459,
      "learning_rate": 4.942218369804902e-05,
      "loss": 3.7657,
      "step": 262100
    },
    {
      "epoch": 0.04624311710355125,
      "grad_norm": 8.842177391052246,
      "learning_rate": 4.942196324077832e-05,
      "loss": 3.7012,
      "step": 262200
    },
    {
      "epoch": 0.04626075368520783,
      "grad_norm": 6.922393798828125,
      "learning_rate": 4.9421742783507614e-05,
      "loss": 3.9052,
      "step": 262300
    },
    {
      "epoch": 0.04627839026686441,
      "grad_norm": 7.429444313049316,
      "learning_rate": 4.94215223262369e-05,
      "loss": 3.8464,
      "step": 262400
    },
    {
      "epoch": 0.04629602684852099,
      "grad_norm": 8.365531921386719,
      "learning_rate": 4.94213018689662e-05,
      "loss": 3.9075,
      "step": 262500
    },
    {
      "epoch": 0.046313663430177565,
      "grad_norm": 5.818085670471191,
      "learning_rate": 4.9421081411695493e-05,
      "loss": 3.8655,
      "step": 262600
    },
    {
      "epoch": 0.046331300011834144,
      "grad_norm": 6.069094657897949,
      "learning_rate": 4.942086095442478e-05,
      "loss": 3.768,
      "step": 262700
    },
    {
      "epoch": 0.04634893659349073,
      "grad_norm": 6.604176998138428,
      "learning_rate": 4.942064049715408e-05,
      "loss": 3.6729,
      "step": 262800
    },
    {
      "epoch": 0.04636657317514731,
      "grad_norm": 5.907709121704102,
      "learning_rate": 4.9420420039883366e-05,
      "loss": 3.8585,
      "step": 262900
    },
    {
      "epoch": 0.046384209756803886,
      "grad_norm": 7.43931770324707,
      "learning_rate": 4.942019958261266e-05,
      "loss": 3.876,
      "step": 263000
    },
    {
      "epoch": 0.046401846338460465,
      "grad_norm": 6.353298187255859,
      "learning_rate": 4.941997912534195e-05,
      "loss": 3.7687,
      "step": 263100
    },
    {
      "epoch": 0.04641948292011704,
      "grad_norm": 6.062141418457031,
      "learning_rate": 4.9419758668071246e-05,
      "loss": 3.7726,
      "step": 263200
    },
    {
      "epoch": 0.04643711950177362,
      "grad_norm": 8.444308280944824,
      "learning_rate": 4.9419538210800535e-05,
      "loss": 3.8685,
      "step": 263300
    },
    {
      "epoch": 0.0464547560834302,
      "grad_norm": 6.734417915344238,
      "learning_rate": 4.941931775352983e-05,
      "loss": 3.798,
      "step": 263400
    },
    {
      "epoch": 0.04647239266508678,
      "grad_norm": 9.46090316772461,
      "learning_rate": 4.9419097296259126e-05,
      "loss": 3.8731,
      "step": 263500
    },
    {
      "epoch": 0.046490029246743364,
      "grad_norm": 9.27686595916748,
      "learning_rate": 4.9418876838988414e-05,
      "loss": 3.8593,
      "step": 263600
    },
    {
      "epoch": 0.04650766582839994,
      "grad_norm": 7.536101818084717,
      "learning_rate": 4.941865638171771e-05,
      "loss": 3.7977,
      "step": 263700
    },
    {
      "epoch": 0.04652530241005652,
      "grad_norm": 6.268775939941406,
      "learning_rate": 4.9418435924447005e-05,
      "loss": 3.873,
      "step": 263800
    },
    {
      "epoch": 0.0465429389917131,
      "grad_norm": 4.9449615478515625,
      "learning_rate": 4.9418215467176294e-05,
      "loss": 3.8352,
      "step": 263900
    },
    {
      "epoch": 0.04656057557336968,
      "grad_norm": 5.843754768371582,
      "learning_rate": 4.941799500990559e-05,
      "loss": 3.8055,
      "step": 264000
    },
    {
      "epoch": 0.046578212155026257,
      "grad_norm": 8.587483406066895,
      "learning_rate": 4.9417774552634885e-05,
      "loss": 3.8982,
      "step": 264100
    },
    {
      "epoch": 0.046595848736682835,
      "grad_norm": 8.615253448486328,
      "learning_rate": 4.9417554095364174e-05,
      "loss": 3.9425,
      "step": 264200
    },
    {
      "epoch": 0.04661348531833942,
      "grad_norm": 6.453746318817139,
      "learning_rate": 4.941733363809347e-05,
      "loss": 3.8135,
      "step": 264300
    },
    {
      "epoch": 0.046631121899996,
      "grad_norm": 7.050350189208984,
      "learning_rate": 4.941711318082276e-05,
      "loss": 4.0117,
      "step": 264400
    },
    {
      "epoch": 0.04664875848165258,
      "grad_norm": 10.908226013183594,
      "learning_rate": 4.9416892723552046e-05,
      "loss": 3.9307,
      "step": 264500
    },
    {
      "epoch": 0.046666395063309156,
      "grad_norm": 4.668972492218018,
      "learning_rate": 4.941667226628134e-05,
      "loss": 3.6925,
      "step": 264600
    },
    {
      "epoch": 0.046684031644965734,
      "grad_norm": 6.005098819732666,
      "learning_rate": 4.941645180901064e-05,
      "loss": 3.8138,
      "step": 264700
    },
    {
      "epoch": 0.04670166822662231,
      "grad_norm": 6.732658386230469,
      "learning_rate": 4.9416231351739926e-05,
      "loss": 3.8461,
      "step": 264800
    },
    {
      "epoch": 0.04671930480827889,
      "grad_norm": 6.829380512237549,
      "learning_rate": 4.941601089446922e-05,
      "loss": 3.8511,
      "step": 264900
    },
    {
      "epoch": 0.04673694138993547,
      "grad_norm": 7.228491306304932,
      "learning_rate": 4.941579043719852e-05,
      "loss": 3.8045,
      "step": 265000
    },
    {
      "epoch": 0.046754577971592055,
      "grad_norm": 8.219011306762695,
      "learning_rate": 4.9415569979927806e-05,
      "loss": 3.8231,
      "step": 265100
    },
    {
      "epoch": 0.046772214553248634,
      "grad_norm": 7.482449531555176,
      "learning_rate": 4.94153495226571e-05,
      "loss": 3.8576,
      "step": 265200
    },
    {
      "epoch": 0.04678985113490521,
      "grad_norm": 6.719925403594971,
      "learning_rate": 4.9415129065386397e-05,
      "loss": 3.7992,
      "step": 265300
    },
    {
      "epoch": 0.04680748771656179,
      "grad_norm": 7.291539669036865,
      "learning_rate": 4.941490860811569e-05,
      "loss": 3.8141,
      "step": 265400
    },
    {
      "epoch": 0.04682512429821837,
      "grad_norm": 8.328438758850098,
      "learning_rate": 4.941468815084498e-05,
      "loss": 3.8283,
      "step": 265500
    },
    {
      "epoch": 0.04684276087987495,
      "grad_norm": 6.472358703613281,
      "learning_rate": 4.9414467693574276e-05,
      "loss": 3.8614,
      "step": 265600
    },
    {
      "epoch": 0.046860397461531526,
      "grad_norm": 7.211410999298096,
      "learning_rate": 4.9414247236303565e-05,
      "loss": 3.7969,
      "step": 265700
    },
    {
      "epoch": 0.04687803404318811,
      "grad_norm": 7.830506324768066,
      "learning_rate": 4.941402677903286e-05,
      "loss": 3.8619,
      "step": 265800
    },
    {
      "epoch": 0.04689567062484469,
      "grad_norm": 8.426056861877441,
      "learning_rate": 4.941380632176215e-05,
      "loss": 3.9187,
      "step": 265900
    },
    {
      "epoch": 0.04691330720650127,
      "grad_norm": 7.528064250946045,
      "learning_rate": 4.9413585864491445e-05,
      "loss": 3.7839,
      "step": 266000
    },
    {
      "epoch": 0.04693094378815785,
      "grad_norm": 7.308210372924805,
      "learning_rate": 4.941336540722073e-05,
      "loss": 3.7907,
      "step": 266100
    },
    {
      "epoch": 0.046948580369814426,
      "grad_norm": 6.433085918426514,
      "learning_rate": 4.941314494995003e-05,
      "loss": 3.8052,
      "step": 266200
    },
    {
      "epoch": 0.046966216951471004,
      "grad_norm": 9.414775848388672,
      "learning_rate": 4.9412924492679324e-05,
      "loss": 3.7065,
      "step": 266300
    },
    {
      "epoch": 0.04698385353312758,
      "grad_norm": 7.179811954498291,
      "learning_rate": 4.941270403540861e-05,
      "loss": 3.7572,
      "step": 266400
    },
    {
      "epoch": 0.04700149011478416,
      "grad_norm": 5.473998546600342,
      "learning_rate": 4.941248357813791e-05,
      "loss": 3.7812,
      "step": 266500
    },
    {
      "epoch": 0.04701912669644075,
      "grad_norm": 6.904870986938477,
      "learning_rate": 4.9412263120867204e-05,
      "loss": 3.8809,
      "step": 266600
    },
    {
      "epoch": 0.047036763278097325,
      "grad_norm": 5.886017322540283,
      "learning_rate": 4.941204266359649e-05,
      "loss": 3.794,
      "step": 266700
    },
    {
      "epoch": 0.047054399859753904,
      "grad_norm": 7.748331069946289,
      "learning_rate": 4.941182220632579e-05,
      "loss": 3.7763,
      "step": 266800
    },
    {
      "epoch": 0.04707203644141048,
      "grad_norm": 10.98639965057373,
      "learning_rate": 4.9411601749055083e-05,
      "loss": 3.7452,
      "step": 266900
    },
    {
      "epoch": 0.04708967302306706,
      "grad_norm": 6.943963527679443,
      "learning_rate": 4.941138129178437e-05,
      "loss": 3.8812,
      "step": 267000
    },
    {
      "epoch": 0.04710730960472364,
      "grad_norm": 7.588156700134277,
      "learning_rate": 4.941116083451367e-05,
      "loss": 3.776,
      "step": 267100
    },
    {
      "epoch": 0.04712494618638022,
      "grad_norm": 8.575557708740234,
      "learning_rate": 4.9410940377242956e-05,
      "loss": 3.8068,
      "step": 267200
    },
    {
      "epoch": 0.0471425827680368,
      "grad_norm": 6.800861358642578,
      "learning_rate": 4.9410719919972245e-05,
      "loss": 3.6742,
      "step": 267300
    },
    {
      "epoch": 0.04716021934969338,
      "grad_norm": 7.253435134887695,
      "learning_rate": 4.941049946270154e-05,
      "loss": 3.8522,
      "step": 267400
    },
    {
      "epoch": 0.04717785593134996,
      "grad_norm": 8.50384521484375,
      "learning_rate": 4.9410279005430836e-05,
      "loss": 3.7783,
      "step": 267500
    },
    {
      "epoch": 0.04719549251300654,
      "grad_norm": 6.000443935394287,
      "learning_rate": 4.9410058548160125e-05,
      "loss": 3.8004,
      "step": 267600
    },
    {
      "epoch": 0.04721312909466312,
      "grad_norm": 7.298534870147705,
      "learning_rate": 4.940983809088942e-05,
      "loss": 3.8117,
      "step": 267700
    },
    {
      "epoch": 0.047230765676319696,
      "grad_norm": 8.68896770477295,
      "learning_rate": 4.9409617633618716e-05,
      "loss": 3.9611,
      "step": 267800
    },
    {
      "epoch": 0.047248402257976274,
      "grad_norm": 9.765854835510254,
      "learning_rate": 4.9409397176348004e-05,
      "loss": 3.9048,
      "step": 267900
    },
    {
      "epoch": 0.04726603883963285,
      "grad_norm": 6.771575450897217,
      "learning_rate": 4.94091767190773e-05,
      "loss": 3.8502,
      "step": 268000
    },
    {
      "epoch": 0.04728367542128944,
      "grad_norm": 12.300946235656738,
      "learning_rate": 4.9408956261806595e-05,
      "loss": 3.9207,
      "step": 268100
    },
    {
      "epoch": 0.04730131200294602,
      "grad_norm": 7.609684467315674,
      "learning_rate": 4.9408735804535884e-05,
      "loss": 3.7728,
      "step": 268200
    },
    {
      "epoch": 0.047318948584602595,
      "grad_norm": 5.92277717590332,
      "learning_rate": 4.940851534726518e-05,
      "loss": 3.8124,
      "step": 268300
    },
    {
      "epoch": 0.047336585166259174,
      "grad_norm": 7.934424877166748,
      "learning_rate": 4.9408294889994475e-05,
      "loss": 3.9007,
      "step": 268400
    },
    {
      "epoch": 0.04735422174791575,
      "grad_norm": 9.784831047058105,
      "learning_rate": 4.9408074432723763e-05,
      "loss": 3.8501,
      "step": 268500
    },
    {
      "epoch": 0.04737185832957233,
      "grad_norm": 8.429459571838379,
      "learning_rate": 4.940785397545306e-05,
      "loss": 3.8954,
      "step": 268600
    },
    {
      "epoch": 0.04738949491122891,
      "grad_norm": 6.029760837554932,
      "learning_rate": 4.940763351818235e-05,
      "loss": 3.907,
      "step": 268700
    },
    {
      "epoch": 0.047407131492885494,
      "grad_norm": 5.662801742553711,
      "learning_rate": 4.9407413060911636e-05,
      "loss": 3.8068,
      "step": 268800
    },
    {
      "epoch": 0.04742476807454207,
      "grad_norm": 8.166072845458984,
      "learning_rate": 4.940719260364093e-05,
      "loss": 3.766,
      "step": 268900
    },
    {
      "epoch": 0.04744240465619865,
      "grad_norm": 6.039344310760498,
      "learning_rate": 4.940697214637023e-05,
      "loss": 3.7719,
      "step": 269000
    },
    {
      "epoch": 0.04746004123785523,
      "grad_norm": 6.967318534851074,
      "learning_rate": 4.9406751689099516e-05,
      "loss": 3.7863,
      "step": 269100
    },
    {
      "epoch": 0.04747767781951181,
      "grad_norm": 10.435194969177246,
      "learning_rate": 4.940653123182881e-05,
      "loss": 3.8013,
      "step": 269200
    },
    {
      "epoch": 0.04749531440116839,
      "grad_norm": 5.60943603515625,
      "learning_rate": 4.940631077455811e-05,
      "loss": 3.8655,
      "step": 269300
    },
    {
      "epoch": 0.047512950982824965,
      "grad_norm": 9.201314926147461,
      "learning_rate": 4.9406090317287396e-05,
      "loss": 3.8063,
      "step": 269400
    },
    {
      "epoch": 0.047530587564481544,
      "grad_norm": 7.6304826736450195,
      "learning_rate": 4.940586986001669e-05,
      "loss": 3.8616,
      "step": 269500
    },
    {
      "epoch": 0.04754822414613813,
      "grad_norm": 10.931927680969238,
      "learning_rate": 4.9405649402745987e-05,
      "loss": 3.8503,
      "step": 269600
    },
    {
      "epoch": 0.04756586072779471,
      "grad_norm": 7.038604736328125,
      "learning_rate": 4.9405428945475275e-05,
      "loss": 3.7217,
      "step": 269700
    },
    {
      "epoch": 0.047583497309451286,
      "grad_norm": 7.319185256958008,
      "learning_rate": 4.940520848820457e-05,
      "loss": 3.8252,
      "step": 269800
    },
    {
      "epoch": 0.047601133891107865,
      "grad_norm": 7.165927886962891,
      "learning_rate": 4.9404988030933866e-05,
      "loss": 3.7442,
      "step": 269900
    },
    {
      "epoch": 0.04761877047276444,
      "grad_norm": 10.989160537719727,
      "learning_rate": 4.9404767573663155e-05,
      "loss": 3.7796,
      "step": 270000
    },
    {
      "epoch": 0.04763640705442102,
      "grad_norm": 8.257041931152344,
      "learning_rate": 4.9404547116392444e-05,
      "loss": 3.8243,
      "step": 270100
    },
    {
      "epoch": 0.0476540436360776,
      "grad_norm": 7.767772197723389,
      "learning_rate": 4.940432665912174e-05,
      "loss": 3.8471,
      "step": 270200
    },
    {
      "epoch": 0.047671680217734186,
      "grad_norm": 6.7450995445251465,
      "learning_rate": 4.940410620185103e-05,
      "loss": 3.8103,
      "step": 270300
    },
    {
      "epoch": 0.047689316799390764,
      "grad_norm": 7.228923797607422,
      "learning_rate": 4.940388574458032e-05,
      "loss": 3.7755,
      "step": 270400
    },
    {
      "epoch": 0.04770695338104734,
      "grad_norm": 6.7070698738098145,
      "learning_rate": 4.940366528730962e-05,
      "loss": 3.8395,
      "step": 270500
    },
    {
      "epoch": 0.04772458996270392,
      "grad_norm": 6.780328273773193,
      "learning_rate": 4.940344483003891e-05,
      "loss": 3.8176,
      "step": 270600
    },
    {
      "epoch": 0.0477422265443605,
      "grad_norm": 6.241909980773926,
      "learning_rate": 4.94032243727682e-05,
      "loss": 3.8902,
      "step": 270700
    },
    {
      "epoch": 0.04775986312601708,
      "grad_norm": 7.612326145172119,
      "learning_rate": 4.94030039154975e-05,
      "loss": 3.8066,
      "step": 270800
    },
    {
      "epoch": 0.04777749970767366,
      "grad_norm": 8.387763023376465,
      "learning_rate": 4.940278345822679e-05,
      "loss": 3.8924,
      "step": 270900
    },
    {
      "epoch": 0.047795136289330235,
      "grad_norm": 10.845412254333496,
      "learning_rate": 4.940256300095608e-05,
      "loss": 3.8477,
      "step": 271000
    },
    {
      "epoch": 0.04781277287098682,
      "grad_norm": 6.692364692687988,
      "learning_rate": 4.940234254368538e-05,
      "loss": 3.7462,
      "step": 271100
    },
    {
      "epoch": 0.0478304094526434,
      "grad_norm": 7.000276565551758,
      "learning_rate": 4.9402122086414667e-05,
      "loss": 3.8055,
      "step": 271200
    },
    {
      "epoch": 0.04784804603429998,
      "grad_norm": 10.848995208740234,
      "learning_rate": 4.940190162914396e-05,
      "loss": 3.8547,
      "step": 271300
    },
    {
      "epoch": 0.047865682615956556,
      "grad_norm": 6.223498821258545,
      "learning_rate": 4.940168117187326e-05,
      "loss": 3.7163,
      "step": 271400
    },
    {
      "epoch": 0.047883319197613135,
      "grad_norm": 6.010849475860596,
      "learning_rate": 4.9401460714602546e-05,
      "loss": 3.7686,
      "step": 271500
    },
    {
      "epoch": 0.04790095577926971,
      "grad_norm": 6.116968154907227,
      "learning_rate": 4.9401240257331835e-05,
      "loss": 3.8715,
      "step": 271600
    },
    {
      "epoch": 0.04791859236092629,
      "grad_norm": 10.4368314743042,
      "learning_rate": 4.940101980006113e-05,
      "loss": 3.9185,
      "step": 271700
    },
    {
      "epoch": 0.04793622894258288,
      "grad_norm": 5.138176918029785,
      "learning_rate": 4.940079934279042e-05,
      "loss": 3.7439,
      "step": 271800
    },
    {
      "epoch": 0.047953865524239456,
      "grad_norm": 7.700058460235596,
      "learning_rate": 4.9400578885519715e-05,
      "loss": 3.7904,
      "step": 271900
    },
    {
      "epoch": 0.047971502105896034,
      "grad_norm": 5.21931266784668,
      "learning_rate": 4.940035842824901e-05,
      "loss": 3.7134,
      "step": 272000
    },
    {
      "epoch": 0.04798913868755261,
      "grad_norm": 6.313577651977539,
      "learning_rate": 4.94001379709783e-05,
      "loss": 3.8416,
      "step": 272100
    },
    {
      "epoch": 0.04800677526920919,
      "grad_norm": 6.750517845153809,
      "learning_rate": 4.9399917513707594e-05,
      "loss": 3.7722,
      "step": 272200
    },
    {
      "epoch": 0.04802441185086577,
      "grad_norm": 8.25092601776123,
      "learning_rate": 4.939969705643689e-05,
      "loss": 3.82,
      "step": 272300
    },
    {
      "epoch": 0.04804204843252235,
      "grad_norm": 6.724236488342285,
      "learning_rate": 4.939947659916618e-05,
      "loss": 3.7708,
      "step": 272400
    },
    {
      "epoch": 0.04805968501417893,
      "grad_norm": 7.3816046714782715,
      "learning_rate": 4.9399256141895474e-05,
      "loss": 3.857,
      "step": 272500
    },
    {
      "epoch": 0.04807732159583551,
      "grad_norm": 7.0695037841796875,
      "learning_rate": 4.939903568462477e-05,
      "loss": 3.849,
      "step": 272600
    },
    {
      "epoch": 0.04809495817749209,
      "grad_norm": 6.998348712921143,
      "learning_rate": 4.939881522735406e-05,
      "loss": 3.791,
      "step": 272700
    },
    {
      "epoch": 0.04811259475914867,
      "grad_norm": 6.141467571258545,
      "learning_rate": 4.9398594770083353e-05,
      "loss": 3.842,
      "step": 272800
    },
    {
      "epoch": 0.04813023134080525,
      "grad_norm": 7.025646686553955,
      "learning_rate": 4.939837431281264e-05,
      "loss": 3.7821,
      "step": 272900
    },
    {
      "epoch": 0.048147867922461826,
      "grad_norm": 6.863890647888184,
      "learning_rate": 4.939815385554194e-05,
      "loss": 3.7295,
      "step": 273000
    },
    {
      "epoch": 0.048165504504118405,
      "grad_norm": 10.660886764526367,
      "learning_rate": 4.9397933398271226e-05,
      "loss": 3.836,
      "step": 273100
    },
    {
      "epoch": 0.04818314108577498,
      "grad_norm": 6.30218505859375,
      "learning_rate": 4.939771294100052e-05,
      "loss": 3.8801,
      "step": 273200
    },
    {
      "epoch": 0.04820077766743157,
      "grad_norm": 9.256681442260742,
      "learning_rate": 4.939749248372981e-05,
      "loss": 3.7548,
      "step": 273300
    },
    {
      "epoch": 0.04821841424908815,
      "grad_norm": 7.269128799438477,
      "learning_rate": 4.9397272026459106e-05,
      "loss": 3.7419,
      "step": 273400
    },
    {
      "epoch": 0.048236050830744726,
      "grad_norm": 6.815064430236816,
      "learning_rate": 4.93970515691884e-05,
      "loss": 3.9168,
      "step": 273500
    },
    {
      "epoch": 0.048253687412401304,
      "grad_norm": 7.184151649475098,
      "learning_rate": 4.939683111191769e-05,
      "loss": 3.7622,
      "step": 273600
    },
    {
      "epoch": 0.04827132399405788,
      "grad_norm": 12.300812721252441,
      "learning_rate": 4.9396610654646986e-05,
      "loss": 3.8366,
      "step": 273700
    },
    {
      "epoch": 0.04828896057571446,
      "grad_norm": 6.967639446258545,
      "learning_rate": 4.939639019737628e-05,
      "loss": 3.881,
      "step": 273800
    },
    {
      "epoch": 0.04830659715737104,
      "grad_norm": 7.441606044769287,
      "learning_rate": 4.939616974010557e-05,
      "loss": 3.8753,
      "step": 273900
    },
    {
      "epoch": 0.04832423373902762,
      "grad_norm": 4.883002758026123,
      "learning_rate": 4.9395949282834865e-05,
      "loss": 3.8457,
      "step": 274000
    },
    {
      "epoch": 0.048341870320684203,
      "grad_norm": 6.178522109985352,
      "learning_rate": 4.939572882556416e-05,
      "loss": 3.8197,
      "step": 274100
    },
    {
      "epoch": 0.04835950690234078,
      "grad_norm": 6.299166202545166,
      "learning_rate": 4.939550836829345e-05,
      "loss": 3.8168,
      "step": 274200
    },
    {
      "epoch": 0.04837714348399736,
      "grad_norm": 8.717310905456543,
      "learning_rate": 4.9395287911022745e-05,
      "loss": 3.8226,
      "step": 274300
    },
    {
      "epoch": 0.04839478006565394,
      "grad_norm": 11.22120475769043,
      "learning_rate": 4.9395067453752033e-05,
      "loss": 3.8634,
      "step": 274400
    },
    {
      "epoch": 0.04841241664731052,
      "grad_norm": 7.807533264160156,
      "learning_rate": 4.939484699648132e-05,
      "loss": 3.8199,
      "step": 274500
    },
    {
      "epoch": 0.048430053228967096,
      "grad_norm": 5.825372219085693,
      "learning_rate": 4.939462653921062e-05,
      "loss": 3.6781,
      "step": 274600
    },
    {
      "epoch": 0.048447689810623674,
      "grad_norm": 7.98043155670166,
      "learning_rate": 4.939440608193991e-05,
      "loss": 3.7818,
      "step": 274700
    },
    {
      "epoch": 0.04846532639228026,
      "grad_norm": 4.53777551651001,
      "learning_rate": 4.93941856246692e-05,
      "loss": 3.8456,
      "step": 274800
    },
    {
      "epoch": 0.04848296297393684,
      "grad_norm": 8.35807991027832,
      "learning_rate": 4.93939651673985e-05,
      "loss": 3.831,
      "step": 274900
    },
    {
      "epoch": 0.04850059955559342,
      "grad_norm": 8.142704010009766,
      "learning_rate": 4.939374471012779e-05,
      "loss": 3.831,
      "step": 275000
    },
    {
      "epoch": 0.048518236137249995,
      "grad_norm": 6.267306804656982,
      "learning_rate": 4.939352425285708e-05,
      "loss": 3.7591,
      "step": 275100
    },
    {
      "epoch": 0.048535872718906574,
      "grad_norm": 8.024884223937988,
      "learning_rate": 4.939330379558638e-05,
      "loss": 3.6882,
      "step": 275200
    },
    {
      "epoch": 0.04855350930056315,
      "grad_norm": 7.821133136749268,
      "learning_rate": 4.939308333831567e-05,
      "loss": 3.7521,
      "step": 275300
    },
    {
      "epoch": 0.04857114588221973,
      "grad_norm": 8.376211166381836,
      "learning_rate": 4.939286288104496e-05,
      "loss": 3.7256,
      "step": 275400
    },
    {
      "epoch": 0.04858878246387631,
      "grad_norm": 6.596561431884766,
      "learning_rate": 4.9392642423774257e-05,
      "loss": 3.8858,
      "step": 275500
    },
    {
      "epoch": 0.048606419045532895,
      "grad_norm": 6.24705171585083,
      "learning_rate": 4.939242196650355e-05,
      "loss": 3.8474,
      "step": 275600
    },
    {
      "epoch": 0.04862405562718947,
      "grad_norm": 7.117159843444824,
      "learning_rate": 4.939220150923284e-05,
      "loss": 3.7552,
      "step": 275700
    },
    {
      "epoch": 0.04864169220884605,
      "grad_norm": 5.747297763824463,
      "learning_rate": 4.9391981051962136e-05,
      "loss": 3.8011,
      "step": 275800
    },
    {
      "epoch": 0.04865932879050263,
      "grad_norm": 9.071423530578613,
      "learning_rate": 4.9391760594691425e-05,
      "loss": 3.8742,
      "step": 275900
    },
    {
      "epoch": 0.04867696537215921,
      "grad_norm": 6.666932106018066,
      "learning_rate": 4.939154013742072e-05,
      "loss": 3.734,
      "step": 276000
    },
    {
      "epoch": 0.04869460195381579,
      "grad_norm": 8.313365936279297,
      "learning_rate": 4.939131968015001e-05,
      "loss": 3.8259,
      "step": 276100
    },
    {
      "epoch": 0.048712238535472366,
      "grad_norm": 5.409659385681152,
      "learning_rate": 4.9391099222879304e-05,
      "loss": 3.8152,
      "step": 276200
    },
    {
      "epoch": 0.04872987511712895,
      "grad_norm": 6.510919570922852,
      "learning_rate": 4.93908787656086e-05,
      "loss": 3.762,
      "step": 276300
    },
    {
      "epoch": 0.04874751169878553,
      "grad_norm": 7.227108001708984,
      "learning_rate": 4.939065830833789e-05,
      "loss": 3.7531,
      "step": 276400
    },
    {
      "epoch": 0.04876514828044211,
      "grad_norm": 6.29630184173584,
      "learning_rate": 4.9390437851067184e-05,
      "loss": 3.7693,
      "step": 276500
    },
    {
      "epoch": 0.04878278486209869,
      "grad_norm": 8.738157272338867,
      "learning_rate": 4.939021739379648e-05,
      "loss": 3.8539,
      "step": 276600
    },
    {
      "epoch": 0.048800421443755265,
      "grad_norm": 6.05345344543457,
      "learning_rate": 4.938999693652577e-05,
      "loss": 3.8732,
      "step": 276700
    },
    {
      "epoch": 0.048818058025411844,
      "grad_norm": 6.592251777648926,
      "learning_rate": 4.9389776479255064e-05,
      "loss": 3.7891,
      "step": 276800
    },
    {
      "epoch": 0.04883569460706842,
      "grad_norm": 8.542719841003418,
      "learning_rate": 4.938955602198436e-05,
      "loss": 3.819,
      "step": 276900
    },
    {
      "epoch": 0.048853331188725,
      "grad_norm": 6.9768290519714355,
      "learning_rate": 4.938933556471365e-05,
      "loss": 3.8969,
      "step": 277000
    },
    {
      "epoch": 0.048870967770381586,
      "grad_norm": 7.1177473068237305,
      "learning_rate": 4.938911510744294e-05,
      "loss": 3.7835,
      "step": 277100
    },
    {
      "epoch": 0.048888604352038165,
      "grad_norm": 6.971645832061768,
      "learning_rate": 4.938889465017223e-05,
      "loss": 3.7898,
      "step": 277200
    },
    {
      "epoch": 0.04890624093369474,
      "grad_norm": 10.992897033691406,
      "learning_rate": 4.938867419290152e-05,
      "loss": 3.8719,
      "step": 277300
    },
    {
      "epoch": 0.04892387751535132,
      "grad_norm": 9.692060470581055,
      "learning_rate": 4.9388453735630816e-05,
      "loss": 3.7707,
      "step": 277400
    },
    {
      "epoch": 0.0489415140970079,
      "grad_norm": 7.177724838256836,
      "learning_rate": 4.938823327836011e-05,
      "loss": 3.7619,
      "step": 277500
    },
    {
      "epoch": 0.04895915067866448,
      "grad_norm": 6.387118339538574,
      "learning_rate": 4.93880128210894e-05,
      "loss": 3.7867,
      "step": 277600
    },
    {
      "epoch": 0.04897678726032106,
      "grad_norm": 7.0473480224609375,
      "learning_rate": 4.9387792363818696e-05,
      "loss": 3.8191,
      "step": 277700
    },
    {
      "epoch": 0.04899442384197764,
      "grad_norm": 6.212815284729004,
      "learning_rate": 4.938757190654799e-05,
      "loss": 3.8216,
      "step": 277800
    },
    {
      "epoch": 0.04901206042363422,
      "grad_norm": 6.465844631195068,
      "learning_rate": 4.938735144927728e-05,
      "loss": 3.8275,
      "step": 277900
    },
    {
      "epoch": 0.0490296970052908,
      "grad_norm": 5.971502780914307,
      "learning_rate": 4.9387130992006575e-05,
      "loss": 3.8579,
      "step": 278000
    },
    {
      "epoch": 0.04904733358694738,
      "grad_norm": 6.853618144989014,
      "learning_rate": 4.938691053473587e-05,
      "loss": 3.8655,
      "step": 278100
    },
    {
      "epoch": 0.04906497016860396,
      "grad_norm": 8.163467407226562,
      "learning_rate": 4.938669007746516e-05,
      "loss": 3.8137,
      "step": 278200
    },
    {
      "epoch": 0.049082606750260535,
      "grad_norm": 9.299246788024902,
      "learning_rate": 4.9386469620194455e-05,
      "loss": 3.7376,
      "step": 278300
    },
    {
      "epoch": 0.049100243331917114,
      "grad_norm": 6.453317165374756,
      "learning_rate": 4.938624916292375e-05,
      "loss": 3.7797,
      "step": 278400
    },
    {
      "epoch": 0.04911787991357369,
      "grad_norm": 8.950345993041992,
      "learning_rate": 4.938602870565304e-05,
      "loss": 3.7139,
      "step": 278500
    },
    {
      "epoch": 0.04913551649523028,
      "grad_norm": 15.012852668762207,
      "learning_rate": 4.9385808248382335e-05,
      "loss": 3.8706,
      "step": 278600
    },
    {
      "epoch": 0.049153153076886856,
      "grad_norm": 7.999581813812256,
      "learning_rate": 4.9385587791111623e-05,
      "loss": 3.8674,
      "step": 278700
    },
    {
      "epoch": 0.049170789658543435,
      "grad_norm": 6.966564655303955,
      "learning_rate": 4.938536733384091e-05,
      "loss": 3.8099,
      "step": 278800
    },
    {
      "epoch": 0.04918842624020001,
      "grad_norm": 5.873568534851074,
      "learning_rate": 4.938514687657021e-05,
      "loss": 3.7903,
      "step": 278900
    },
    {
      "epoch": 0.04920606282185659,
      "grad_norm": 6.1718034744262695,
      "learning_rate": 4.93849264192995e-05,
      "loss": 3.7024,
      "step": 279000
    },
    {
      "epoch": 0.04922369940351317,
      "grad_norm": 5.294618129730225,
      "learning_rate": 4.938470596202879e-05,
      "loss": 3.8944,
      "step": 279100
    },
    {
      "epoch": 0.04924133598516975,
      "grad_norm": 4.981637001037598,
      "learning_rate": 4.938448550475809e-05,
      "loss": 3.8342,
      "step": 279200
    },
    {
      "epoch": 0.049258972566826334,
      "grad_norm": 9.067610740661621,
      "learning_rate": 4.938426504748738e-05,
      "loss": 3.8287,
      "step": 279300
    },
    {
      "epoch": 0.04927660914848291,
      "grad_norm": 6.054286003112793,
      "learning_rate": 4.938404459021667e-05,
      "loss": 3.9003,
      "step": 279400
    },
    {
      "epoch": 0.04929424573013949,
      "grad_norm": 8.148858070373535,
      "learning_rate": 4.938382413294597e-05,
      "loss": 3.7389,
      "step": 279500
    },
    {
      "epoch": 0.04931188231179607,
      "grad_norm": 5.239684581756592,
      "learning_rate": 4.938360367567526e-05,
      "loss": 3.905,
      "step": 279600
    },
    {
      "epoch": 0.04932951889345265,
      "grad_norm": 9.321554183959961,
      "learning_rate": 4.938338321840455e-05,
      "loss": 3.7509,
      "step": 279700
    },
    {
      "epoch": 0.049347155475109226,
      "grad_norm": 6.584101676940918,
      "learning_rate": 4.9383162761133846e-05,
      "loss": 3.8932,
      "step": 279800
    },
    {
      "epoch": 0.049364792056765805,
      "grad_norm": 7.774539947509766,
      "learning_rate": 4.938294230386314e-05,
      "loss": 3.8059,
      "step": 279900
    },
    {
      "epoch": 0.04938242863842238,
      "grad_norm": 6.008896350860596,
      "learning_rate": 4.938272184659243e-05,
      "loss": 3.7159,
      "step": 280000
    },
    {
      "epoch": 0.04940006522007897,
      "grad_norm": 8.946270942687988,
      "learning_rate": 4.938250138932172e-05,
      "loss": 3.8772,
      "step": 280100
    },
    {
      "epoch": 0.04941770180173555,
      "grad_norm": 8.935681343078613,
      "learning_rate": 4.9382280932051015e-05,
      "loss": 3.7882,
      "step": 280200
    },
    {
      "epoch": 0.049435338383392126,
      "grad_norm": 8.806510925292969,
      "learning_rate": 4.9382060474780303e-05,
      "loss": 3.8351,
      "step": 280300
    },
    {
      "epoch": 0.049452974965048704,
      "grad_norm": 7.948183536529541,
      "learning_rate": 4.93818400175096e-05,
      "loss": 3.8153,
      "step": 280400
    },
    {
      "epoch": 0.04947061154670528,
      "grad_norm": 4.781657695770264,
      "learning_rate": 4.9381619560238894e-05,
      "loss": 3.8389,
      "step": 280500
    },
    {
      "epoch": 0.04948824812836186,
      "grad_norm": 6.182093143463135,
      "learning_rate": 4.938139910296818e-05,
      "loss": 3.8547,
      "step": 280600
    },
    {
      "epoch": 0.04950588471001844,
      "grad_norm": 6.478462219238281,
      "learning_rate": 4.938117864569748e-05,
      "loss": 3.742,
      "step": 280700
    },
    {
      "epoch": 0.049523521291675025,
      "grad_norm": 6.1707611083984375,
      "learning_rate": 4.9380958188426774e-05,
      "loss": 3.9148,
      "step": 280800
    },
    {
      "epoch": 0.049541157873331604,
      "grad_norm": 8.004400253295898,
      "learning_rate": 4.938073773115606e-05,
      "loss": 3.745,
      "step": 280900
    },
    {
      "epoch": 0.04955879445498818,
      "grad_norm": 8.049574851989746,
      "learning_rate": 4.938051727388536e-05,
      "loss": 3.8993,
      "step": 281000
    },
    {
      "epoch": 0.04957643103664476,
      "grad_norm": 6.429286479949951,
      "learning_rate": 4.9380296816614654e-05,
      "loss": 3.9295,
      "step": 281100
    },
    {
      "epoch": 0.04959406761830134,
      "grad_norm": 6.499159812927246,
      "learning_rate": 4.938007635934394e-05,
      "loss": 3.7558,
      "step": 281200
    },
    {
      "epoch": 0.04961170419995792,
      "grad_norm": 7.996886730194092,
      "learning_rate": 4.937985590207324e-05,
      "loss": 3.695,
      "step": 281300
    },
    {
      "epoch": 0.049629340781614496,
      "grad_norm": 8.800505638122559,
      "learning_rate": 4.937963544480253e-05,
      "loss": 3.7549,
      "step": 281400
    },
    {
      "epoch": 0.049646977363271075,
      "grad_norm": 5.94477653503418,
      "learning_rate": 4.937941498753182e-05,
      "loss": 3.7019,
      "step": 281500
    },
    {
      "epoch": 0.04966461394492766,
      "grad_norm": 8.142223358154297,
      "learning_rate": 4.937919453026111e-05,
      "loss": 3.8939,
      "step": 281600
    },
    {
      "epoch": 0.04968225052658424,
      "grad_norm": 4.963144779205322,
      "learning_rate": 4.9378974072990406e-05,
      "loss": 3.8823,
      "step": 281700
    },
    {
      "epoch": 0.04969988710824082,
      "grad_norm": 8.318524360656738,
      "learning_rate": 4.9378753615719695e-05,
      "loss": 3.8075,
      "step": 281800
    },
    {
      "epoch": 0.049717523689897396,
      "grad_norm": 8.245524406433105,
      "learning_rate": 4.937853315844899e-05,
      "loss": 3.7427,
      "step": 281900
    },
    {
      "epoch": 0.049735160271553974,
      "grad_norm": 6.832765102386475,
      "learning_rate": 4.9378312701178286e-05,
      "loss": 3.8242,
      "step": 282000
    },
    {
      "epoch": 0.04975279685321055,
      "grad_norm": 6.809195041656494,
      "learning_rate": 4.9378092243907574e-05,
      "loss": 3.7805,
      "step": 282100
    },
    {
      "epoch": 0.04977043343486713,
      "grad_norm": 7.821868419647217,
      "learning_rate": 4.937787178663687e-05,
      "loss": 3.8296,
      "step": 282200
    },
    {
      "epoch": 0.04978807001652372,
      "grad_norm": 7.932534217834473,
      "learning_rate": 4.9377651329366165e-05,
      "loss": 3.6441,
      "step": 282300
    },
    {
      "epoch": 0.049805706598180295,
      "grad_norm": 6.452662944793701,
      "learning_rate": 4.9377430872095454e-05,
      "loss": 3.7561,
      "step": 282400
    },
    {
      "epoch": 0.049823343179836874,
      "grad_norm": 7.217107772827148,
      "learning_rate": 4.937721041482475e-05,
      "loss": 3.7944,
      "step": 282500
    },
    {
      "epoch": 0.04984097976149345,
      "grad_norm": 6.051760673522949,
      "learning_rate": 4.9376989957554045e-05,
      "loss": 3.7966,
      "step": 282600
    },
    {
      "epoch": 0.04985861634315003,
      "grad_norm": 7.43934965133667,
      "learning_rate": 4.9376769500283334e-05,
      "loss": 3.8319,
      "step": 282700
    },
    {
      "epoch": 0.04987625292480661,
      "grad_norm": 9.261351585388184,
      "learning_rate": 4.937654904301263e-05,
      "loss": 3.8573,
      "step": 282800
    },
    {
      "epoch": 0.04989388950646319,
      "grad_norm": 5.692983627319336,
      "learning_rate": 4.937632858574192e-05,
      "loss": 3.7258,
      "step": 282900
    },
    {
      "epoch": 0.049911526088119766,
      "grad_norm": 5.596556663513184,
      "learning_rate": 4.937610812847121e-05,
      "loss": 3.8106,
      "step": 283000
    },
    {
      "epoch": 0.04992916266977635,
      "grad_norm": 7.840824604034424,
      "learning_rate": 4.93758876712005e-05,
      "loss": 3.869,
      "step": 283100
    },
    {
      "epoch": 0.04994679925143293,
      "grad_norm": 7.772075176239014,
      "learning_rate": 4.93756672139298e-05,
      "loss": 3.836,
      "step": 283200
    },
    {
      "epoch": 0.04996443583308951,
      "grad_norm": 6.260948181152344,
      "learning_rate": 4.9375446756659086e-05,
      "loss": 3.8026,
      "step": 283300
    },
    {
      "epoch": 0.04998207241474609,
      "grad_norm": 6.8172125816345215,
      "learning_rate": 4.937522629938838e-05,
      "loss": 3.8466,
      "step": 283400
    },
    {
      "epoch": 0.049999708996402666,
      "grad_norm": 10.078128814697266,
      "learning_rate": 4.937500584211768e-05,
      "loss": 3.8923,
      "step": 283500
    },
    {
      "epoch": 0.050017345578059244,
      "grad_norm": 5.098606109619141,
      "learning_rate": 4.9374785384846966e-05,
      "loss": 3.7118,
      "step": 283600
    },
    {
      "epoch": 0.05003498215971582,
      "grad_norm": 5.714226722717285,
      "learning_rate": 4.937456492757626e-05,
      "loss": 3.8072,
      "step": 283700
    },
    {
      "epoch": 0.05005261874137241,
      "grad_norm": 8.424817085266113,
      "learning_rate": 4.937434447030556e-05,
      "loss": 3.7163,
      "step": 283800
    },
    {
      "epoch": 0.050070255323028987,
      "grad_norm": 5.39387845993042,
      "learning_rate": 4.9374124013034845e-05,
      "loss": 3.8155,
      "step": 283900
    },
    {
      "epoch": 0.050087891904685565,
      "grad_norm": 6.387217998504639,
      "learning_rate": 4.937390355576414e-05,
      "loss": 3.7192,
      "step": 284000
    },
    {
      "epoch": 0.050105528486342144,
      "grad_norm": 7.037935256958008,
      "learning_rate": 4.9373683098493436e-05,
      "loss": 3.7062,
      "step": 284100
    },
    {
      "epoch": 0.05012316506799872,
      "grad_norm": 7.098381042480469,
      "learning_rate": 4.9373462641222725e-05,
      "loss": 3.6796,
      "step": 284200
    },
    {
      "epoch": 0.0501408016496553,
      "grad_norm": 8.105600357055664,
      "learning_rate": 4.937324218395202e-05,
      "loss": 3.7938,
      "step": 284300
    },
    {
      "epoch": 0.05015843823131188,
      "grad_norm": 6.31407356262207,
      "learning_rate": 4.937302172668131e-05,
      "loss": 3.8706,
      "step": 284400
    },
    {
      "epoch": 0.05017607481296846,
      "grad_norm": 8.50644302368164,
      "learning_rate": 4.93728012694106e-05,
      "loss": 3.8443,
      "step": 284500
    },
    {
      "epoch": 0.05019371139462504,
      "grad_norm": 9.054360389709473,
      "learning_rate": 4.9372580812139893e-05,
      "loss": 3.712,
      "step": 284600
    },
    {
      "epoch": 0.05021134797628162,
      "grad_norm": 5.462251663208008,
      "learning_rate": 4.937236035486919e-05,
      "loss": 3.7808,
      "step": 284700
    },
    {
      "epoch": 0.0502289845579382,
      "grad_norm": 8.437084197998047,
      "learning_rate": 4.937213989759848e-05,
      "loss": 3.8963,
      "step": 284800
    },
    {
      "epoch": 0.05024662113959478,
      "grad_norm": 7.160171031951904,
      "learning_rate": 4.937191944032777e-05,
      "loss": 3.7539,
      "step": 284900
    },
    {
      "epoch": 0.05026425772125136,
      "grad_norm": 6.502964019775391,
      "learning_rate": 4.937169898305707e-05,
      "loss": 3.8574,
      "step": 285000
    },
    {
      "epoch": 0.050281894302907935,
      "grad_norm": 7.360185623168945,
      "learning_rate": 4.937147852578636e-05,
      "loss": 3.7906,
      "step": 285100
    },
    {
      "epoch": 0.050299530884564514,
      "grad_norm": 8.096510887145996,
      "learning_rate": 4.937125806851565e-05,
      "loss": 3.8101,
      "step": 285200
    },
    {
      "epoch": 0.0503171674662211,
      "grad_norm": 7.8065924644470215,
      "learning_rate": 4.937103761124495e-05,
      "loss": 3.8311,
      "step": 285300
    },
    {
      "epoch": 0.05033480404787768,
      "grad_norm": 6.411307334899902,
      "learning_rate": 4.937081715397424e-05,
      "loss": 3.8347,
      "step": 285400
    },
    {
      "epoch": 0.050352440629534256,
      "grad_norm": 6.143601894378662,
      "learning_rate": 4.937059669670353e-05,
      "loss": 3.7623,
      "step": 285500
    },
    {
      "epoch": 0.050370077211190835,
      "grad_norm": 6.156732559204102,
      "learning_rate": 4.937037623943283e-05,
      "loss": 3.8221,
      "step": 285600
    },
    {
      "epoch": 0.05038771379284741,
      "grad_norm": 8.177315711975098,
      "learning_rate": 4.9370155782162116e-05,
      "loss": 3.6954,
      "step": 285700
    },
    {
      "epoch": 0.05040535037450399,
      "grad_norm": 7.724449634552002,
      "learning_rate": 4.936993532489141e-05,
      "loss": 3.7037,
      "step": 285800
    },
    {
      "epoch": 0.05042298695616057,
      "grad_norm": 5.010810852050781,
      "learning_rate": 4.93697148676207e-05,
      "loss": 3.7686,
      "step": 285900
    },
    {
      "epoch": 0.05044062353781715,
      "grad_norm": 7.867138385772705,
      "learning_rate": 4.936949441034999e-05,
      "loss": 3.757,
      "step": 286000
    },
    {
      "epoch": 0.050458260119473734,
      "grad_norm": 7.071381568908691,
      "learning_rate": 4.9369273953079285e-05,
      "loss": 3.7515,
      "step": 286100
    },
    {
      "epoch": 0.05047589670113031,
      "grad_norm": 6.015459060668945,
      "learning_rate": 4.936905349580858e-05,
      "loss": 3.8309,
      "step": 286200
    },
    {
      "epoch": 0.05049353328278689,
      "grad_norm": 5.514883995056152,
      "learning_rate": 4.936883303853787e-05,
      "loss": 3.7428,
      "step": 286300
    },
    {
      "epoch": 0.05051116986444347,
      "grad_norm": 5.948193550109863,
      "learning_rate": 4.9368612581267164e-05,
      "loss": 3.7751,
      "step": 286400
    },
    {
      "epoch": 0.05052880644610005,
      "grad_norm": 8.171100616455078,
      "learning_rate": 4.936839212399646e-05,
      "loss": 3.906,
      "step": 286500
    },
    {
      "epoch": 0.05054644302775663,
      "grad_norm": 5.761720180511475,
      "learning_rate": 4.9368171666725755e-05,
      "loss": 3.6814,
      "step": 286600
    },
    {
      "epoch": 0.050564079609413205,
      "grad_norm": 7.385166645050049,
      "learning_rate": 4.9367951209455044e-05,
      "loss": 3.9056,
      "step": 286700
    },
    {
      "epoch": 0.05058171619106979,
      "grad_norm": 6.220409870147705,
      "learning_rate": 4.936773075218434e-05,
      "loss": 3.7585,
      "step": 286800
    },
    {
      "epoch": 0.05059935277272637,
      "grad_norm": 6.1054863929748535,
      "learning_rate": 4.9367510294913635e-05,
      "loss": 3.7845,
      "step": 286900
    },
    {
      "epoch": 0.05061698935438295,
      "grad_norm": 8.247794151306152,
      "learning_rate": 4.9367289837642924e-05,
      "loss": 3.8782,
      "step": 287000
    },
    {
      "epoch": 0.050634625936039526,
      "grad_norm": 6.526100158691406,
      "learning_rate": 4.936706938037222e-05,
      "loss": 3.7943,
      "step": 287100
    },
    {
      "epoch": 0.050652262517696105,
      "grad_norm": 7.16778564453125,
      "learning_rate": 4.936684892310151e-05,
      "loss": 3.8025,
      "step": 287200
    },
    {
      "epoch": 0.05066989909935268,
      "grad_norm": 7.616987705230713,
      "learning_rate": 4.9366628465830797e-05,
      "loss": 3.7894,
      "step": 287300
    },
    {
      "epoch": 0.05068753568100926,
      "grad_norm": 7.551713943481445,
      "learning_rate": 4.936640800856009e-05,
      "loss": 3.8057,
      "step": 287400
    },
    {
      "epoch": 0.05070517226266584,
      "grad_norm": 8.472246170043945,
      "learning_rate": 4.936618755128939e-05,
      "loss": 3.7589,
      "step": 287500
    },
    {
      "epoch": 0.050722808844322426,
      "grad_norm": 7.055597305297852,
      "learning_rate": 4.9365967094018676e-05,
      "loss": 3.7793,
      "step": 287600
    },
    {
      "epoch": 0.050740445425979004,
      "grad_norm": 5.437378406524658,
      "learning_rate": 4.936574663674797e-05,
      "loss": 3.6985,
      "step": 287700
    },
    {
      "epoch": 0.05075808200763558,
      "grad_norm": 8.391921997070312,
      "learning_rate": 4.936552617947727e-05,
      "loss": 3.8202,
      "step": 287800
    },
    {
      "epoch": 0.05077571858929216,
      "grad_norm": 7.179731369018555,
      "learning_rate": 4.9365305722206556e-05,
      "loss": 3.8545,
      "step": 287900
    },
    {
      "epoch": 0.05079335517094874,
      "grad_norm": 6.754101753234863,
      "learning_rate": 4.936508526493585e-05,
      "loss": 3.7459,
      "step": 288000
    },
    {
      "epoch": 0.05081099175260532,
      "grad_norm": 7.293684959411621,
      "learning_rate": 4.936486480766515e-05,
      "loss": 3.7772,
      "step": 288100
    },
    {
      "epoch": 0.0508286283342619,
      "grad_norm": 6.130543231964111,
      "learning_rate": 4.9364644350394435e-05,
      "loss": 3.8724,
      "step": 288200
    },
    {
      "epoch": 0.05084626491591848,
      "grad_norm": 7.742160797119141,
      "learning_rate": 4.936442389312373e-05,
      "loss": 3.7698,
      "step": 288300
    },
    {
      "epoch": 0.05086390149757506,
      "grad_norm": 7.364014148712158,
      "learning_rate": 4.9364203435853026e-05,
      "loss": 3.7764,
      "step": 288400
    },
    {
      "epoch": 0.05088153807923164,
      "grad_norm": 5.897683143615723,
      "learning_rate": 4.9363982978582315e-05,
      "loss": 3.6877,
      "step": 288500
    },
    {
      "epoch": 0.05089917466088822,
      "grad_norm": 4.925845146179199,
      "learning_rate": 4.936376252131161e-05,
      "loss": 3.741,
      "step": 288600
    },
    {
      "epoch": 0.050916811242544796,
      "grad_norm": 6.5914177894592285,
      "learning_rate": 4.93635420640409e-05,
      "loss": 3.7873,
      "step": 288700
    },
    {
      "epoch": 0.050934447824201375,
      "grad_norm": 11.965259552001953,
      "learning_rate": 4.936332160677019e-05,
      "loss": 3.9151,
      "step": 288800
    },
    {
      "epoch": 0.05095208440585795,
      "grad_norm": 7.448995113372803,
      "learning_rate": 4.936310114949948e-05,
      "loss": 3.7197,
      "step": 288900
    },
    {
      "epoch": 0.05096972098751453,
      "grad_norm": 7.845843315124512,
      "learning_rate": 4.936288069222878e-05,
      "loss": 3.837,
      "step": 289000
    },
    {
      "epoch": 0.05098735756917112,
      "grad_norm": 6.396045207977295,
      "learning_rate": 4.936266023495807e-05,
      "loss": 3.8549,
      "step": 289100
    },
    {
      "epoch": 0.051004994150827695,
      "grad_norm": 11.75684642791748,
      "learning_rate": 4.936243977768736e-05,
      "loss": 3.7791,
      "step": 289200
    },
    {
      "epoch": 0.051022630732484274,
      "grad_norm": 5.684412479400635,
      "learning_rate": 4.936221932041666e-05,
      "loss": 3.8378,
      "step": 289300
    },
    {
      "epoch": 0.05104026731414085,
      "grad_norm": 7.279500484466553,
      "learning_rate": 4.936199886314595e-05,
      "loss": 3.8716,
      "step": 289400
    },
    {
      "epoch": 0.05105790389579743,
      "grad_norm": 7.17695426940918,
      "learning_rate": 4.936177840587524e-05,
      "loss": 3.89,
      "step": 289500
    },
    {
      "epoch": 0.05107554047745401,
      "grad_norm": 8.464468002319336,
      "learning_rate": 4.936155794860454e-05,
      "loss": 3.842,
      "step": 289600
    },
    {
      "epoch": 0.05109317705911059,
      "grad_norm": 9.900006294250488,
      "learning_rate": 4.936133749133383e-05,
      "loss": 3.8089,
      "step": 289700
    },
    {
      "epoch": 0.05111081364076717,
      "grad_norm": 7.000983715057373,
      "learning_rate": 4.936111703406312e-05,
      "loss": 3.8424,
      "step": 289800
    },
    {
      "epoch": 0.05112845022242375,
      "grad_norm": 6.02218770980835,
      "learning_rate": 4.936089657679242e-05,
      "loss": 3.861,
      "step": 289900
    },
    {
      "epoch": 0.05114608680408033,
      "grad_norm": 8.754170417785645,
      "learning_rate": 4.9360676119521706e-05,
      "loss": 3.8035,
      "step": 290000
    },
    {
      "epoch": 0.05116372338573691,
      "grad_norm": 5.839159965515137,
      "learning_rate": 4.9360455662250995e-05,
      "loss": 3.809,
      "step": 290100
    },
    {
      "epoch": 0.05118135996739349,
      "grad_norm": 7.522488594055176,
      "learning_rate": 4.936023520498029e-05,
      "loss": 3.8705,
      "step": 290200
    },
    {
      "epoch": 0.051198996549050066,
      "grad_norm": 10.86595344543457,
      "learning_rate": 4.936001474770958e-05,
      "loss": 3.8326,
      "step": 290300
    },
    {
      "epoch": 0.051216633130706644,
      "grad_norm": 5.991713523864746,
      "learning_rate": 4.9359794290438875e-05,
      "loss": 3.7765,
      "step": 290400
    },
    {
      "epoch": 0.05123426971236322,
      "grad_norm": 6.35781717300415,
      "learning_rate": 4.935957383316817e-05,
      "loss": 3.8113,
      "step": 290500
    },
    {
      "epoch": 0.05125190629401981,
      "grad_norm": 9.021300315856934,
      "learning_rate": 4.935935337589746e-05,
      "loss": 3.7588,
      "step": 290600
    },
    {
      "epoch": 0.05126954287567639,
      "grad_norm": 6.622096061706543,
      "learning_rate": 4.9359132918626754e-05,
      "loss": 3.7698,
      "step": 290700
    },
    {
      "epoch": 0.051287179457332965,
      "grad_norm": 5.447916507720947,
      "learning_rate": 4.935891246135605e-05,
      "loss": 3.9065,
      "step": 290800
    },
    {
      "epoch": 0.051304816038989544,
      "grad_norm": 6.4342241287231445,
      "learning_rate": 4.935869200408534e-05,
      "loss": 3.7627,
      "step": 290900
    },
    {
      "epoch": 0.05132245262064612,
      "grad_norm": 7.3346734046936035,
      "learning_rate": 4.9358471546814634e-05,
      "loss": 3.8144,
      "step": 291000
    },
    {
      "epoch": 0.0513400892023027,
      "grad_norm": 7.821092128753662,
      "learning_rate": 4.935825108954393e-05,
      "loss": 3.8707,
      "step": 291100
    },
    {
      "epoch": 0.05135772578395928,
      "grad_norm": 11.62153434753418,
      "learning_rate": 4.935803063227322e-05,
      "loss": 3.7535,
      "step": 291200
    },
    {
      "epoch": 0.051375362365615865,
      "grad_norm": 7.620581150054932,
      "learning_rate": 4.9357810175002514e-05,
      "loss": 3.7871,
      "step": 291300
    },
    {
      "epoch": 0.05139299894727244,
      "grad_norm": 8.348520278930664,
      "learning_rate": 4.935758971773181e-05,
      "loss": 3.7877,
      "step": 291400
    },
    {
      "epoch": 0.05141063552892902,
      "grad_norm": 7.876293659210205,
      "learning_rate": 4.93573692604611e-05,
      "loss": 3.9571,
      "step": 291500
    },
    {
      "epoch": 0.0514282721105856,
      "grad_norm": 6.8521199226379395,
      "learning_rate": 4.9357148803190386e-05,
      "loss": 3.8421,
      "step": 291600
    },
    {
      "epoch": 0.05144590869224218,
      "grad_norm": 6.7361860275268555,
      "learning_rate": 4.935692834591968e-05,
      "loss": 3.8312,
      "step": 291700
    },
    {
      "epoch": 0.05146354527389876,
      "grad_norm": 7.673239707946777,
      "learning_rate": 4.935670788864897e-05,
      "loss": 3.8008,
      "step": 291800
    },
    {
      "epoch": 0.051481181855555336,
      "grad_norm": 9.524487495422363,
      "learning_rate": 4.9356487431378266e-05,
      "loss": 3.9189,
      "step": 291900
    },
    {
      "epoch": 0.051498818437211914,
      "grad_norm": 6.165417194366455,
      "learning_rate": 4.935626697410756e-05,
      "loss": 3.8269,
      "step": 292000
    },
    {
      "epoch": 0.0515164550188685,
      "grad_norm": 11.323477745056152,
      "learning_rate": 4.935604651683685e-05,
      "loss": 3.8844,
      "step": 292100
    },
    {
      "epoch": 0.05153409160052508,
      "grad_norm": 6.853017330169678,
      "learning_rate": 4.9355826059566146e-05,
      "loss": 3.8053,
      "step": 292200
    },
    {
      "epoch": 0.05155172818218166,
      "grad_norm": 7.548285961151123,
      "learning_rate": 4.935560560229544e-05,
      "loss": 3.7106,
      "step": 292300
    },
    {
      "epoch": 0.051569364763838235,
      "grad_norm": 6.517601013183594,
      "learning_rate": 4.935538514502473e-05,
      "loss": 3.7004,
      "step": 292400
    },
    {
      "epoch": 0.051587001345494814,
      "grad_norm": 9.595296859741211,
      "learning_rate": 4.9355164687754025e-05,
      "loss": 3.8228,
      "step": 292500
    },
    {
      "epoch": 0.05160463792715139,
      "grad_norm": 7.543625354766846,
      "learning_rate": 4.935494423048332e-05,
      "loss": 3.9008,
      "step": 292600
    },
    {
      "epoch": 0.05162227450880797,
      "grad_norm": 6.427265167236328,
      "learning_rate": 4.935472377321261e-05,
      "loss": 3.8293,
      "step": 292700
    },
    {
      "epoch": 0.051639911090464556,
      "grad_norm": 6.193447113037109,
      "learning_rate": 4.9354503315941905e-05,
      "loss": 3.6821,
      "step": 292800
    },
    {
      "epoch": 0.051657547672121135,
      "grad_norm": 11.416682243347168,
      "learning_rate": 4.9354282858671194e-05,
      "loss": 3.7229,
      "step": 292900
    },
    {
      "epoch": 0.05167518425377771,
      "grad_norm": 8.193558692932129,
      "learning_rate": 4.935406240140049e-05,
      "loss": 3.7732,
      "step": 293000
    },
    {
      "epoch": 0.05169282083543429,
      "grad_norm": 5.490866184234619,
      "learning_rate": 4.935384194412978e-05,
      "loss": 3.8329,
      "step": 293100
    },
    {
      "epoch": 0.05171045741709087,
      "grad_norm": 6.196450233459473,
      "learning_rate": 4.935362148685907e-05,
      "loss": 3.8426,
      "step": 293200
    },
    {
      "epoch": 0.05172809399874745,
      "grad_norm": 4.906078815460205,
      "learning_rate": 4.935340102958836e-05,
      "loss": 3.7879,
      "step": 293300
    },
    {
      "epoch": 0.05174573058040403,
      "grad_norm": 5.572305202484131,
      "learning_rate": 4.935318057231766e-05,
      "loss": 3.7205,
      "step": 293400
    },
    {
      "epoch": 0.051763367162060606,
      "grad_norm": 8.86373519897461,
      "learning_rate": 4.935296011504695e-05,
      "loss": 3.9478,
      "step": 293500
    },
    {
      "epoch": 0.05178100374371719,
      "grad_norm": 7.915070056915283,
      "learning_rate": 4.935273965777624e-05,
      "loss": 3.8645,
      "step": 293600
    },
    {
      "epoch": 0.05179864032537377,
      "grad_norm": 9.284465789794922,
      "learning_rate": 4.935251920050554e-05,
      "loss": 3.6731,
      "step": 293700
    },
    {
      "epoch": 0.05181627690703035,
      "grad_norm": 7.170982837677002,
      "learning_rate": 4.935229874323483e-05,
      "loss": 3.7715,
      "step": 293800
    },
    {
      "epoch": 0.05183391348868693,
      "grad_norm": 9.894224166870117,
      "learning_rate": 4.935207828596412e-05,
      "loss": 3.7407,
      "step": 293900
    },
    {
      "epoch": 0.051851550070343505,
      "grad_norm": 7.838983058929443,
      "learning_rate": 4.935185782869342e-05,
      "loss": 3.8438,
      "step": 294000
    },
    {
      "epoch": 0.051869186652000084,
      "grad_norm": 5.694455623626709,
      "learning_rate": 4.935163737142271e-05,
      "loss": 3.9113,
      "step": 294100
    },
    {
      "epoch": 0.05188682323365666,
      "grad_norm": 6.966238021850586,
      "learning_rate": 4.9351416914152e-05,
      "loss": 3.7409,
      "step": 294200
    },
    {
      "epoch": 0.05190445981531325,
      "grad_norm": 6.850048542022705,
      "learning_rate": 4.9351196456881296e-05,
      "loss": 3.7736,
      "step": 294300
    },
    {
      "epoch": 0.051922096396969826,
      "grad_norm": 7.185439109802246,
      "learning_rate": 4.9350975999610585e-05,
      "loss": 3.7464,
      "step": 294400
    },
    {
      "epoch": 0.051939732978626404,
      "grad_norm": 6.6239848136901855,
      "learning_rate": 4.9350755542339874e-05,
      "loss": 3.8371,
      "step": 294500
    },
    {
      "epoch": 0.05195736956028298,
      "grad_norm": 7.658340930938721,
      "learning_rate": 4.935053508506917e-05,
      "loss": 3.8185,
      "step": 294600
    },
    {
      "epoch": 0.05197500614193956,
      "grad_norm": 6.87136697769165,
      "learning_rate": 4.9350314627798465e-05,
      "loss": 3.8802,
      "step": 294700
    },
    {
      "epoch": 0.05199264272359614,
      "grad_norm": 7.076325416564941,
      "learning_rate": 4.935009417052775e-05,
      "loss": 3.7107,
      "step": 294800
    },
    {
      "epoch": 0.05201027930525272,
      "grad_norm": 7.0442986488342285,
      "learning_rate": 4.934987371325705e-05,
      "loss": 3.7347,
      "step": 294900
    },
    {
      "epoch": 0.0520279158869093,
      "grad_norm": 7.712565898895264,
      "learning_rate": 4.9349653255986344e-05,
      "loss": 3.6904,
      "step": 295000
    },
    {
      "epoch": 0.05204555246856588,
      "grad_norm": 6.231156349182129,
      "learning_rate": 4.934943279871563e-05,
      "loss": 3.7726,
      "step": 295100
    },
    {
      "epoch": 0.05206318905022246,
      "grad_norm": 7.1331868171691895,
      "learning_rate": 4.934921234144493e-05,
      "loss": 3.7988,
      "step": 295200
    },
    {
      "epoch": 0.05208082563187904,
      "grad_norm": 7.006073951721191,
      "learning_rate": 4.9348991884174224e-05,
      "loss": 3.798,
      "step": 295300
    },
    {
      "epoch": 0.05209846221353562,
      "grad_norm": 7.054513931274414,
      "learning_rate": 4.934877142690351e-05,
      "loss": 3.7664,
      "step": 295400
    },
    {
      "epoch": 0.052116098795192196,
      "grad_norm": 9.475616455078125,
      "learning_rate": 4.934855096963281e-05,
      "loss": 3.6357,
      "step": 295500
    },
    {
      "epoch": 0.052133735376848775,
      "grad_norm": 6.756696701049805,
      "learning_rate": 4.9348330512362104e-05,
      "loss": 3.7763,
      "step": 295600
    },
    {
      "epoch": 0.05215137195850535,
      "grad_norm": 6.182986259460449,
      "learning_rate": 4.934811005509139e-05,
      "loss": 3.8057,
      "step": 295700
    },
    {
      "epoch": 0.05216900854016194,
      "grad_norm": 8.814291954040527,
      "learning_rate": 4.934788959782069e-05,
      "loss": 3.6804,
      "step": 295800
    },
    {
      "epoch": 0.05218664512181852,
      "grad_norm": 7.272101402282715,
      "learning_rate": 4.9347669140549976e-05,
      "loss": 3.7949,
      "step": 295900
    },
    {
      "epoch": 0.052204281703475096,
      "grad_norm": 10.011601448059082,
      "learning_rate": 4.9347448683279265e-05,
      "loss": 3.744,
      "step": 296000
    },
    {
      "epoch": 0.052221918285131674,
      "grad_norm": 7.934563636779785,
      "learning_rate": 4.934722822600856e-05,
      "loss": 3.8829,
      "step": 296100
    },
    {
      "epoch": 0.05223955486678825,
      "grad_norm": 5.803226947784424,
      "learning_rate": 4.9347007768737856e-05,
      "loss": 3.8972,
      "step": 296200
    },
    {
      "epoch": 0.05225719144844483,
      "grad_norm": 8.345248222351074,
      "learning_rate": 4.9346787311467145e-05,
      "loss": 3.7389,
      "step": 296300
    },
    {
      "epoch": 0.05227482803010141,
      "grad_norm": 6.888656139373779,
      "learning_rate": 4.934656685419644e-05,
      "loss": 3.8301,
      "step": 296400
    },
    {
      "epoch": 0.05229246461175799,
      "grad_norm": 7.618644714355469,
      "learning_rate": 4.9346346396925736e-05,
      "loss": 3.7436,
      "step": 296500
    },
    {
      "epoch": 0.052310101193414574,
      "grad_norm": 9.392992973327637,
      "learning_rate": 4.9346125939655024e-05,
      "loss": 3.8387,
      "step": 296600
    },
    {
      "epoch": 0.05232773777507115,
      "grad_norm": 8.258986473083496,
      "learning_rate": 4.934590548238432e-05,
      "loss": 3.794,
      "step": 296700
    },
    {
      "epoch": 0.05234537435672773,
      "grad_norm": 7.632602691650391,
      "learning_rate": 4.9345685025113615e-05,
      "loss": 3.838,
      "step": 296800
    },
    {
      "epoch": 0.05236301093838431,
      "grad_norm": 10.019379615783691,
      "learning_rate": 4.9345464567842904e-05,
      "loss": 3.7492,
      "step": 296900
    },
    {
      "epoch": 0.05238064752004089,
      "grad_norm": 7.1133599281311035,
      "learning_rate": 4.93452441105722e-05,
      "loss": 3.7737,
      "step": 297000
    },
    {
      "epoch": 0.052398284101697466,
      "grad_norm": 5.912423133850098,
      "learning_rate": 4.9345023653301495e-05,
      "loss": 3.773,
      "step": 297100
    },
    {
      "epoch": 0.052415920683354045,
      "grad_norm": 6.862151622772217,
      "learning_rate": 4.9344803196030784e-05,
      "loss": 3.7016,
      "step": 297200
    },
    {
      "epoch": 0.05243355726501063,
      "grad_norm": 7.442943096160889,
      "learning_rate": 4.934458273876007e-05,
      "loss": 3.7855,
      "step": 297300
    },
    {
      "epoch": 0.05245119384666721,
      "grad_norm": 8.126022338867188,
      "learning_rate": 4.934436228148937e-05,
      "loss": 3.7749,
      "step": 297400
    },
    {
      "epoch": 0.05246883042832379,
      "grad_norm": 8.319220542907715,
      "learning_rate": 4.934414182421866e-05,
      "loss": 3.7298,
      "step": 297500
    },
    {
      "epoch": 0.052486467009980366,
      "grad_norm": 7.3846306800842285,
      "learning_rate": 4.934392136694795e-05,
      "loss": 3.7963,
      "step": 297600
    },
    {
      "epoch": 0.052504103591636944,
      "grad_norm": 5.4740309715271,
      "learning_rate": 4.934370090967725e-05,
      "loss": 3.6793,
      "step": 297700
    },
    {
      "epoch": 0.05252174017329352,
      "grad_norm": 6.699826717376709,
      "learning_rate": 4.934348045240654e-05,
      "loss": 3.7844,
      "step": 297800
    },
    {
      "epoch": 0.0525393767549501,
      "grad_norm": 6.2008490562438965,
      "learning_rate": 4.934325999513583e-05,
      "loss": 3.7043,
      "step": 297900
    },
    {
      "epoch": 0.05255701333660668,
      "grad_norm": 10.960675239562988,
      "learning_rate": 4.934303953786513e-05,
      "loss": 3.6753,
      "step": 298000
    },
    {
      "epoch": 0.052574649918263265,
      "grad_norm": 6.904557228088379,
      "learning_rate": 4.934281908059442e-05,
      "loss": 3.805,
      "step": 298100
    },
    {
      "epoch": 0.052592286499919844,
      "grad_norm": 6.952970504760742,
      "learning_rate": 4.934259862332371e-05,
      "loss": 3.7442,
      "step": 298200
    },
    {
      "epoch": 0.05260992308157642,
      "grad_norm": 12.986239433288574,
      "learning_rate": 4.934237816605301e-05,
      "loss": 3.7394,
      "step": 298300
    },
    {
      "epoch": 0.052627559663233,
      "grad_norm": 6.744261264801025,
      "learning_rate": 4.93421577087823e-05,
      "loss": 3.8087,
      "step": 298400
    },
    {
      "epoch": 0.05264519624488958,
      "grad_norm": 5.996745586395264,
      "learning_rate": 4.934193725151159e-05,
      "loss": 3.7995,
      "step": 298500
    },
    {
      "epoch": 0.05266283282654616,
      "grad_norm": 6.119041442871094,
      "learning_rate": 4.9341716794240886e-05,
      "loss": 3.774,
      "step": 298600
    },
    {
      "epoch": 0.052680469408202736,
      "grad_norm": 8.676469802856445,
      "learning_rate": 4.9341496336970175e-05,
      "loss": 3.8045,
      "step": 298700
    },
    {
      "epoch": 0.05269810598985932,
      "grad_norm": 8.409029006958008,
      "learning_rate": 4.9341275879699464e-05,
      "loss": 3.7359,
      "step": 298800
    },
    {
      "epoch": 0.0527157425715159,
      "grad_norm": 6.098300457000732,
      "learning_rate": 4.934105542242876e-05,
      "loss": 3.767,
      "step": 298900
    },
    {
      "epoch": 0.05273337915317248,
      "grad_norm": 9.82558822631836,
      "learning_rate": 4.9340834965158055e-05,
      "loss": 3.6769,
      "step": 299000
    },
    {
      "epoch": 0.05275101573482906,
      "grad_norm": 7.558035373687744,
      "learning_rate": 4.934061450788734e-05,
      "loss": 3.801,
      "step": 299100
    },
    {
      "epoch": 0.052768652316485636,
      "grad_norm": 6.3772382736206055,
      "learning_rate": 4.934039405061664e-05,
      "loss": 3.8725,
      "step": 299200
    },
    {
      "epoch": 0.052786288898142214,
      "grad_norm": 5.714488983154297,
      "learning_rate": 4.9340173593345934e-05,
      "loss": 3.7939,
      "step": 299300
    },
    {
      "epoch": 0.05280392547979879,
      "grad_norm": 5.941741466522217,
      "learning_rate": 4.933995313607522e-05,
      "loss": 3.7851,
      "step": 299400
    },
    {
      "epoch": 0.05282156206145537,
      "grad_norm": 6.839719295501709,
      "learning_rate": 4.933973267880452e-05,
      "loss": 3.829,
      "step": 299500
    },
    {
      "epoch": 0.052839198643111956,
      "grad_norm": 8.966604232788086,
      "learning_rate": 4.9339512221533814e-05,
      "loss": 3.7883,
      "step": 299600
    },
    {
      "epoch": 0.052856835224768535,
      "grad_norm": 7.085408687591553,
      "learning_rate": 4.93392917642631e-05,
      "loss": 3.7792,
      "step": 299700
    },
    {
      "epoch": 0.05287447180642511,
      "grad_norm": 6.545825481414795,
      "learning_rate": 4.93390713069924e-05,
      "loss": 3.7889,
      "step": 299800
    },
    {
      "epoch": 0.05289210838808169,
      "grad_norm": 8.73269271850586,
      "learning_rate": 4.9338850849721694e-05,
      "loss": 3.819,
      "step": 299900
    },
    {
      "epoch": 0.05290974496973827,
      "grad_norm": 8.940168380737305,
      "learning_rate": 4.933863039245098e-05,
      "loss": 3.7884,
      "step": 300000
    },
    {
      "epoch": 0.05292738155139485,
      "grad_norm": 9.52955150604248,
      "learning_rate": 4.933840993518027e-05,
      "loss": 3.8036,
      "step": 300100
    },
    {
      "epoch": 0.05294501813305143,
      "grad_norm": 9.222635269165039,
      "learning_rate": 4.9338189477909566e-05,
      "loss": 3.8093,
      "step": 300200
    },
    {
      "epoch": 0.05296265471470801,
      "grad_norm": 5.889760971069336,
      "learning_rate": 4.9337969020638855e-05,
      "loss": 3.696,
      "step": 300300
    },
    {
      "epoch": 0.05298029129636459,
      "grad_norm": 6.820961952209473,
      "learning_rate": 4.933774856336815e-05,
      "loss": 3.8031,
      "step": 300400
    },
    {
      "epoch": 0.05299792787802117,
      "grad_norm": 12.656932830810547,
      "learning_rate": 4.9337528106097446e-05,
      "loss": 3.8178,
      "step": 300500
    },
    {
      "epoch": 0.05301556445967775,
      "grad_norm": 5.558810710906982,
      "learning_rate": 4.9337307648826735e-05,
      "loss": 3.8515,
      "step": 300600
    },
    {
      "epoch": 0.05303320104133433,
      "grad_norm": 5.607730388641357,
      "learning_rate": 4.933708719155603e-05,
      "loss": 3.7538,
      "step": 300700
    },
    {
      "epoch": 0.053050837622990905,
      "grad_norm": 5.76077127456665,
      "learning_rate": 4.9336866734285326e-05,
      "loss": 3.8127,
      "step": 300800
    },
    {
      "epoch": 0.053068474204647484,
      "grad_norm": 7.762339115142822,
      "learning_rate": 4.9336646277014614e-05,
      "loss": 3.6599,
      "step": 300900
    },
    {
      "epoch": 0.05308611078630406,
      "grad_norm": 6.630784511566162,
      "learning_rate": 4.933642581974391e-05,
      "loss": 3.9281,
      "step": 301000
    },
    {
      "epoch": 0.05310374736796065,
      "grad_norm": 4.785046100616455,
      "learning_rate": 4.9336205362473205e-05,
      "loss": 3.6985,
      "step": 301100
    },
    {
      "epoch": 0.053121383949617226,
      "grad_norm": 6.615071773529053,
      "learning_rate": 4.9335984905202494e-05,
      "loss": 3.7626,
      "step": 301200
    },
    {
      "epoch": 0.053139020531273805,
      "grad_norm": 7.545271873474121,
      "learning_rate": 4.933576444793179e-05,
      "loss": 3.8605,
      "step": 301300
    },
    {
      "epoch": 0.05315665711293038,
      "grad_norm": 8.313116073608398,
      "learning_rate": 4.9335543990661085e-05,
      "loss": 3.799,
      "step": 301400
    },
    {
      "epoch": 0.05317429369458696,
      "grad_norm": 12.767817497253418,
      "learning_rate": 4.9335323533390374e-05,
      "loss": 3.6276,
      "step": 301500
    },
    {
      "epoch": 0.05319193027624354,
      "grad_norm": 4.844892501831055,
      "learning_rate": 4.933510307611966e-05,
      "loss": 3.7777,
      "step": 301600
    },
    {
      "epoch": 0.05320956685790012,
      "grad_norm": 7.0700483322143555,
      "learning_rate": 4.933488261884896e-05,
      "loss": 3.7235,
      "step": 301700
    },
    {
      "epoch": 0.053227203439556704,
      "grad_norm": 8.804636001586914,
      "learning_rate": 4.9334662161578246e-05,
      "loss": 3.8369,
      "step": 301800
    },
    {
      "epoch": 0.05324484002121328,
      "grad_norm": 7.428201675415039,
      "learning_rate": 4.933444170430754e-05,
      "loss": 3.8348,
      "step": 301900
    },
    {
      "epoch": 0.05326247660286986,
      "grad_norm": 6.711430072784424,
      "learning_rate": 4.933422124703684e-05,
      "loss": 3.7972,
      "step": 302000
    },
    {
      "epoch": 0.05328011318452644,
      "grad_norm": 7.899375915527344,
      "learning_rate": 4.9334000789766126e-05,
      "loss": 3.8268,
      "step": 302100
    },
    {
      "epoch": 0.05329774976618302,
      "grad_norm": 7.549953937530518,
      "learning_rate": 4.933378033249542e-05,
      "loss": 3.6952,
      "step": 302200
    },
    {
      "epoch": 0.0533153863478396,
      "grad_norm": 8.110517501831055,
      "learning_rate": 4.933355987522472e-05,
      "loss": 3.7229,
      "step": 302300
    },
    {
      "epoch": 0.053333022929496175,
      "grad_norm": 7.447738170623779,
      "learning_rate": 4.9333339417954006e-05,
      "loss": 3.7232,
      "step": 302400
    },
    {
      "epoch": 0.053350659511152754,
      "grad_norm": 9.725265502929688,
      "learning_rate": 4.93331189606833e-05,
      "loss": 3.8175,
      "step": 302500
    },
    {
      "epoch": 0.05336829609280934,
      "grad_norm": 10.651984214782715,
      "learning_rate": 4.93328985034126e-05,
      "loss": 3.7706,
      "step": 302600
    },
    {
      "epoch": 0.05338593267446592,
      "grad_norm": 6.524327278137207,
      "learning_rate": 4.9332678046141885e-05,
      "loss": 3.7944,
      "step": 302700
    },
    {
      "epoch": 0.053403569256122496,
      "grad_norm": 5.780178546905518,
      "learning_rate": 4.933245758887118e-05,
      "loss": 3.7199,
      "step": 302800
    },
    {
      "epoch": 0.053421205837779075,
      "grad_norm": 8.04891300201416,
      "learning_rate": 4.933223713160047e-05,
      "loss": 3.8763,
      "step": 302900
    },
    {
      "epoch": 0.05343884241943565,
      "grad_norm": 6.446044445037842,
      "learning_rate": 4.9332016674329765e-05,
      "loss": 3.7927,
      "step": 303000
    },
    {
      "epoch": 0.05345647900109223,
      "grad_norm": 6.542093753814697,
      "learning_rate": 4.9331796217059054e-05,
      "loss": 3.628,
      "step": 303100
    },
    {
      "epoch": 0.05347411558274881,
      "grad_norm": 6.212488651275635,
      "learning_rate": 4.933157575978835e-05,
      "loss": 3.7953,
      "step": 303200
    },
    {
      "epoch": 0.053491752164405396,
      "grad_norm": 8.560837745666504,
      "learning_rate": 4.933135530251764e-05,
      "loss": 3.77,
      "step": 303300
    },
    {
      "epoch": 0.053509388746061974,
      "grad_norm": 6.639617443084717,
      "learning_rate": 4.933113484524693e-05,
      "loss": 3.7874,
      "step": 303400
    },
    {
      "epoch": 0.05352702532771855,
      "grad_norm": 8.976969718933105,
      "learning_rate": 4.933091438797623e-05,
      "loss": 3.717,
      "step": 303500
    },
    {
      "epoch": 0.05354466190937513,
      "grad_norm": 6.442269802093506,
      "learning_rate": 4.933069393070552e-05,
      "loss": 3.6759,
      "step": 303600
    },
    {
      "epoch": 0.05356229849103171,
      "grad_norm": 7.066695690155029,
      "learning_rate": 4.933047347343481e-05,
      "loss": 3.8242,
      "step": 303700
    },
    {
      "epoch": 0.05357993507268829,
      "grad_norm": 8.870697975158691,
      "learning_rate": 4.933025301616411e-05,
      "loss": 3.7059,
      "step": 303800
    },
    {
      "epoch": 0.05359757165434487,
      "grad_norm": 7.977606773376465,
      "learning_rate": 4.93300325588934e-05,
      "loss": 3.8071,
      "step": 303900
    },
    {
      "epoch": 0.053615208236001445,
      "grad_norm": 5.456815242767334,
      "learning_rate": 4.932981210162269e-05,
      "loss": 3.7747,
      "step": 304000
    },
    {
      "epoch": 0.05363284481765803,
      "grad_norm": 5.492070198059082,
      "learning_rate": 4.932959164435199e-05,
      "loss": 3.8083,
      "step": 304100
    },
    {
      "epoch": 0.05365048139931461,
      "grad_norm": 5.117137908935547,
      "learning_rate": 4.932937118708128e-05,
      "loss": 3.7507,
      "step": 304200
    },
    {
      "epoch": 0.05366811798097119,
      "grad_norm": 9.574591636657715,
      "learning_rate": 4.932915072981057e-05,
      "loss": 3.7638,
      "step": 304300
    },
    {
      "epoch": 0.053685754562627766,
      "grad_norm": 6.092116832733154,
      "learning_rate": 4.932893027253986e-05,
      "loss": 3.8923,
      "step": 304400
    },
    {
      "epoch": 0.053703391144284345,
      "grad_norm": 9.37044906616211,
      "learning_rate": 4.932870981526915e-05,
      "loss": 3.8588,
      "step": 304500
    },
    {
      "epoch": 0.05372102772594092,
      "grad_norm": 7.409124851226807,
      "learning_rate": 4.9328489357998445e-05,
      "loss": 3.6759,
      "step": 304600
    },
    {
      "epoch": 0.0537386643075975,
      "grad_norm": 7.322339057922363,
      "learning_rate": 4.932826890072774e-05,
      "loss": 3.7383,
      "step": 304700
    },
    {
      "epoch": 0.05375630088925409,
      "grad_norm": 10.639183044433594,
      "learning_rate": 4.932804844345703e-05,
      "loss": 3.802,
      "step": 304800
    },
    {
      "epoch": 0.053773937470910665,
      "grad_norm": 7.891964912414551,
      "learning_rate": 4.9327827986186325e-05,
      "loss": 3.7587,
      "step": 304900
    },
    {
      "epoch": 0.053791574052567244,
      "grad_norm": 7.29105281829834,
      "learning_rate": 4.932760752891562e-05,
      "loss": 3.9829,
      "step": 305000
    },
    {
      "epoch": 0.05380921063422382,
      "grad_norm": 9.360034942626953,
      "learning_rate": 4.932738707164491e-05,
      "loss": 3.7779,
      "step": 305100
    },
    {
      "epoch": 0.0538268472158804,
      "grad_norm": 7.0604376792907715,
      "learning_rate": 4.9327166614374204e-05,
      "loss": 3.726,
      "step": 305200
    },
    {
      "epoch": 0.05384448379753698,
      "grad_norm": 6.071659564971924,
      "learning_rate": 4.93269461571035e-05,
      "loss": 3.79,
      "step": 305300
    },
    {
      "epoch": 0.05386212037919356,
      "grad_norm": 6.336104393005371,
      "learning_rate": 4.932672569983279e-05,
      "loss": 3.7246,
      "step": 305400
    },
    {
      "epoch": 0.053879756960850136,
      "grad_norm": 5.705779075622559,
      "learning_rate": 4.9326505242562084e-05,
      "loss": 3.7734,
      "step": 305500
    },
    {
      "epoch": 0.05389739354250672,
      "grad_norm": 6.985772609710693,
      "learning_rate": 4.932628478529138e-05,
      "loss": 3.7784,
      "step": 305600
    },
    {
      "epoch": 0.0539150301241633,
      "grad_norm": 8.978555679321289,
      "learning_rate": 4.932606432802067e-05,
      "loss": 3.6105,
      "step": 305700
    },
    {
      "epoch": 0.05393266670581988,
      "grad_norm": 7.947198867797852,
      "learning_rate": 4.9325843870749964e-05,
      "loss": 3.7584,
      "step": 305800
    },
    {
      "epoch": 0.05395030328747646,
      "grad_norm": 5.552781581878662,
      "learning_rate": 4.932562341347925e-05,
      "loss": 3.8144,
      "step": 305900
    },
    {
      "epoch": 0.053967939869133036,
      "grad_norm": 8.249349594116211,
      "learning_rate": 4.932540295620854e-05,
      "loss": 3.8381,
      "step": 306000
    },
    {
      "epoch": 0.053985576450789614,
      "grad_norm": 6.409824371337891,
      "learning_rate": 4.9325182498937836e-05,
      "loss": 3.8079,
      "step": 306100
    },
    {
      "epoch": 0.05400321303244619,
      "grad_norm": 8.665823936462402,
      "learning_rate": 4.932496204166713e-05,
      "loss": 3.7684,
      "step": 306200
    },
    {
      "epoch": 0.05402084961410278,
      "grad_norm": 10.296154022216797,
      "learning_rate": 4.932474158439642e-05,
      "loss": 3.7774,
      "step": 306300
    },
    {
      "epoch": 0.05403848619575936,
      "grad_norm": 9.4345064163208,
      "learning_rate": 4.9324521127125716e-05,
      "loss": 3.8211,
      "step": 306400
    },
    {
      "epoch": 0.054056122777415935,
      "grad_norm": 6.828439235687256,
      "learning_rate": 4.932430066985501e-05,
      "loss": 3.7762,
      "step": 306500
    },
    {
      "epoch": 0.054073759359072514,
      "grad_norm": 7.5154709815979,
      "learning_rate": 4.93240802125843e-05,
      "loss": 3.7605,
      "step": 306600
    },
    {
      "epoch": 0.05409139594072909,
      "grad_norm": 5.815271377563477,
      "learning_rate": 4.9323859755313596e-05,
      "loss": 3.7977,
      "step": 306700
    },
    {
      "epoch": 0.05410903252238567,
      "grad_norm": 9.570856094360352,
      "learning_rate": 4.932363929804289e-05,
      "loss": 3.8719,
      "step": 306800
    },
    {
      "epoch": 0.05412666910404225,
      "grad_norm": 6.7365851402282715,
      "learning_rate": 4.932341884077218e-05,
      "loss": 3.7703,
      "step": 306900
    },
    {
      "epoch": 0.05414430568569883,
      "grad_norm": 7.1954121589660645,
      "learning_rate": 4.9323198383501475e-05,
      "loss": 3.8275,
      "step": 307000
    },
    {
      "epoch": 0.05416194226735541,
      "grad_norm": 8.519620895385742,
      "learning_rate": 4.932297792623077e-05,
      "loss": 3.7757,
      "step": 307100
    },
    {
      "epoch": 0.05417957884901199,
      "grad_norm": 6.347900867462158,
      "learning_rate": 4.932275746896006e-05,
      "loss": 3.716,
      "step": 307200
    },
    {
      "epoch": 0.05419721543066857,
      "grad_norm": 8.715060234069824,
      "learning_rate": 4.932253701168935e-05,
      "loss": 3.718,
      "step": 307300
    },
    {
      "epoch": 0.05421485201232515,
      "grad_norm": 7.176887035369873,
      "learning_rate": 4.9322316554418644e-05,
      "loss": 3.8232,
      "step": 307400
    },
    {
      "epoch": 0.05423248859398173,
      "grad_norm": 6.372053146362305,
      "learning_rate": 4.932209609714793e-05,
      "loss": 3.7993,
      "step": 307500
    },
    {
      "epoch": 0.054250125175638306,
      "grad_norm": 7.175247669219971,
      "learning_rate": 4.932187563987723e-05,
      "loss": 3.7449,
      "step": 307600
    },
    {
      "epoch": 0.054267761757294884,
      "grad_norm": 8.752551078796387,
      "learning_rate": 4.932165518260652e-05,
      "loss": 3.7226,
      "step": 307700
    },
    {
      "epoch": 0.05428539833895147,
      "grad_norm": 9.161717414855957,
      "learning_rate": 4.932143472533581e-05,
      "loss": 3.6913,
      "step": 307800
    },
    {
      "epoch": 0.05430303492060805,
      "grad_norm": 7.238637924194336,
      "learning_rate": 4.932121426806511e-05,
      "loss": 3.7653,
      "step": 307900
    },
    {
      "epoch": 0.05432067150226463,
      "grad_norm": 7.2866740226745605,
      "learning_rate": 4.93209938107944e-05,
      "loss": 3.7477,
      "step": 308000
    },
    {
      "epoch": 0.054338308083921205,
      "grad_norm": 6.608392238616943,
      "learning_rate": 4.93207733535237e-05,
      "loss": 3.7713,
      "step": 308100
    },
    {
      "epoch": 0.054355944665577784,
      "grad_norm": 8.013254165649414,
      "learning_rate": 4.932055289625299e-05,
      "loss": 3.8048,
      "step": 308200
    },
    {
      "epoch": 0.05437358124723436,
      "grad_norm": 9.509692192077637,
      "learning_rate": 4.932033243898228e-05,
      "loss": 3.67,
      "step": 308300
    },
    {
      "epoch": 0.05439121782889094,
      "grad_norm": 8.439270973205566,
      "learning_rate": 4.932011198171158e-05,
      "loss": 3.7108,
      "step": 308400
    },
    {
      "epoch": 0.05440885441054752,
      "grad_norm": 8.841569900512695,
      "learning_rate": 4.931989152444087e-05,
      "loss": 3.8309,
      "step": 308500
    },
    {
      "epoch": 0.054426490992204105,
      "grad_norm": 9.409120559692383,
      "learning_rate": 4.931967106717016e-05,
      "loss": 3.7737,
      "step": 308600
    },
    {
      "epoch": 0.05444412757386068,
      "grad_norm": 7.202775955200195,
      "learning_rate": 4.931945060989945e-05,
      "loss": 3.7334,
      "step": 308700
    },
    {
      "epoch": 0.05446176415551726,
      "grad_norm": 6.526743412017822,
      "learning_rate": 4.931923015262874e-05,
      "loss": 3.679,
      "step": 308800
    },
    {
      "epoch": 0.05447940073717384,
      "grad_norm": 5.7335052490234375,
      "learning_rate": 4.9319009695358035e-05,
      "loss": 3.7545,
      "step": 308900
    },
    {
      "epoch": 0.05449703731883042,
      "grad_norm": 13.886405944824219,
      "learning_rate": 4.931878923808733e-05,
      "loss": 3.7653,
      "step": 309000
    },
    {
      "epoch": 0.054514673900487,
      "grad_norm": 6.208724021911621,
      "learning_rate": 4.931856878081662e-05,
      "loss": 3.7391,
      "step": 309100
    },
    {
      "epoch": 0.054532310482143576,
      "grad_norm": 5.909900188446045,
      "learning_rate": 4.9318348323545915e-05,
      "loss": 3.7402,
      "step": 309200
    },
    {
      "epoch": 0.05454994706380016,
      "grad_norm": 5.036021709442139,
      "learning_rate": 4.931812786627521e-05,
      "loss": 3.8178,
      "step": 309300
    },
    {
      "epoch": 0.05456758364545674,
      "grad_norm": 7.11853551864624,
      "learning_rate": 4.93179074090045e-05,
      "loss": 3.6807,
      "step": 309400
    },
    {
      "epoch": 0.05458522022711332,
      "grad_norm": 7.116866111755371,
      "learning_rate": 4.9317686951733794e-05,
      "loss": 3.7476,
      "step": 309500
    },
    {
      "epoch": 0.054602856808769897,
      "grad_norm": 6.604700565338135,
      "learning_rate": 4.931746649446309e-05,
      "loss": 3.8076,
      "step": 309600
    },
    {
      "epoch": 0.054620493390426475,
      "grad_norm": 6.709282398223877,
      "learning_rate": 4.931724603719238e-05,
      "loss": 3.6976,
      "step": 309700
    },
    {
      "epoch": 0.054638129972083053,
      "grad_norm": 5.931800842285156,
      "learning_rate": 4.9317025579921674e-05,
      "loss": 3.8341,
      "step": 309800
    },
    {
      "epoch": 0.05465576655373963,
      "grad_norm": 8.801111221313477,
      "learning_rate": 4.931680512265097e-05,
      "loss": 3.7554,
      "step": 309900
    },
    {
      "epoch": 0.05467340313539621,
      "grad_norm": 5.803996562957764,
      "learning_rate": 4.931658466538026e-05,
      "loss": 3.8328,
      "step": 310000
    },
    {
      "epoch": 0.054691039717052796,
      "grad_norm": 8.0256986618042,
      "learning_rate": 4.931636420810955e-05,
      "loss": 3.7433,
      "step": 310100
    },
    {
      "epoch": 0.054708676298709374,
      "grad_norm": 7.645759582519531,
      "learning_rate": 4.931614375083884e-05,
      "loss": 3.6885,
      "step": 310200
    },
    {
      "epoch": 0.05472631288036595,
      "grad_norm": 8.407390594482422,
      "learning_rate": 4.931592329356813e-05,
      "loss": 3.6889,
      "step": 310300
    },
    {
      "epoch": 0.05474394946202253,
      "grad_norm": 7.246765613555908,
      "learning_rate": 4.9315702836297426e-05,
      "loss": 3.8399,
      "step": 310400
    },
    {
      "epoch": 0.05476158604367911,
      "grad_norm": 11.335081100463867,
      "learning_rate": 4.931548237902672e-05,
      "loss": 3.7479,
      "step": 310500
    },
    {
      "epoch": 0.05477922262533569,
      "grad_norm": 7.522907257080078,
      "learning_rate": 4.931526192175601e-05,
      "loss": 3.7463,
      "step": 310600
    },
    {
      "epoch": 0.05479685920699227,
      "grad_norm": 5.795997619628906,
      "learning_rate": 4.9315041464485306e-05,
      "loss": 3.8046,
      "step": 310700
    },
    {
      "epoch": 0.05481449578864885,
      "grad_norm": 6.741024494171143,
      "learning_rate": 4.93148210072146e-05,
      "loss": 3.8209,
      "step": 310800
    },
    {
      "epoch": 0.05483213237030543,
      "grad_norm": 7.90355110168457,
      "learning_rate": 4.931460054994389e-05,
      "loss": 3.7061,
      "step": 310900
    },
    {
      "epoch": 0.05484976895196201,
      "grad_norm": 6.053273677825928,
      "learning_rate": 4.9314380092673186e-05,
      "loss": 3.6908,
      "step": 311000
    },
    {
      "epoch": 0.05486740553361859,
      "grad_norm": 7.325275897979736,
      "learning_rate": 4.931415963540248e-05,
      "loss": 3.6957,
      "step": 311100
    },
    {
      "epoch": 0.054885042115275166,
      "grad_norm": 6.171573162078857,
      "learning_rate": 4.931393917813177e-05,
      "loss": 3.8475,
      "step": 311200
    },
    {
      "epoch": 0.054902678696931745,
      "grad_norm": 9.384042739868164,
      "learning_rate": 4.9313718720861065e-05,
      "loss": 3.7924,
      "step": 311300
    },
    {
      "epoch": 0.05492031527858832,
      "grad_norm": 5.485127925872803,
      "learning_rate": 4.931349826359036e-05,
      "loss": 3.8572,
      "step": 311400
    },
    {
      "epoch": 0.0549379518602449,
      "grad_norm": 6.156095504760742,
      "learning_rate": 4.931327780631965e-05,
      "loss": 3.7489,
      "step": 311500
    },
    {
      "epoch": 0.05495558844190149,
      "grad_norm": 6.325335502624512,
      "learning_rate": 4.931305734904894e-05,
      "loss": 3.7814,
      "step": 311600
    },
    {
      "epoch": 0.054973225023558066,
      "grad_norm": 6.650742053985596,
      "learning_rate": 4.9312836891778234e-05,
      "loss": 3.7226,
      "step": 311700
    },
    {
      "epoch": 0.054990861605214644,
      "grad_norm": 7.758073806762695,
      "learning_rate": 4.931261643450752e-05,
      "loss": 3.8461,
      "step": 311800
    },
    {
      "epoch": 0.05500849818687122,
      "grad_norm": 12.812666893005371,
      "learning_rate": 4.931239597723682e-05,
      "loss": 3.8705,
      "step": 311900
    },
    {
      "epoch": 0.0550261347685278,
      "grad_norm": 8.467926979064941,
      "learning_rate": 4.931217551996611e-05,
      "loss": 3.8542,
      "step": 312000
    },
    {
      "epoch": 0.05504377135018438,
      "grad_norm": 9.968737602233887,
      "learning_rate": 4.93119550626954e-05,
      "loss": 3.8009,
      "step": 312100
    },
    {
      "epoch": 0.05506140793184096,
      "grad_norm": 10.55451488494873,
      "learning_rate": 4.93117346054247e-05,
      "loss": 3.8165,
      "step": 312200
    },
    {
      "epoch": 0.055079044513497544,
      "grad_norm": 8.056674003601074,
      "learning_rate": 4.931151414815399e-05,
      "loss": 3.8992,
      "step": 312300
    },
    {
      "epoch": 0.05509668109515412,
      "grad_norm": 7.638803005218506,
      "learning_rate": 4.931129369088328e-05,
      "loss": 3.7919,
      "step": 312400
    },
    {
      "epoch": 0.0551143176768107,
      "grad_norm": 12.24791145324707,
      "learning_rate": 4.931107323361258e-05,
      "loss": 3.691,
      "step": 312500
    },
    {
      "epoch": 0.05513195425846728,
      "grad_norm": 6.791423797607422,
      "learning_rate": 4.931085277634187e-05,
      "loss": 3.8519,
      "step": 312600
    },
    {
      "epoch": 0.05514959084012386,
      "grad_norm": 7.546932697296143,
      "learning_rate": 4.931063231907116e-05,
      "loss": 3.6744,
      "step": 312700
    },
    {
      "epoch": 0.055167227421780436,
      "grad_norm": 9.285024642944336,
      "learning_rate": 4.9310411861800457e-05,
      "loss": 3.8588,
      "step": 312800
    },
    {
      "epoch": 0.055184864003437015,
      "grad_norm": 10.021020889282227,
      "learning_rate": 4.9310191404529745e-05,
      "loss": 3.8537,
      "step": 312900
    },
    {
      "epoch": 0.05520250058509359,
      "grad_norm": 7.157227993011475,
      "learning_rate": 4.930997094725904e-05,
      "loss": 3.8105,
      "step": 313000
    },
    {
      "epoch": 0.05522013716675018,
      "grad_norm": 6.142873764038086,
      "learning_rate": 4.930975048998833e-05,
      "loss": 3.7422,
      "step": 313100
    },
    {
      "epoch": 0.05523777374840676,
      "grad_norm": 6.92900276184082,
      "learning_rate": 4.9309530032717625e-05,
      "loss": 3.8197,
      "step": 313200
    },
    {
      "epoch": 0.055255410330063336,
      "grad_norm": 9.6627836227417,
      "learning_rate": 4.9309309575446914e-05,
      "loss": 3.7749,
      "step": 313300
    },
    {
      "epoch": 0.055273046911719914,
      "grad_norm": 5.442070007324219,
      "learning_rate": 4.930908911817621e-05,
      "loss": 3.7862,
      "step": 313400
    },
    {
      "epoch": 0.05529068349337649,
      "grad_norm": 5.826889514923096,
      "learning_rate": 4.9308868660905505e-05,
      "loss": 3.7981,
      "step": 313500
    },
    {
      "epoch": 0.05530832007503307,
      "grad_norm": 5.870038032531738,
      "learning_rate": 4.930864820363479e-05,
      "loss": 3.8008,
      "step": 313600
    },
    {
      "epoch": 0.05532595665668965,
      "grad_norm": 8.403902053833008,
      "learning_rate": 4.930842774636409e-05,
      "loss": 3.7991,
      "step": 313700
    },
    {
      "epoch": 0.055343593238346235,
      "grad_norm": 7.166866302490234,
      "learning_rate": 4.9308207289093384e-05,
      "loss": 3.712,
      "step": 313800
    },
    {
      "epoch": 0.055361229820002814,
      "grad_norm": 11.542618751525879,
      "learning_rate": 4.930798683182267e-05,
      "loss": 3.8335,
      "step": 313900
    },
    {
      "epoch": 0.05537886640165939,
      "grad_norm": 7.173199653625488,
      "learning_rate": 4.930776637455197e-05,
      "loss": 3.8018,
      "step": 314000
    },
    {
      "epoch": 0.05539650298331597,
      "grad_norm": 9.53253173828125,
      "learning_rate": 4.9307545917281264e-05,
      "loss": 3.7799,
      "step": 314100
    },
    {
      "epoch": 0.05541413956497255,
      "grad_norm": 5.911957263946533,
      "learning_rate": 4.930732546001055e-05,
      "loss": 3.7414,
      "step": 314200
    },
    {
      "epoch": 0.05543177614662913,
      "grad_norm": 6.573966979980469,
      "learning_rate": 4.930710500273985e-05,
      "loss": 3.7362,
      "step": 314300
    },
    {
      "epoch": 0.055449412728285706,
      "grad_norm": 7.380790710449219,
      "learning_rate": 4.930688454546914e-05,
      "loss": 3.813,
      "step": 314400
    },
    {
      "epoch": 0.055467049309942285,
      "grad_norm": 4.924807071685791,
      "learning_rate": 4.930666408819843e-05,
      "loss": 3.7813,
      "step": 314500
    },
    {
      "epoch": 0.05548468589159887,
      "grad_norm": 6.458393096923828,
      "learning_rate": 4.930644363092772e-05,
      "loss": 3.7031,
      "step": 314600
    },
    {
      "epoch": 0.05550232247325545,
      "grad_norm": 7.228975296020508,
      "learning_rate": 4.9306223173657016e-05,
      "loss": 3.7695,
      "step": 314700
    },
    {
      "epoch": 0.05551995905491203,
      "grad_norm": 7.9494547843933105,
      "learning_rate": 4.9306002716386305e-05,
      "loss": 3.7856,
      "step": 314800
    },
    {
      "epoch": 0.055537595636568605,
      "grad_norm": 7.610551834106445,
      "learning_rate": 4.93057822591156e-05,
      "loss": 3.7139,
      "step": 314900
    },
    {
      "epoch": 0.055555232218225184,
      "grad_norm": 6.834371089935303,
      "learning_rate": 4.9305561801844896e-05,
      "loss": 3.8995,
      "step": 315000
    },
    {
      "epoch": 0.05557286879988176,
      "grad_norm": 7.606583595275879,
      "learning_rate": 4.9305341344574185e-05,
      "loss": 3.7693,
      "step": 315100
    },
    {
      "epoch": 0.05559050538153834,
      "grad_norm": 5.773957252502441,
      "learning_rate": 4.930512088730348e-05,
      "loss": 3.7049,
      "step": 315200
    },
    {
      "epoch": 0.055608141963194926,
      "grad_norm": 7.194046497344971,
      "learning_rate": 4.9304900430032776e-05,
      "loss": 3.7375,
      "step": 315300
    },
    {
      "epoch": 0.055625778544851505,
      "grad_norm": 8.692204475402832,
      "learning_rate": 4.9304679972762064e-05,
      "loss": 3.7433,
      "step": 315400
    },
    {
      "epoch": 0.05564341512650808,
      "grad_norm": 5.882479667663574,
      "learning_rate": 4.930445951549136e-05,
      "loss": 3.7974,
      "step": 315500
    },
    {
      "epoch": 0.05566105170816466,
      "grad_norm": 5.94064474105835,
      "learning_rate": 4.9304239058220655e-05,
      "loss": 3.7858,
      "step": 315600
    },
    {
      "epoch": 0.05567868828982124,
      "grad_norm": 5.650874137878418,
      "learning_rate": 4.9304018600949944e-05,
      "loss": 3.7347,
      "step": 315700
    },
    {
      "epoch": 0.05569632487147782,
      "grad_norm": 4.953477382659912,
      "learning_rate": 4.930379814367924e-05,
      "loss": 3.7621,
      "step": 315800
    },
    {
      "epoch": 0.0557139614531344,
      "grad_norm": 6.08418607711792,
      "learning_rate": 4.930357768640853e-05,
      "loss": 3.6468,
      "step": 315900
    },
    {
      "epoch": 0.055731598034790976,
      "grad_norm": 8.975300788879395,
      "learning_rate": 4.930335722913782e-05,
      "loss": 3.7119,
      "step": 316000
    },
    {
      "epoch": 0.05574923461644756,
      "grad_norm": 6.5968337059021,
      "learning_rate": 4.930313677186711e-05,
      "loss": 3.838,
      "step": 316100
    },
    {
      "epoch": 0.05576687119810414,
      "grad_norm": 7.762139320373535,
      "learning_rate": 4.930291631459641e-05,
      "loss": 3.8281,
      "step": 316200
    },
    {
      "epoch": 0.05578450777976072,
      "grad_norm": 9.018904685974121,
      "learning_rate": 4.9302695857325696e-05,
      "loss": 3.7927,
      "step": 316300
    },
    {
      "epoch": 0.0558021443614173,
      "grad_norm": 6.90460729598999,
      "learning_rate": 4.930247540005499e-05,
      "loss": 3.8142,
      "step": 316400
    },
    {
      "epoch": 0.055819780943073875,
      "grad_norm": 10.603910446166992,
      "learning_rate": 4.930225494278429e-05,
      "loss": 3.8488,
      "step": 316500
    },
    {
      "epoch": 0.055837417524730454,
      "grad_norm": 7.594685077667236,
      "learning_rate": 4.9302034485513576e-05,
      "loss": 3.8172,
      "step": 316600
    },
    {
      "epoch": 0.05585505410638703,
      "grad_norm": 5.976531505584717,
      "learning_rate": 4.930181402824287e-05,
      "loss": 3.8054,
      "step": 316700
    },
    {
      "epoch": 0.05587269068804362,
      "grad_norm": 7.481276035308838,
      "learning_rate": 4.930159357097217e-05,
      "loss": 3.8313,
      "step": 316800
    },
    {
      "epoch": 0.055890327269700196,
      "grad_norm": 7.005457878112793,
      "learning_rate": 4.9301373113701456e-05,
      "loss": 3.7708,
      "step": 316900
    },
    {
      "epoch": 0.055907963851356775,
      "grad_norm": 6.761196136474609,
      "learning_rate": 4.930115265643075e-05,
      "loss": 3.8172,
      "step": 317000
    },
    {
      "epoch": 0.05592560043301335,
      "grad_norm": 7.568409442901611,
      "learning_rate": 4.9300932199160047e-05,
      "loss": 3.7563,
      "step": 317100
    },
    {
      "epoch": 0.05594323701466993,
      "grad_norm": 6.696749687194824,
      "learning_rate": 4.9300711741889335e-05,
      "loss": 3.7563,
      "step": 317200
    },
    {
      "epoch": 0.05596087359632651,
      "grad_norm": 7.393428325653076,
      "learning_rate": 4.930049128461863e-05,
      "loss": 3.7339,
      "step": 317300
    },
    {
      "epoch": 0.05597851017798309,
      "grad_norm": 9.275212287902832,
      "learning_rate": 4.930027082734792e-05,
      "loss": 3.8319,
      "step": 317400
    },
    {
      "epoch": 0.05599614675963967,
      "grad_norm": 7.21140193939209,
      "learning_rate": 4.930005037007721e-05,
      "loss": 3.755,
      "step": 317500
    },
    {
      "epoch": 0.05601378334129625,
      "grad_norm": 8.412603378295898,
      "learning_rate": 4.9299829912806504e-05,
      "loss": 3.7955,
      "step": 317600
    },
    {
      "epoch": 0.05603141992295283,
      "grad_norm": 6.629931926727295,
      "learning_rate": 4.92996094555358e-05,
      "loss": 3.8069,
      "step": 317700
    },
    {
      "epoch": 0.05604905650460941,
      "grad_norm": 5.392752170562744,
      "learning_rate": 4.929938899826509e-05,
      "loss": 3.7287,
      "step": 317800
    },
    {
      "epoch": 0.05606669308626599,
      "grad_norm": 6.0499701499938965,
      "learning_rate": 4.929916854099438e-05,
      "loss": 3.7685,
      "step": 317900
    },
    {
      "epoch": 0.05608432966792257,
      "grad_norm": 7.153133392333984,
      "learning_rate": 4.929894808372368e-05,
      "loss": 3.7516,
      "step": 318000
    },
    {
      "epoch": 0.056101966249579145,
      "grad_norm": 4.631698131561279,
      "learning_rate": 4.929872762645297e-05,
      "loss": 3.8973,
      "step": 318100
    },
    {
      "epoch": 0.056119602831235724,
      "grad_norm": 7.01552677154541,
      "learning_rate": 4.929850716918226e-05,
      "loss": 3.7964,
      "step": 318200
    },
    {
      "epoch": 0.05613723941289231,
      "grad_norm": 7.810211658477783,
      "learning_rate": 4.929828671191156e-05,
      "loss": 3.8455,
      "step": 318300
    },
    {
      "epoch": 0.05615487599454889,
      "grad_norm": 9.071742057800293,
      "learning_rate": 4.929806625464085e-05,
      "loss": 3.8345,
      "step": 318400
    },
    {
      "epoch": 0.056172512576205466,
      "grad_norm": 8.707259178161621,
      "learning_rate": 4.929784579737014e-05,
      "loss": 3.7643,
      "step": 318500
    },
    {
      "epoch": 0.056190149157862045,
      "grad_norm": 7.642786979675293,
      "learning_rate": 4.929762534009944e-05,
      "loss": 3.7706,
      "step": 318600
    },
    {
      "epoch": 0.05620778573951862,
      "grad_norm": 6.391592979431152,
      "learning_rate": 4.9297404882828727e-05,
      "loss": 3.6953,
      "step": 318700
    },
    {
      "epoch": 0.0562254223211752,
      "grad_norm": 6.940497398376465,
      "learning_rate": 4.9297184425558015e-05,
      "loss": 3.8059,
      "step": 318800
    },
    {
      "epoch": 0.05624305890283178,
      "grad_norm": 11.025238037109375,
      "learning_rate": 4.929696396828731e-05,
      "loss": 3.7377,
      "step": 318900
    },
    {
      "epoch": 0.05626069548448836,
      "grad_norm": 8.199198722839355,
      "learning_rate": 4.9296743511016606e-05,
      "loss": 3.883,
      "step": 319000
    },
    {
      "epoch": 0.056278332066144944,
      "grad_norm": 7.3928961753845215,
      "learning_rate": 4.9296523053745895e-05,
      "loss": 3.7489,
      "step": 319100
    },
    {
      "epoch": 0.05629596864780152,
      "grad_norm": 8.309423446655273,
      "learning_rate": 4.929630259647519e-05,
      "loss": 3.8375,
      "step": 319200
    },
    {
      "epoch": 0.0563136052294581,
      "grad_norm": 7.222981929779053,
      "learning_rate": 4.9296082139204486e-05,
      "loss": 3.6439,
      "step": 319300
    },
    {
      "epoch": 0.05633124181111468,
      "grad_norm": 10.094696044921875,
      "learning_rate": 4.9295861681933775e-05,
      "loss": 3.7639,
      "step": 319400
    },
    {
      "epoch": 0.05634887839277126,
      "grad_norm": 6.7247538566589355,
      "learning_rate": 4.929564122466307e-05,
      "loss": 3.7697,
      "step": 319500
    },
    {
      "epoch": 0.05636651497442784,
      "grad_norm": 9.871216773986816,
      "learning_rate": 4.9295420767392365e-05,
      "loss": 3.814,
      "step": 319600
    },
    {
      "epoch": 0.056384151556084415,
      "grad_norm": 7.2572174072265625,
      "learning_rate": 4.9295200310121654e-05,
      "loss": 3.7208,
      "step": 319700
    },
    {
      "epoch": 0.056401788137741,
      "grad_norm": 6.351672649383545,
      "learning_rate": 4.929497985285095e-05,
      "loss": 3.7526,
      "step": 319800
    },
    {
      "epoch": 0.05641942471939758,
      "grad_norm": 6.544184684753418,
      "learning_rate": 4.9294759395580245e-05,
      "loss": 3.7799,
      "step": 319900
    },
    {
      "epoch": 0.05643706130105416,
      "grad_norm": 7.0077314376831055,
      "learning_rate": 4.9294538938309534e-05,
      "loss": 3.8511,
      "step": 320000
    },
    {
      "epoch": 0.056454697882710736,
      "grad_norm": 7.59346866607666,
      "learning_rate": 4.929431848103883e-05,
      "loss": 3.6791,
      "step": 320100
    },
    {
      "epoch": 0.056472334464367314,
      "grad_norm": 8.023757934570312,
      "learning_rate": 4.929409802376812e-05,
      "loss": 3.7791,
      "step": 320200
    },
    {
      "epoch": 0.05648997104602389,
      "grad_norm": 5.926943302154541,
      "learning_rate": 4.929387756649741e-05,
      "loss": 3.7134,
      "step": 320300
    },
    {
      "epoch": 0.05650760762768047,
      "grad_norm": 7.057656288146973,
      "learning_rate": 4.92936571092267e-05,
      "loss": 3.7053,
      "step": 320400
    },
    {
      "epoch": 0.05652524420933705,
      "grad_norm": 8.069137573242188,
      "learning_rate": 4.9293436651956e-05,
      "loss": 3.7892,
      "step": 320500
    },
    {
      "epoch": 0.056542880790993635,
      "grad_norm": 7.741543769836426,
      "learning_rate": 4.9293216194685286e-05,
      "loss": 3.6988,
      "step": 320600
    },
    {
      "epoch": 0.056560517372650214,
      "grad_norm": 6.2032270431518555,
      "learning_rate": 4.929299573741458e-05,
      "loss": 3.7345,
      "step": 320700
    },
    {
      "epoch": 0.05657815395430679,
      "grad_norm": 7.137546062469482,
      "learning_rate": 4.929277528014388e-05,
      "loss": 3.79,
      "step": 320800
    },
    {
      "epoch": 0.05659579053596337,
      "grad_norm": 7.3860883712768555,
      "learning_rate": 4.9292554822873166e-05,
      "loss": 3.832,
      "step": 320900
    },
    {
      "epoch": 0.05661342711761995,
      "grad_norm": 8.189903259277344,
      "learning_rate": 4.929233436560246e-05,
      "loss": 3.8418,
      "step": 321000
    },
    {
      "epoch": 0.05663106369927653,
      "grad_norm": 7.583385944366455,
      "learning_rate": 4.929211390833176e-05,
      "loss": 3.6875,
      "step": 321100
    },
    {
      "epoch": 0.056648700280933106,
      "grad_norm": 9.461089134216309,
      "learning_rate": 4.9291893451061046e-05,
      "loss": 3.74,
      "step": 321200
    },
    {
      "epoch": 0.05666633686258969,
      "grad_norm": 7.822536468505859,
      "learning_rate": 4.929167299379034e-05,
      "loss": 3.7155,
      "step": 321300
    },
    {
      "epoch": 0.05668397344424627,
      "grad_norm": 7.9032368659973145,
      "learning_rate": 4.9291452536519636e-05,
      "loss": 3.8232,
      "step": 321400
    },
    {
      "epoch": 0.05670161002590285,
      "grad_norm": 7.978944778442383,
      "learning_rate": 4.9291232079248925e-05,
      "loss": 3.869,
      "step": 321500
    },
    {
      "epoch": 0.05671924660755943,
      "grad_norm": 4.7670817375183105,
      "learning_rate": 4.9291011621978214e-05,
      "loss": 3.7027,
      "step": 321600
    },
    {
      "epoch": 0.056736883189216006,
      "grad_norm": 8.119240760803223,
      "learning_rate": 4.929079116470751e-05,
      "loss": 3.6756,
      "step": 321700
    },
    {
      "epoch": 0.056754519770872584,
      "grad_norm": 7.084442615509033,
      "learning_rate": 4.92905707074368e-05,
      "loss": 3.779,
      "step": 321800
    },
    {
      "epoch": 0.05677215635252916,
      "grad_norm": 7.296848297119141,
      "learning_rate": 4.9290350250166093e-05,
      "loss": 3.8079,
      "step": 321900
    },
    {
      "epoch": 0.05678979293418574,
      "grad_norm": 8.91573429107666,
      "learning_rate": 4.929012979289539e-05,
      "loss": 3.6826,
      "step": 322000
    },
    {
      "epoch": 0.05680742951584233,
      "grad_norm": 6.895864963531494,
      "learning_rate": 4.928990933562468e-05,
      "loss": 3.7267,
      "step": 322100
    },
    {
      "epoch": 0.056825066097498905,
      "grad_norm": 7.935935974121094,
      "learning_rate": 4.928968887835397e-05,
      "loss": 3.7788,
      "step": 322200
    },
    {
      "epoch": 0.056842702679155484,
      "grad_norm": 10.644266128540039,
      "learning_rate": 4.928946842108327e-05,
      "loss": 3.7713,
      "step": 322300
    },
    {
      "epoch": 0.05686033926081206,
      "grad_norm": 6.498663425445557,
      "learning_rate": 4.928924796381256e-05,
      "loss": 3.7888,
      "step": 322400
    },
    {
      "epoch": 0.05687797584246864,
      "grad_norm": 7.5605316162109375,
      "learning_rate": 4.928902750654185e-05,
      "loss": 3.792,
      "step": 322500
    },
    {
      "epoch": 0.05689561242412522,
      "grad_norm": 7.031543731689453,
      "learning_rate": 4.928880704927115e-05,
      "loss": 3.8049,
      "step": 322600
    },
    {
      "epoch": 0.0569132490057818,
      "grad_norm": 7.6945881843566895,
      "learning_rate": 4.928858659200044e-05,
      "loss": 3.7487,
      "step": 322700
    },
    {
      "epoch": 0.05693088558743838,
      "grad_norm": 9.07047176361084,
      "learning_rate": 4.928836613472973e-05,
      "loss": 3.8177,
      "step": 322800
    },
    {
      "epoch": 0.05694852216909496,
      "grad_norm": 5.677465438842773,
      "learning_rate": 4.928814567745902e-05,
      "loss": 3.8012,
      "step": 322900
    },
    {
      "epoch": 0.05696615875075154,
      "grad_norm": 6.033688545227051,
      "learning_rate": 4.9287925220188317e-05,
      "loss": 3.8807,
      "step": 323000
    },
    {
      "epoch": 0.05698379533240812,
      "grad_norm": 10.167851448059082,
      "learning_rate": 4.9287704762917605e-05,
      "loss": 3.8043,
      "step": 323100
    },
    {
      "epoch": 0.0570014319140647,
      "grad_norm": 6.823400497436523,
      "learning_rate": 4.92874843056469e-05,
      "loss": 3.689,
      "step": 323200
    },
    {
      "epoch": 0.057019068495721276,
      "grad_norm": 10.634390830993652,
      "learning_rate": 4.928726384837619e-05,
      "loss": 3.7368,
      "step": 323300
    },
    {
      "epoch": 0.057036705077377854,
      "grad_norm": 7.187836170196533,
      "learning_rate": 4.9287043391105485e-05,
      "loss": 3.7454,
      "step": 323400
    },
    {
      "epoch": 0.05705434165903443,
      "grad_norm": 5.86405611038208,
      "learning_rate": 4.928682293383478e-05,
      "loss": 3.8042,
      "step": 323500
    },
    {
      "epoch": 0.05707197824069102,
      "grad_norm": 5.6599931716918945,
      "learning_rate": 4.928660247656407e-05,
      "loss": 3.7738,
      "step": 323600
    },
    {
      "epoch": 0.0570896148223476,
      "grad_norm": 7.829216957092285,
      "learning_rate": 4.9286382019293364e-05,
      "loss": 3.7251,
      "step": 323700
    },
    {
      "epoch": 0.057107251404004175,
      "grad_norm": 6.936190128326416,
      "learning_rate": 4.928616156202266e-05,
      "loss": 3.7046,
      "step": 323800
    },
    {
      "epoch": 0.057124887985660754,
      "grad_norm": 7.126327991485596,
      "learning_rate": 4.928594110475195e-05,
      "loss": 3.7196,
      "step": 323900
    },
    {
      "epoch": 0.05714252456731733,
      "grad_norm": 6.9683918952941895,
      "learning_rate": 4.9285720647481244e-05,
      "loss": 3.8065,
      "step": 324000
    },
    {
      "epoch": 0.05716016114897391,
      "grad_norm": 6.629579067230225,
      "learning_rate": 4.928550019021054e-05,
      "loss": 3.8041,
      "step": 324100
    },
    {
      "epoch": 0.05717779773063049,
      "grad_norm": 6.5254011154174805,
      "learning_rate": 4.928527973293983e-05,
      "loss": 3.8105,
      "step": 324200
    },
    {
      "epoch": 0.057195434312287075,
      "grad_norm": 6.648772716522217,
      "learning_rate": 4.9285059275669124e-05,
      "loss": 3.7658,
      "step": 324300
    },
    {
      "epoch": 0.05721307089394365,
      "grad_norm": 8.434374809265137,
      "learning_rate": 4.928483881839841e-05,
      "loss": 3.7968,
      "step": 324400
    },
    {
      "epoch": 0.05723070747560023,
      "grad_norm": 7.07083797454834,
      "learning_rate": 4.928461836112771e-05,
      "loss": 3.7974,
      "step": 324500
    },
    {
      "epoch": 0.05724834405725681,
      "grad_norm": 8.953047752380371,
      "learning_rate": 4.9284397903856997e-05,
      "loss": 3.5977,
      "step": 324600
    },
    {
      "epoch": 0.05726598063891339,
      "grad_norm": 10.552515983581543,
      "learning_rate": 4.928417744658629e-05,
      "loss": 3.7841,
      "step": 324700
    },
    {
      "epoch": 0.05728361722056997,
      "grad_norm": 8.369606018066406,
      "learning_rate": 4.928395698931558e-05,
      "loss": 3.7685,
      "step": 324800
    },
    {
      "epoch": 0.057301253802226546,
      "grad_norm": 10.467023849487305,
      "learning_rate": 4.9283736532044876e-05,
      "loss": 3.721,
      "step": 324900
    },
    {
      "epoch": 0.057318890383883124,
      "grad_norm": 11.409527778625488,
      "learning_rate": 4.928351607477417e-05,
      "loss": 3.8176,
      "step": 325000
    },
    {
      "epoch": 0.05733652696553971,
      "grad_norm": 9.142106056213379,
      "learning_rate": 4.928329561750346e-05,
      "loss": 3.8586,
      "step": 325100
    },
    {
      "epoch": 0.05735416354719629,
      "grad_norm": 6.435899257659912,
      "learning_rate": 4.9283075160232756e-05,
      "loss": 3.6911,
      "step": 325200
    },
    {
      "epoch": 0.057371800128852866,
      "grad_norm": 5.60671854019165,
      "learning_rate": 4.928285470296205e-05,
      "loss": 3.7734,
      "step": 325300
    },
    {
      "epoch": 0.057389436710509445,
      "grad_norm": 9.372231483459473,
      "learning_rate": 4.928263424569134e-05,
      "loss": 3.6891,
      "step": 325400
    },
    {
      "epoch": 0.05740707329216602,
      "grad_norm": 6.310053825378418,
      "learning_rate": 4.9282413788420635e-05,
      "loss": 3.7609,
      "step": 325500
    },
    {
      "epoch": 0.0574247098738226,
      "grad_norm": 7.205848217010498,
      "learning_rate": 4.928219333114993e-05,
      "loss": 3.7823,
      "step": 325600
    },
    {
      "epoch": 0.05744234645547918,
      "grad_norm": 8.544562339782715,
      "learning_rate": 4.928197287387922e-05,
      "loss": 3.7618,
      "step": 325700
    },
    {
      "epoch": 0.057459983037135766,
      "grad_norm": 8.760807991027832,
      "learning_rate": 4.9281752416608515e-05,
      "loss": 3.768,
      "step": 325800
    },
    {
      "epoch": 0.057477619618792344,
      "grad_norm": 7.17964506149292,
      "learning_rate": 4.9281531959337804e-05,
      "loss": 3.6792,
      "step": 325900
    },
    {
      "epoch": 0.05749525620044892,
      "grad_norm": 5.742992401123047,
      "learning_rate": 4.928131150206709e-05,
      "loss": 3.7257,
      "step": 326000
    },
    {
      "epoch": 0.0575128927821055,
      "grad_norm": 7.023001670837402,
      "learning_rate": 4.928109104479639e-05,
      "loss": 3.7783,
      "step": 326100
    },
    {
      "epoch": 0.05753052936376208,
      "grad_norm": 5.8188276290893555,
      "learning_rate": 4.9280870587525683e-05,
      "loss": 3.7993,
      "step": 326200
    },
    {
      "epoch": 0.05754816594541866,
      "grad_norm": 8.800178527832031,
      "learning_rate": 4.928065013025497e-05,
      "loss": 3.771,
      "step": 326300
    },
    {
      "epoch": 0.05756580252707524,
      "grad_norm": 6.346086025238037,
      "learning_rate": 4.928042967298427e-05,
      "loss": 3.8097,
      "step": 326400
    },
    {
      "epoch": 0.057583439108731815,
      "grad_norm": 6.117018699645996,
      "learning_rate": 4.928020921571356e-05,
      "loss": 3.8914,
      "step": 326500
    },
    {
      "epoch": 0.0576010756903884,
      "grad_norm": 4.8364081382751465,
      "learning_rate": 4.927998875844285e-05,
      "loss": 3.8097,
      "step": 326600
    },
    {
      "epoch": 0.05761871227204498,
      "grad_norm": 9.117749214172363,
      "learning_rate": 4.927976830117215e-05,
      "loss": 3.8142,
      "step": 326700
    },
    {
      "epoch": 0.05763634885370156,
      "grad_norm": 7.5215253829956055,
      "learning_rate": 4.927954784390144e-05,
      "loss": 3.7791,
      "step": 326800
    },
    {
      "epoch": 0.057653985435358136,
      "grad_norm": 5.4616522789001465,
      "learning_rate": 4.927932738663073e-05,
      "loss": 3.688,
      "step": 326900
    },
    {
      "epoch": 0.057671622017014715,
      "grad_norm": 6.609074115753174,
      "learning_rate": 4.927910692936003e-05,
      "loss": 3.6724,
      "step": 327000
    },
    {
      "epoch": 0.05768925859867129,
      "grad_norm": 9.556530952453613,
      "learning_rate": 4.927888647208932e-05,
      "loss": 3.8638,
      "step": 327100
    },
    {
      "epoch": 0.05770689518032787,
      "grad_norm": 7.581765651702881,
      "learning_rate": 4.927866601481861e-05,
      "loss": 3.7508,
      "step": 327200
    },
    {
      "epoch": 0.05772453176198446,
      "grad_norm": 5.612247467041016,
      "learning_rate": 4.9278445557547906e-05,
      "loss": 3.7812,
      "step": 327300
    },
    {
      "epoch": 0.057742168343641036,
      "grad_norm": 7.150717735290527,
      "learning_rate": 4.9278225100277195e-05,
      "loss": 3.7933,
      "step": 327400
    },
    {
      "epoch": 0.057759804925297614,
      "grad_norm": 7.641959190368652,
      "learning_rate": 4.9278004643006484e-05,
      "loss": 3.8815,
      "step": 327500
    },
    {
      "epoch": 0.05777744150695419,
      "grad_norm": 6.985860347747803,
      "learning_rate": 4.927778418573578e-05,
      "loss": 3.7692,
      "step": 327600
    },
    {
      "epoch": 0.05779507808861077,
      "grad_norm": 8.672551155090332,
      "learning_rate": 4.9277563728465075e-05,
      "loss": 3.7989,
      "step": 327700
    },
    {
      "epoch": 0.05781271467026735,
      "grad_norm": 7.252488613128662,
      "learning_rate": 4.9277343271194363e-05,
      "loss": 3.8039,
      "step": 327800
    },
    {
      "epoch": 0.05783035125192393,
      "grad_norm": 11.001867294311523,
      "learning_rate": 4.927712281392366e-05,
      "loss": 3.7424,
      "step": 327900
    },
    {
      "epoch": 0.05784798783358051,
      "grad_norm": 6.307226657867432,
      "learning_rate": 4.9276902356652954e-05,
      "loss": 3.7798,
      "step": 328000
    },
    {
      "epoch": 0.05786562441523709,
      "grad_norm": 10.882843017578125,
      "learning_rate": 4.927668189938224e-05,
      "loss": 3.7313,
      "step": 328100
    },
    {
      "epoch": 0.05788326099689367,
      "grad_norm": 9.309816360473633,
      "learning_rate": 4.927646144211154e-05,
      "loss": 3.7079,
      "step": 328200
    },
    {
      "epoch": 0.05790089757855025,
      "grad_norm": 5.8024797439575195,
      "learning_rate": 4.9276240984840834e-05,
      "loss": 3.8449,
      "step": 328300
    },
    {
      "epoch": 0.05791853416020683,
      "grad_norm": 6.129226207733154,
      "learning_rate": 4.927602052757012e-05,
      "loss": 3.7983,
      "step": 328400
    },
    {
      "epoch": 0.057936170741863406,
      "grad_norm": 11.78221321105957,
      "learning_rate": 4.927580007029942e-05,
      "loss": 3.919,
      "step": 328500
    },
    {
      "epoch": 0.057953807323519985,
      "grad_norm": 7.29962158203125,
      "learning_rate": 4.9275579613028714e-05,
      "loss": 3.7956,
      "step": 328600
    },
    {
      "epoch": 0.05797144390517656,
      "grad_norm": 8.022439956665039,
      "learning_rate": 4.9275359155758e-05,
      "loss": 3.7463,
      "step": 328700
    },
    {
      "epoch": 0.05798908048683315,
      "grad_norm": 12.443371772766113,
      "learning_rate": 4.927513869848729e-05,
      "loss": 3.687,
      "step": 328800
    },
    {
      "epoch": 0.05800671706848973,
      "grad_norm": 8.851612091064453,
      "learning_rate": 4.9274918241216587e-05,
      "loss": 3.8,
      "step": 328900
    },
    {
      "epoch": 0.058024353650146306,
      "grad_norm": 7.80549430847168,
      "learning_rate": 4.9274697783945875e-05,
      "loss": 3.8006,
      "step": 329000
    },
    {
      "epoch": 0.058041990231802884,
      "grad_norm": 7.388244152069092,
      "learning_rate": 4.927447732667517e-05,
      "loss": 3.691,
      "step": 329100
    },
    {
      "epoch": 0.05805962681345946,
      "grad_norm": 6.350114345550537,
      "learning_rate": 4.9274256869404466e-05,
      "loss": 3.7372,
      "step": 329200
    },
    {
      "epoch": 0.05807726339511604,
      "grad_norm": 8.15089225769043,
      "learning_rate": 4.927403641213376e-05,
      "loss": 3.7311,
      "step": 329300
    },
    {
      "epoch": 0.05809489997677262,
      "grad_norm": 7.794012069702148,
      "learning_rate": 4.927381595486305e-05,
      "loss": 3.7651,
      "step": 329400
    },
    {
      "epoch": 0.0581125365584292,
      "grad_norm": 8.16973876953125,
      "learning_rate": 4.9273595497592346e-05,
      "loss": 3.7349,
      "step": 329500
    },
    {
      "epoch": 0.058130173140085784,
      "grad_norm": 8.700772285461426,
      "learning_rate": 4.927337504032164e-05,
      "loss": 3.8094,
      "step": 329600
    },
    {
      "epoch": 0.05814780972174236,
      "grad_norm": 9.03039836883545,
      "learning_rate": 4.927315458305093e-05,
      "loss": 3.7432,
      "step": 329700
    },
    {
      "epoch": 0.05816544630339894,
      "grad_norm": 7.360448360443115,
      "learning_rate": 4.9272934125780225e-05,
      "loss": 3.8269,
      "step": 329800
    },
    {
      "epoch": 0.05818308288505552,
      "grad_norm": 10.276251792907715,
      "learning_rate": 4.927271366850952e-05,
      "loss": 3.7692,
      "step": 329900
    },
    {
      "epoch": 0.0582007194667121,
      "grad_norm": 7.23275899887085,
      "learning_rate": 4.927249321123881e-05,
      "loss": 3.7493,
      "step": 330000
    },
    {
      "epoch": 0.058218356048368676,
      "grad_norm": 9.92667007446289,
      "learning_rate": 4.9272272753968105e-05,
      "loss": 3.6734,
      "step": 330100
    },
    {
      "epoch": 0.058235992630025255,
      "grad_norm": 7.879982948303223,
      "learning_rate": 4.9272052296697394e-05,
      "loss": 3.7239,
      "step": 330200
    },
    {
      "epoch": 0.05825362921168184,
      "grad_norm": 7.945446491241455,
      "learning_rate": 4.927183183942668e-05,
      "loss": 3.6858,
      "step": 330300
    },
    {
      "epoch": 0.05827126579333842,
      "grad_norm": 7.400527477264404,
      "learning_rate": 4.927161138215598e-05,
      "loss": 3.7434,
      "step": 330400
    },
    {
      "epoch": 0.058288902374995,
      "grad_norm": 7.449358940124512,
      "learning_rate": 4.927139092488527e-05,
      "loss": 3.7324,
      "step": 330500
    },
    {
      "epoch": 0.058306538956651575,
      "grad_norm": 8.03432559967041,
      "learning_rate": 4.927117046761456e-05,
      "loss": 3.7828,
      "step": 330600
    },
    {
      "epoch": 0.058324175538308154,
      "grad_norm": 6.821742534637451,
      "learning_rate": 4.927095001034386e-05,
      "loss": 3.7371,
      "step": 330700
    },
    {
      "epoch": 0.05834181211996473,
      "grad_norm": 8.171815872192383,
      "learning_rate": 4.927072955307315e-05,
      "loss": 3.7242,
      "step": 330800
    },
    {
      "epoch": 0.05835944870162131,
      "grad_norm": 6.708266258239746,
      "learning_rate": 4.927050909580244e-05,
      "loss": 3.8084,
      "step": 330900
    },
    {
      "epoch": 0.05837708528327789,
      "grad_norm": 8.539813995361328,
      "learning_rate": 4.927028863853174e-05,
      "loss": 3.6631,
      "step": 331000
    },
    {
      "epoch": 0.058394721864934475,
      "grad_norm": 9.162328720092773,
      "learning_rate": 4.927006818126103e-05,
      "loss": 3.6621,
      "step": 331100
    },
    {
      "epoch": 0.05841235844659105,
      "grad_norm": 6.509259223937988,
      "learning_rate": 4.926984772399032e-05,
      "loss": 3.8557,
      "step": 331200
    },
    {
      "epoch": 0.05842999502824763,
      "grad_norm": 6.892364501953125,
      "learning_rate": 4.926962726671962e-05,
      "loss": 3.7638,
      "step": 331300
    },
    {
      "epoch": 0.05844763160990421,
      "grad_norm": 8.174046516418457,
      "learning_rate": 4.926940680944891e-05,
      "loss": 3.7854,
      "step": 331400
    },
    {
      "epoch": 0.05846526819156079,
      "grad_norm": 6.712615966796875,
      "learning_rate": 4.92691863521782e-05,
      "loss": 3.8432,
      "step": 331500
    },
    {
      "epoch": 0.05848290477321737,
      "grad_norm": 7.742337703704834,
      "learning_rate": 4.926896589490749e-05,
      "loss": 3.663,
      "step": 331600
    },
    {
      "epoch": 0.058500541354873946,
      "grad_norm": 6.382655143737793,
      "learning_rate": 4.9268745437636785e-05,
      "loss": 3.7191,
      "step": 331700
    },
    {
      "epoch": 0.05851817793653053,
      "grad_norm": 6.204744338989258,
      "learning_rate": 4.9268524980366074e-05,
      "loss": 3.7588,
      "step": 331800
    },
    {
      "epoch": 0.05853581451818711,
      "grad_norm": 6.160109043121338,
      "learning_rate": 4.926830452309537e-05,
      "loss": 3.7985,
      "step": 331900
    },
    {
      "epoch": 0.05855345109984369,
      "grad_norm": 6.106138706207275,
      "learning_rate": 4.9268084065824665e-05,
      "loss": 3.7249,
      "step": 332000
    },
    {
      "epoch": 0.05857108768150027,
      "grad_norm": 7.272943496704102,
      "learning_rate": 4.9267863608553953e-05,
      "loss": 3.7087,
      "step": 332100
    },
    {
      "epoch": 0.058588724263156845,
      "grad_norm": 5.155791282653809,
      "learning_rate": 4.926764315128325e-05,
      "loss": 3.7928,
      "step": 332200
    },
    {
      "epoch": 0.058606360844813424,
      "grad_norm": 10.48605728149414,
      "learning_rate": 4.9267422694012544e-05,
      "loss": 3.7335,
      "step": 332300
    },
    {
      "epoch": 0.05862399742647,
      "grad_norm": 8.318161964416504,
      "learning_rate": 4.926720223674183e-05,
      "loss": 3.6207,
      "step": 332400
    },
    {
      "epoch": 0.05864163400812658,
      "grad_norm": 7.070163249969482,
      "learning_rate": 4.926698177947113e-05,
      "loss": 3.6757,
      "step": 332500
    },
    {
      "epoch": 0.058659270589783166,
      "grad_norm": 7.181363582611084,
      "learning_rate": 4.9266761322200424e-05,
      "loss": 3.8078,
      "step": 332600
    },
    {
      "epoch": 0.058676907171439745,
      "grad_norm": 9.510724067687988,
      "learning_rate": 4.926654086492971e-05,
      "loss": 3.8239,
      "step": 332700
    },
    {
      "epoch": 0.05869454375309632,
      "grad_norm": 7.096585750579834,
      "learning_rate": 4.926632040765901e-05,
      "loss": 3.8167,
      "step": 332800
    },
    {
      "epoch": 0.0587121803347529,
      "grad_norm": 6.634393692016602,
      "learning_rate": 4.9266099950388304e-05,
      "loss": 3.7756,
      "step": 332900
    },
    {
      "epoch": 0.05872981691640948,
      "grad_norm": 7.225319862365723,
      "learning_rate": 4.926587949311759e-05,
      "loss": 3.7729,
      "step": 333000
    },
    {
      "epoch": 0.05874745349806606,
      "grad_norm": 10.467079162597656,
      "learning_rate": 4.926565903584688e-05,
      "loss": 3.665,
      "step": 333100
    },
    {
      "epoch": 0.05876509007972264,
      "grad_norm": 8.340472221374512,
      "learning_rate": 4.9265438578576176e-05,
      "loss": 3.698,
      "step": 333200
    },
    {
      "epoch": 0.05878272666137922,
      "grad_norm": 5.937167167663574,
      "learning_rate": 4.9265218121305465e-05,
      "loss": 3.6951,
      "step": 333300
    },
    {
      "epoch": 0.0588003632430358,
      "grad_norm": 6.661717414855957,
      "learning_rate": 4.926499766403476e-05,
      "loss": 3.7308,
      "step": 333400
    },
    {
      "epoch": 0.05881799982469238,
      "grad_norm": 5.954048156738281,
      "learning_rate": 4.9264777206764056e-05,
      "loss": 3.7571,
      "step": 333500
    },
    {
      "epoch": 0.05883563640634896,
      "grad_norm": 9.252968788146973,
      "learning_rate": 4.9264556749493345e-05,
      "loss": 3.6821,
      "step": 333600
    },
    {
      "epoch": 0.05885327298800554,
      "grad_norm": 10.032158851623535,
      "learning_rate": 4.926433629222264e-05,
      "loss": 3.6547,
      "step": 333700
    },
    {
      "epoch": 0.058870909569662115,
      "grad_norm": 8.178404808044434,
      "learning_rate": 4.9264115834951936e-05,
      "loss": 3.7247,
      "step": 333800
    },
    {
      "epoch": 0.058888546151318694,
      "grad_norm": 7.038850784301758,
      "learning_rate": 4.9263895377681224e-05,
      "loss": 3.7592,
      "step": 333900
    },
    {
      "epoch": 0.05890618273297527,
      "grad_norm": 7.309412002563477,
      "learning_rate": 4.926367492041052e-05,
      "loss": 3.7316,
      "step": 334000
    },
    {
      "epoch": 0.05892381931463186,
      "grad_norm": 6.198890686035156,
      "learning_rate": 4.9263454463139815e-05,
      "loss": 3.774,
      "step": 334100
    },
    {
      "epoch": 0.058941455896288436,
      "grad_norm": 8.555800437927246,
      "learning_rate": 4.9263234005869104e-05,
      "loss": 3.8854,
      "step": 334200
    },
    {
      "epoch": 0.058959092477945015,
      "grad_norm": 6.325084686279297,
      "learning_rate": 4.92630135485984e-05,
      "loss": 3.7108,
      "step": 334300
    },
    {
      "epoch": 0.05897672905960159,
      "grad_norm": 8.493451118469238,
      "learning_rate": 4.926279309132769e-05,
      "loss": 3.7197,
      "step": 334400
    },
    {
      "epoch": 0.05899436564125817,
      "grad_norm": 13.374551773071289,
      "learning_rate": 4.9262572634056984e-05,
      "loss": 3.7385,
      "step": 334500
    },
    {
      "epoch": 0.05901200222291475,
      "grad_norm": 6.905546188354492,
      "learning_rate": 4.926235217678627e-05,
      "loss": 3.6985,
      "step": 334600
    },
    {
      "epoch": 0.05902963880457133,
      "grad_norm": 9.180167198181152,
      "learning_rate": 4.926213171951557e-05,
      "loss": 3.7306,
      "step": 334700
    },
    {
      "epoch": 0.059047275386227914,
      "grad_norm": 7.611682415008545,
      "learning_rate": 4.9261911262244857e-05,
      "loss": 3.5564,
      "step": 334800
    },
    {
      "epoch": 0.05906491196788449,
      "grad_norm": 6.291686058044434,
      "learning_rate": 4.926169080497415e-05,
      "loss": 3.672,
      "step": 334900
    },
    {
      "epoch": 0.05908254854954107,
      "grad_norm": 6.553642272949219,
      "learning_rate": 4.926147034770345e-05,
      "loss": 3.7413,
      "step": 335000
    },
    {
      "epoch": 0.05910018513119765,
      "grad_norm": 6.498518943786621,
      "learning_rate": 4.9261249890432736e-05,
      "loss": 3.8044,
      "step": 335100
    },
    {
      "epoch": 0.05911782171285423,
      "grad_norm": 6.826023101806641,
      "learning_rate": 4.926102943316203e-05,
      "loss": 3.817,
      "step": 335200
    },
    {
      "epoch": 0.059135458294510806,
      "grad_norm": 6.738882541656494,
      "learning_rate": 4.926080897589133e-05,
      "loss": 3.7391,
      "step": 335300
    },
    {
      "epoch": 0.059153094876167385,
      "grad_norm": 6.873105525970459,
      "learning_rate": 4.9260588518620616e-05,
      "loss": 3.7961,
      "step": 335400
    },
    {
      "epoch": 0.059170731457823963,
      "grad_norm": 6.004534721374512,
      "learning_rate": 4.926036806134991e-05,
      "loss": 3.7246,
      "step": 335500
    },
    {
      "epoch": 0.05918836803948055,
      "grad_norm": 9.826592445373535,
      "learning_rate": 4.926014760407921e-05,
      "loss": 3.7949,
      "step": 335600
    },
    {
      "epoch": 0.05920600462113713,
      "grad_norm": 6.404085636138916,
      "learning_rate": 4.9259927146808495e-05,
      "loss": 3.7662,
      "step": 335700
    },
    {
      "epoch": 0.059223641202793706,
      "grad_norm": 7.999170303344727,
      "learning_rate": 4.925970668953779e-05,
      "loss": 3.7956,
      "step": 335800
    },
    {
      "epoch": 0.059241277784450284,
      "grad_norm": 5.683852195739746,
      "learning_rate": 4.925948623226708e-05,
      "loss": 3.8649,
      "step": 335900
    },
    {
      "epoch": 0.05925891436610686,
      "grad_norm": 7.370704174041748,
      "learning_rate": 4.925926577499637e-05,
      "loss": 3.7459,
      "step": 336000
    },
    {
      "epoch": 0.05927655094776344,
      "grad_norm": 8.078605651855469,
      "learning_rate": 4.9259045317725664e-05,
      "loss": 3.7453,
      "step": 336100
    },
    {
      "epoch": 0.05929418752942002,
      "grad_norm": 8.0162992477417,
      "learning_rate": 4.925882486045496e-05,
      "loss": 3.8497,
      "step": 336200
    },
    {
      "epoch": 0.059311824111076605,
      "grad_norm": 8.143898963928223,
      "learning_rate": 4.925860440318425e-05,
      "loss": 3.7583,
      "step": 336300
    },
    {
      "epoch": 0.059329460692733184,
      "grad_norm": 6.356661796569824,
      "learning_rate": 4.9258383945913543e-05,
      "loss": 3.7891,
      "step": 336400
    },
    {
      "epoch": 0.05934709727438976,
      "grad_norm": 5.588716506958008,
      "learning_rate": 4.925816348864284e-05,
      "loss": 3.8065,
      "step": 336500
    },
    {
      "epoch": 0.05936473385604634,
      "grad_norm": 7.686548709869385,
      "learning_rate": 4.925794303137213e-05,
      "loss": 3.7502,
      "step": 336600
    },
    {
      "epoch": 0.05938237043770292,
      "grad_norm": 6.719829559326172,
      "learning_rate": 4.925772257410142e-05,
      "loss": 3.7961,
      "step": 336700
    },
    {
      "epoch": 0.0594000070193595,
      "grad_norm": 7.9087653160095215,
      "learning_rate": 4.925750211683072e-05,
      "loss": 3.7687,
      "step": 336800
    },
    {
      "epoch": 0.059417643601016076,
      "grad_norm": 9.576961517333984,
      "learning_rate": 4.925728165956001e-05,
      "loss": 3.69,
      "step": 336900
    },
    {
      "epoch": 0.059435280182672655,
      "grad_norm": 7.470233917236328,
      "learning_rate": 4.92570612022893e-05,
      "loss": 3.7369,
      "step": 337000
    },
    {
      "epoch": 0.05945291676432924,
      "grad_norm": 5.13494873046875,
      "learning_rate": 4.92568407450186e-05,
      "loss": 3.8164,
      "step": 337100
    },
    {
      "epoch": 0.05947055334598582,
      "grad_norm": 7.1374287605285645,
      "learning_rate": 4.925662028774789e-05,
      "loss": 3.749,
      "step": 337200
    },
    {
      "epoch": 0.0594881899276424,
      "grad_norm": 7.9932942390441895,
      "learning_rate": 4.925639983047718e-05,
      "loss": 3.7191,
      "step": 337300
    },
    {
      "epoch": 0.059505826509298976,
      "grad_norm": 5.81447172164917,
      "learning_rate": 4.925617937320647e-05,
      "loss": 3.6981,
      "step": 337400
    },
    {
      "epoch": 0.059523463090955554,
      "grad_norm": 7.12362813949585,
      "learning_rate": 4.925595891593576e-05,
      "loss": 3.8003,
      "step": 337500
    },
    {
      "epoch": 0.05954109967261213,
      "grad_norm": 11.038017272949219,
      "learning_rate": 4.9255738458665055e-05,
      "loss": 3.7453,
      "step": 337600
    },
    {
      "epoch": 0.05955873625426871,
      "grad_norm": 7.6229352951049805,
      "learning_rate": 4.925551800139435e-05,
      "loss": 3.706,
      "step": 337700
    },
    {
      "epoch": 0.0595763728359253,
      "grad_norm": 6.748722553253174,
      "learning_rate": 4.925529754412364e-05,
      "loss": 3.662,
      "step": 337800
    },
    {
      "epoch": 0.059594009417581875,
      "grad_norm": 6.357499599456787,
      "learning_rate": 4.9255077086852935e-05,
      "loss": 3.806,
      "step": 337900
    },
    {
      "epoch": 0.059611645999238454,
      "grad_norm": 7.78161096572876,
      "learning_rate": 4.925485662958223e-05,
      "loss": 3.731,
      "step": 338000
    },
    {
      "epoch": 0.05962928258089503,
      "grad_norm": 8.355549812316895,
      "learning_rate": 4.925463617231152e-05,
      "loss": 3.6605,
      "step": 338100
    },
    {
      "epoch": 0.05964691916255161,
      "grad_norm": 7.227962017059326,
      "learning_rate": 4.9254415715040814e-05,
      "loss": 3.7348,
      "step": 338200
    },
    {
      "epoch": 0.05966455574420819,
      "grad_norm": 8.098552703857422,
      "learning_rate": 4.925419525777011e-05,
      "loss": 3.5984,
      "step": 338300
    },
    {
      "epoch": 0.05968219232586477,
      "grad_norm": 10.477150917053223,
      "learning_rate": 4.92539748004994e-05,
      "loss": 3.7355,
      "step": 338400
    },
    {
      "epoch": 0.059699828907521346,
      "grad_norm": 7.773017406463623,
      "learning_rate": 4.9253754343228694e-05,
      "loss": 3.8112,
      "step": 338500
    },
    {
      "epoch": 0.05971746548917793,
      "grad_norm": 5.856801986694336,
      "learning_rate": 4.925353388595799e-05,
      "loss": 3.7504,
      "step": 338600
    },
    {
      "epoch": 0.05973510207083451,
      "grad_norm": 6.542690277099609,
      "learning_rate": 4.925331342868728e-05,
      "loss": 3.7595,
      "step": 338700
    },
    {
      "epoch": 0.05975273865249109,
      "grad_norm": 6.310145854949951,
      "learning_rate": 4.925309297141657e-05,
      "loss": 3.8285,
      "step": 338800
    },
    {
      "epoch": 0.05977037523414767,
      "grad_norm": 6.135852813720703,
      "learning_rate": 4.925287251414586e-05,
      "loss": 3.7254,
      "step": 338900
    },
    {
      "epoch": 0.059788011815804246,
      "grad_norm": 8.179734230041504,
      "learning_rate": 4.925265205687515e-05,
      "loss": 3.6425,
      "step": 339000
    },
    {
      "epoch": 0.059805648397460824,
      "grad_norm": 7.436174392700195,
      "learning_rate": 4.9252431599604446e-05,
      "loss": 3.7956,
      "step": 339100
    },
    {
      "epoch": 0.0598232849791174,
      "grad_norm": 7.494320869445801,
      "learning_rate": 4.925221114233374e-05,
      "loss": 3.7045,
      "step": 339200
    },
    {
      "epoch": 0.05984092156077399,
      "grad_norm": 6.562629699707031,
      "learning_rate": 4.925199068506303e-05,
      "loss": 3.879,
      "step": 339300
    },
    {
      "epoch": 0.05985855814243057,
      "grad_norm": 6.211453437805176,
      "learning_rate": 4.9251770227792326e-05,
      "loss": 3.8513,
      "step": 339400
    },
    {
      "epoch": 0.059876194724087145,
      "grad_norm": 7.927415370941162,
      "learning_rate": 4.925154977052162e-05,
      "loss": 3.7615,
      "step": 339500
    },
    {
      "epoch": 0.059893831305743724,
      "grad_norm": 6.350909233093262,
      "learning_rate": 4.925132931325091e-05,
      "loss": 3.7411,
      "step": 339600
    },
    {
      "epoch": 0.0599114678874003,
      "grad_norm": 8.208812713623047,
      "learning_rate": 4.9251108855980206e-05,
      "loss": 3.6862,
      "step": 339700
    },
    {
      "epoch": 0.05992910446905688,
      "grad_norm": 7.172741413116455,
      "learning_rate": 4.92508883987095e-05,
      "loss": 3.7432,
      "step": 339800
    },
    {
      "epoch": 0.05994674105071346,
      "grad_norm": 6.614645004272461,
      "learning_rate": 4.92506679414388e-05,
      "loss": 3.7826,
      "step": 339900
    },
    {
      "epoch": 0.05996437763237004,
      "grad_norm": 5.076901435852051,
      "learning_rate": 4.9250447484168085e-05,
      "loss": 3.7272,
      "step": 340000
    },
    {
      "epoch": 0.05998201421402662,
      "grad_norm": 9.443957328796387,
      "learning_rate": 4.925022702689738e-05,
      "loss": 3.7568,
      "step": 340100
    },
    {
      "epoch": 0.0599996507956832,
      "grad_norm": 8.774757385253906,
      "learning_rate": 4.925000656962667e-05,
      "loss": 3.8554,
      "step": 340200
    },
    {
      "epoch": 0.06001728737733978,
      "grad_norm": 6.313534736633301,
      "learning_rate": 4.924978611235596e-05,
      "loss": 3.7853,
      "step": 340300
    },
    {
      "epoch": 0.06003492395899636,
      "grad_norm": 7.964385032653809,
      "learning_rate": 4.9249565655085254e-05,
      "loss": 3.7131,
      "step": 340400
    },
    {
      "epoch": 0.06005256054065294,
      "grad_norm": 8.447517395019531,
      "learning_rate": 4.924934519781455e-05,
      "loss": 3.6925,
      "step": 340500
    },
    {
      "epoch": 0.060070197122309515,
      "grad_norm": 6.789041042327881,
      "learning_rate": 4.924912474054384e-05,
      "loss": 3.765,
      "step": 340600
    },
    {
      "epoch": 0.060087833703966094,
      "grad_norm": 6.050089359283447,
      "learning_rate": 4.924890428327313e-05,
      "loss": 3.7881,
      "step": 340700
    },
    {
      "epoch": 0.06010547028562268,
      "grad_norm": 11.60807991027832,
      "learning_rate": 4.924868382600243e-05,
      "loss": 3.7848,
      "step": 340800
    },
    {
      "epoch": 0.06012310686727926,
      "grad_norm": 13.1608247756958,
      "learning_rate": 4.924846336873172e-05,
      "loss": 3.7333,
      "step": 340900
    },
    {
      "epoch": 0.060140743448935836,
      "grad_norm": 5.910364627838135,
      "learning_rate": 4.924824291146101e-05,
      "loss": 3.8467,
      "step": 341000
    },
    {
      "epoch": 0.060158380030592415,
      "grad_norm": 7.256924152374268,
      "learning_rate": 4.924802245419031e-05,
      "loss": 3.7321,
      "step": 341100
    },
    {
      "epoch": 0.06017601661224899,
      "grad_norm": 6.913788795471191,
      "learning_rate": 4.92478019969196e-05,
      "loss": 3.766,
      "step": 341200
    },
    {
      "epoch": 0.06019365319390557,
      "grad_norm": 9.945686340332031,
      "learning_rate": 4.924758153964889e-05,
      "loss": 3.8093,
      "step": 341300
    },
    {
      "epoch": 0.06021128977556215,
      "grad_norm": 8.38996696472168,
      "learning_rate": 4.924736108237819e-05,
      "loss": 3.7334,
      "step": 341400
    },
    {
      "epoch": 0.06022892635721873,
      "grad_norm": 6.458978176116943,
      "learning_rate": 4.924714062510748e-05,
      "loss": 3.8415,
      "step": 341500
    },
    {
      "epoch": 0.060246562938875314,
      "grad_norm": 7.399052143096924,
      "learning_rate": 4.9246920167836765e-05,
      "loss": 3.721,
      "step": 341600
    },
    {
      "epoch": 0.06026419952053189,
      "grad_norm": 7.515855312347412,
      "learning_rate": 4.924669971056606e-05,
      "loss": 3.8433,
      "step": 341700
    },
    {
      "epoch": 0.06028183610218847,
      "grad_norm": 5.81825590133667,
      "learning_rate": 4.924647925329535e-05,
      "loss": 3.7245,
      "step": 341800
    },
    {
      "epoch": 0.06029947268384505,
      "grad_norm": 7.910508632659912,
      "learning_rate": 4.9246258796024645e-05,
      "loss": 3.7646,
      "step": 341900
    },
    {
      "epoch": 0.06031710926550163,
      "grad_norm": 8.065067291259766,
      "learning_rate": 4.924603833875394e-05,
      "loss": 3.6377,
      "step": 342000
    },
    {
      "epoch": 0.06033474584715821,
      "grad_norm": 6.021277904510498,
      "learning_rate": 4.924581788148323e-05,
      "loss": 3.6566,
      "step": 342100
    },
    {
      "epoch": 0.060352382428814785,
      "grad_norm": 6.882036209106445,
      "learning_rate": 4.9245597424212525e-05,
      "loss": 3.7552,
      "step": 342200
    },
    {
      "epoch": 0.06037001901047137,
      "grad_norm": 9.944628715515137,
      "learning_rate": 4.924537696694182e-05,
      "loss": 3.6377,
      "step": 342300
    },
    {
      "epoch": 0.06038765559212795,
      "grad_norm": 6.4760637283325195,
      "learning_rate": 4.924515650967111e-05,
      "loss": 3.7902,
      "step": 342400
    },
    {
      "epoch": 0.06040529217378453,
      "grad_norm": 5.81082010269165,
      "learning_rate": 4.9244936052400404e-05,
      "loss": 3.6824,
      "step": 342500
    },
    {
      "epoch": 0.060422928755441106,
      "grad_norm": 9.011990547180176,
      "learning_rate": 4.92447155951297e-05,
      "loss": 3.7531,
      "step": 342600
    },
    {
      "epoch": 0.060440565337097685,
      "grad_norm": 8.688769340515137,
      "learning_rate": 4.924449513785899e-05,
      "loss": 3.7316,
      "step": 342700
    },
    {
      "epoch": 0.06045820191875426,
      "grad_norm": 5.345545291900635,
      "learning_rate": 4.9244274680588284e-05,
      "loss": 3.7682,
      "step": 342800
    },
    {
      "epoch": 0.06047583850041084,
      "grad_norm": 6.591670989990234,
      "learning_rate": 4.924405422331758e-05,
      "loss": 3.6849,
      "step": 342900
    },
    {
      "epoch": 0.06049347508206742,
      "grad_norm": 10.619576454162598,
      "learning_rate": 4.924383376604687e-05,
      "loss": 3.677,
      "step": 343000
    },
    {
      "epoch": 0.060511111663724006,
      "grad_norm": 8.728523254394531,
      "learning_rate": 4.924361330877616e-05,
      "loss": 3.8029,
      "step": 343100
    },
    {
      "epoch": 0.060528748245380584,
      "grad_norm": 7.9964985847473145,
      "learning_rate": 4.924339285150545e-05,
      "loss": 3.7838,
      "step": 343200
    },
    {
      "epoch": 0.06054638482703716,
      "grad_norm": 7.992275238037109,
      "learning_rate": 4.924317239423474e-05,
      "loss": 3.7065,
      "step": 343300
    },
    {
      "epoch": 0.06056402140869374,
      "grad_norm": 8.055719375610352,
      "learning_rate": 4.9242951936964036e-05,
      "loss": 3.7503,
      "step": 343400
    },
    {
      "epoch": 0.06058165799035032,
      "grad_norm": 5.167973518371582,
      "learning_rate": 4.924273147969333e-05,
      "loss": 3.7238,
      "step": 343500
    },
    {
      "epoch": 0.0605992945720069,
      "grad_norm": 10.943835258483887,
      "learning_rate": 4.924251102242262e-05,
      "loss": 3.7752,
      "step": 343600
    },
    {
      "epoch": 0.06061693115366348,
      "grad_norm": 9.349505424499512,
      "learning_rate": 4.9242290565151916e-05,
      "loss": 3.7163,
      "step": 343700
    },
    {
      "epoch": 0.06063456773532006,
      "grad_norm": 5.089837074279785,
      "learning_rate": 4.924207010788121e-05,
      "loss": 3.7737,
      "step": 343800
    },
    {
      "epoch": 0.06065220431697664,
      "grad_norm": 7.319516181945801,
      "learning_rate": 4.92418496506105e-05,
      "loss": 3.6914,
      "step": 343900
    },
    {
      "epoch": 0.06066984089863322,
      "grad_norm": 6.234694004058838,
      "learning_rate": 4.9241629193339796e-05,
      "loss": 3.781,
      "step": 344000
    },
    {
      "epoch": 0.0606874774802898,
      "grad_norm": 7.1076154708862305,
      "learning_rate": 4.924140873606909e-05,
      "loss": 3.7001,
      "step": 344100
    },
    {
      "epoch": 0.060705114061946376,
      "grad_norm": 5.278623104095459,
      "learning_rate": 4.924118827879838e-05,
      "loss": 3.7426,
      "step": 344200
    },
    {
      "epoch": 0.060722750643602955,
      "grad_norm": 8.510019302368164,
      "learning_rate": 4.9240967821527675e-05,
      "loss": 3.7691,
      "step": 344300
    },
    {
      "epoch": 0.06074038722525953,
      "grad_norm": 5.914671421051025,
      "learning_rate": 4.9240747364256964e-05,
      "loss": 3.6884,
      "step": 344400
    },
    {
      "epoch": 0.06075802380691611,
      "grad_norm": 7.163676738739014,
      "learning_rate": 4.924052690698626e-05,
      "loss": 3.7686,
      "step": 344500
    },
    {
      "epoch": 0.0607756603885727,
      "grad_norm": 7.14947509765625,
      "learning_rate": 4.924030644971555e-05,
      "loss": 3.7164,
      "step": 344600
    },
    {
      "epoch": 0.060793296970229276,
      "grad_norm": 10.02947998046875,
      "learning_rate": 4.9240085992444844e-05,
      "loss": 3.808,
      "step": 344700
    },
    {
      "epoch": 0.060810933551885854,
      "grad_norm": 6.298553466796875,
      "learning_rate": 4.923986553517413e-05,
      "loss": 3.8265,
      "step": 344800
    },
    {
      "epoch": 0.06082857013354243,
      "grad_norm": 6.953925132751465,
      "learning_rate": 4.923964507790343e-05,
      "loss": 3.6426,
      "step": 344900
    },
    {
      "epoch": 0.06084620671519901,
      "grad_norm": 6.575968265533447,
      "learning_rate": 4.923942462063272e-05,
      "loss": 3.7546,
      "step": 345000
    },
    {
      "epoch": 0.06086384329685559,
      "grad_norm": 9.553204536437988,
      "learning_rate": 4.923920416336201e-05,
      "loss": 3.8468,
      "step": 345100
    },
    {
      "epoch": 0.06088147987851217,
      "grad_norm": 6.803539752960205,
      "learning_rate": 4.923898370609131e-05,
      "loss": 3.7713,
      "step": 345200
    },
    {
      "epoch": 0.06089911646016875,
      "grad_norm": 6.439903259277344,
      "learning_rate": 4.92387632488206e-05,
      "loss": 3.7105,
      "step": 345300
    },
    {
      "epoch": 0.06091675304182533,
      "grad_norm": 6.222912311553955,
      "learning_rate": 4.923854279154989e-05,
      "loss": 3.6968,
      "step": 345400
    },
    {
      "epoch": 0.06093438962348191,
      "grad_norm": 6.593644618988037,
      "learning_rate": 4.923832233427919e-05,
      "loss": 3.8014,
      "step": 345500
    },
    {
      "epoch": 0.06095202620513849,
      "grad_norm": 7.3010358810424805,
      "learning_rate": 4.923810187700848e-05,
      "loss": 3.8428,
      "step": 345600
    },
    {
      "epoch": 0.06096966278679507,
      "grad_norm": 10.523694038391113,
      "learning_rate": 4.923788141973777e-05,
      "loss": 3.7813,
      "step": 345700
    },
    {
      "epoch": 0.060987299368451646,
      "grad_norm": 8.316019058227539,
      "learning_rate": 4.923766096246707e-05,
      "loss": 3.7027,
      "step": 345800
    },
    {
      "epoch": 0.061004935950108224,
      "grad_norm": 5.035008430480957,
      "learning_rate": 4.9237440505196355e-05,
      "loss": 3.7094,
      "step": 345900
    },
    {
      "epoch": 0.0610225725317648,
      "grad_norm": 5.443902015686035,
      "learning_rate": 4.9237220047925644e-05,
      "loss": 3.8577,
      "step": 346000
    },
    {
      "epoch": 0.06104020911342139,
      "grad_norm": 6.1904168128967285,
      "learning_rate": 4.923699959065494e-05,
      "loss": 3.749,
      "step": 346100
    },
    {
      "epoch": 0.06105784569507797,
      "grad_norm": 8.62984848022461,
      "learning_rate": 4.9236779133384235e-05,
      "loss": 3.7872,
      "step": 346200
    },
    {
      "epoch": 0.061075482276734545,
      "grad_norm": 5.9284820556640625,
      "learning_rate": 4.9236558676113524e-05,
      "loss": 3.8218,
      "step": 346300
    },
    {
      "epoch": 0.061093118858391124,
      "grad_norm": 9.547062873840332,
      "learning_rate": 4.923633821884282e-05,
      "loss": 3.7663,
      "step": 346400
    },
    {
      "epoch": 0.0611107554400477,
      "grad_norm": 8.029521942138672,
      "learning_rate": 4.9236117761572115e-05,
      "loss": 3.7935,
      "step": 346500
    },
    {
      "epoch": 0.06112839202170428,
      "grad_norm": 6.112310886383057,
      "learning_rate": 4.92358973043014e-05,
      "loss": 3.7461,
      "step": 346600
    },
    {
      "epoch": 0.06114602860336086,
      "grad_norm": 6.575381755828857,
      "learning_rate": 4.92356768470307e-05,
      "loss": 3.7143,
      "step": 346700
    },
    {
      "epoch": 0.061163665185017445,
      "grad_norm": 10.727660179138184,
      "learning_rate": 4.9235456389759994e-05,
      "loss": 3.7391,
      "step": 346800
    },
    {
      "epoch": 0.06118130176667402,
      "grad_norm": 5.747212886810303,
      "learning_rate": 4.923523593248928e-05,
      "loss": 3.7822,
      "step": 346900
    },
    {
      "epoch": 0.0611989383483306,
      "grad_norm": 11.939757347106934,
      "learning_rate": 4.923501547521858e-05,
      "loss": 3.7567,
      "step": 347000
    },
    {
      "epoch": 0.06121657492998718,
      "grad_norm": 7.411578178405762,
      "learning_rate": 4.9234795017947874e-05,
      "loss": 3.7122,
      "step": 347100
    },
    {
      "epoch": 0.06123421151164376,
      "grad_norm": 8.301542282104492,
      "learning_rate": 4.923457456067716e-05,
      "loss": 3.8526,
      "step": 347200
    },
    {
      "epoch": 0.06125184809330034,
      "grad_norm": 7.238471031188965,
      "learning_rate": 4.923435410340646e-05,
      "loss": 3.7473,
      "step": 347300
    },
    {
      "epoch": 0.061269484674956916,
      "grad_norm": 6.486738681793213,
      "learning_rate": 4.923413364613575e-05,
      "loss": 3.7087,
      "step": 347400
    },
    {
      "epoch": 0.061287121256613494,
      "grad_norm": 6.300641059875488,
      "learning_rate": 4.9233913188865035e-05,
      "loss": 3.752,
      "step": 347500
    },
    {
      "epoch": 0.06130475783827008,
      "grad_norm": 8.143951416015625,
      "learning_rate": 4.923369273159433e-05,
      "loss": 3.6972,
      "step": 347600
    },
    {
      "epoch": 0.06132239441992666,
      "grad_norm": 8.847925186157227,
      "learning_rate": 4.9233472274323626e-05,
      "loss": 3.7912,
      "step": 347700
    },
    {
      "epoch": 0.06134003100158324,
      "grad_norm": 6.645965576171875,
      "learning_rate": 4.9233251817052915e-05,
      "loss": 3.8342,
      "step": 347800
    },
    {
      "epoch": 0.061357667583239815,
      "grad_norm": 6.052443981170654,
      "learning_rate": 4.923303135978221e-05,
      "loss": 3.6748,
      "step": 347900
    },
    {
      "epoch": 0.061375304164896394,
      "grad_norm": 9.493585586547852,
      "learning_rate": 4.9232810902511506e-05,
      "loss": 3.7686,
      "step": 348000
    },
    {
      "epoch": 0.06139294074655297,
      "grad_norm": 8.441768646240234,
      "learning_rate": 4.9232590445240795e-05,
      "loss": 3.7322,
      "step": 348100
    },
    {
      "epoch": 0.06141057732820955,
      "grad_norm": 7.184335231781006,
      "learning_rate": 4.923236998797009e-05,
      "loss": 3.8068,
      "step": 348200
    },
    {
      "epoch": 0.061428213909866136,
      "grad_norm": 6.062726974487305,
      "learning_rate": 4.9232149530699386e-05,
      "loss": 3.7234,
      "step": 348300
    },
    {
      "epoch": 0.061445850491522715,
      "grad_norm": 8.793047904968262,
      "learning_rate": 4.9231929073428674e-05,
      "loss": 3.7329,
      "step": 348400
    },
    {
      "epoch": 0.06146348707317929,
      "grad_norm": 9.549275398254395,
      "learning_rate": 4.923170861615797e-05,
      "loss": 3.6467,
      "step": 348500
    },
    {
      "epoch": 0.06148112365483587,
      "grad_norm": 8.741385459899902,
      "learning_rate": 4.9231488158887265e-05,
      "loss": 3.7314,
      "step": 348600
    },
    {
      "epoch": 0.06149876023649245,
      "grad_norm": 7.546419143676758,
      "learning_rate": 4.9231267701616554e-05,
      "loss": 3.6666,
      "step": 348700
    },
    {
      "epoch": 0.06151639681814903,
      "grad_norm": 7.750029563903809,
      "learning_rate": 4.923104724434584e-05,
      "loss": 3.7638,
      "step": 348800
    },
    {
      "epoch": 0.06153403339980561,
      "grad_norm": 5.582911014556885,
      "learning_rate": 4.923082678707514e-05,
      "loss": 3.716,
      "step": 348900
    },
    {
      "epoch": 0.061551669981462186,
      "grad_norm": 6.25344181060791,
      "learning_rate": 4.923060632980443e-05,
      "loss": 3.7389,
      "step": 349000
    },
    {
      "epoch": 0.06156930656311877,
      "grad_norm": 5.625121593475342,
      "learning_rate": 4.923038587253372e-05,
      "loss": 3.6143,
      "step": 349100
    },
    {
      "epoch": 0.06158694314477535,
      "grad_norm": 7.070201396942139,
      "learning_rate": 4.923016541526302e-05,
      "loss": 3.7597,
      "step": 349200
    },
    {
      "epoch": 0.06160457972643193,
      "grad_norm": 7.205389976501465,
      "learning_rate": 4.9229944957992306e-05,
      "loss": 3.6539,
      "step": 349300
    },
    {
      "epoch": 0.06162221630808851,
      "grad_norm": 9.113938331604004,
      "learning_rate": 4.92297245007216e-05,
      "loss": 3.8319,
      "step": 349400
    },
    {
      "epoch": 0.061639852889745085,
      "grad_norm": 12.369701385498047,
      "learning_rate": 4.92295040434509e-05,
      "loss": 3.7622,
      "step": 349500
    },
    {
      "epoch": 0.061657489471401664,
      "grad_norm": 6.844741344451904,
      "learning_rate": 4.9229283586180186e-05,
      "loss": 3.7044,
      "step": 349600
    },
    {
      "epoch": 0.06167512605305824,
      "grad_norm": 7.304211139678955,
      "learning_rate": 4.922906312890948e-05,
      "loss": 3.7336,
      "step": 349700
    },
    {
      "epoch": 0.06169276263471483,
      "grad_norm": 5.456146240234375,
      "learning_rate": 4.922884267163878e-05,
      "loss": 3.8379,
      "step": 349800
    },
    {
      "epoch": 0.061710399216371406,
      "grad_norm": 8.907281875610352,
      "learning_rate": 4.9228622214368066e-05,
      "loss": 3.6886,
      "step": 349900
    },
    {
      "epoch": 0.061728035798027985,
      "grad_norm": 7.891829013824463,
      "learning_rate": 4.922840175709736e-05,
      "loss": 3.8077,
      "step": 350000
    },
    {
      "epoch": 0.06174567237968456,
      "grad_norm": 7.765510559082031,
      "learning_rate": 4.922818129982666e-05,
      "loss": 3.7552,
      "step": 350100
    },
    {
      "epoch": 0.06176330896134114,
      "grad_norm": 5.990499973297119,
      "learning_rate": 4.9227960842555945e-05,
      "loss": 3.7841,
      "step": 350200
    },
    {
      "epoch": 0.06178094554299772,
      "grad_norm": 6.409586429595947,
      "learning_rate": 4.9227740385285234e-05,
      "loss": 3.8141,
      "step": 350300
    },
    {
      "epoch": 0.0617985821246543,
      "grad_norm": 8.107853889465332,
      "learning_rate": 4.922751992801453e-05,
      "loss": 3.7291,
      "step": 350400
    },
    {
      "epoch": 0.06181621870631088,
      "grad_norm": 7.2544379234313965,
      "learning_rate": 4.9227299470743825e-05,
      "loss": 3.6788,
      "step": 350500
    },
    {
      "epoch": 0.06183385528796746,
      "grad_norm": 5.627926349639893,
      "learning_rate": 4.9227079013473114e-05,
      "loss": 3.7534,
      "step": 350600
    },
    {
      "epoch": 0.06185149186962404,
      "grad_norm": 6.388124942779541,
      "learning_rate": 4.922685855620241e-05,
      "loss": 3.6756,
      "step": 350700
    },
    {
      "epoch": 0.06186912845128062,
      "grad_norm": 9.27502155303955,
      "learning_rate": 4.9226638098931705e-05,
      "loss": 3.7932,
      "step": 350800
    },
    {
      "epoch": 0.0618867650329372,
      "grad_norm": 6.540542125701904,
      "learning_rate": 4.922641764166099e-05,
      "loss": 3.8329,
      "step": 350900
    },
    {
      "epoch": 0.061904401614593776,
      "grad_norm": 8.92776107788086,
      "learning_rate": 4.922619718439029e-05,
      "loss": 3.7558,
      "step": 351000
    },
    {
      "epoch": 0.061922038196250355,
      "grad_norm": 5.791131496429443,
      "learning_rate": 4.9225976727119584e-05,
      "loss": 3.7597,
      "step": 351100
    },
    {
      "epoch": 0.06193967477790693,
      "grad_norm": 6.600589275360107,
      "learning_rate": 4.922575626984887e-05,
      "loss": 3.7707,
      "step": 351200
    },
    {
      "epoch": 0.06195731135956352,
      "grad_norm": 7.600331783294678,
      "learning_rate": 4.922553581257817e-05,
      "loss": 3.7312,
      "step": 351300
    },
    {
      "epoch": 0.0619749479412201,
      "grad_norm": 6.145722389221191,
      "learning_rate": 4.9225315355307464e-05,
      "loss": 3.7488,
      "step": 351400
    },
    {
      "epoch": 0.061992584522876676,
      "grad_norm": 7.608340740203857,
      "learning_rate": 4.922509489803675e-05,
      "loss": 3.7994,
      "step": 351500
    },
    {
      "epoch": 0.062010221104533254,
      "grad_norm": 7.882720947265625,
      "learning_rate": 4.922487444076604e-05,
      "loss": 3.7107,
      "step": 351600
    },
    {
      "epoch": 0.06202785768618983,
      "grad_norm": 7.622560977935791,
      "learning_rate": 4.922465398349534e-05,
      "loss": 3.7235,
      "step": 351700
    },
    {
      "epoch": 0.06204549426784641,
      "grad_norm": 6.810075283050537,
      "learning_rate": 4.9224433526224625e-05,
      "loss": 3.8139,
      "step": 351800
    },
    {
      "epoch": 0.06206313084950299,
      "grad_norm": 7.699434757232666,
      "learning_rate": 4.922421306895392e-05,
      "loss": 3.7525,
      "step": 351900
    },
    {
      "epoch": 0.06208076743115957,
      "grad_norm": 8.78919792175293,
      "learning_rate": 4.9223992611683216e-05,
      "loss": 3.7825,
      "step": 352000
    },
    {
      "epoch": 0.062098404012816154,
      "grad_norm": 9.41281795501709,
      "learning_rate": 4.9223772154412505e-05,
      "loss": 3.7795,
      "step": 352100
    },
    {
      "epoch": 0.06211604059447273,
      "grad_norm": 12.180741310119629,
      "learning_rate": 4.92235516971418e-05,
      "loss": 3.7525,
      "step": 352200
    },
    {
      "epoch": 0.06213367717612931,
      "grad_norm": 10.005213737487793,
      "learning_rate": 4.9223331239871096e-05,
      "loss": 3.6379,
      "step": 352300
    },
    {
      "epoch": 0.06215131375778589,
      "grad_norm": 6.817615985870361,
      "learning_rate": 4.9223110782600385e-05,
      "loss": 3.6157,
      "step": 352400
    },
    {
      "epoch": 0.06216895033944247,
      "grad_norm": 7.622710704803467,
      "learning_rate": 4.922289032532968e-05,
      "loss": 3.8429,
      "step": 352500
    },
    {
      "epoch": 0.062186586921099046,
      "grad_norm": 6.746375560760498,
      "learning_rate": 4.9222669868058976e-05,
      "loss": 3.7933,
      "step": 352600
    },
    {
      "epoch": 0.062204223502755625,
      "grad_norm": 7.969291687011719,
      "learning_rate": 4.9222449410788264e-05,
      "loss": 3.6988,
      "step": 352700
    },
    {
      "epoch": 0.06222186008441221,
      "grad_norm": 6.738916397094727,
      "learning_rate": 4.922222895351756e-05,
      "loss": 3.6774,
      "step": 352800
    },
    {
      "epoch": 0.06223949666606879,
      "grad_norm": 7.777612686157227,
      "learning_rate": 4.9222008496246855e-05,
      "loss": 3.7598,
      "step": 352900
    },
    {
      "epoch": 0.06225713324772537,
      "grad_norm": 7.789290904998779,
      "learning_rate": 4.9221788038976144e-05,
      "loss": 3.8635,
      "step": 353000
    },
    {
      "epoch": 0.062274769829381946,
      "grad_norm": 9.293352127075195,
      "learning_rate": 4.922156758170543e-05,
      "loss": 3.7034,
      "step": 353100
    },
    {
      "epoch": 0.062292406411038524,
      "grad_norm": 9.551055908203125,
      "learning_rate": 4.922134712443473e-05,
      "loss": 3.7493,
      "step": 353200
    },
    {
      "epoch": 0.0623100429926951,
      "grad_norm": 6.869476318359375,
      "learning_rate": 4.922112666716402e-05,
      "loss": 3.6783,
      "step": 353300
    },
    {
      "epoch": 0.06232767957435168,
      "grad_norm": 10.323326110839844,
      "learning_rate": 4.922090620989331e-05,
      "loss": 3.7647,
      "step": 353400
    },
    {
      "epoch": 0.06234531615600826,
      "grad_norm": 8.840083122253418,
      "learning_rate": 4.922068575262261e-05,
      "loss": 3.6665,
      "step": 353500
    },
    {
      "epoch": 0.062362952737664845,
      "grad_norm": 6.824641704559326,
      "learning_rate": 4.9220465295351896e-05,
      "loss": 3.6426,
      "step": 353600
    },
    {
      "epoch": 0.062380589319321424,
      "grad_norm": 9.582900047302246,
      "learning_rate": 4.922024483808119e-05,
      "loss": 3.6922,
      "step": 353700
    },
    {
      "epoch": 0.062398225900978,
      "grad_norm": 7.241106986999512,
      "learning_rate": 4.922002438081049e-05,
      "loss": 3.7327,
      "step": 353800
    },
    {
      "epoch": 0.06241586248263458,
      "grad_norm": 6.471708297729492,
      "learning_rate": 4.9219803923539776e-05,
      "loss": 3.6187,
      "step": 353900
    },
    {
      "epoch": 0.06243349906429116,
      "grad_norm": 7.060819625854492,
      "learning_rate": 4.921958346626907e-05,
      "loss": 3.6141,
      "step": 354000
    },
    {
      "epoch": 0.06245113564594774,
      "grad_norm": 6.754120826721191,
      "learning_rate": 4.921936300899837e-05,
      "loss": 3.7301,
      "step": 354100
    },
    {
      "epoch": 0.062468772227604316,
      "grad_norm": 8.70278263092041,
      "learning_rate": 4.9219142551727656e-05,
      "loss": 3.628,
      "step": 354200
    },
    {
      "epoch": 0.0624864088092609,
      "grad_norm": 6.530391693115234,
      "learning_rate": 4.921892209445695e-05,
      "loss": 3.7426,
      "step": 354300
    },
    {
      "epoch": 0.06250404539091747,
      "grad_norm": 6.104455947875977,
      "learning_rate": 4.921870163718624e-05,
      "loss": 3.7003,
      "step": 354400
    },
    {
      "epoch": 0.06252168197257406,
      "grad_norm": 8.13643741607666,
      "learning_rate": 4.9218481179915535e-05,
      "loss": 3.7343,
      "step": 354500
    },
    {
      "epoch": 0.06253931855423063,
      "grad_norm": 6.008492946624756,
      "learning_rate": 4.9218260722644824e-05,
      "loss": 3.7661,
      "step": 354600
    },
    {
      "epoch": 0.06255695513588722,
      "grad_norm": 9.727729797363281,
      "learning_rate": 4.921804026537412e-05,
      "loss": 3.6845,
      "step": 354700
    },
    {
      "epoch": 0.0625745917175438,
      "grad_norm": 11.827803611755371,
      "learning_rate": 4.921781980810341e-05,
      "loss": 3.6989,
      "step": 354800
    },
    {
      "epoch": 0.06259222829920037,
      "grad_norm": 7.861766338348389,
      "learning_rate": 4.9217599350832704e-05,
      "loss": 3.8273,
      "step": 354900
    },
    {
      "epoch": 0.06260986488085696,
      "grad_norm": 10.52082347869873,
      "learning_rate": 4.9217378893562e-05,
      "loss": 3.6681,
      "step": 355000
    },
    {
      "epoch": 0.06262750146251353,
      "grad_norm": 9.381030082702637,
      "learning_rate": 4.921715843629129e-05,
      "loss": 3.7394,
      "step": 355100
    },
    {
      "epoch": 0.06264513804417011,
      "grad_norm": 9.864814758300781,
      "learning_rate": 4.921693797902058e-05,
      "loss": 3.7192,
      "step": 355200
    },
    {
      "epoch": 0.06266277462582669,
      "grad_norm": 7.877662181854248,
      "learning_rate": 4.921671752174988e-05,
      "loss": 3.7142,
      "step": 355300
    },
    {
      "epoch": 0.06268041120748327,
      "grad_norm": 6.736419200897217,
      "learning_rate": 4.921649706447917e-05,
      "loss": 3.7644,
      "step": 355400
    },
    {
      "epoch": 0.06269804778913986,
      "grad_norm": 6.296275615692139,
      "learning_rate": 4.921627660720846e-05,
      "loss": 3.6431,
      "step": 355500
    },
    {
      "epoch": 0.06271568437079643,
      "grad_norm": 10.214816093444824,
      "learning_rate": 4.921605614993776e-05,
      "loss": 3.7755,
      "step": 355600
    },
    {
      "epoch": 0.06273332095245301,
      "grad_norm": 5.848552703857422,
      "learning_rate": 4.921583569266705e-05,
      "loss": 3.7887,
      "step": 355700
    },
    {
      "epoch": 0.06275095753410959,
      "grad_norm": 10.32754135131836,
      "learning_rate": 4.921561523539634e-05,
      "loss": 3.8688,
      "step": 355800
    },
    {
      "epoch": 0.06276859411576617,
      "grad_norm": 7.2678656578063965,
      "learning_rate": 4.921539477812563e-05,
      "loss": 3.6704,
      "step": 355900
    },
    {
      "epoch": 0.06278623069742274,
      "grad_norm": 7.234210014343262,
      "learning_rate": 4.921517432085492e-05,
      "loss": 3.7366,
      "step": 356000
    },
    {
      "epoch": 0.06280386727907933,
      "grad_norm": 6.553335189819336,
      "learning_rate": 4.9214953863584215e-05,
      "loss": 3.7374,
      "step": 356100
    },
    {
      "epoch": 0.06282150386073591,
      "grad_norm": 9.355865478515625,
      "learning_rate": 4.921473340631351e-05,
      "loss": 3.6861,
      "step": 356200
    },
    {
      "epoch": 0.06283914044239249,
      "grad_norm": 12.244987487792969,
      "learning_rate": 4.92145129490428e-05,
      "loss": 3.5403,
      "step": 356300
    },
    {
      "epoch": 0.06285677702404907,
      "grad_norm": 7.094109535217285,
      "learning_rate": 4.9214292491772095e-05,
      "loss": 3.7727,
      "step": 356400
    },
    {
      "epoch": 0.06287441360570564,
      "grad_norm": 5.54910945892334,
      "learning_rate": 4.921407203450139e-05,
      "loss": 3.651,
      "step": 356500
    },
    {
      "epoch": 0.06289205018736223,
      "grad_norm": 12.823349952697754,
      "learning_rate": 4.921385157723068e-05,
      "loss": 3.6733,
      "step": 356600
    },
    {
      "epoch": 0.0629096867690188,
      "grad_norm": 7.789220809936523,
      "learning_rate": 4.9213631119959975e-05,
      "loss": 3.7606,
      "step": 356700
    },
    {
      "epoch": 0.06292732335067538,
      "grad_norm": 8.975091934204102,
      "learning_rate": 4.921341066268927e-05,
      "loss": 3.7723,
      "step": 356800
    },
    {
      "epoch": 0.06294495993233197,
      "grad_norm": 6.252924919128418,
      "learning_rate": 4.921319020541856e-05,
      "loss": 3.7219,
      "step": 356900
    },
    {
      "epoch": 0.06296259651398854,
      "grad_norm": 7.919688701629639,
      "learning_rate": 4.9212969748147854e-05,
      "loss": 3.7221,
      "step": 357000
    },
    {
      "epoch": 0.06298023309564513,
      "grad_norm": 8.982651710510254,
      "learning_rate": 4.921274929087715e-05,
      "loss": 3.7305,
      "step": 357100
    },
    {
      "epoch": 0.0629978696773017,
      "grad_norm": 7.650363445281982,
      "learning_rate": 4.921252883360644e-05,
      "loss": 3.7209,
      "step": 357200
    },
    {
      "epoch": 0.06301550625895828,
      "grad_norm": 6.3671135902404785,
      "learning_rate": 4.9212308376335734e-05,
      "loss": 3.6894,
      "step": 357300
    },
    {
      "epoch": 0.06303314284061486,
      "grad_norm": 7.29524040222168,
      "learning_rate": 4.921208791906502e-05,
      "loss": 3.7496,
      "step": 357400
    },
    {
      "epoch": 0.06305077942227144,
      "grad_norm": 12.6641845703125,
      "learning_rate": 4.921186746179431e-05,
      "loss": 3.7266,
      "step": 357500
    },
    {
      "epoch": 0.06306841600392801,
      "grad_norm": 6.260845184326172,
      "learning_rate": 4.921164700452361e-05,
      "loss": 3.6902,
      "step": 357600
    },
    {
      "epoch": 0.0630860525855846,
      "grad_norm": 8.889199256896973,
      "learning_rate": 4.92114265472529e-05,
      "loss": 3.7023,
      "step": 357700
    },
    {
      "epoch": 0.06310368916724118,
      "grad_norm": 7.985721111297607,
      "learning_rate": 4.921120608998219e-05,
      "loss": 3.7538,
      "step": 357800
    },
    {
      "epoch": 0.06312132574889776,
      "grad_norm": 8.719188690185547,
      "learning_rate": 4.9210985632711486e-05,
      "loss": 3.8541,
      "step": 357900
    },
    {
      "epoch": 0.06313896233055434,
      "grad_norm": 6.890449047088623,
      "learning_rate": 4.921076517544078e-05,
      "loss": 3.7561,
      "step": 358000
    },
    {
      "epoch": 0.06315659891221091,
      "grad_norm": 6.300353050231934,
      "learning_rate": 4.921054471817007e-05,
      "loss": 3.7753,
      "step": 358100
    },
    {
      "epoch": 0.0631742354938675,
      "grad_norm": 6.881195068359375,
      "learning_rate": 4.9210324260899366e-05,
      "loss": 3.5941,
      "step": 358200
    },
    {
      "epoch": 0.06319187207552407,
      "grad_norm": 8.369613647460938,
      "learning_rate": 4.921010380362866e-05,
      "loss": 3.7416,
      "step": 358300
    },
    {
      "epoch": 0.06320950865718065,
      "grad_norm": 8.440408706665039,
      "learning_rate": 4.920988334635795e-05,
      "loss": 3.7394,
      "step": 358400
    },
    {
      "epoch": 0.06322714523883724,
      "grad_norm": 6.374676704406738,
      "learning_rate": 4.9209662889087246e-05,
      "loss": 3.7003,
      "step": 358500
    },
    {
      "epoch": 0.06324478182049381,
      "grad_norm": 7.646557331085205,
      "learning_rate": 4.920944243181654e-05,
      "loss": 3.7722,
      "step": 358600
    },
    {
      "epoch": 0.0632624184021504,
      "grad_norm": 5.875401496887207,
      "learning_rate": 4.920922197454583e-05,
      "loss": 3.7779,
      "step": 358700
    },
    {
      "epoch": 0.06328005498380697,
      "grad_norm": 8.60869312286377,
      "learning_rate": 4.920900151727512e-05,
      "loss": 3.725,
      "step": 358800
    },
    {
      "epoch": 0.06329769156546355,
      "grad_norm": 9.610249519348145,
      "learning_rate": 4.9208781060004414e-05,
      "loss": 3.8101,
      "step": 358900
    },
    {
      "epoch": 0.06331532814712013,
      "grad_norm": 8.333880424499512,
      "learning_rate": 4.92085606027337e-05,
      "loss": 3.5986,
      "step": 359000
    },
    {
      "epoch": 0.06333296472877671,
      "grad_norm": 6.211960315704346,
      "learning_rate": 4.9208340145463e-05,
      "loss": 3.6727,
      "step": 359100
    },
    {
      "epoch": 0.0633506013104333,
      "grad_norm": 8.900796890258789,
      "learning_rate": 4.9208119688192294e-05,
      "loss": 3.8128,
      "step": 359200
    },
    {
      "epoch": 0.06336823789208987,
      "grad_norm": 6.763883590698242,
      "learning_rate": 4.920789923092158e-05,
      "loss": 3.7606,
      "step": 359300
    },
    {
      "epoch": 0.06338587447374645,
      "grad_norm": 8.683380126953125,
      "learning_rate": 4.920767877365088e-05,
      "loss": 3.6717,
      "step": 359400
    },
    {
      "epoch": 0.06340351105540303,
      "grad_norm": 5.705942153930664,
      "learning_rate": 4.920745831638017e-05,
      "loss": 3.7046,
      "step": 359500
    },
    {
      "epoch": 0.06342114763705961,
      "grad_norm": 6.053652763366699,
      "learning_rate": 4.920723785910946e-05,
      "loss": 3.7272,
      "step": 359600
    },
    {
      "epoch": 0.06343878421871618,
      "grad_norm": 6.219960689544678,
      "learning_rate": 4.920701740183876e-05,
      "loss": 3.8401,
      "step": 359700
    },
    {
      "epoch": 0.06345642080037277,
      "grad_norm": 6.096341133117676,
      "learning_rate": 4.920679694456805e-05,
      "loss": 3.6667,
      "step": 359800
    },
    {
      "epoch": 0.06347405738202935,
      "grad_norm": 10.09280776977539,
      "learning_rate": 4.920657648729734e-05,
      "loss": 3.7783,
      "step": 359900
    },
    {
      "epoch": 0.06349169396368592,
      "grad_norm": 6.364275932312012,
      "learning_rate": 4.920635603002664e-05,
      "loss": 3.6704,
      "step": 360000
    },
    {
      "epoch": 0.06350933054534251,
      "grad_norm": 7.241750717163086,
      "learning_rate": 4.920613557275593e-05,
      "loss": 3.8002,
      "step": 360100
    },
    {
      "epoch": 0.06352696712699908,
      "grad_norm": 6.926883220672607,
      "learning_rate": 4.920591511548522e-05,
      "loss": 3.8247,
      "step": 360200
    },
    {
      "epoch": 0.06354460370865567,
      "grad_norm": 6.902170658111572,
      "learning_rate": 4.920569465821451e-05,
      "loss": 3.8564,
      "step": 360300
    },
    {
      "epoch": 0.06356224029031224,
      "grad_norm": 5.636000633239746,
      "learning_rate": 4.9205474200943805e-05,
      "loss": 3.7706,
      "step": 360400
    },
    {
      "epoch": 0.06357987687196882,
      "grad_norm": 5.030237674713135,
      "learning_rate": 4.9205253743673094e-05,
      "loss": 3.7508,
      "step": 360500
    },
    {
      "epoch": 0.0635975134536254,
      "grad_norm": 6.457301616668701,
      "learning_rate": 4.920503328640239e-05,
      "loss": 3.7918,
      "step": 360600
    },
    {
      "epoch": 0.06361515003528198,
      "grad_norm": 6.679937839508057,
      "learning_rate": 4.9204812829131685e-05,
      "loss": 3.7763,
      "step": 360700
    },
    {
      "epoch": 0.06363278661693857,
      "grad_norm": 7.610947608947754,
      "learning_rate": 4.9204592371860974e-05,
      "loss": 3.7203,
      "step": 360800
    },
    {
      "epoch": 0.06365042319859514,
      "grad_norm": 10.361973762512207,
      "learning_rate": 4.920437191459027e-05,
      "loss": 3.7133,
      "step": 360900
    },
    {
      "epoch": 0.06366805978025172,
      "grad_norm": 6.65485954284668,
      "learning_rate": 4.9204151457319565e-05,
      "loss": 3.851,
      "step": 361000
    },
    {
      "epoch": 0.0636856963619083,
      "grad_norm": 7.015462875366211,
      "learning_rate": 4.920393100004885e-05,
      "loss": 3.7207,
      "step": 361100
    },
    {
      "epoch": 0.06370333294356488,
      "grad_norm": 8.016936302185059,
      "learning_rate": 4.920371054277815e-05,
      "loss": 3.8071,
      "step": 361200
    },
    {
      "epoch": 0.06372096952522145,
      "grad_norm": 5.980036735534668,
      "learning_rate": 4.9203490085507444e-05,
      "loss": 3.727,
      "step": 361300
    },
    {
      "epoch": 0.06373860610687804,
      "grad_norm": 7.278775215148926,
      "learning_rate": 4.920326962823674e-05,
      "loss": 3.7213,
      "step": 361400
    },
    {
      "epoch": 0.06375624268853462,
      "grad_norm": 10.561996459960938,
      "learning_rate": 4.920304917096603e-05,
      "loss": 3.7079,
      "step": 361500
    },
    {
      "epoch": 0.0637738792701912,
      "grad_norm": 6.07479190826416,
      "learning_rate": 4.920282871369532e-05,
      "loss": 3.727,
      "step": 361600
    },
    {
      "epoch": 0.06379151585184778,
      "grad_norm": 7.745866298675537,
      "learning_rate": 4.920260825642461e-05,
      "loss": 3.617,
      "step": 361700
    },
    {
      "epoch": 0.06380915243350435,
      "grad_norm": 6.610787391662598,
      "learning_rate": 4.92023877991539e-05,
      "loss": 3.7557,
      "step": 361800
    },
    {
      "epoch": 0.06382678901516094,
      "grad_norm": 9.16538143157959,
      "learning_rate": 4.92021673418832e-05,
      "loss": 3.6211,
      "step": 361900
    },
    {
      "epoch": 0.06384442559681751,
      "grad_norm": 6.925276279449463,
      "learning_rate": 4.920194688461249e-05,
      "loss": 3.794,
      "step": 362000
    },
    {
      "epoch": 0.0638620621784741,
      "grad_norm": 7.5937910079956055,
      "learning_rate": 4.920172642734178e-05,
      "loss": 3.6532,
      "step": 362100
    },
    {
      "epoch": 0.06387969876013068,
      "grad_norm": 8.265151023864746,
      "learning_rate": 4.9201505970071076e-05,
      "loss": 3.6922,
      "step": 362200
    },
    {
      "epoch": 0.06389733534178725,
      "grad_norm": 10.891761779785156,
      "learning_rate": 4.920128551280037e-05,
      "loss": 3.8692,
      "step": 362300
    },
    {
      "epoch": 0.06391497192344384,
      "grad_norm": 7.561089992523193,
      "learning_rate": 4.920106505552966e-05,
      "loss": 3.8412,
      "step": 362400
    },
    {
      "epoch": 0.06393260850510041,
      "grad_norm": 8.181053161621094,
      "learning_rate": 4.9200844598258956e-05,
      "loss": 3.7302,
      "step": 362500
    },
    {
      "epoch": 0.063950245086757,
      "grad_norm": 6.220739841461182,
      "learning_rate": 4.920062414098825e-05,
      "loss": 3.7503,
      "step": 362600
    },
    {
      "epoch": 0.06396788166841356,
      "grad_norm": 7.2982964515686035,
      "learning_rate": 4.920040368371754e-05,
      "loss": 3.7642,
      "step": 362700
    },
    {
      "epoch": 0.06398551825007015,
      "grad_norm": 6.8257036209106445,
      "learning_rate": 4.9200183226446836e-05,
      "loss": 3.7015,
      "step": 362800
    },
    {
      "epoch": 0.06400315483172674,
      "grad_norm": 9.105173110961914,
      "learning_rate": 4.919996276917613e-05,
      "loss": 3.6312,
      "step": 362900
    },
    {
      "epoch": 0.06402079141338331,
      "grad_norm": 6.962555408477783,
      "learning_rate": 4.919974231190542e-05,
      "loss": 3.791,
      "step": 363000
    },
    {
      "epoch": 0.06403842799503989,
      "grad_norm": 6.525742530822754,
      "learning_rate": 4.919952185463471e-05,
      "loss": 3.6564,
      "step": 363100
    },
    {
      "epoch": 0.06405606457669646,
      "grad_norm": 7.783652305603027,
      "learning_rate": 4.9199301397364004e-05,
      "loss": 3.6778,
      "step": 363200
    },
    {
      "epoch": 0.06407370115835305,
      "grad_norm": 8.675312995910645,
      "learning_rate": 4.919908094009329e-05,
      "loss": 3.6423,
      "step": 363300
    },
    {
      "epoch": 0.06409133774000962,
      "grad_norm": 7.491997718811035,
      "learning_rate": 4.919886048282259e-05,
      "loss": 3.823,
      "step": 363400
    },
    {
      "epoch": 0.0641089743216662,
      "grad_norm": 7.613797664642334,
      "learning_rate": 4.9198640025551883e-05,
      "loss": 3.7369,
      "step": 363500
    },
    {
      "epoch": 0.06412661090332278,
      "grad_norm": 7.371021747589111,
      "learning_rate": 4.919841956828117e-05,
      "loss": 3.7165,
      "step": 363600
    },
    {
      "epoch": 0.06414424748497936,
      "grad_norm": 7.614805221557617,
      "learning_rate": 4.919819911101047e-05,
      "loss": 3.8346,
      "step": 363700
    },
    {
      "epoch": 0.06416188406663595,
      "grad_norm": 7.159060478210449,
      "learning_rate": 4.919797865373976e-05,
      "loss": 3.8186,
      "step": 363800
    },
    {
      "epoch": 0.06417952064829252,
      "grad_norm": 6.25709342956543,
      "learning_rate": 4.919775819646905e-05,
      "loss": 3.8188,
      "step": 363900
    },
    {
      "epoch": 0.0641971572299491,
      "grad_norm": 8.469935417175293,
      "learning_rate": 4.919753773919835e-05,
      "loss": 3.7539,
      "step": 364000
    },
    {
      "epoch": 0.06421479381160568,
      "grad_norm": 12.487884521484375,
      "learning_rate": 4.919731728192764e-05,
      "loss": 3.701,
      "step": 364100
    },
    {
      "epoch": 0.06423243039326226,
      "grad_norm": 6.577757358551025,
      "learning_rate": 4.919709682465693e-05,
      "loss": 3.8075,
      "step": 364200
    },
    {
      "epoch": 0.06425006697491883,
      "grad_norm": 6.812063217163086,
      "learning_rate": 4.919687636738623e-05,
      "loss": 3.8716,
      "step": 364300
    },
    {
      "epoch": 0.06426770355657542,
      "grad_norm": 7.1713151931762695,
      "learning_rate": 4.9196655910115516e-05,
      "loss": 3.8103,
      "step": 364400
    },
    {
      "epoch": 0.064285340138232,
      "grad_norm": 7.536594867706299,
      "learning_rate": 4.919643545284481e-05,
      "loss": 3.7321,
      "step": 364500
    },
    {
      "epoch": 0.06430297671988858,
      "grad_norm": 8.328235626220703,
      "learning_rate": 4.91962149955741e-05,
      "loss": 3.7164,
      "step": 364600
    },
    {
      "epoch": 0.06432061330154516,
      "grad_norm": 6.632905960083008,
      "learning_rate": 4.9195994538303395e-05,
      "loss": 3.7326,
      "step": 364700
    },
    {
      "epoch": 0.06433824988320173,
      "grad_norm": 8.38687515258789,
      "learning_rate": 4.9195774081032684e-05,
      "loss": 3.8065,
      "step": 364800
    },
    {
      "epoch": 0.06435588646485832,
      "grad_norm": 6.829151630401611,
      "learning_rate": 4.919555362376198e-05,
      "loss": 3.7258,
      "step": 364900
    },
    {
      "epoch": 0.06437352304651489,
      "grad_norm": 5.117142677307129,
      "learning_rate": 4.9195333166491275e-05,
      "loss": 3.7371,
      "step": 365000
    },
    {
      "epoch": 0.06439115962817148,
      "grad_norm": 6.505436897277832,
      "learning_rate": 4.9195112709220564e-05,
      "loss": 3.7147,
      "step": 365100
    },
    {
      "epoch": 0.06440879620982806,
      "grad_norm": 7.1050124168396,
      "learning_rate": 4.919489225194986e-05,
      "loss": 3.7204,
      "step": 365200
    },
    {
      "epoch": 0.06442643279148463,
      "grad_norm": 6.8099188804626465,
      "learning_rate": 4.9194671794679154e-05,
      "loss": 3.7369,
      "step": 365300
    },
    {
      "epoch": 0.06444406937314122,
      "grad_norm": 8.429821014404297,
      "learning_rate": 4.919445133740844e-05,
      "loss": 3.7282,
      "step": 365400
    },
    {
      "epoch": 0.06446170595479779,
      "grad_norm": 5.550673484802246,
      "learning_rate": 4.919423088013774e-05,
      "loss": 3.7202,
      "step": 365500
    },
    {
      "epoch": 0.06447934253645438,
      "grad_norm": 9.427590370178223,
      "learning_rate": 4.9194010422867034e-05,
      "loss": 3.6413,
      "step": 365600
    },
    {
      "epoch": 0.06449697911811095,
      "grad_norm": 6.241054058074951,
      "learning_rate": 4.919378996559632e-05,
      "loss": 3.6611,
      "step": 365700
    },
    {
      "epoch": 0.06451461569976753,
      "grad_norm": 15.076991081237793,
      "learning_rate": 4.919356950832562e-05,
      "loss": 3.7668,
      "step": 365800
    },
    {
      "epoch": 0.06453225228142412,
      "grad_norm": 7.246580123901367,
      "learning_rate": 4.919334905105491e-05,
      "loss": 3.6866,
      "step": 365900
    },
    {
      "epoch": 0.06454988886308069,
      "grad_norm": 5.631277561187744,
      "learning_rate": 4.9193128593784196e-05,
      "loss": 3.7587,
      "step": 366000
    },
    {
      "epoch": 0.06456752544473728,
      "grad_norm": 10.31214427947998,
      "learning_rate": 4.919290813651349e-05,
      "loss": 3.7693,
      "step": 366100
    },
    {
      "epoch": 0.06458516202639385,
      "grad_norm": 5.507120609283447,
      "learning_rate": 4.9192687679242787e-05,
      "loss": 3.7486,
      "step": 366200
    },
    {
      "epoch": 0.06460279860805043,
      "grad_norm": 7.69681453704834,
      "learning_rate": 4.9192467221972075e-05,
      "loss": 3.7456,
      "step": 366300
    },
    {
      "epoch": 0.064620435189707,
      "grad_norm": 7.817460536956787,
      "learning_rate": 4.919224676470137e-05,
      "loss": 3.7643,
      "step": 366400
    },
    {
      "epoch": 0.06463807177136359,
      "grad_norm": 6.8830246925354,
      "learning_rate": 4.9192026307430666e-05,
      "loss": 3.6939,
      "step": 366500
    },
    {
      "epoch": 0.06465570835302016,
      "grad_norm": 7.1218671798706055,
      "learning_rate": 4.9191805850159955e-05,
      "loss": 3.7619,
      "step": 366600
    },
    {
      "epoch": 0.06467334493467675,
      "grad_norm": 5.648975372314453,
      "learning_rate": 4.919158539288925e-05,
      "loss": 3.7377,
      "step": 366700
    },
    {
      "epoch": 0.06469098151633333,
      "grad_norm": 6.271511554718018,
      "learning_rate": 4.9191364935618546e-05,
      "loss": 3.6665,
      "step": 366800
    },
    {
      "epoch": 0.0647086180979899,
      "grad_norm": 11.396541595458984,
      "learning_rate": 4.9191144478347835e-05,
      "loss": 3.7594,
      "step": 366900
    },
    {
      "epoch": 0.06472625467964649,
      "grad_norm": 7.788951873779297,
      "learning_rate": 4.919092402107713e-05,
      "loss": 3.6937,
      "step": 367000
    },
    {
      "epoch": 0.06474389126130306,
      "grad_norm": 6.883339881896973,
      "learning_rate": 4.9190703563806425e-05,
      "loss": 3.7873,
      "step": 367100
    },
    {
      "epoch": 0.06476152784295965,
      "grad_norm": 7.2699785232543945,
      "learning_rate": 4.9190483106535714e-05,
      "loss": 3.7327,
      "step": 367200
    },
    {
      "epoch": 0.06477916442461622,
      "grad_norm": 7.350027561187744,
      "learning_rate": 4.919026264926501e-05,
      "loss": 3.7358,
      "step": 367300
    },
    {
      "epoch": 0.0647968010062728,
      "grad_norm": 5.971551895141602,
      "learning_rate": 4.91900421919943e-05,
      "loss": 3.79,
      "step": 367400
    },
    {
      "epoch": 0.06481443758792939,
      "grad_norm": 7.187914848327637,
      "learning_rate": 4.918982173472359e-05,
      "loss": 3.7929,
      "step": 367500
    },
    {
      "epoch": 0.06483207416958596,
      "grad_norm": 8.942702293395996,
      "learning_rate": 4.918960127745288e-05,
      "loss": 3.6537,
      "step": 367600
    },
    {
      "epoch": 0.06484971075124255,
      "grad_norm": 7.149214267730713,
      "learning_rate": 4.918938082018218e-05,
      "loss": 3.766,
      "step": 367700
    },
    {
      "epoch": 0.06486734733289912,
      "grad_norm": 8.112969398498535,
      "learning_rate": 4.918916036291147e-05,
      "loss": 3.7961,
      "step": 367800
    },
    {
      "epoch": 0.0648849839145557,
      "grad_norm": 7.9688544273376465,
      "learning_rate": 4.918893990564076e-05,
      "loss": 3.7928,
      "step": 367900
    },
    {
      "epoch": 0.06490262049621227,
      "grad_norm": 5.6361894607543945,
      "learning_rate": 4.918871944837006e-05,
      "loss": 3.7402,
      "step": 368000
    },
    {
      "epoch": 0.06492025707786886,
      "grad_norm": 9.994897842407227,
      "learning_rate": 4.9188498991099346e-05,
      "loss": 3.6958,
      "step": 368100
    },
    {
      "epoch": 0.06493789365952544,
      "grad_norm": 9.927319526672363,
      "learning_rate": 4.918827853382864e-05,
      "loss": 3.716,
      "step": 368200
    },
    {
      "epoch": 0.06495553024118202,
      "grad_norm": 7.7181243896484375,
      "learning_rate": 4.918805807655794e-05,
      "loss": 3.6814,
      "step": 368300
    },
    {
      "epoch": 0.0649731668228386,
      "grad_norm": 6.733609676361084,
      "learning_rate": 4.9187837619287226e-05,
      "loss": 3.7615,
      "step": 368400
    },
    {
      "epoch": 0.06499080340449517,
      "grad_norm": 6.406012058258057,
      "learning_rate": 4.918761716201652e-05,
      "loss": 3.6306,
      "step": 368500
    },
    {
      "epoch": 0.06500843998615176,
      "grad_norm": 7.615968704223633,
      "learning_rate": 4.918739670474582e-05,
      "loss": 3.7554,
      "step": 368600
    },
    {
      "epoch": 0.06502607656780833,
      "grad_norm": 7.870471477508545,
      "learning_rate": 4.9187176247475106e-05,
      "loss": 3.7513,
      "step": 368700
    },
    {
      "epoch": 0.06504371314946492,
      "grad_norm": 9.087803840637207,
      "learning_rate": 4.9186955790204394e-05,
      "loss": 3.7466,
      "step": 368800
    },
    {
      "epoch": 0.0650613497311215,
      "grad_norm": 4.651310920715332,
      "learning_rate": 4.918673533293369e-05,
      "loss": 3.8176,
      "step": 368900
    },
    {
      "epoch": 0.06507898631277807,
      "grad_norm": 7.166528701782227,
      "learning_rate": 4.918651487566298e-05,
      "loss": 3.7829,
      "step": 369000
    },
    {
      "epoch": 0.06509662289443466,
      "grad_norm": 7.116058826446533,
      "learning_rate": 4.9186294418392274e-05,
      "loss": 3.7678,
      "step": 369100
    },
    {
      "epoch": 0.06511425947609123,
      "grad_norm": 6.908496379852295,
      "learning_rate": 4.918607396112157e-05,
      "loss": 3.7844,
      "step": 369200
    },
    {
      "epoch": 0.06513189605774782,
      "grad_norm": 7.806757926940918,
      "learning_rate": 4.918585350385086e-05,
      "loss": 3.6708,
      "step": 369300
    },
    {
      "epoch": 0.06514953263940439,
      "grad_norm": 6.1425909996032715,
      "learning_rate": 4.9185633046580154e-05,
      "loss": 3.7071,
      "step": 369400
    },
    {
      "epoch": 0.06516716922106097,
      "grad_norm": 14.842976570129395,
      "learning_rate": 4.918541258930945e-05,
      "loss": 3.7791,
      "step": 369500
    },
    {
      "epoch": 0.06518480580271754,
      "grad_norm": 7.010107040405273,
      "learning_rate": 4.918519213203874e-05,
      "loss": 3.669,
      "step": 369600
    },
    {
      "epoch": 0.06520244238437413,
      "grad_norm": 8.12656307220459,
      "learning_rate": 4.918497167476803e-05,
      "loss": 3.6603,
      "step": 369700
    },
    {
      "epoch": 0.06522007896603071,
      "grad_norm": 7.017056941986084,
      "learning_rate": 4.918475121749733e-05,
      "loss": 3.8083,
      "step": 369800
    },
    {
      "epoch": 0.06523771554768729,
      "grad_norm": 7.183920383453369,
      "learning_rate": 4.918453076022662e-05,
      "loss": 3.7201,
      "step": 369900
    },
    {
      "epoch": 0.06525535212934387,
      "grad_norm": 6.177273750305176,
      "learning_rate": 4.918431030295591e-05,
      "loss": 3.7925,
      "step": 370000
    },
    {
      "epoch": 0.06527298871100044,
      "grad_norm": 8.424572944641113,
      "learning_rate": 4.918408984568521e-05,
      "loss": 3.7512,
      "step": 370100
    },
    {
      "epoch": 0.06529062529265703,
      "grad_norm": 5.612263202667236,
      "learning_rate": 4.91838693884145e-05,
      "loss": 3.7155,
      "step": 370200
    },
    {
      "epoch": 0.0653082618743136,
      "grad_norm": 7.489047527313232,
      "learning_rate": 4.9183648931143786e-05,
      "loss": 3.8485,
      "step": 370300
    },
    {
      "epoch": 0.06532589845597019,
      "grad_norm": 7.960988521575928,
      "learning_rate": 4.918342847387308e-05,
      "loss": 3.7525,
      "step": 370400
    },
    {
      "epoch": 0.06534353503762677,
      "grad_norm": 8.946788787841797,
      "learning_rate": 4.918320801660237e-05,
      "loss": 3.6453,
      "step": 370500
    },
    {
      "epoch": 0.06536117161928334,
      "grad_norm": 10.290329933166504,
      "learning_rate": 4.9182987559331665e-05,
      "loss": 3.645,
      "step": 370600
    },
    {
      "epoch": 0.06537880820093993,
      "grad_norm": 7.0604567527771,
      "learning_rate": 4.918276710206096e-05,
      "loss": 3.6761,
      "step": 370700
    },
    {
      "epoch": 0.0653964447825965,
      "grad_norm": 4.856434345245361,
      "learning_rate": 4.918254664479025e-05,
      "loss": 3.6723,
      "step": 370800
    },
    {
      "epoch": 0.06541408136425308,
      "grad_norm": 6.082362174987793,
      "learning_rate": 4.9182326187519545e-05,
      "loss": 3.7538,
      "step": 370900
    },
    {
      "epoch": 0.06543171794590966,
      "grad_norm": 6.29967737197876,
      "learning_rate": 4.918210573024884e-05,
      "loss": 3.6495,
      "step": 371000
    },
    {
      "epoch": 0.06544935452756624,
      "grad_norm": 9.340665817260742,
      "learning_rate": 4.918188527297813e-05,
      "loss": 3.6068,
      "step": 371100
    },
    {
      "epoch": 0.06546699110922283,
      "grad_norm": 7.704825401306152,
      "learning_rate": 4.9181664815707425e-05,
      "loss": 3.7071,
      "step": 371200
    },
    {
      "epoch": 0.0654846276908794,
      "grad_norm": 6.681672096252441,
      "learning_rate": 4.918144435843672e-05,
      "loss": 3.6528,
      "step": 371300
    },
    {
      "epoch": 0.06550226427253598,
      "grad_norm": 7.514021873474121,
      "learning_rate": 4.918122390116601e-05,
      "loss": 3.6903,
      "step": 371400
    },
    {
      "epoch": 0.06551990085419256,
      "grad_norm": 7.243884086608887,
      "learning_rate": 4.9181003443895304e-05,
      "loss": 3.7774,
      "step": 371500
    },
    {
      "epoch": 0.06553753743584914,
      "grad_norm": 6.559919834136963,
      "learning_rate": 4.918078298662459e-05,
      "loss": 3.7852,
      "step": 371600
    },
    {
      "epoch": 0.06555517401750571,
      "grad_norm": 5.882087707519531,
      "learning_rate": 4.918056252935389e-05,
      "loss": 3.7472,
      "step": 371700
    },
    {
      "epoch": 0.0655728105991623,
      "grad_norm": 8.029996871948242,
      "learning_rate": 4.918034207208318e-05,
      "loss": 3.7358,
      "step": 371800
    },
    {
      "epoch": 0.06559044718081888,
      "grad_norm": 8.739194869995117,
      "learning_rate": 4.918012161481247e-05,
      "loss": 3.6036,
      "step": 371900
    },
    {
      "epoch": 0.06560808376247546,
      "grad_norm": 16.335132598876953,
      "learning_rate": 4.917990115754177e-05,
      "loss": 3.8123,
      "step": 372000
    },
    {
      "epoch": 0.06562572034413204,
      "grad_norm": 6.629660129547119,
      "learning_rate": 4.917968070027106e-05,
      "loss": 3.6194,
      "step": 372100
    },
    {
      "epoch": 0.06564335692578861,
      "grad_norm": 8.175972938537598,
      "learning_rate": 4.917946024300035e-05,
      "loss": 3.7593,
      "step": 372200
    },
    {
      "epoch": 0.0656609935074452,
      "grad_norm": 7.520557880401611,
      "learning_rate": 4.917923978572965e-05,
      "loss": 3.7825,
      "step": 372300
    },
    {
      "epoch": 0.06567863008910177,
      "grad_norm": 5.441642761230469,
      "learning_rate": 4.9179019328458936e-05,
      "loss": 3.7243,
      "step": 372400
    },
    {
      "epoch": 0.06569626667075835,
      "grad_norm": 6.5418572425842285,
      "learning_rate": 4.917879887118823e-05,
      "loss": 3.7576,
      "step": 372500
    },
    {
      "epoch": 0.06571390325241493,
      "grad_norm": 6.52208137512207,
      "learning_rate": 4.917857841391753e-05,
      "loss": 3.705,
      "step": 372600
    },
    {
      "epoch": 0.06573153983407151,
      "grad_norm": 5.133516788482666,
      "learning_rate": 4.9178357956646816e-05,
      "loss": 3.7199,
      "step": 372700
    },
    {
      "epoch": 0.0657491764157281,
      "grad_norm": 8.514922142028809,
      "learning_rate": 4.917813749937611e-05,
      "loss": 3.6702,
      "step": 372800
    },
    {
      "epoch": 0.06576681299738467,
      "grad_norm": 11.323524475097656,
      "learning_rate": 4.917791704210541e-05,
      "loss": 3.6539,
      "step": 372900
    },
    {
      "epoch": 0.06578444957904125,
      "grad_norm": 7.379795074462891,
      "learning_rate": 4.9177696584834696e-05,
      "loss": 3.7291,
      "step": 373000
    },
    {
      "epoch": 0.06580208616069783,
      "grad_norm": 6.635201930999756,
      "learning_rate": 4.9177476127563984e-05,
      "loss": 3.8734,
      "step": 373100
    },
    {
      "epoch": 0.06581972274235441,
      "grad_norm": 7.251353740692139,
      "learning_rate": 4.917725567029328e-05,
      "loss": 3.7118,
      "step": 373200
    },
    {
      "epoch": 0.06583735932401098,
      "grad_norm": 7.394643783569336,
      "learning_rate": 4.917703521302257e-05,
      "loss": 3.7402,
      "step": 373300
    },
    {
      "epoch": 0.06585499590566757,
      "grad_norm": 9.33100414276123,
      "learning_rate": 4.9176814755751864e-05,
      "loss": 3.7058,
      "step": 373400
    },
    {
      "epoch": 0.06587263248732415,
      "grad_norm": 6.731003284454346,
      "learning_rate": 4.917659429848116e-05,
      "loss": 3.7226,
      "step": 373500
    },
    {
      "epoch": 0.06589026906898073,
      "grad_norm": 5.935976028442383,
      "learning_rate": 4.917637384121045e-05,
      "loss": 3.7463,
      "step": 373600
    },
    {
      "epoch": 0.06590790565063731,
      "grad_norm": 5.471180438995361,
      "learning_rate": 4.9176153383939743e-05,
      "loss": 3.7818,
      "step": 373700
    },
    {
      "epoch": 0.06592554223229388,
      "grad_norm": 7.538700103759766,
      "learning_rate": 4.917593292666904e-05,
      "loss": 3.7077,
      "step": 373800
    },
    {
      "epoch": 0.06594317881395047,
      "grad_norm": 6.75834321975708,
      "learning_rate": 4.917571246939833e-05,
      "loss": 3.6651,
      "step": 373900
    },
    {
      "epoch": 0.06596081539560704,
      "grad_norm": 11.578814506530762,
      "learning_rate": 4.917549201212762e-05,
      "loss": 3.6867,
      "step": 374000
    },
    {
      "epoch": 0.06597845197726362,
      "grad_norm": 7.750483512878418,
      "learning_rate": 4.917527155485692e-05,
      "loss": 3.7979,
      "step": 374100
    },
    {
      "epoch": 0.06599608855892021,
      "grad_norm": 6.221967697143555,
      "learning_rate": 4.917505109758621e-05,
      "loss": 3.7612,
      "step": 374200
    },
    {
      "epoch": 0.06601372514057678,
      "grad_norm": 6.3923211097717285,
      "learning_rate": 4.91748306403155e-05,
      "loss": 3.7445,
      "step": 374300
    },
    {
      "epoch": 0.06603136172223337,
      "grad_norm": 6.181362628936768,
      "learning_rate": 4.917461018304479e-05,
      "loss": 3.6445,
      "step": 374400
    },
    {
      "epoch": 0.06604899830388994,
      "grad_norm": 6.638273239135742,
      "learning_rate": 4.917438972577409e-05,
      "loss": 3.7122,
      "step": 374500
    },
    {
      "epoch": 0.06606663488554652,
      "grad_norm": 10.044553756713867,
      "learning_rate": 4.9174169268503376e-05,
      "loss": 3.7314,
      "step": 374600
    },
    {
      "epoch": 0.0660842714672031,
      "grad_norm": 6.261441707611084,
      "learning_rate": 4.917394881123267e-05,
      "loss": 3.7195,
      "step": 374700
    },
    {
      "epoch": 0.06610190804885968,
      "grad_norm": 9.494189262390137,
      "learning_rate": 4.917372835396196e-05,
      "loss": 3.772,
      "step": 374800
    },
    {
      "epoch": 0.06611954463051627,
      "grad_norm": 9.127131462097168,
      "learning_rate": 4.9173507896691255e-05,
      "loss": 3.6534,
      "step": 374900
    },
    {
      "epoch": 0.06613718121217284,
      "grad_norm": 7.430210113525391,
      "learning_rate": 4.917328743942055e-05,
      "loss": 3.7348,
      "step": 375000
    },
    {
      "epoch": 0.06615481779382942,
      "grad_norm": 10.265532493591309,
      "learning_rate": 4.917306698214984e-05,
      "loss": 3.7523,
      "step": 375100
    },
    {
      "epoch": 0.066172454375486,
      "grad_norm": 6.0521063804626465,
      "learning_rate": 4.9172846524879135e-05,
      "loss": 3.6989,
      "step": 375200
    },
    {
      "epoch": 0.06619009095714258,
      "grad_norm": 7.157875061035156,
      "learning_rate": 4.917262606760843e-05,
      "loss": 3.6774,
      "step": 375300
    },
    {
      "epoch": 0.06620772753879915,
      "grad_norm": 8.78227710723877,
      "learning_rate": 4.917240561033772e-05,
      "loss": 3.7126,
      "step": 375400
    },
    {
      "epoch": 0.06622536412045574,
      "grad_norm": 7.267744541168213,
      "learning_rate": 4.9172185153067014e-05,
      "loss": 3.7031,
      "step": 375500
    },
    {
      "epoch": 0.06624300070211231,
      "grad_norm": 6.3563947677612305,
      "learning_rate": 4.917196469579631e-05,
      "loss": 3.7171,
      "step": 375600
    },
    {
      "epoch": 0.0662606372837689,
      "grad_norm": 6.9249587059021,
      "learning_rate": 4.91717442385256e-05,
      "loss": 3.8046,
      "step": 375700
    },
    {
      "epoch": 0.06627827386542548,
      "grad_norm": 9.28396987915039,
      "learning_rate": 4.9171523781254894e-05,
      "loss": 3.6829,
      "step": 375800
    },
    {
      "epoch": 0.06629591044708205,
      "grad_norm": 6.3008880615234375,
      "learning_rate": 4.917130332398418e-05,
      "loss": 3.7206,
      "step": 375900
    },
    {
      "epoch": 0.06631354702873864,
      "grad_norm": 11.776065826416016,
      "learning_rate": 4.917108286671348e-05,
      "loss": 3.7075,
      "step": 376000
    },
    {
      "epoch": 0.06633118361039521,
      "grad_norm": 6.337625503540039,
      "learning_rate": 4.917086240944277e-05,
      "loss": 3.7523,
      "step": 376100
    },
    {
      "epoch": 0.0663488201920518,
      "grad_norm": 7.538369655609131,
      "learning_rate": 4.917064195217206e-05,
      "loss": 3.7571,
      "step": 376200
    },
    {
      "epoch": 0.06636645677370837,
      "grad_norm": 9.07320785522461,
      "learning_rate": 4.917042149490135e-05,
      "loss": 3.779,
      "step": 376300
    },
    {
      "epoch": 0.06638409335536495,
      "grad_norm": 8.731524467468262,
      "learning_rate": 4.9170201037630647e-05,
      "loss": 3.7703,
      "step": 376400
    },
    {
      "epoch": 0.06640172993702154,
      "grad_norm": 4.589964866638184,
      "learning_rate": 4.916998058035994e-05,
      "loss": 3.7084,
      "step": 376500
    },
    {
      "epoch": 0.06641936651867811,
      "grad_norm": 6.38817024230957,
      "learning_rate": 4.916976012308923e-05,
      "loss": 3.7536,
      "step": 376600
    },
    {
      "epoch": 0.0664370031003347,
      "grad_norm": 8.6016206741333,
      "learning_rate": 4.9169539665818526e-05,
      "loss": 3.6586,
      "step": 376700
    },
    {
      "epoch": 0.06645463968199126,
      "grad_norm": 7.026094913482666,
      "learning_rate": 4.916931920854782e-05,
      "loss": 3.6469,
      "step": 376800
    },
    {
      "epoch": 0.06647227626364785,
      "grad_norm": 6.209458827972412,
      "learning_rate": 4.916909875127711e-05,
      "loss": 3.7489,
      "step": 376900
    },
    {
      "epoch": 0.06648991284530442,
      "grad_norm": 12.483701705932617,
      "learning_rate": 4.9168878294006406e-05,
      "loss": 3.7457,
      "step": 377000
    },
    {
      "epoch": 0.06650754942696101,
      "grad_norm": 8.856493949890137,
      "learning_rate": 4.91686578367357e-05,
      "loss": 3.7016,
      "step": 377100
    },
    {
      "epoch": 0.06652518600861759,
      "grad_norm": 7.982220649719238,
      "learning_rate": 4.916843737946499e-05,
      "loss": 3.7769,
      "step": 377200
    },
    {
      "epoch": 0.06654282259027416,
      "grad_norm": 6.624660968780518,
      "learning_rate": 4.9168216922194285e-05,
      "loss": 3.7996,
      "step": 377300
    },
    {
      "epoch": 0.06656045917193075,
      "grad_norm": 7.160035133361816,
      "learning_rate": 4.9167996464923574e-05,
      "loss": 3.7267,
      "step": 377400
    },
    {
      "epoch": 0.06657809575358732,
      "grad_norm": 7.6985764503479,
      "learning_rate": 4.916777600765286e-05,
      "loss": 3.8881,
      "step": 377500
    },
    {
      "epoch": 0.0665957323352439,
      "grad_norm": 5.17958402633667,
      "learning_rate": 4.916755555038216e-05,
      "loss": 3.6694,
      "step": 377600
    },
    {
      "epoch": 0.06661336891690048,
      "grad_norm": 11.28260612487793,
      "learning_rate": 4.9167335093111454e-05,
      "loss": 3.7503,
      "step": 377700
    },
    {
      "epoch": 0.06663100549855706,
      "grad_norm": 6.288686275482178,
      "learning_rate": 4.916711463584074e-05,
      "loss": 3.6984,
      "step": 377800
    },
    {
      "epoch": 0.06664864208021365,
      "grad_norm": 8.09912395477295,
      "learning_rate": 4.916689417857004e-05,
      "loss": 3.7339,
      "step": 377900
    },
    {
      "epoch": 0.06666627866187022,
      "grad_norm": 7.250552654266357,
      "learning_rate": 4.9166673721299333e-05,
      "loss": 3.6866,
      "step": 378000
    },
    {
      "epoch": 0.0666839152435268,
      "grad_norm": 7.334185600280762,
      "learning_rate": 4.916645326402862e-05,
      "loss": 3.6511,
      "step": 378100
    },
    {
      "epoch": 0.06670155182518338,
      "grad_norm": 6.262397766113281,
      "learning_rate": 4.916623280675792e-05,
      "loss": 3.8061,
      "step": 378200
    },
    {
      "epoch": 0.06671918840683996,
      "grad_norm": 8.260042190551758,
      "learning_rate": 4.916601234948721e-05,
      "loss": 3.6317,
      "step": 378300
    },
    {
      "epoch": 0.06673682498849653,
      "grad_norm": 5.517086982727051,
      "learning_rate": 4.91657918922165e-05,
      "loss": 3.8542,
      "step": 378400
    },
    {
      "epoch": 0.06675446157015312,
      "grad_norm": 8.817752838134766,
      "learning_rate": 4.91655714349458e-05,
      "loss": 3.6871,
      "step": 378500
    },
    {
      "epoch": 0.06677209815180969,
      "grad_norm": 7.5681257247924805,
      "learning_rate": 4.916535097767509e-05,
      "loss": 3.7171,
      "step": 378600
    },
    {
      "epoch": 0.06678973473346628,
      "grad_norm": 7.938612461090088,
      "learning_rate": 4.916513052040438e-05,
      "loss": 3.6689,
      "step": 378700
    },
    {
      "epoch": 0.06680737131512286,
      "grad_norm": 7.980437278747559,
      "learning_rate": 4.916491006313367e-05,
      "loss": 3.7598,
      "step": 378800
    },
    {
      "epoch": 0.06682500789677943,
      "grad_norm": 13.055797576904297,
      "learning_rate": 4.9164689605862966e-05,
      "loss": 3.752,
      "step": 378900
    },
    {
      "epoch": 0.06684264447843602,
      "grad_norm": 7.016619682312012,
      "learning_rate": 4.9164469148592254e-05,
      "loss": 3.6654,
      "step": 379000
    },
    {
      "epoch": 0.06686028106009259,
      "grad_norm": 7.789445877075195,
      "learning_rate": 4.916424869132155e-05,
      "loss": 3.7361,
      "step": 379100
    },
    {
      "epoch": 0.06687791764174918,
      "grad_norm": 7.0342230796813965,
      "learning_rate": 4.9164028234050845e-05,
      "loss": 3.6766,
      "step": 379200
    },
    {
      "epoch": 0.06689555422340575,
      "grad_norm": 12.478890419006348,
      "learning_rate": 4.9163807776780134e-05,
      "loss": 3.7964,
      "step": 379300
    },
    {
      "epoch": 0.06691319080506233,
      "grad_norm": 11.695404052734375,
      "learning_rate": 4.916358731950943e-05,
      "loss": 3.6879,
      "step": 379400
    },
    {
      "epoch": 0.06693082738671892,
      "grad_norm": 7.570422172546387,
      "learning_rate": 4.9163366862238725e-05,
      "loss": 3.6097,
      "step": 379500
    },
    {
      "epoch": 0.06694846396837549,
      "grad_norm": 7.178614139556885,
      "learning_rate": 4.9163146404968013e-05,
      "loss": 3.6903,
      "step": 379600
    },
    {
      "epoch": 0.06696610055003208,
      "grad_norm": 6.879251003265381,
      "learning_rate": 4.916292594769731e-05,
      "loss": 3.7627,
      "step": 379700
    },
    {
      "epoch": 0.06698373713168865,
      "grad_norm": 8.093931198120117,
      "learning_rate": 4.9162705490426604e-05,
      "loss": 3.6945,
      "step": 379800
    },
    {
      "epoch": 0.06700137371334523,
      "grad_norm": 7.766705513000488,
      "learning_rate": 4.916248503315589e-05,
      "loss": 3.7084,
      "step": 379900
    },
    {
      "epoch": 0.0670190102950018,
      "grad_norm": 7.511780738830566,
      "learning_rate": 4.916226457588519e-05,
      "loss": 3.7062,
      "step": 380000
    },
    {
      "epoch": 0.06703664687665839,
      "grad_norm": 8.171366691589355,
      "learning_rate": 4.9162044118614484e-05,
      "loss": 3.6528,
      "step": 380100
    },
    {
      "epoch": 0.06705428345831498,
      "grad_norm": 7.783137321472168,
      "learning_rate": 4.916182366134377e-05,
      "loss": 3.6955,
      "step": 380200
    },
    {
      "epoch": 0.06707192003997155,
      "grad_norm": 6.704426288604736,
      "learning_rate": 4.916160320407306e-05,
      "loss": 3.6926,
      "step": 380300
    },
    {
      "epoch": 0.06708955662162813,
      "grad_norm": 9.598064422607422,
      "learning_rate": 4.916138274680236e-05,
      "loss": 3.7061,
      "step": 380400
    },
    {
      "epoch": 0.0671071932032847,
      "grad_norm": 6.189016342163086,
      "learning_rate": 4.9161162289531646e-05,
      "loss": 3.7781,
      "step": 380500
    },
    {
      "epoch": 0.06712482978494129,
      "grad_norm": 7.114518642425537,
      "learning_rate": 4.916094183226094e-05,
      "loss": 3.691,
      "step": 380600
    },
    {
      "epoch": 0.06714246636659786,
      "grad_norm": 8.99112606048584,
      "learning_rate": 4.9160721374990237e-05,
      "loss": 3.6857,
      "step": 380700
    },
    {
      "epoch": 0.06716010294825445,
      "grad_norm": 7.007252216339111,
      "learning_rate": 4.9160500917719525e-05,
      "loss": 3.8102,
      "step": 380800
    },
    {
      "epoch": 0.06717773952991103,
      "grad_norm": 7.579390525817871,
      "learning_rate": 4.916028046044882e-05,
      "loss": 3.6992,
      "step": 380900
    },
    {
      "epoch": 0.0671953761115676,
      "grad_norm": 6.519526481628418,
      "learning_rate": 4.9160060003178116e-05,
      "loss": 3.6474,
      "step": 381000
    },
    {
      "epoch": 0.06721301269322419,
      "grad_norm": 5.391659736633301,
      "learning_rate": 4.9159839545907405e-05,
      "loss": 3.7779,
      "step": 381100
    },
    {
      "epoch": 0.06723064927488076,
      "grad_norm": 10.084928512573242,
      "learning_rate": 4.91596190886367e-05,
      "loss": 3.6566,
      "step": 381200
    },
    {
      "epoch": 0.06724828585653735,
      "grad_norm": 6.945054531097412,
      "learning_rate": 4.9159398631365996e-05,
      "loss": 3.731,
      "step": 381300
    },
    {
      "epoch": 0.06726592243819392,
      "grad_norm": 6.066847801208496,
      "learning_rate": 4.9159178174095284e-05,
      "loss": 3.7475,
      "step": 381400
    },
    {
      "epoch": 0.0672835590198505,
      "grad_norm": 7.879674911499023,
      "learning_rate": 4.915895771682458e-05,
      "loss": 3.6125,
      "step": 381500
    },
    {
      "epoch": 0.06730119560150707,
      "grad_norm": 7.41195821762085,
      "learning_rate": 4.915873725955387e-05,
      "loss": 3.6957,
      "step": 381600
    },
    {
      "epoch": 0.06731883218316366,
      "grad_norm": 8.573721885681152,
      "learning_rate": 4.9158516802283164e-05,
      "loss": 3.624,
      "step": 381700
    },
    {
      "epoch": 0.06733646876482025,
      "grad_norm": 8.49123764038086,
      "learning_rate": 4.915829634501245e-05,
      "loss": 3.5889,
      "step": 381800
    },
    {
      "epoch": 0.06735410534647682,
      "grad_norm": 7.283878803253174,
      "learning_rate": 4.915807588774175e-05,
      "loss": 3.8906,
      "step": 381900
    },
    {
      "epoch": 0.0673717419281334,
      "grad_norm": 8.13622760772705,
      "learning_rate": 4.915785543047104e-05,
      "loss": 3.7323,
      "step": 382000
    },
    {
      "epoch": 0.06738937850978997,
      "grad_norm": 7.296215057373047,
      "learning_rate": 4.915763497320033e-05,
      "loss": 3.7537,
      "step": 382100
    },
    {
      "epoch": 0.06740701509144656,
      "grad_norm": 8.295256614685059,
      "learning_rate": 4.915741451592963e-05,
      "loss": 3.6521,
      "step": 382200
    },
    {
      "epoch": 0.06742465167310313,
      "grad_norm": 11.538521766662598,
      "learning_rate": 4.9157194058658917e-05,
      "loss": 3.7186,
      "step": 382300
    },
    {
      "epoch": 0.06744228825475972,
      "grad_norm": 6.684504985809326,
      "learning_rate": 4.915697360138821e-05,
      "loss": 3.668,
      "step": 382400
    },
    {
      "epoch": 0.0674599248364163,
      "grad_norm": 7.574419975280762,
      "learning_rate": 4.915675314411751e-05,
      "loss": 3.7245,
      "step": 382500
    },
    {
      "epoch": 0.06747756141807287,
      "grad_norm": 6.816363334655762,
      "learning_rate": 4.91565326868468e-05,
      "loss": 3.6681,
      "step": 382600
    },
    {
      "epoch": 0.06749519799972946,
      "grad_norm": 7.099475383758545,
      "learning_rate": 4.915631222957609e-05,
      "loss": 3.7103,
      "step": 382700
    },
    {
      "epoch": 0.06751283458138603,
      "grad_norm": 8.03563404083252,
      "learning_rate": 4.915609177230539e-05,
      "loss": 3.6906,
      "step": 382800
    },
    {
      "epoch": 0.06753047116304262,
      "grad_norm": 7.8056640625,
      "learning_rate": 4.915587131503468e-05,
      "loss": 3.6658,
      "step": 382900
    },
    {
      "epoch": 0.06754810774469919,
      "grad_norm": 8.1216402053833,
      "learning_rate": 4.915565085776397e-05,
      "loss": 3.6815,
      "step": 383000
    },
    {
      "epoch": 0.06756574432635577,
      "grad_norm": 5.548676013946533,
      "learning_rate": 4.915543040049326e-05,
      "loss": 3.7208,
      "step": 383100
    },
    {
      "epoch": 0.06758338090801236,
      "grad_norm": 11.639344215393066,
      "learning_rate": 4.9155209943222555e-05,
      "loss": 3.6604,
      "step": 383200
    },
    {
      "epoch": 0.06760101748966893,
      "grad_norm": 8.67759895324707,
      "learning_rate": 4.9154989485951844e-05,
      "loss": 3.7873,
      "step": 383300
    },
    {
      "epoch": 0.06761865407132552,
      "grad_norm": 6.399173259735107,
      "learning_rate": 4.915476902868114e-05,
      "loss": 3.7593,
      "step": 383400
    },
    {
      "epoch": 0.06763629065298209,
      "grad_norm": 9.32809829711914,
      "learning_rate": 4.9154548571410435e-05,
      "loss": 3.7954,
      "step": 383500
    },
    {
      "epoch": 0.06765392723463867,
      "grad_norm": 8.405075073242188,
      "learning_rate": 4.9154328114139724e-05,
      "loss": 3.7063,
      "step": 383600
    },
    {
      "epoch": 0.06767156381629524,
      "grad_norm": 7.459236145019531,
      "learning_rate": 4.915410765686902e-05,
      "loss": 3.6946,
      "step": 383700
    },
    {
      "epoch": 0.06768920039795183,
      "grad_norm": 5.7501044273376465,
      "learning_rate": 4.9153887199598315e-05,
      "loss": 3.6839,
      "step": 383800
    },
    {
      "epoch": 0.06770683697960841,
      "grad_norm": 7.798663139343262,
      "learning_rate": 4.9153666742327603e-05,
      "loss": 3.7715,
      "step": 383900
    },
    {
      "epoch": 0.06772447356126499,
      "grad_norm": 7.717845439910889,
      "learning_rate": 4.91534462850569e-05,
      "loss": 3.7255,
      "step": 384000
    },
    {
      "epoch": 0.06774211014292157,
      "grad_norm": 8.047258377075195,
      "learning_rate": 4.9153225827786194e-05,
      "loss": 3.7146,
      "step": 384100
    },
    {
      "epoch": 0.06775974672457814,
      "grad_norm": 9.403502464294434,
      "learning_rate": 4.915300537051548e-05,
      "loss": 3.7673,
      "step": 384200
    },
    {
      "epoch": 0.06777738330623473,
      "grad_norm": 7.412606239318848,
      "learning_rate": 4.915278491324478e-05,
      "loss": 3.826,
      "step": 384300
    },
    {
      "epoch": 0.0677950198878913,
      "grad_norm": 6.113533973693848,
      "learning_rate": 4.915256445597407e-05,
      "loss": 3.7092,
      "step": 384400
    },
    {
      "epoch": 0.06781265646954789,
      "grad_norm": 7.913381099700928,
      "learning_rate": 4.915234399870336e-05,
      "loss": 3.6625,
      "step": 384500
    },
    {
      "epoch": 0.06783029305120446,
      "grad_norm": 7.794310569763184,
      "learning_rate": 4.915212354143265e-05,
      "loss": 3.7245,
      "step": 384600
    },
    {
      "epoch": 0.06784792963286104,
      "grad_norm": 8.144535064697266,
      "learning_rate": 4.915190308416195e-05,
      "loss": 3.6493,
      "step": 384700
    },
    {
      "epoch": 0.06786556621451763,
      "grad_norm": 8.813645362854004,
      "learning_rate": 4.9151682626891236e-05,
      "loss": 3.6971,
      "step": 384800
    },
    {
      "epoch": 0.0678832027961742,
      "grad_norm": 5.378270626068115,
      "learning_rate": 4.915146216962053e-05,
      "loss": 3.6282,
      "step": 384900
    },
    {
      "epoch": 0.06790083937783079,
      "grad_norm": 5.51471471786499,
      "learning_rate": 4.9151241712349826e-05,
      "loss": 3.7254,
      "step": 385000
    },
    {
      "epoch": 0.06791847595948736,
      "grad_norm": 8.381650924682617,
      "learning_rate": 4.9151021255079115e-05,
      "loss": 3.8022,
      "step": 385100
    },
    {
      "epoch": 0.06793611254114394,
      "grad_norm": 6.1304497718811035,
      "learning_rate": 4.915080079780841e-05,
      "loss": 3.785,
      "step": 385200
    },
    {
      "epoch": 0.06795374912280051,
      "grad_norm": 9.123141288757324,
      "learning_rate": 4.9150580340537706e-05,
      "loss": 3.6653,
      "step": 385300
    },
    {
      "epoch": 0.0679713857044571,
      "grad_norm": 7.199790954589844,
      "learning_rate": 4.9150359883266995e-05,
      "loss": 3.7366,
      "step": 385400
    },
    {
      "epoch": 0.06798902228611368,
      "grad_norm": 6.947322368621826,
      "learning_rate": 4.915013942599629e-05,
      "loss": 3.6947,
      "step": 385500
    },
    {
      "epoch": 0.06800665886777026,
      "grad_norm": 5.769318580627441,
      "learning_rate": 4.9149918968725586e-05,
      "loss": 3.6896,
      "step": 385600
    },
    {
      "epoch": 0.06802429544942684,
      "grad_norm": 8.343483924865723,
      "learning_rate": 4.9149698511454874e-05,
      "loss": 3.694,
      "step": 385700
    },
    {
      "epoch": 0.06804193203108341,
      "grad_norm": 10.769645690917969,
      "learning_rate": 4.914947805418417e-05,
      "loss": 3.6228,
      "step": 385800
    },
    {
      "epoch": 0.06805956861274,
      "grad_norm": 6.0678277015686035,
      "learning_rate": 4.914925759691346e-05,
      "loss": 3.5802,
      "step": 385900
    },
    {
      "epoch": 0.06807720519439657,
      "grad_norm": 6.517558574676514,
      "learning_rate": 4.9149037139642754e-05,
      "loss": 3.7804,
      "step": 386000
    },
    {
      "epoch": 0.06809484177605316,
      "grad_norm": 6.31580924987793,
      "learning_rate": 4.914881668237204e-05,
      "loss": 3.6584,
      "step": 386100
    },
    {
      "epoch": 0.06811247835770974,
      "grad_norm": 7.955623149871826,
      "learning_rate": 4.914859622510134e-05,
      "loss": 3.7477,
      "step": 386200
    },
    {
      "epoch": 0.06813011493936631,
      "grad_norm": 7.552979469299316,
      "learning_rate": 4.914837576783063e-05,
      "loss": 3.7412,
      "step": 386300
    },
    {
      "epoch": 0.0681477515210229,
      "grad_norm": 6.491350173950195,
      "learning_rate": 4.914815531055992e-05,
      "loss": 3.7174,
      "step": 386400
    },
    {
      "epoch": 0.06816538810267947,
      "grad_norm": 8.406439781188965,
      "learning_rate": 4.914793485328922e-05,
      "loss": 3.7117,
      "step": 386500
    },
    {
      "epoch": 0.06818302468433605,
      "grad_norm": 10.424241065979004,
      "learning_rate": 4.9147714396018507e-05,
      "loss": 3.7189,
      "step": 386600
    },
    {
      "epoch": 0.06820066126599263,
      "grad_norm": 6.042880535125732,
      "learning_rate": 4.91474939387478e-05,
      "loss": 3.6593,
      "step": 386700
    },
    {
      "epoch": 0.06821829784764921,
      "grad_norm": 12.926065444946289,
      "learning_rate": 4.91472734814771e-05,
      "loss": 3.71,
      "step": 386800
    },
    {
      "epoch": 0.0682359344293058,
      "grad_norm": 8.21342945098877,
      "learning_rate": 4.9147053024206386e-05,
      "loss": 3.7949,
      "step": 386900
    },
    {
      "epoch": 0.06825357101096237,
      "grad_norm": 6.930325508117676,
      "learning_rate": 4.914683256693568e-05,
      "loss": 3.7602,
      "step": 387000
    },
    {
      "epoch": 0.06827120759261895,
      "grad_norm": 7.636091232299805,
      "learning_rate": 4.914661210966498e-05,
      "loss": 3.6829,
      "step": 387100
    },
    {
      "epoch": 0.06828884417427553,
      "grad_norm": 5.80084228515625,
      "learning_rate": 4.9146391652394266e-05,
      "loss": 3.7431,
      "step": 387200
    },
    {
      "epoch": 0.06830648075593211,
      "grad_norm": 7.619333267211914,
      "learning_rate": 4.914617119512356e-05,
      "loss": 3.7054,
      "step": 387300
    },
    {
      "epoch": 0.06832411733758868,
      "grad_norm": 6.930846214294434,
      "learning_rate": 4.914595073785285e-05,
      "loss": 3.7609,
      "step": 387400
    },
    {
      "epoch": 0.06834175391924527,
      "grad_norm": 5.724080562591553,
      "learning_rate": 4.914573028058214e-05,
      "loss": 3.7733,
      "step": 387500
    },
    {
      "epoch": 0.06835939050090184,
      "grad_norm": 6.186201572418213,
      "learning_rate": 4.9145509823311434e-05,
      "loss": 3.7733,
      "step": 387600
    },
    {
      "epoch": 0.06837702708255843,
      "grad_norm": 5.561692237854004,
      "learning_rate": 4.914528936604073e-05,
      "loss": 3.6923,
      "step": 387700
    },
    {
      "epoch": 0.06839466366421501,
      "grad_norm": 5.28828763961792,
      "learning_rate": 4.914506890877002e-05,
      "loss": 3.6549,
      "step": 387800
    },
    {
      "epoch": 0.06841230024587158,
      "grad_norm": 7.338144779205322,
      "learning_rate": 4.9144848451499314e-05,
      "loss": 3.7265,
      "step": 387900
    },
    {
      "epoch": 0.06842993682752817,
      "grad_norm": 6.7556562423706055,
      "learning_rate": 4.914462799422861e-05,
      "loss": 3.6605,
      "step": 388000
    },
    {
      "epoch": 0.06844757340918474,
      "grad_norm": 4.824390888214111,
      "learning_rate": 4.91444075369579e-05,
      "loss": 3.6644,
      "step": 388100
    },
    {
      "epoch": 0.06846520999084132,
      "grad_norm": 6.621760845184326,
      "learning_rate": 4.914418707968719e-05,
      "loss": 3.6634,
      "step": 388200
    },
    {
      "epoch": 0.0684828465724979,
      "grad_norm": 6.89152717590332,
      "learning_rate": 4.914396662241649e-05,
      "loss": 3.6818,
      "step": 388300
    },
    {
      "epoch": 0.06850048315415448,
      "grad_norm": 9.951619148254395,
      "learning_rate": 4.914374616514578e-05,
      "loss": 3.7501,
      "step": 388400
    },
    {
      "epoch": 0.06851811973581107,
      "grad_norm": 6.868344783782959,
      "learning_rate": 4.914352570787507e-05,
      "loss": 3.6837,
      "step": 388500
    },
    {
      "epoch": 0.06853575631746764,
      "grad_norm": 8.156929016113281,
      "learning_rate": 4.914330525060437e-05,
      "loss": 3.6833,
      "step": 388600
    },
    {
      "epoch": 0.06855339289912422,
      "grad_norm": 6.106428146362305,
      "learning_rate": 4.914308479333366e-05,
      "loss": 3.7801,
      "step": 388700
    },
    {
      "epoch": 0.0685710294807808,
      "grad_norm": 12.520970344543457,
      "learning_rate": 4.914286433606295e-05,
      "loss": 3.7097,
      "step": 388800
    },
    {
      "epoch": 0.06858866606243738,
      "grad_norm": 6.949672222137451,
      "learning_rate": 4.914264387879224e-05,
      "loss": 3.7884,
      "step": 388900
    },
    {
      "epoch": 0.06860630264409395,
      "grad_norm": 7.553709506988525,
      "learning_rate": 4.914242342152153e-05,
      "loss": 3.7049,
      "step": 389000
    },
    {
      "epoch": 0.06862393922575054,
      "grad_norm": 9.217886924743652,
      "learning_rate": 4.9142202964250825e-05,
      "loss": 3.7261,
      "step": 389100
    },
    {
      "epoch": 0.06864157580740712,
      "grad_norm": 12.523289680480957,
      "learning_rate": 4.914198250698012e-05,
      "loss": 3.7174,
      "step": 389200
    },
    {
      "epoch": 0.0686592123890637,
      "grad_norm": 5.186746597290039,
      "learning_rate": 4.914176204970941e-05,
      "loss": 3.742,
      "step": 389300
    },
    {
      "epoch": 0.06867684897072028,
      "grad_norm": 6.895259857177734,
      "learning_rate": 4.9141541592438705e-05,
      "loss": 3.6943,
      "step": 389400
    },
    {
      "epoch": 0.06869448555237685,
      "grad_norm": 7.601945877075195,
      "learning_rate": 4.9141321135168e-05,
      "loss": 3.765,
      "step": 389500
    },
    {
      "epoch": 0.06871212213403344,
      "grad_norm": 6.102781295776367,
      "learning_rate": 4.914110067789729e-05,
      "loss": 3.6655,
      "step": 389600
    },
    {
      "epoch": 0.06872975871569001,
      "grad_norm": 9.095032691955566,
      "learning_rate": 4.9140880220626585e-05,
      "loss": 3.8428,
      "step": 389700
    },
    {
      "epoch": 0.0687473952973466,
      "grad_norm": 8.12377643585205,
      "learning_rate": 4.914065976335588e-05,
      "loss": 3.7616,
      "step": 389800
    },
    {
      "epoch": 0.06876503187900318,
      "grad_norm": 5.835875511169434,
      "learning_rate": 4.914043930608517e-05,
      "loss": 3.7717,
      "step": 389900
    },
    {
      "epoch": 0.06878266846065975,
      "grad_norm": 7.507722854614258,
      "learning_rate": 4.9140218848814464e-05,
      "loss": 3.7638,
      "step": 390000
    },
    {
      "epoch": 0.06880030504231634,
      "grad_norm": 7.6039910316467285,
      "learning_rate": 4.913999839154376e-05,
      "loss": 3.6704,
      "step": 390100
    },
    {
      "epoch": 0.06881794162397291,
      "grad_norm": 7.756200790405273,
      "learning_rate": 4.913977793427305e-05,
      "loss": 3.7415,
      "step": 390200
    },
    {
      "epoch": 0.0688355782056295,
      "grad_norm": 6.300668716430664,
      "learning_rate": 4.913955747700234e-05,
      "loss": 3.6756,
      "step": 390300
    },
    {
      "epoch": 0.06885321478728607,
      "grad_norm": 6.88421106338501,
      "learning_rate": 4.913933701973163e-05,
      "loss": 3.6728,
      "step": 390400
    },
    {
      "epoch": 0.06887085136894265,
      "grad_norm": 11.501226425170898,
      "learning_rate": 4.913911656246092e-05,
      "loss": 3.7707,
      "step": 390500
    },
    {
      "epoch": 0.06888848795059922,
      "grad_norm": 5.377995014190674,
      "learning_rate": 4.913889610519022e-05,
      "loss": 3.6469,
      "step": 390600
    },
    {
      "epoch": 0.06890612453225581,
      "grad_norm": 10.073040008544922,
      "learning_rate": 4.913867564791951e-05,
      "loss": 3.685,
      "step": 390700
    },
    {
      "epoch": 0.0689237611139124,
      "grad_norm": 8.168204307556152,
      "learning_rate": 4.91384551906488e-05,
      "loss": 3.6877,
      "step": 390800
    },
    {
      "epoch": 0.06894139769556897,
      "grad_norm": 8.561149597167969,
      "learning_rate": 4.9138234733378096e-05,
      "loss": 3.7291,
      "step": 390900
    },
    {
      "epoch": 0.06895903427722555,
      "grad_norm": 7.480485439300537,
      "learning_rate": 4.913801427610739e-05,
      "loss": 3.6881,
      "step": 391000
    },
    {
      "epoch": 0.06897667085888212,
      "grad_norm": 5.75881290435791,
      "learning_rate": 4.913779381883668e-05,
      "loss": 3.683,
      "step": 391100
    },
    {
      "epoch": 0.06899430744053871,
      "grad_norm": 7.278732776641846,
      "learning_rate": 4.9137573361565976e-05,
      "loss": 3.7095,
      "step": 391200
    },
    {
      "epoch": 0.06901194402219528,
      "grad_norm": 7.275108337402344,
      "learning_rate": 4.913735290429527e-05,
      "loss": 3.622,
      "step": 391300
    },
    {
      "epoch": 0.06902958060385186,
      "grad_norm": 7.059364318847656,
      "learning_rate": 4.913713244702456e-05,
      "loss": 3.7356,
      "step": 391400
    },
    {
      "epoch": 0.06904721718550845,
      "grad_norm": 7.269749164581299,
      "learning_rate": 4.9136911989753856e-05,
      "loss": 3.7192,
      "step": 391500
    },
    {
      "epoch": 0.06906485376716502,
      "grad_norm": 8.8560791015625,
      "learning_rate": 4.913669153248315e-05,
      "loss": 3.7628,
      "step": 391600
    },
    {
      "epoch": 0.0690824903488216,
      "grad_norm": 6.184111595153809,
      "learning_rate": 4.913647107521244e-05,
      "loss": 3.7094,
      "step": 391700
    },
    {
      "epoch": 0.06910012693047818,
      "grad_norm": 7.162466049194336,
      "learning_rate": 4.913625061794173e-05,
      "loss": 3.8206,
      "step": 391800
    },
    {
      "epoch": 0.06911776351213476,
      "grad_norm": 5.301044464111328,
      "learning_rate": 4.9136030160671024e-05,
      "loss": 3.7106,
      "step": 391900
    },
    {
      "epoch": 0.06913540009379134,
      "grad_norm": 5.662224769592285,
      "learning_rate": 4.913580970340031e-05,
      "loss": 3.586,
      "step": 392000
    },
    {
      "epoch": 0.06915303667544792,
      "grad_norm": 5.811949729919434,
      "learning_rate": 4.913558924612961e-05,
      "loss": 3.587,
      "step": 392100
    },
    {
      "epoch": 0.0691706732571045,
      "grad_norm": 6.720030307769775,
      "learning_rate": 4.9135368788858904e-05,
      "loss": 3.7694,
      "step": 392200
    },
    {
      "epoch": 0.06918830983876108,
      "grad_norm": 7.2118072509765625,
      "learning_rate": 4.913514833158819e-05,
      "loss": 3.7245,
      "step": 392300
    },
    {
      "epoch": 0.06920594642041766,
      "grad_norm": 7.065410614013672,
      "learning_rate": 4.913492787431749e-05,
      "loss": 3.6673,
      "step": 392400
    },
    {
      "epoch": 0.06922358300207423,
      "grad_norm": 6.0056471824646,
      "learning_rate": 4.913470741704678e-05,
      "loss": 3.7329,
      "step": 392500
    },
    {
      "epoch": 0.06924121958373082,
      "grad_norm": 7.462639331817627,
      "learning_rate": 4.913448695977607e-05,
      "loss": 3.7007,
      "step": 392600
    },
    {
      "epoch": 0.06925885616538739,
      "grad_norm": 7.664926052093506,
      "learning_rate": 4.913426650250537e-05,
      "loss": 3.7598,
      "step": 392700
    },
    {
      "epoch": 0.06927649274704398,
      "grad_norm": 7.148446559906006,
      "learning_rate": 4.913404604523466e-05,
      "loss": 3.6993,
      "step": 392800
    },
    {
      "epoch": 0.06929412932870056,
      "grad_norm": 8.566877365112305,
      "learning_rate": 4.913382558796395e-05,
      "loss": 3.733,
      "step": 392900
    },
    {
      "epoch": 0.06931176591035713,
      "grad_norm": 7.32655668258667,
      "learning_rate": 4.913360513069325e-05,
      "loss": 3.6928,
      "step": 393000
    },
    {
      "epoch": 0.06932940249201372,
      "grad_norm": 7.646816730499268,
      "learning_rate": 4.9133384673422536e-05,
      "loss": 3.6839,
      "step": 393100
    },
    {
      "epoch": 0.06934703907367029,
      "grad_norm": 6.944309711456299,
      "learning_rate": 4.913316421615183e-05,
      "loss": 3.6851,
      "step": 393200
    },
    {
      "epoch": 0.06936467565532688,
      "grad_norm": 11.264299392700195,
      "learning_rate": 4.913294375888112e-05,
      "loss": 3.7375,
      "step": 393300
    },
    {
      "epoch": 0.06938231223698345,
      "grad_norm": 10.054542541503906,
      "learning_rate": 4.9132723301610415e-05,
      "loss": 3.7563,
      "step": 393400
    },
    {
      "epoch": 0.06939994881864003,
      "grad_norm": 6.795144081115723,
      "learning_rate": 4.913250284433971e-05,
      "loss": 3.7605,
      "step": 393500
    },
    {
      "epoch": 0.0694175854002966,
      "grad_norm": 7.558021068572998,
      "learning_rate": 4.9132282387069e-05,
      "loss": 3.7003,
      "step": 393600
    },
    {
      "epoch": 0.06943522198195319,
      "grad_norm": 8.952591896057129,
      "learning_rate": 4.9132061929798295e-05,
      "loss": 3.7754,
      "step": 393700
    },
    {
      "epoch": 0.06945285856360978,
      "grad_norm": 7.742771625518799,
      "learning_rate": 4.913184147252759e-05,
      "loss": 3.6127,
      "step": 393800
    },
    {
      "epoch": 0.06947049514526635,
      "grad_norm": 6.551666736602783,
      "learning_rate": 4.913162101525688e-05,
      "loss": 3.6511,
      "step": 393900
    },
    {
      "epoch": 0.06948813172692293,
      "grad_norm": 8.152711868286133,
      "learning_rate": 4.9131400557986175e-05,
      "loss": 3.6288,
      "step": 394000
    },
    {
      "epoch": 0.0695057683085795,
      "grad_norm": 5.936465740203857,
      "learning_rate": 4.913118010071547e-05,
      "loss": 3.869,
      "step": 394100
    },
    {
      "epoch": 0.06952340489023609,
      "grad_norm": 6.3681488037109375,
      "learning_rate": 4.913095964344476e-05,
      "loss": 3.713,
      "step": 394200
    },
    {
      "epoch": 0.06954104147189266,
      "grad_norm": 7.0071539878845215,
      "learning_rate": 4.9130739186174054e-05,
      "loss": 3.6779,
      "step": 394300
    },
    {
      "epoch": 0.06955867805354925,
      "grad_norm": 6.639379978179932,
      "learning_rate": 4.913051872890335e-05,
      "loss": 3.8019,
      "step": 394400
    },
    {
      "epoch": 0.06957631463520583,
      "grad_norm": 7.1284284591674805,
      "learning_rate": 4.913029827163264e-05,
      "loss": 3.6152,
      "step": 394500
    },
    {
      "epoch": 0.0695939512168624,
      "grad_norm": 6.182532787322998,
      "learning_rate": 4.913007781436193e-05,
      "loss": 3.7309,
      "step": 394600
    },
    {
      "epoch": 0.06961158779851899,
      "grad_norm": 6.935771465301514,
      "learning_rate": 4.912985735709122e-05,
      "loss": 3.7068,
      "step": 394700
    },
    {
      "epoch": 0.06962922438017556,
      "grad_norm": 9.770130157470703,
      "learning_rate": 4.912963689982051e-05,
      "loss": 3.7678,
      "step": 394800
    },
    {
      "epoch": 0.06964686096183215,
      "grad_norm": 9.15311336517334,
      "learning_rate": 4.912941644254981e-05,
      "loss": 3.7298,
      "step": 394900
    },
    {
      "epoch": 0.06966449754348872,
      "grad_norm": 12.88414478302002,
      "learning_rate": 4.91291959852791e-05,
      "loss": 3.6969,
      "step": 395000
    },
    {
      "epoch": 0.0696821341251453,
      "grad_norm": 7.148166179656982,
      "learning_rate": 4.912897552800839e-05,
      "loss": 3.7373,
      "step": 395100
    },
    {
      "epoch": 0.06969977070680189,
      "grad_norm": 6.701380729675293,
      "learning_rate": 4.9128755070737686e-05,
      "loss": 3.6479,
      "step": 395200
    },
    {
      "epoch": 0.06971740728845846,
      "grad_norm": 9.438552856445312,
      "learning_rate": 4.912853461346698e-05,
      "loss": 3.7714,
      "step": 395300
    },
    {
      "epoch": 0.06973504387011505,
      "grad_norm": 8.50424575805664,
      "learning_rate": 4.912831415619627e-05,
      "loss": 3.718,
      "step": 395400
    },
    {
      "epoch": 0.06975268045177162,
      "grad_norm": 4.790501594543457,
      "learning_rate": 4.9128093698925566e-05,
      "loss": 3.7593,
      "step": 395500
    },
    {
      "epoch": 0.0697703170334282,
      "grad_norm": 11.863574981689453,
      "learning_rate": 4.912787324165486e-05,
      "loss": 3.7602,
      "step": 395600
    },
    {
      "epoch": 0.06978795361508477,
      "grad_norm": 6.560465335845947,
      "learning_rate": 4.912765278438415e-05,
      "loss": 3.6875,
      "step": 395700
    },
    {
      "epoch": 0.06980559019674136,
      "grad_norm": 6.752932071685791,
      "learning_rate": 4.9127432327113446e-05,
      "loss": 3.768,
      "step": 395800
    },
    {
      "epoch": 0.06982322677839795,
      "grad_norm": 6.746065139770508,
      "learning_rate": 4.9127211869842734e-05,
      "loss": 3.7632,
      "step": 395900
    },
    {
      "epoch": 0.06984086336005452,
      "grad_norm": 6.847597599029541,
      "learning_rate": 4.912699141257203e-05,
      "loss": 3.6849,
      "step": 396000
    },
    {
      "epoch": 0.0698584999417111,
      "grad_norm": 6.9178385734558105,
      "learning_rate": 4.912677095530132e-05,
      "loss": 3.7232,
      "step": 396100
    },
    {
      "epoch": 0.06987613652336767,
      "grad_norm": 6.925136089324951,
      "learning_rate": 4.9126550498030614e-05,
      "loss": 3.6957,
      "step": 396200
    },
    {
      "epoch": 0.06989377310502426,
      "grad_norm": 8.88163948059082,
      "learning_rate": 4.91263300407599e-05,
      "loss": 3.8698,
      "step": 396300
    },
    {
      "epoch": 0.06991140968668083,
      "grad_norm": 6.189737319946289,
      "learning_rate": 4.91261095834892e-05,
      "loss": 3.7171,
      "step": 396400
    },
    {
      "epoch": 0.06992904626833742,
      "grad_norm": 10.256255149841309,
      "learning_rate": 4.9125889126218494e-05,
      "loss": 3.7593,
      "step": 396500
    },
    {
      "epoch": 0.06994668284999399,
      "grad_norm": 6.663978576660156,
      "learning_rate": 4.912566866894778e-05,
      "loss": 3.7068,
      "step": 396600
    },
    {
      "epoch": 0.06996431943165057,
      "grad_norm": 5.83235502243042,
      "learning_rate": 4.912544821167708e-05,
      "loss": 3.7435,
      "step": 396700
    },
    {
      "epoch": 0.06998195601330716,
      "grad_norm": 13.315865516662598,
      "learning_rate": 4.912522775440637e-05,
      "loss": 3.7281,
      "step": 396800
    },
    {
      "epoch": 0.06999959259496373,
      "grad_norm": 6.708896160125732,
      "learning_rate": 4.912500729713566e-05,
      "loss": 3.7191,
      "step": 396900
    },
    {
      "epoch": 0.07001722917662032,
      "grad_norm": 5.66539192199707,
      "learning_rate": 4.912478683986496e-05,
      "loss": 3.7117,
      "step": 397000
    },
    {
      "epoch": 0.07003486575827689,
      "grad_norm": 6.91002893447876,
      "learning_rate": 4.912456638259425e-05,
      "loss": 3.6767,
      "step": 397100
    },
    {
      "epoch": 0.07005250233993347,
      "grad_norm": 5.530418395996094,
      "learning_rate": 4.912434592532354e-05,
      "loss": 3.7145,
      "step": 397200
    },
    {
      "epoch": 0.07007013892159004,
      "grad_norm": 6.087483882904053,
      "learning_rate": 4.912412546805284e-05,
      "loss": 3.6199,
      "step": 397300
    },
    {
      "epoch": 0.07008777550324663,
      "grad_norm": 5.265771865844727,
      "learning_rate": 4.9123905010782126e-05,
      "loss": 3.7139,
      "step": 397400
    },
    {
      "epoch": 0.07010541208490322,
      "grad_norm": 8.342131614685059,
      "learning_rate": 4.9123684553511414e-05,
      "loss": 3.6874,
      "step": 397500
    },
    {
      "epoch": 0.07012304866655979,
      "grad_norm": 8.313578605651855,
      "learning_rate": 4.912346409624071e-05,
      "loss": 3.753,
      "step": 397600
    },
    {
      "epoch": 0.07014068524821637,
      "grad_norm": 8.586536407470703,
      "learning_rate": 4.9123243638970005e-05,
      "loss": 3.7044,
      "step": 397700
    },
    {
      "epoch": 0.07015832182987294,
      "grad_norm": 6.624576568603516,
      "learning_rate": 4.9123023181699294e-05,
      "loss": 3.6971,
      "step": 397800
    },
    {
      "epoch": 0.07017595841152953,
      "grad_norm": 6.645515441894531,
      "learning_rate": 4.912280272442859e-05,
      "loss": 3.8047,
      "step": 397900
    },
    {
      "epoch": 0.0701935949931861,
      "grad_norm": 7.7391886711120605,
      "learning_rate": 4.9122582267157885e-05,
      "loss": 3.6692,
      "step": 398000
    },
    {
      "epoch": 0.07021123157484269,
      "grad_norm": 7.222881317138672,
      "learning_rate": 4.9122361809887174e-05,
      "loss": 3.6923,
      "step": 398100
    },
    {
      "epoch": 0.07022886815649927,
      "grad_norm": 7.918066024780273,
      "learning_rate": 4.912214135261647e-05,
      "loss": 3.795,
      "step": 398200
    },
    {
      "epoch": 0.07024650473815584,
      "grad_norm": 5.710524559020996,
      "learning_rate": 4.9121920895345765e-05,
      "loss": 3.7347,
      "step": 398300
    },
    {
      "epoch": 0.07026414131981243,
      "grad_norm": 6.940193176269531,
      "learning_rate": 4.912170043807505e-05,
      "loss": 3.6908,
      "step": 398400
    },
    {
      "epoch": 0.070281777901469,
      "grad_norm": 7.275580883026123,
      "learning_rate": 4.912147998080435e-05,
      "loss": 3.6627,
      "step": 398500
    },
    {
      "epoch": 0.07029941448312559,
      "grad_norm": 7.344522953033447,
      "learning_rate": 4.9121259523533644e-05,
      "loss": 3.7115,
      "step": 398600
    },
    {
      "epoch": 0.07031705106478216,
      "grad_norm": 8.086894989013672,
      "learning_rate": 4.912103906626293e-05,
      "loss": 3.6511,
      "step": 398700
    },
    {
      "epoch": 0.07033468764643874,
      "grad_norm": 7.077473163604736,
      "learning_rate": 4.912081860899223e-05,
      "loss": 3.7148,
      "step": 398800
    },
    {
      "epoch": 0.07035232422809533,
      "grad_norm": 7.64016056060791,
      "learning_rate": 4.912059815172152e-05,
      "loss": 3.7331,
      "step": 398900
    },
    {
      "epoch": 0.0703699608097519,
      "grad_norm": 6.185445785522461,
      "learning_rate": 4.9120377694450806e-05,
      "loss": 3.6728,
      "step": 399000
    },
    {
      "epoch": 0.07038759739140849,
      "grad_norm": 5.652663707733154,
      "learning_rate": 4.91201572371801e-05,
      "loss": 3.6796,
      "step": 399100
    },
    {
      "epoch": 0.07040523397306506,
      "grad_norm": 8.831523895263672,
      "learning_rate": 4.91199367799094e-05,
      "loss": 3.7911,
      "step": 399200
    },
    {
      "epoch": 0.07042287055472164,
      "grad_norm": 10.562214851379395,
      "learning_rate": 4.9119716322638685e-05,
      "loss": 3.6774,
      "step": 399300
    },
    {
      "epoch": 0.07044050713637821,
      "grad_norm": 6.7490458488464355,
      "learning_rate": 4.911949586536798e-05,
      "loss": 3.6597,
      "step": 399400
    },
    {
      "epoch": 0.0704581437180348,
      "grad_norm": 6.121922969818115,
      "learning_rate": 4.9119275408097276e-05,
      "loss": 3.654,
      "step": 399500
    },
    {
      "epoch": 0.07047578029969137,
      "grad_norm": 7.720916271209717,
      "learning_rate": 4.9119054950826565e-05,
      "loss": 3.7928,
      "step": 399600
    },
    {
      "epoch": 0.07049341688134796,
      "grad_norm": 6.958128452301025,
      "learning_rate": 4.911883449355586e-05,
      "loss": 3.6315,
      "step": 399700
    },
    {
      "epoch": 0.07051105346300454,
      "grad_norm": 6.03531551361084,
      "learning_rate": 4.9118614036285156e-05,
      "loss": 3.6621,
      "step": 399800
    },
    {
      "epoch": 0.07052869004466111,
      "grad_norm": 8.063187599182129,
      "learning_rate": 4.9118393579014445e-05,
      "loss": 3.7582,
      "step": 399900
    },
    {
      "epoch": 0.0705463266263177,
      "grad_norm": 10.783177375793457,
      "learning_rate": 4.911817312174374e-05,
      "loss": 3.8798,
      "step": 400000
    },
    {
      "epoch": 0.07056396320797427,
      "grad_norm": 10.728376388549805,
      "learning_rate": 4.9117952664473036e-05,
      "loss": 3.689,
      "step": 400100
    },
    {
      "epoch": 0.07058159978963086,
      "grad_norm": 5.8860955238342285,
      "learning_rate": 4.9117732207202324e-05,
      "loss": 3.762,
      "step": 400200
    },
    {
      "epoch": 0.07059923637128743,
      "grad_norm": 5.456374645233154,
      "learning_rate": 4.911751174993161e-05,
      "loss": 3.6606,
      "step": 400300
    },
    {
      "epoch": 0.07061687295294401,
      "grad_norm": 8.30044937133789,
      "learning_rate": 4.911729129266091e-05,
      "loss": 3.6941,
      "step": 400400
    },
    {
      "epoch": 0.0706345095346006,
      "grad_norm": 11.058866500854492,
      "learning_rate": 4.91170708353902e-05,
      "loss": 3.7129,
      "step": 400500
    },
    {
      "epoch": 0.07065214611625717,
      "grad_norm": 8.15324592590332,
      "learning_rate": 4.911685037811949e-05,
      "loss": 3.7213,
      "step": 400600
    },
    {
      "epoch": 0.07066978269791375,
      "grad_norm": 6.106086254119873,
      "learning_rate": 4.911662992084879e-05,
      "loss": 3.663,
      "step": 400700
    },
    {
      "epoch": 0.07068741927957033,
      "grad_norm": 5.693040370941162,
      "learning_rate": 4.911640946357808e-05,
      "loss": 3.7825,
      "step": 400800
    },
    {
      "epoch": 0.07070505586122691,
      "grad_norm": 6.202041149139404,
      "learning_rate": 4.911618900630737e-05,
      "loss": 3.6433,
      "step": 400900
    },
    {
      "epoch": 0.07072269244288348,
      "grad_norm": 5.097118854522705,
      "learning_rate": 4.911596854903667e-05,
      "loss": 3.7279,
      "step": 401000
    },
    {
      "epoch": 0.07074032902454007,
      "grad_norm": 11.060967445373535,
      "learning_rate": 4.9115748091765956e-05,
      "loss": 3.7255,
      "step": 401100
    },
    {
      "epoch": 0.07075796560619665,
      "grad_norm": 6.036325454711914,
      "learning_rate": 4.911552763449525e-05,
      "loss": 3.7061,
      "step": 401200
    },
    {
      "epoch": 0.07077560218785323,
      "grad_norm": 6.377003192901611,
      "learning_rate": 4.911530717722455e-05,
      "loss": 3.7166,
      "step": 401300
    },
    {
      "epoch": 0.07079323876950981,
      "grad_norm": 9.84842300415039,
      "learning_rate": 4.9115086719953836e-05,
      "loss": 3.7014,
      "step": 401400
    },
    {
      "epoch": 0.07081087535116638,
      "grad_norm": 13.411954879760742,
      "learning_rate": 4.911486626268313e-05,
      "loss": 3.7027,
      "step": 401500
    },
    {
      "epoch": 0.07082851193282297,
      "grad_norm": 7.8801984786987305,
      "learning_rate": 4.911464580541243e-05,
      "loss": 3.6675,
      "step": 401600
    },
    {
      "epoch": 0.07084614851447954,
      "grad_norm": 10.504977226257324,
      "learning_rate": 4.9114425348141716e-05,
      "loss": 3.7982,
      "step": 401700
    },
    {
      "epoch": 0.07086378509613613,
      "grad_norm": 9.627877235412598,
      "learning_rate": 4.9114204890871004e-05,
      "loss": 3.6758,
      "step": 401800
    },
    {
      "epoch": 0.07088142167779271,
      "grad_norm": 8.95875358581543,
      "learning_rate": 4.91139844336003e-05,
      "loss": 3.6435,
      "step": 401900
    },
    {
      "epoch": 0.07089905825944928,
      "grad_norm": 6.9437055587768555,
      "learning_rate": 4.911376397632959e-05,
      "loss": 3.689,
      "step": 402000
    },
    {
      "epoch": 0.07091669484110587,
      "grad_norm": 7.222600936889648,
      "learning_rate": 4.9113543519058884e-05,
      "loss": 3.6769,
      "step": 402100
    },
    {
      "epoch": 0.07093433142276244,
      "grad_norm": 9.24088191986084,
      "learning_rate": 4.911332306178818e-05,
      "loss": 3.6873,
      "step": 402200
    },
    {
      "epoch": 0.07095196800441902,
      "grad_norm": 15.084803581237793,
      "learning_rate": 4.911310260451747e-05,
      "loss": 3.6602,
      "step": 402300
    },
    {
      "epoch": 0.0709696045860756,
      "grad_norm": 5.156674385070801,
      "learning_rate": 4.9112882147246764e-05,
      "loss": 3.7311,
      "step": 402400
    },
    {
      "epoch": 0.07098724116773218,
      "grad_norm": 7.612294673919678,
      "learning_rate": 4.911266168997606e-05,
      "loss": 3.7912,
      "step": 402500
    },
    {
      "epoch": 0.07100487774938875,
      "grad_norm": 7.729225158691406,
      "learning_rate": 4.911244123270535e-05,
      "loss": 3.6941,
      "step": 402600
    },
    {
      "epoch": 0.07102251433104534,
      "grad_norm": 7.338106155395508,
      "learning_rate": 4.911222077543464e-05,
      "loss": 3.7047,
      "step": 402700
    },
    {
      "epoch": 0.07104015091270192,
      "grad_norm": 6.065176963806152,
      "learning_rate": 4.911200031816394e-05,
      "loss": 3.727,
      "step": 402800
    },
    {
      "epoch": 0.0710577874943585,
      "grad_norm": 6.561781883239746,
      "learning_rate": 4.911177986089323e-05,
      "loss": 3.7252,
      "step": 402900
    },
    {
      "epoch": 0.07107542407601508,
      "grad_norm": 8.427323341369629,
      "learning_rate": 4.911155940362252e-05,
      "loss": 3.7224,
      "step": 403000
    },
    {
      "epoch": 0.07109306065767165,
      "grad_norm": 7.406466960906982,
      "learning_rate": 4.911133894635181e-05,
      "loss": 3.5524,
      "step": 403100
    },
    {
      "epoch": 0.07111069723932824,
      "grad_norm": 10.022547721862793,
      "learning_rate": 4.911111848908111e-05,
      "loss": 3.8381,
      "step": 403200
    },
    {
      "epoch": 0.07112833382098481,
      "grad_norm": 6.973726272583008,
      "learning_rate": 4.9110898031810396e-05,
      "loss": 3.8148,
      "step": 403300
    },
    {
      "epoch": 0.0711459704026414,
      "grad_norm": 6.11339807510376,
      "learning_rate": 4.911067757453969e-05,
      "loss": 3.6253,
      "step": 403400
    },
    {
      "epoch": 0.07116360698429798,
      "grad_norm": 10.293334007263184,
      "learning_rate": 4.911045711726898e-05,
      "loss": 3.756,
      "step": 403500
    },
    {
      "epoch": 0.07118124356595455,
      "grad_norm": 13.037920951843262,
      "learning_rate": 4.9110236659998275e-05,
      "loss": 3.6867,
      "step": 403600
    },
    {
      "epoch": 0.07119888014761114,
      "grad_norm": 7.093772888183594,
      "learning_rate": 4.911001620272757e-05,
      "loss": 3.6742,
      "step": 403700
    },
    {
      "epoch": 0.07121651672926771,
      "grad_norm": 7.542945861816406,
      "learning_rate": 4.910979574545686e-05,
      "loss": 3.6566,
      "step": 403800
    },
    {
      "epoch": 0.0712341533109243,
      "grad_norm": 5.275257587432861,
      "learning_rate": 4.9109575288186155e-05,
      "loss": 3.8303,
      "step": 403900
    },
    {
      "epoch": 0.07125178989258087,
      "grad_norm": 9.69286060333252,
      "learning_rate": 4.910935483091545e-05,
      "loss": 3.761,
      "step": 404000
    },
    {
      "epoch": 0.07126942647423745,
      "grad_norm": 5.138851642608643,
      "learning_rate": 4.9109134373644746e-05,
      "loss": 3.623,
      "step": 404100
    },
    {
      "epoch": 0.07128706305589404,
      "grad_norm": 7.053598880767822,
      "learning_rate": 4.9108913916374035e-05,
      "loss": 3.7014,
      "step": 404200
    },
    {
      "epoch": 0.07130469963755061,
      "grad_norm": 6.8813652992248535,
      "learning_rate": 4.910869345910333e-05,
      "loss": 3.8205,
      "step": 404300
    },
    {
      "epoch": 0.0713223362192072,
      "grad_norm": 6.323674201965332,
      "learning_rate": 4.9108473001832626e-05,
      "loss": 3.8399,
      "step": 404400
    },
    {
      "epoch": 0.07133997280086377,
      "grad_norm": 8.675958633422852,
      "learning_rate": 4.9108252544561914e-05,
      "loss": 3.6655,
      "step": 404500
    },
    {
      "epoch": 0.07135760938252035,
      "grad_norm": 9.627309799194336,
      "learning_rate": 4.91080320872912e-05,
      "loss": 3.6621,
      "step": 404600
    },
    {
      "epoch": 0.07137524596417692,
      "grad_norm": 5.695810317993164,
      "learning_rate": 4.91078116300205e-05,
      "loss": 3.7865,
      "step": 404700
    },
    {
      "epoch": 0.07139288254583351,
      "grad_norm": 7.866023540496826,
      "learning_rate": 4.910759117274979e-05,
      "loss": 3.7025,
      "step": 404800
    },
    {
      "epoch": 0.0714105191274901,
      "grad_norm": 7.973536014556885,
      "learning_rate": 4.910737071547908e-05,
      "loss": 3.779,
      "step": 404900
    },
    {
      "epoch": 0.07142815570914667,
      "grad_norm": 7.371437072753906,
      "learning_rate": 4.910715025820838e-05,
      "loss": 3.7298,
      "step": 405000
    },
    {
      "epoch": 0.07144579229080325,
      "grad_norm": 10.060458183288574,
      "learning_rate": 4.910692980093767e-05,
      "loss": 3.6635,
      "step": 405100
    },
    {
      "epoch": 0.07146342887245982,
      "grad_norm": 6.815676212310791,
      "learning_rate": 4.910670934366696e-05,
      "loss": 3.7237,
      "step": 405200
    },
    {
      "epoch": 0.07148106545411641,
      "grad_norm": 9.459431648254395,
      "learning_rate": 4.910648888639626e-05,
      "loss": 3.7451,
      "step": 405300
    },
    {
      "epoch": 0.07149870203577298,
      "grad_norm": 5.966718673706055,
      "learning_rate": 4.9106268429125546e-05,
      "loss": 3.6233,
      "step": 405400
    },
    {
      "epoch": 0.07151633861742956,
      "grad_norm": 6.638185977935791,
      "learning_rate": 4.910604797185484e-05,
      "loss": 3.6719,
      "step": 405500
    },
    {
      "epoch": 0.07153397519908614,
      "grad_norm": 7.098903656005859,
      "learning_rate": 4.910582751458414e-05,
      "loss": 3.6276,
      "step": 405600
    },
    {
      "epoch": 0.07155161178074272,
      "grad_norm": 5.9847187995910645,
      "learning_rate": 4.9105607057313426e-05,
      "loss": 3.6799,
      "step": 405700
    },
    {
      "epoch": 0.07156924836239931,
      "grad_norm": 8.198453903198242,
      "learning_rate": 4.910538660004272e-05,
      "loss": 3.6762,
      "step": 405800
    },
    {
      "epoch": 0.07158688494405588,
      "grad_norm": 7.017387390136719,
      "learning_rate": 4.910516614277201e-05,
      "loss": 3.6922,
      "step": 405900
    },
    {
      "epoch": 0.07160452152571246,
      "grad_norm": 6.216415882110596,
      "learning_rate": 4.9104945685501306e-05,
      "loss": 3.5919,
      "step": 406000
    },
    {
      "epoch": 0.07162215810736904,
      "grad_norm": 4.795767784118652,
      "learning_rate": 4.9104725228230594e-05,
      "loss": 3.7529,
      "step": 406100
    },
    {
      "epoch": 0.07163979468902562,
      "grad_norm": 7.340813636779785,
      "learning_rate": 4.910450477095989e-05,
      "loss": 3.7394,
      "step": 406200
    },
    {
      "epoch": 0.07165743127068219,
      "grad_norm": 8.351438522338867,
      "learning_rate": 4.910428431368918e-05,
      "loss": 3.607,
      "step": 406300
    },
    {
      "epoch": 0.07167506785233878,
      "grad_norm": 9.781754493713379,
      "learning_rate": 4.9104063856418474e-05,
      "loss": 3.7375,
      "step": 406400
    },
    {
      "epoch": 0.07169270443399536,
      "grad_norm": 7.557653903961182,
      "learning_rate": 4.910384339914777e-05,
      "loss": 3.7694,
      "step": 406500
    },
    {
      "epoch": 0.07171034101565193,
      "grad_norm": 8.136774063110352,
      "learning_rate": 4.910362294187706e-05,
      "loss": 3.5916,
      "step": 406600
    },
    {
      "epoch": 0.07172797759730852,
      "grad_norm": 7.148400783538818,
      "learning_rate": 4.9103402484606354e-05,
      "loss": 3.6476,
      "step": 406700
    },
    {
      "epoch": 0.07174561417896509,
      "grad_norm": 9.795811653137207,
      "learning_rate": 4.910318202733565e-05,
      "loss": 3.7133,
      "step": 406800
    },
    {
      "epoch": 0.07176325076062168,
      "grad_norm": 8.168766021728516,
      "learning_rate": 4.910296157006494e-05,
      "loss": 3.6749,
      "step": 406900
    },
    {
      "epoch": 0.07178088734227825,
      "grad_norm": 7.524574279785156,
      "learning_rate": 4.910274111279423e-05,
      "loss": 3.7023,
      "step": 407000
    },
    {
      "epoch": 0.07179852392393483,
      "grad_norm": 5.6265764236450195,
      "learning_rate": 4.910252065552353e-05,
      "loss": 3.7343,
      "step": 407100
    },
    {
      "epoch": 0.07181616050559142,
      "grad_norm": 6.800334930419922,
      "learning_rate": 4.910230019825282e-05,
      "loss": 3.652,
      "step": 407200
    },
    {
      "epoch": 0.07183379708724799,
      "grad_norm": 8.122955322265625,
      "learning_rate": 4.910207974098211e-05,
      "loss": 3.6957,
      "step": 407300
    },
    {
      "epoch": 0.07185143366890458,
      "grad_norm": 7.69876766204834,
      "learning_rate": 4.91018592837114e-05,
      "loss": 3.8142,
      "step": 407400
    },
    {
      "epoch": 0.07186907025056115,
      "grad_norm": 6.803778648376465,
      "learning_rate": 4.910163882644069e-05,
      "loss": 3.6963,
      "step": 407500
    },
    {
      "epoch": 0.07188670683221773,
      "grad_norm": 11.415600776672363,
      "learning_rate": 4.9101418369169986e-05,
      "loss": 3.739,
      "step": 407600
    },
    {
      "epoch": 0.0719043434138743,
      "grad_norm": 5.516503810882568,
      "learning_rate": 4.910119791189928e-05,
      "loss": 3.5878,
      "step": 407700
    },
    {
      "epoch": 0.07192197999553089,
      "grad_norm": 7.664919376373291,
      "learning_rate": 4.910097745462857e-05,
      "loss": 3.7661,
      "step": 407800
    },
    {
      "epoch": 0.07193961657718748,
      "grad_norm": 8.884929656982422,
      "learning_rate": 4.9100756997357865e-05,
      "loss": 3.7423,
      "step": 407900
    },
    {
      "epoch": 0.07195725315884405,
      "grad_norm": 8.116252899169922,
      "learning_rate": 4.910053654008716e-05,
      "loss": 3.722,
      "step": 408000
    },
    {
      "epoch": 0.07197488974050063,
      "grad_norm": 11.935447692871094,
      "learning_rate": 4.910031608281645e-05,
      "loss": 3.7465,
      "step": 408100
    },
    {
      "epoch": 0.0719925263221572,
      "grad_norm": 6.049992084503174,
      "learning_rate": 4.9100095625545745e-05,
      "loss": 3.5757,
      "step": 408200
    },
    {
      "epoch": 0.07201016290381379,
      "grad_norm": 6.796011924743652,
      "learning_rate": 4.909987516827504e-05,
      "loss": 3.7712,
      "step": 408300
    },
    {
      "epoch": 0.07202779948547036,
      "grad_norm": 7.1714067459106445,
      "learning_rate": 4.909965471100433e-05,
      "loss": 3.5924,
      "step": 408400
    },
    {
      "epoch": 0.07204543606712695,
      "grad_norm": 6.672080039978027,
      "learning_rate": 4.9099434253733625e-05,
      "loss": 3.696,
      "step": 408500
    },
    {
      "epoch": 0.07206307264878352,
      "grad_norm": 6.8541083335876465,
      "learning_rate": 4.909921379646292e-05,
      "loss": 3.7171,
      "step": 408600
    },
    {
      "epoch": 0.0720807092304401,
      "grad_norm": 5.412881851196289,
      "learning_rate": 4.909899333919221e-05,
      "loss": 3.6775,
      "step": 408700
    },
    {
      "epoch": 0.07209834581209669,
      "grad_norm": 7.261142730712891,
      "learning_rate": 4.9098772881921504e-05,
      "loss": 3.6804,
      "step": 408800
    },
    {
      "epoch": 0.07211598239375326,
      "grad_norm": 7.148397922515869,
      "learning_rate": 4.909855242465079e-05,
      "loss": 3.6919,
      "step": 408900
    },
    {
      "epoch": 0.07213361897540985,
      "grad_norm": 8.766236305236816,
      "learning_rate": 4.909833196738008e-05,
      "loss": 3.7777,
      "step": 409000
    },
    {
      "epoch": 0.07215125555706642,
      "grad_norm": 7.4585137367248535,
      "learning_rate": 4.909811151010938e-05,
      "loss": 3.7899,
      "step": 409100
    },
    {
      "epoch": 0.072168892138723,
      "grad_norm": 6.684548377990723,
      "learning_rate": 4.909789105283867e-05,
      "loss": 3.7072,
      "step": 409200
    },
    {
      "epoch": 0.07218652872037958,
      "grad_norm": 8.031394004821777,
      "learning_rate": 4.909767059556796e-05,
      "loss": 3.7221,
      "step": 409300
    },
    {
      "epoch": 0.07220416530203616,
      "grad_norm": 10.349508285522461,
      "learning_rate": 4.909745013829726e-05,
      "loss": 3.8209,
      "step": 409400
    },
    {
      "epoch": 0.07222180188369275,
      "grad_norm": 6.604257106781006,
      "learning_rate": 4.909722968102655e-05,
      "loss": 3.6306,
      "step": 409500
    },
    {
      "epoch": 0.07223943846534932,
      "grad_norm": 8.274561882019043,
      "learning_rate": 4.909700922375584e-05,
      "loss": 3.6546,
      "step": 409600
    },
    {
      "epoch": 0.0722570750470059,
      "grad_norm": 5.624370098114014,
      "learning_rate": 4.9096788766485136e-05,
      "loss": 3.8281,
      "step": 409700
    },
    {
      "epoch": 0.07227471162866247,
      "grad_norm": 5.768036365509033,
      "learning_rate": 4.909656830921443e-05,
      "loss": 3.6491,
      "step": 409800
    },
    {
      "epoch": 0.07229234821031906,
      "grad_norm": 9.214250564575195,
      "learning_rate": 4.909634785194372e-05,
      "loss": 3.7792,
      "step": 409900
    },
    {
      "epoch": 0.07230998479197563,
      "grad_norm": 9.511454582214355,
      "learning_rate": 4.9096127394673016e-05,
      "loss": 3.7171,
      "step": 410000
    },
    {
      "epoch": 0.07232762137363222,
      "grad_norm": 11.779867172241211,
      "learning_rate": 4.909590693740231e-05,
      "loss": 3.6202,
      "step": 410100
    },
    {
      "epoch": 0.0723452579552888,
      "grad_norm": 8.321614265441895,
      "learning_rate": 4.90956864801316e-05,
      "loss": 3.6897,
      "step": 410200
    },
    {
      "epoch": 0.07236289453694537,
      "grad_norm": 5.107521057128906,
      "learning_rate": 4.909546602286089e-05,
      "loss": 3.7425,
      "step": 410300
    },
    {
      "epoch": 0.07238053111860196,
      "grad_norm": 6.176507472991943,
      "learning_rate": 4.9095245565590184e-05,
      "loss": 3.7866,
      "step": 410400
    },
    {
      "epoch": 0.07239816770025853,
      "grad_norm": 10.278783798217773,
      "learning_rate": 4.909502510831947e-05,
      "loss": 3.7183,
      "step": 410500
    },
    {
      "epoch": 0.07241580428191512,
      "grad_norm": 12.020923614501953,
      "learning_rate": 4.909480465104877e-05,
      "loss": 3.7511,
      "step": 410600
    },
    {
      "epoch": 0.07243344086357169,
      "grad_norm": 6.714798450469971,
      "learning_rate": 4.9094584193778064e-05,
      "loss": 3.7136,
      "step": 410700
    },
    {
      "epoch": 0.07245107744522827,
      "grad_norm": 7.876855373382568,
      "learning_rate": 4.909436373650735e-05,
      "loss": 3.7393,
      "step": 410800
    },
    {
      "epoch": 0.07246871402688486,
      "grad_norm": 7.0460991859436035,
      "learning_rate": 4.909414327923665e-05,
      "loss": 3.6946,
      "step": 410900
    },
    {
      "epoch": 0.07248635060854143,
      "grad_norm": 5.760002136230469,
      "learning_rate": 4.9093922821965944e-05,
      "loss": 3.7156,
      "step": 411000
    },
    {
      "epoch": 0.07250398719019802,
      "grad_norm": 8.155281066894531,
      "learning_rate": 4.909370236469523e-05,
      "loss": 3.6253,
      "step": 411100
    },
    {
      "epoch": 0.07252162377185459,
      "grad_norm": 6.150388240814209,
      "learning_rate": 4.909348190742453e-05,
      "loss": 3.6019,
      "step": 411200
    },
    {
      "epoch": 0.07253926035351117,
      "grad_norm": 7.332658290863037,
      "learning_rate": 4.909326145015382e-05,
      "loss": 3.7313,
      "step": 411300
    },
    {
      "epoch": 0.07255689693516774,
      "grad_norm": 6.617134094238281,
      "learning_rate": 4.909304099288311e-05,
      "loss": 3.5726,
      "step": 411400
    },
    {
      "epoch": 0.07257453351682433,
      "grad_norm": 8.307900428771973,
      "learning_rate": 4.909282053561241e-05,
      "loss": 3.7893,
      "step": 411500
    },
    {
      "epoch": 0.0725921700984809,
      "grad_norm": 7.054543972015381,
      "learning_rate": 4.90926000783417e-05,
      "loss": 3.7805,
      "step": 411600
    },
    {
      "epoch": 0.07260980668013749,
      "grad_norm": 6.950910568237305,
      "learning_rate": 4.909237962107099e-05,
      "loss": 3.8189,
      "step": 411700
    },
    {
      "epoch": 0.07262744326179407,
      "grad_norm": 8.197530746459961,
      "learning_rate": 4.909215916380028e-05,
      "loss": 3.6961,
      "step": 411800
    },
    {
      "epoch": 0.07264507984345064,
      "grad_norm": 7.227577209472656,
      "learning_rate": 4.9091938706529576e-05,
      "loss": 3.7438,
      "step": 411900
    },
    {
      "epoch": 0.07266271642510723,
      "grad_norm": 5.89053201675415,
      "learning_rate": 4.9091718249258864e-05,
      "loss": 3.7439,
      "step": 412000
    },
    {
      "epoch": 0.0726803530067638,
      "grad_norm": 6.488180160522461,
      "learning_rate": 4.909149779198816e-05,
      "loss": 3.622,
      "step": 412100
    },
    {
      "epoch": 0.07269798958842039,
      "grad_norm": 5.112366676330566,
      "learning_rate": 4.9091277334717455e-05,
      "loss": 3.7325,
      "step": 412200
    },
    {
      "epoch": 0.07271562617007696,
      "grad_norm": 9.727319717407227,
      "learning_rate": 4.9091056877446744e-05,
      "loss": 3.6791,
      "step": 412300
    },
    {
      "epoch": 0.07273326275173354,
      "grad_norm": 6.833263874053955,
      "learning_rate": 4.909083642017604e-05,
      "loss": 3.7546,
      "step": 412400
    },
    {
      "epoch": 0.07275089933339013,
      "grad_norm": 5.571951866149902,
      "learning_rate": 4.9090615962905335e-05,
      "loss": 3.6656,
      "step": 412500
    },
    {
      "epoch": 0.0727685359150467,
      "grad_norm": 9.103586196899414,
      "learning_rate": 4.9090395505634624e-05,
      "loss": 3.6265,
      "step": 412600
    },
    {
      "epoch": 0.07278617249670329,
      "grad_norm": 6.589406490325928,
      "learning_rate": 4.909017504836392e-05,
      "loss": 3.7338,
      "step": 412700
    },
    {
      "epoch": 0.07280380907835986,
      "grad_norm": 5.946054458618164,
      "learning_rate": 4.9089954591093215e-05,
      "loss": 3.7029,
      "step": 412800
    },
    {
      "epoch": 0.07282144566001644,
      "grad_norm": 8.125171661376953,
      "learning_rate": 4.90897341338225e-05,
      "loss": 3.6571,
      "step": 412900
    },
    {
      "epoch": 0.07283908224167301,
      "grad_norm": 5.158370494842529,
      "learning_rate": 4.90895136765518e-05,
      "loss": 3.6719,
      "step": 413000
    },
    {
      "epoch": 0.0728567188233296,
      "grad_norm": 7.324016094207764,
      "learning_rate": 4.908929321928109e-05,
      "loss": 3.6326,
      "step": 413100
    },
    {
      "epoch": 0.07287435540498619,
      "grad_norm": 7.587601184844971,
      "learning_rate": 4.908907276201038e-05,
      "loss": 3.6877,
      "step": 413200
    },
    {
      "epoch": 0.07289199198664276,
      "grad_norm": 7.474213123321533,
      "learning_rate": 4.908885230473967e-05,
      "loss": 3.729,
      "step": 413300
    },
    {
      "epoch": 0.07290962856829934,
      "grad_norm": 6.166245937347412,
      "learning_rate": 4.908863184746897e-05,
      "loss": 3.6819,
      "step": 413400
    },
    {
      "epoch": 0.07292726514995591,
      "grad_norm": 7.209005832672119,
      "learning_rate": 4.9088411390198256e-05,
      "loss": 3.7116,
      "step": 413500
    },
    {
      "epoch": 0.0729449017316125,
      "grad_norm": 6.551790237426758,
      "learning_rate": 4.908819093292755e-05,
      "loss": 3.7807,
      "step": 413600
    },
    {
      "epoch": 0.07296253831326907,
      "grad_norm": 6.971299648284912,
      "learning_rate": 4.908797047565685e-05,
      "loss": 3.6896,
      "step": 413700
    },
    {
      "epoch": 0.07298017489492566,
      "grad_norm": 7.12523889541626,
      "learning_rate": 4.9087750018386135e-05,
      "loss": 3.7177,
      "step": 413800
    },
    {
      "epoch": 0.07299781147658224,
      "grad_norm": 7.619770050048828,
      "learning_rate": 4.908752956111543e-05,
      "loss": 3.7159,
      "step": 413900
    },
    {
      "epoch": 0.07301544805823881,
      "grad_norm": 5.0272297859191895,
      "learning_rate": 4.9087309103844726e-05,
      "loss": 3.7015,
      "step": 414000
    },
    {
      "epoch": 0.0730330846398954,
      "grad_norm": 9.293680191040039,
      "learning_rate": 4.9087088646574015e-05,
      "loss": 3.7075,
      "step": 414100
    },
    {
      "epoch": 0.07305072122155197,
      "grad_norm": 6.778716087341309,
      "learning_rate": 4.908686818930331e-05,
      "loss": 3.7276,
      "step": 414200
    },
    {
      "epoch": 0.07306835780320856,
      "grad_norm": 9.720865249633789,
      "learning_rate": 4.9086647732032606e-05,
      "loss": 3.7676,
      "step": 414300
    },
    {
      "epoch": 0.07308599438486513,
      "grad_norm": 9.505133628845215,
      "learning_rate": 4.9086427274761895e-05,
      "loss": 3.7541,
      "step": 414400
    },
    {
      "epoch": 0.07310363096652171,
      "grad_norm": 7.675838470458984,
      "learning_rate": 4.908620681749119e-05,
      "loss": 3.7201,
      "step": 414500
    },
    {
      "epoch": 0.07312126754817828,
      "grad_norm": 6.6727752685546875,
      "learning_rate": 4.908598636022048e-05,
      "loss": 3.8245,
      "step": 414600
    },
    {
      "epoch": 0.07313890412983487,
      "grad_norm": 6.149900913238525,
      "learning_rate": 4.9085765902949774e-05,
      "loss": 3.5958,
      "step": 414700
    },
    {
      "epoch": 0.07315654071149146,
      "grad_norm": 8.67900276184082,
      "learning_rate": 4.908554544567906e-05,
      "loss": 3.7167,
      "step": 414800
    },
    {
      "epoch": 0.07317417729314803,
      "grad_norm": 5.976672649383545,
      "learning_rate": 4.908532498840836e-05,
      "loss": 3.7469,
      "step": 414900
    },
    {
      "epoch": 0.07319181387480461,
      "grad_norm": 7.869958877563477,
      "learning_rate": 4.9085104531137654e-05,
      "loss": 3.7523,
      "step": 415000
    },
    {
      "epoch": 0.07320945045646118,
      "grad_norm": 10.265687942504883,
      "learning_rate": 4.908488407386694e-05,
      "loss": 3.6422,
      "step": 415100
    },
    {
      "epoch": 0.07322708703811777,
      "grad_norm": 8.55848217010498,
      "learning_rate": 4.908466361659624e-05,
      "loss": 3.6317,
      "step": 415200
    },
    {
      "epoch": 0.07324472361977434,
      "grad_norm": 8.928523063659668,
      "learning_rate": 4.9084443159325533e-05,
      "loss": 3.6426,
      "step": 415300
    },
    {
      "epoch": 0.07326236020143093,
      "grad_norm": 6.1357269287109375,
      "learning_rate": 4.908422270205482e-05,
      "loss": 3.6606,
      "step": 415400
    },
    {
      "epoch": 0.07327999678308751,
      "grad_norm": 12.596688270568848,
      "learning_rate": 4.908400224478412e-05,
      "loss": 3.658,
      "step": 415500
    },
    {
      "epoch": 0.07329763336474408,
      "grad_norm": 13.749735832214355,
      "learning_rate": 4.908378178751341e-05,
      "loss": 3.6437,
      "step": 415600
    },
    {
      "epoch": 0.07331526994640067,
      "grad_norm": 7.383937835693359,
      "learning_rate": 4.90835613302427e-05,
      "loss": 3.7649,
      "step": 415700
    },
    {
      "epoch": 0.07333290652805724,
      "grad_norm": 5.9699835777282715,
      "learning_rate": 4.9083340872972e-05,
      "loss": 3.7583,
      "step": 415800
    },
    {
      "epoch": 0.07335054310971383,
      "grad_norm": 9.298205375671387,
      "learning_rate": 4.9083120415701286e-05,
      "loss": 3.7571,
      "step": 415900
    },
    {
      "epoch": 0.0733681796913704,
      "grad_norm": 4.912570476531982,
      "learning_rate": 4.908289995843058e-05,
      "loss": 3.6944,
      "step": 416000
    },
    {
      "epoch": 0.07338581627302698,
      "grad_norm": 9.335928916931152,
      "learning_rate": 4.908267950115987e-05,
      "loss": 3.6962,
      "step": 416100
    },
    {
      "epoch": 0.07340345285468357,
      "grad_norm": 6.46626615524292,
      "learning_rate": 4.9082459043889166e-05,
      "loss": 3.7282,
      "step": 416200
    },
    {
      "epoch": 0.07342108943634014,
      "grad_norm": 7.611582279205322,
      "learning_rate": 4.9082238586618454e-05,
      "loss": 3.6939,
      "step": 416300
    },
    {
      "epoch": 0.07343872601799672,
      "grad_norm": 6.665298938751221,
      "learning_rate": 4.908201812934775e-05,
      "loss": 3.7093,
      "step": 416400
    },
    {
      "epoch": 0.0734563625996533,
      "grad_norm": 6.963576316833496,
      "learning_rate": 4.9081797672077045e-05,
      "loss": 3.7824,
      "step": 416500
    },
    {
      "epoch": 0.07347399918130988,
      "grad_norm": 7.274116039276123,
      "learning_rate": 4.9081577214806334e-05,
      "loss": 3.6451,
      "step": 416600
    },
    {
      "epoch": 0.07349163576296645,
      "grad_norm": 6.8381266593933105,
      "learning_rate": 4.908135675753563e-05,
      "loss": 3.765,
      "step": 416700
    },
    {
      "epoch": 0.07350927234462304,
      "grad_norm": 5.651116371154785,
      "learning_rate": 4.9081136300264925e-05,
      "loss": 3.6836,
      "step": 416800
    },
    {
      "epoch": 0.07352690892627962,
      "grad_norm": 7.376379013061523,
      "learning_rate": 4.9080915842994214e-05,
      "loss": 3.6902,
      "step": 416900
    },
    {
      "epoch": 0.0735445455079362,
      "grad_norm": 7.039083957672119,
      "learning_rate": 4.908069538572351e-05,
      "loss": 3.7155,
      "step": 417000
    },
    {
      "epoch": 0.07356218208959278,
      "grad_norm": 5.7094550132751465,
      "learning_rate": 4.9080474928452804e-05,
      "loss": 3.5629,
      "step": 417100
    },
    {
      "epoch": 0.07357981867124935,
      "grad_norm": 8.427834510803223,
      "learning_rate": 4.908025447118209e-05,
      "loss": 3.7056,
      "step": 417200
    },
    {
      "epoch": 0.07359745525290594,
      "grad_norm": 9.400764465332031,
      "learning_rate": 4.908003401391139e-05,
      "loss": 3.7149,
      "step": 417300
    },
    {
      "epoch": 0.07361509183456251,
      "grad_norm": 7.57944393157959,
      "learning_rate": 4.907981355664068e-05,
      "loss": 3.7369,
      "step": 417400
    },
    {
      "epoch": 0.0736327284162191,
      "grad_norm": 6.291823387145996,
      "learning_rate": 4.9079593099369966e-05,
      "loss": 3.7176,
      "step": 417500
    },
    {
      "epoch": 0.07365036499787567,
      "grad_norm": 7.214149475097656,
      "learning_rate": 4.907937264209926e-05,
      "loss": 3.6476,
      "step": 417600
    },
    {
      "epoch": 0.07366800157953225,
      "grad_norm": 6.101755619049072,
      "learning_rate": 4.907915218482856e-05,
      "loss": 3.6644,
      "step": 417700
    },
    {
      "epoch": 0.07368563816118884,
      "grad_norm": 9.5487060546875,
      "learning_rate": 4.9078931727557846e-05,
      "loss": 3.5889,
      "step": 417800
    },
    {
      "epoch": 0.07370327474284541,
      "grad_norm": 7.267298698425293,
      "learning_rate": 4.907871127028714e-05,
      "loss": 3.6136,
      "step": 417900
    },
    {
      "epoch": 0.073720911324502,
      "grad_norm": 10.131036758422852,
      "learning_rate": 4.9078490813016437e-05,
      "loss": 3.6839,
      "step": 418000
    },
    {
      "epoch": 0.07373854790615857,
      "grad_norm": 5.73970890045166,
      "learning_rate": 4.9078270355745725e-05,
      "loss": 3.673,
      "step": 418100
    },
    {
      "epoch": 0.07375618448781515,
      "grad_norm": 6.132469654083252,
      "learning_rate": 4.907804989847502e-05,
      "loss": 3.6696,
      "step": 418200
    },
    {
      "epoch": 0.07377382106947172,
      "grad_norm": 8.524506568908691,
      "learning_rate": 4.9077829441204316e-05,
      "loss": 3.6801,
      "step": 418300
    },
    {
      "epoch": 0.07379145765112831,
      "grad_norm": 8.44619369506836,
      "learning_rate": 4.9077608983933605e-05,
      "loss": 3.7316,
      "step": 418400
    },
    {
      "epoch": 0.0738090942327849,
      "grad_norm": 6.982933044433594,
      "learning_rate": 4.90773885266629e-05,
      "loss": 3.7548,
      "step": 418500
    },
    {
      "epoch": 0.07382673081444147,
      "grad_norm": 6.854853630065918,
      "learning_rate": 4.9077168069392196e-05,
      "loss": 3.7923,
      "step": 418600
    },
    {
      "epoch": 0.07384436739609805,
      "grad_norm": 11.227926254272461,
      "learning_rate": 4.9076947612121485e-05,
      "loss": 3.7244,
      "step": 418700
    },
    {
      "epoch": 0.07386200397775462,
      "grad_norm": 9.829475402832031,
      "learning_rate": 4.907672715485078e-05,
      "loss": 3.6738,
      "step": 418800
    },
    {
      "epoch": 0.07387964055941121,
      "grad_norm": 9.713373184204102,
      "learning_rate": 4.907650669758007e-05,
      "loss": 3.64,
      "step": 418900
    },
    {
      "epoch": 0.07389727714106778,
      "grad_norm": 5.69442892074585,
      "learning_rate": 4.907628624030936e-05,
      "loss": 3.6761,
      "step": 419000
    },
    {
      "epoch": 0.07391491372272437,
      "grad_norm": 7.045242786407471,
      "learning_rate": 4.907606578303865e-05,
      "loss": 3.6058,
      "step": 419100
    },
    {
      "epoch": 0.07393255030438095,
      "grad_norm": 8.599491119384766,
      "learning_rate": 4.907584532576795e-05,
      "loss": 3.63,
      "step": 419200
    },
    {
      "epoch": 0.07395018688603752,
      "grad_norm": 7.82052755355835,
      "learning_rate": 4.907562486849724e-05,
      "loss": 3.6777,
      "step": 419300
    },
    {
      "epoch": 0.07396782346769411,
      "grad_norm": 8.669271469116211,
      "learning_rate": 4.907540441122653e-05,
      "loss": 3.724,
      "step": 419400
    },
    {
      "epoch": 0.07398546004935068,
      "grad_norm": 7.962042808532715,
      "learning_rate": 4.907518395395583e-05,
      "loss": 3.7283,
      "step": 419500
    },
    {
      "epoch": 0.07400309663100726,
      "grad_norm": 8.514822959899902,
      "learning_rate": 4.907496349668512e-05,
      "loss": 3.7628,
      "step": 419600
    },
    {
      "epoch": 0.07402073321266384,
      "grad_norm": 5.669084072113037,
      "learning_rate": 4.907474303941441e-05,
      "loss": 3.7546,
      "step": 419700
    },
    {
      "epoch": 0.07403836979432042,
      "grad_norm": 10.495827674865723,
      "learning_rate": 4.907452258214371e-05,
      "loss": 3.5725,
      "step": 419800
    },
    {
      "epoch": 0.07405600637597701,
      "grad_norm": 6.711654186248779,
      "learning_rate": 4.9074302124872996e-05,
      "loss": 3.6876,
      "step": 419900
    },
    {
      "epoch": 0.07407364295763358,
      "grad_norm": 9.624077796936035,
      "learning_rate": 4.907408166760229e-05,
      "loss": 3.6233,
      "step": 420000
    },
    {
      "epoch": 0.07409127953929016,
      "grad_norm": 6.206813335418701,
      "learning_rate": 4.907386121033159e-05,
      "loss": 3.7048,
      "step": 420100
    },
    {
      "epoch": 0.07410891612094674,
      "grad_norm": 6.6700263023376465,
      "learning_rate": 4.9073640753060876e-05,
      "loss": 3.6135,
      "step": 420200
    },
    {
      "epoch": 0.07412655270260332,
      "grad_norm": 13.629373550415039,
      "learning_rate": 4.9073420295790165e-05,
      "loss": 3.6881,
      "step": 420300
    },
    {
      "epoch": 0.07414418928425989,
      "grad_norm": 6.34145450592041,
      "learning_rate": 4.907319983851946e-05,
      "loss": 3.666,
      "step": 420400
    },
    {
      "epoch": 0.07416182586591648,
      "grad_norm": 10.386804580688477,
      "learning_rate": 4.907297938124875e-05,
      "loss": 3.7029,
      "step": 420500
    },
    {
      "epoch": 0.07417946244757305,
      "grad_norm": 9.01113510131836,
      "learning_rate": 4.9072758923978044e-05,
      "loss": 3.7804,
      "step": 420600
    },
    {
      "epoch": 0.07419709902922964,
      "grad_norm": 8.31973648071289,
      "learning_rate": 4.907253846670734e-05,
      "loss": 3.7217,
      "step": 420700
    },
    {
      "epoch": 0.07421473561088622,
      "grad_norm": 7.6748528480529785,
      "learning_rate": 4.907231800943663e-05,
      "loss": 3.6507,
      "step": 420800
    },
    {
      "epoch": 0.07423237219254279,
      "grad_norm": 7.025804042816162,
      "learning_rate": 4.9072097552165924e-05,
      "loss": 3.7672,
      "step": 420900
    },
    {
      "epoch": 0.07425000877419938,
      "grad_norm": 8.510685920715332,
      "learning_rate": 4.907187709489522e-05,
      "loss": 3.6637,
      "step": 421000
    },
    {
      "epoch": 0.07426764535585595,
      "grad_norm": 5.948276996612549,
      "learning_rate": 4.907165663762451e-05,
      "loss": 3.7528,
      "step": 421100
    },
    {
      "epoch": 0.07428528193751253,
      "grad_norm": 5.721589088439941,
      "learning_rate": 4.9071436180353803e-05,
      "loss": 3.682,
      "step": 421200
    },
    {
      "epoch": 0.0743029185191691,
      "grad_norm": 6.753368377685547,
      "learning_rate": 4.90712157230831e-05,
      "loss": 3.7436,
      "step": 421300
    },
    {
      "epoch": 0.07432055510082569,
      "grad_norm": 6.195378303527832,
      "learning_rate": 4.907099526581239e-05,
      "loss": 3.6998,
      "step": 421400
    },
    {
      "epoch": 0.07433819168248228,
      "grad_norm": 5.512063980102539,
      "learning_rate": 4.907077480854168e-05,
      "loss": 3.7476,
      "step": 421500
    },
    {
      "epoch": 0.07435582826413885,
      "grad_norm": 7.343189239501953,
      "learning_rate": 4.907055435127098e-05,
      "loss": 3.733,
      "step": 421600
    },
    {
      "epoch": 0.07437346484579543,
      "grad_norm": 8.793229103088379,
      "learning_rate": 4.907033389400027e-05,
      "loss": 3.6955,
      "step": 421700
    },
    {
      "epoch": 0.074391101427452,
      "grad_norm": 5.986274242401123,
      "learning_rate": 4.9070113436729556e-05,
      "loss": 3.7347,
      "step": 421800
    },
    {
      "epoch": 0.07440873800910859,
      "grad_norm": 5.588979244232178,
      "learning_rate": 4.906989297945885e-05,
      "loss": 3.7387,
      "step": 421900
    },
    {
      "epoch": 0.07442637459076516,
      "grad_norm": 7.501157760620117,
      "learning_rate": 4.906967252218814e-05,
      "loss": 3.6272,
      "step": 422000
    },
    {
      "epoch": 0.07444401117242175,
      "grad_norm": 7.0064544677734375,
      "learning_rate": 4.9069452064917436e-05,
      "loss": 3.4941,
      "step": 422100
    },
    {
      "epoch": 0.07446164775407833,
      "grad_norm": 7.71759557723999,
      "learning_rate": 4.906923160764673e-05,
      "loss": 3.7036,
      "step": 422200
    },
    {
      "epoch": 0.0744792843357349,
      "grad_norm": 6.193471431732178,
      "learning_rate": 4.906901115037602e-05,
      "loss": 3.7143,
      "step": 422300
    },
    {
      "epoch": 0.07449692091739149,
      "grad_norm": 7.394794464111328,
      "learning_rate": 4.9068790693105315e-05,
      "loss": 3.6726,
      "step": 422400
    },
    {
      "epoch": 0.07451455749904806,
      "grad_norm": 5.581480979919434,
      "learning_rate": 4.906857023583461e-05,
      "loss": 3.6779,
      "step": 422500
    },
    {
      "epoch": 0.07453219408070465,
      "grad_norm": 7.841042518615723,
      "learning_rate": 4.90683497785639e-05,
      "loss": 3.6252,
      "step": 422600
    },
    {
      "epoch": 0.07454983066236122,
      "grad_norm": 6.165823459625244,
      "learning_rate": 4.9068129321293195e-05,
      "loss": 3.584,
      "step": 422700
    },
    {
      "epoch": 0.0745674672440178,
      "grad_norm": 9.29411506652832,
      "learning_rate": 4.906790886402249e-05,
      "loss": 3.6683,
      "step": 422800
    },
    {
      "epoch": 0.07458510382567439,
      "grad_norm": 8.670144081115723,
      "learning_rate": 4.906768840675178e-05,
      "loss": 3.7262,
      "step": 422900
    },
    {
      "epoch": 0.07460274040733096,
      "grad_norm": 7.155746936798096,
      "learning_rate": 4.9067467949481074e-05,
      "loss": 3.7383,
      "step": 423000
    },
    {
      "epoch": 0.07462037698898755,
      "grad_norm": 8.906420707702637,
      "learning_rate": 4.906724749221036e-05,
      "loss": 3.6076,
      "step": 423100
    },
    {
      "epoch": 0.07463801357064412,
      "grad_norm": 8.10021686553955,
      "learning_rate": 4.906702703493966e-05,
      "loss": 3.7362,
      "step": 423200
    },
    {
      "epoch": 0.0746556501523007,
      "grad_norm": 5.768420219421387,
      "learning_rate": 4.906680657766895e-05,
      "loss": 3.737,
      "step": 423300
    },
    {
      "epoch": 0.07467328673395728,
      "grad_norm": 10.266963005065918,
      "learning_rate": 4.906658612039824e-05,
      "loss": 3.715,
      "step": 423400
    },
    {
      "epoch": 0.07469092331561386,
      "grad_norm": 8.857121467590332,
      "learning_rate": 4.906636566312753e-05,
      "loss": 3.7037,
      "step": 423500
    },
    {
      "epoch": 0.07470855989727043,
      "grad_norm": 6.681493282318115,
      "learning_rate": 4.906614520585683e-05,
      "loss": 3.6734,
      "step": 423600
    },
    {
      "epoch": 0.07472619647892702,
      "grad_norm": 9.153873443603516,
      "learning_rate": 4.906592474858612e-05,
      "loss": 3.6758,
      "step": 423700
    },
    {
      "epoch": 0.0747438330605836,
      "grad_norm": 5.409324645996094,
      "learning_rate": 4.906570429131541e-05,
      "loss": 3.5727,
      "step": 423800
    },
    {
      "epoch": 0.07476146964224017,
      "grad_norm": 6.543639183044434,
      "learning_rate": 4.9065483834044707e-05,
      "loss": 3.6932,
      "step": 423900
    },
    {
      "epoch": 0.07477910622389676,
      "grad_norm": 6.945319652557373,
      "learning_rate": 4.9065263376774e-05,
      "loss": 3.6393,
      "step": 424000
    },
    {
      "epoch": 0.07479674280555333,
      "grad_norm": 8.739155769348145,
      "learning_rate": 4.906504291950329e-05,
      "loss": 3.6835,
      "step": 424100
    },
    {
      "epoch": 0.07481437938720992,
      "grad_norm": 7.062309741973877,
      "learning_rate": 4.9064822462232586e-05,
      "loss": 3.8063,
      "step": 424200
    },
    {
      "epoch": 0.07483201596886649,
      "grad_norm": 5.928861618041992,
      "learning_rate": 4.906460200496188e-05,
      "loss": 3.6562,
      "step": 424300
    },
    {
      "epoch": 0.07484965255052307,
      "grad_norm": 9.330730438232422,
      "learning_rate": 4.906438154769117e-05,
      "loss": 3.6811,
      "step": 424400
    },
    {
      "epoch": 0.07486728913217966,
      "grad_norm": 7.510271072387695,
      "learning_rate": 4.9064161090420466e-05,
      "loss": 3.6278,
      "step": 424500
    },
    {
      "epoch": 0.07488492571383623,
      "grad_norm": 9.916378021240234,
      "learning_rate": 4.9063940633149755e-05,
      "loss": 3.7268,
      "step": 424600
    },
    {
      "epoch": 0.07490256229549282,
      "grad_norm": 6.786191940307617,
      "learning_rate": 4.906372017587904e-05,
      "loss": 3.7307,
      "step": 424700
    },
    {
      "epoch": 0.07492019887714939,
      "grad_norm": 6.864049434661865,
      "learning_rate": 4.906349971860834e-05,
      "loss": 3.6231,
      "step": 424800
    },
    {
      "epoch": 0.07493783545880597,
      "grad_norm": 8.866050720214844,
      "learning_rate": 4.9063279261337634e-05,
      "loss": 3.7921,
      "step": 424900
    },
    {
      "epoch": 0.07495547204046255,
      "grad_norm": 9.570176124572754,
      "learning_rate": 4.906305880406692e-05,
      "loss": 3.6216,
      "step": 425000
    },
    {
      "epoch": 0.07497310862211913,
      "grad_norm": 7.672544002532959,
      "learning_rate": 4.906283834679622e-05,
      "loss": 3.6341,
      "step": 425100
    },
    {
      "epoch": 0.07499074520377572,
      "grad_norm": 6.241598129272461,
      "learning_rate": 4.9062617889525514e-05,
      "loss": 3.5447,
      "step": 425200
    },
    {
      "epoch": 0.07500838178543229,
      "grad_norm": 6.648309230804443,
      "learning_rate": 4.906239743225481e-05,
      "loss": 3.6601,
      "step": 425300
    },
    {
      "epoch": 0.07502601836708887,
      "grad_norm": 5.66330623626709,
      "learning_rate": 4.90621769749841e-05,
      "loss": 3.6222,
      "step": 425400
    },
    {
      "epoch": 0.07504365494874544,
      "grad_norm": 7.6191582679748535,
      "learning_rate": 4.9061956517713393e-05,
      "loss": 3.7182,
      "step": 425500
    },
    {
      "epoch": 0.07506129153040203,
      "grad_norm": 6.176968097686768,
      "learning_rate": 4.906173606044269e-05,
      "loss": 3.6292,
      "step": 425600
    },
    {
      "epoch": 0.0750789281120586,
      "grad_norm": 7.52605676651001,
      "learning_rate": 4.906151560317198e-05,
      "loss": 3.7405,
      "step": 425700
    },
    {
      "epoch": 0.07509656469371519,
      "grad_norm": 7.808462619781494,
      "learning_rate": 4.906129514590127e-05,
      "loss": 3.7241,
      "step": 425800
    },
    {
      "epoch": 0.07511420127537177,
      "grad_norm": 6.2580389976501465,
      "learning_rate": 4.906107468863056e-05,
      "loss": 3.6762,
      "step": 425900
    },
    {
      "epoch": 0.07513183785702834,
      "grad_norm": 7.074069976806641,
      "learning_rate": 4.906085423135986e-05,
      "loss": 3.7683,
      "step": 426000
    },
    {
      "epoch": 0.07514947443868493,
      "grad_norm": 6.825689315795898,
      "learning_rate": 4.9060633774089146e-05,
      "loss": 3.6581,
      "step": 426100
    },
    {
      "epoch": 0.0751671110203415,
      "grad_norm": 6.373474597930908,
      "learning_rate": 4.906041331681844e-05,
      "loss": 3.6546,
      "step": 426200
    },
    {
      "epoch": 0.07518474760199809,
      "grad_norm": 6.687862873077393,
      "learning_rate": 4.906019285954773e-05,
      "loss": 3.592,
      "step": 426300
    },
    {
      "epoch": 0.07520238418365466,
      "grad_norm": 11.100208282470703,
      "learning_rate": 4.9059972402277026e-05,
      "loss": 3.6831,
      "step": 426400
    },
    {
      "epoch": 0.07522002076531124,
      "grad_norm": 7.249714374542236,
      "learning_rate": 4.905975194500632e-05,
      "loss": 3.7145,
      "step": 426500
    },
    {
      "epoch": 0.07523765734696782,
      "grad_norm": 9.405200004577637,
      "learning_rate": 4.905953148773561e-05,
      "loss": 3.6091,
      "step": 426600
    },
    {
      "epoch": 0.0752552939286244,
      "grad_norm": 6.624743461608887,
      "learning_rate": 4.9059311030464905e-05,
      "loss": 3.7978,
      "step": 426700
    },
    {
      "epoch": 0.07527293051028099,
      "grad_norm": 6.338552474975586,
      "learning_rate": 4.90590905731942e-05,
      "loss": 3.6353,
      "step": 426800
    },
    {
      "epoch": 0.07529056709193756,
      "grad_norm": 7.3243184089660645,
      "learning_rate": 4.905887011592349e-05,
      "loss": 3.7166,
      "step": 426900
    },
    {
      "epoch": 0.07530820367359414,
      "grad_norm": 6.857998847961426,
      "learning_rate": 4.9058649658652785e-05,
      "loss": 3.7119,
      "step": 427000
    },
    {
      "epoch": 0.07532584025525071,
      "grad_norm": 7.816969871520996,
      "learning_rate": 4.905842920138208e-05,
      "loss": 3.6452,
      "step": 427100
    },
    {
      "epoch": 0.0753434768369073,
      "grad_norm": 7.3863844871521,
      "learning_rate": 4.905820874411137e-05,
      "loss": 3.673,
      "step": 427200
    },
    {
      "epoch": 0.07536111341856387,
      "grad_norm": 7.342135906219482,
      "learning_rate": 4.9057988286840664e-05,
      "loss": 3.6904,
      "step": 427300
    },
    {
      "epoch": 0.07537875000022046,
      "grad_norm": 6.476795673370361,
      "learning_rate": 4.905776782956995e-05,
      "loss": 3.6089,
      "step": 427400
    },
    {
      "epoch": 0.07539638658187704,
      "grad_norm": 5.523098945617676,
      "learning_rate": 4.905754737229924e-05,
      "loss": 3.7145,
      "step": 427500
    },
    {
      "epoch": 0.07541402316353361,
      "grad_norm": 11.23131275177002,
      "learning_rate": 4.905732691502854e-05,
      "loss": 3.73,
      "step": 427600
    },
    {
      "epoch": 0.0754316597451902,
      "grad_norm": 6.218132495880127,
      "learning_rate": 4.905710645775783e-05,
      "loss": 3.7153,
      "step": 427700
    },
    {
      "epoch": 0.07544929632684677,
      "grad_norm": 7.192466735839844,
      "learning_rate": 4.905688600048712e-05,
      "loss": 3.6246,
      "step": 427800
    },
    {
      "epoch": 0.07546693290850336,
      "grad_norm": 9.661992073059082,
      "learning_rate": 4.905666554321642e-05,
      "loss": 3.6704,
      "step": 427900
    },
    {
      "epoch": 0.07548456949015993,
      "grad_norm": 5.666386604309082,
      "learning_rate": 4.905644508594571e-05,
      "loss": 3.7381,
      "step": 428000
    },
    {
      "epoch": 0.07550220607181651,
      "grad_norm": 5.91207218170166,
      "learning_rate": 4.9056224628675e-05,
      "loss": 3.678,
      "step": 428100
    },
    {
      "epoch": 0.0755198426534731,
      "grad_norm": 6.07150936126709,
      "learning_rate": 4.9056004171404297e-05,
      "loss": 3.7437,
      "step": 428200
    },
    {
      "epoch": 0.07553747923512967,
      "grad_norm": 11.546930313110352,
      "learning_rate": 4.905578371413359e-05,
      "loss": 3.6741,
      "step": 428300
    },
    {
      "epoch": 0.07555511581678626,
      "grad_norm": 10.340828895568848,
      "learning_rate": 4.905556325686288e-05,
      "loss": 3.6664,
      "step": 428400
    },
    {
      "epoch": 0.07557275239844283,
      "grad_norm": 8.184953689575195,
      "learning_rate": 4.9055342799592176e-05,
      "loss": 3.7633,
      "step": 428500
    },
    {
      "epoch": 0.07559038898009941,
      "grad_norm": 7.3981475830078125,
      "learning_rate": 4.905512234232147e-05,
      "loss": 3.6472,
      "step": 428600
    },
    {
      "epoch": 0.07560802556175598,
      "grad_norm": 5.948454856872559,
      "learning_rate": 4.905490188505076e-05,
      "loss": 3.6756,
      "step": 428700
    },
    {
      "epoch": 0.07562566214341257,
      "grad_norm": 6.5792951583862305,
      "learning_rate": 4.9054681427780056e-05,
      "loss": 3.6505,
      "step": 428800
    },
    {
      "epoch": 0.07564329872506916,
      "grad_norm": 9.795599937438965,
      "learning_rate": 4.9054460970509344e-05,
      "loss": 3.6535,
      "step": 428900
    },
    {
      "epoch": 0.07566093530672573,
      "grad_norm": 6.1599040031433105,
      "learning_rate": 4.905424051323863e-05,
      "loss": 3.6639,
      "step": 429000
    },
    {
      "epoch": 0.07567857188838231,
      "grad_norm": 7.923549175262451,
      "learning_rate": 4.905402005596793e-05,
      "loss": 3.7633,
      "step": 429100
    },
    {
      "epoch": 0.07569620847003888,
      "grad_norm": 7.335886478424072,
      "learning_rate": 4.9053799598697224e-05,
      "loss": 3.676,
      "step": 429200
    },
    {
      "epoch": 0.07571384505169547,
      "grad_norm": 6.131198883056641,
      "learning_rate": 4.905357914142651e-05,
      "loss": 3.6841,
      "step": 429300
    },
    {
      "epoch": 0.07573148163335204,
      "grad_norm": 10.753243446350098,
      "learning_rate": 4.905335868415581e-05,
      "loss": 3.6715,
      "step": 429400
    },
    {
      "epoch": 0.07574911821500863,
      "grad_norm": 7.807533264160156,
      "learning_rate": 4.9053138226885104e-05,
      "loss": 3.7092,
      "step": 429500
    },
    {
      "epoch": 0.0757667547966652,
      "grad_norm": 7.111656188964844,
      "learning_rate": 4.905291776961439e-05,
      "loss": 3.6321,
      "step": 429600
    },
    {
      "epoch": 0.07578439137832178,
      "grad_norm": 6.277650833129883,
      "learning_rate": 4.905269731234369e-05,
      "loss": 3.5594,
      "step": 429700
    },
    {
      "epoch": 0.07580202795997837,
      "grad_norm": 8.000397682189941,
      "learning_rate": 4.905247685507298e-05,
      "loss": 3.7651,
      "step": 429800
    },
    {
      "epoch": 0.07581966454163494,
      "grad_norm": 8.822888374328613,
      "learning_rate": 4.905225639780227e-05,
      "loss": 3.6891,
      "step": 429900
    },
    {
      "epoch": 0.07583730112329153,
      "grad_norm": 11.566459655761719,
      "learning_rate": 4.905203594053157e-05,
      "loss": 3.6232,
      "step": 430000
    },
    {
      "epoch": 0.0758549377049481,
      "grad_norm": 6.5999836921691895,
      "learning_rate": 4.905181548326086e-05,
      "loss": 3.7134,
      "step": 430100
    },
    {
      "epoch": 0.07587257428660468,
      "grad_norm": 5.734289169311523,
      "learning_rate": 4.905159502599015e-05,
      "loss": 3.6648,
      "step": 430200
    },
    {
      "epoch": 0.07589021086826125,
      "grad_norm": 7.8770246505737305,
      "learning_rate": 4.905137456871944e-05,
      "loss": 3.8244,
      "step": 430300
    },
    {
      "epoch": 0.07590784744991784,
      "grad_norm": 7.890102386474609,
      "learning_rate": 4.9051154111448736e-05,
      "loss": 3.7154,
      "step": 430400
    },
    {
      "epoch": 0.07592548403157443,
      "grad_norm": 6.108577728271484,
      "learning_rate": 4.9050933654178025e-05,
      "loss": 3.758,
      "step": 430500
    },
    {
      "epoch": 0.075943120613231,
      "grad_norm": 7.7383036613464355,
      "learning_rate": 4.905071319690732e-05,
      "loss": 3.7986,
      "step": 430600
    },
    {
      "epoch": 0.07596075719488758,
      "grad_norm": 6.359632968902588,
      "learning_rate": 4.9050492739636615e-05,
      "loss": 3.7242,
      "step": 430700
    },
    {
      "epoch": 0.07597839377654415,
      "grad_norm": 9.716217994689941,
      "learning_rate": 4.9050272282365904e-05,
      "loss": 3.7141,
      "step": 430800
    },
    {
      "epoch": 0.07599603035820074,
      "grad_norm": 9.031644821166992,
      "learning_rate": 4.90500518250952e-05,
      "loss": 3.6451,
      "step": 430900
    },
    {
      "epoch": 0.07601366693985731,
      "grad_norm": 9.447396278381348,
      "learning_rate": 4.9049831367824495e-05,
      "loss": 3.6826,
      "step": 431000
    },
    {
      "epoch": 0.0760313035215139,
      "grad_norm": 6.409650802612305,
      "learning_rate": 4.9049610910553784e-05,
      "loss": 3.7567,
      "step": 431100
    },
    {
      "epoch": 0.07604894010317048,
      "grad_norm": 6.945878028869629,
      "learning_rate": 4.904939045328308e-05,
      "loss": 3.693,
      "step": 431200
    },
    {
      "epoch": 0.07606657668482705,
      "grad_norm": 6.952209949493408,
      "learning_rate": 4.9049169996012375e-05,
      "loss": 3.5572,
      "step": 431300
    },
    {
      "epoch": 0.07608421326648364,
      "grad_norm": 7.324037075042725,
      "learning_rate": 4.9048949538741663e-05,
      "loss": 3.6706,
      "step": 431400
    },
    {
      "epoch": 0.07610184984814021,
      "grad_norm": 7.9093828201293945,
      "learning_rate": 4.904872908147096e-05,
      "loss": 3.7123,
      "step": 431500
    },
    {
      "epoch": 0.0761194864297968,
      "grad_norm": 8.585838317871094,
      "learning_rate": 4.9048508624200254e-05,
      "loss": 3.8029,
      "step": 431600
    },
    {
      "epoch": 0.07613712301145337,
      "grad_norm": 6.744194030761719,
      "learning_rate": 4.904828816692954e-05,
      "loss": 3.7208,
      "step": 431700
    },
    {
      "epoch": 0.07615475959310995,
      "grad_norm": 7.798087120056152,
      "learning_rate": 4.904806770965883e-05,
      "loss": 3.7084,
      "step": 431800
    },
    {
      "epoch": 0.07617239617476654,
      "grad_norm": 6.582857131958008,
      "learning_rate": 4.904784725238813e-05,
      "loss": 3.6909,
      "step": 431900
    },
    {
      "epoch": 0.07619003275642311,
      "grad_norm": 7.065747261047363,
      "learning_rate": 4.9047626795117416e-05,
      "loss": 3.7233,
      "step": 432000
    },
    {
      "epoch": 0.0762076693380797,
      "grad_norm": 7.037389278411865,
      "learning_rate": 4.904740633784671e-05,
      "loss": 3.5739,
      "step": 432100
    },
    {
      "epoch": 0.07622530591973627,
      "grad_norm": 7.376760959625244,
      "learning_rate": 4.904718588057601e-05,
      "loss": 3.7362,
      "step": 432200
    },
    {
      "epoch": 0.07624294250139285,
      "grad_norm": 8.586041450500488,
      "learning_rate": 4.9046965423305296e-05,
      "loss": 3.6289,
      "step": 432300
    },
    {
      "epoch": 0.07626057908304942,
      "grad_norm": 5.794370174407959,
      "learning_rate": 4.904674496603459e-05,
      "loss": 3.6424,
      "step": 432400
    },
    {
      "epoch": 0.07627821566470601,
      "grad_norm": 9.161907196044922,
      "learning_rate": 4.9046524508763886e-05,
      "loss": 3.5674,
      "step": 432500
    },
    {
      "epoch": 0.07629585224636258,
      "grad_norm": 7.4860382080078125,
      "learning_rate": 4.9046304051493175e-05,
      "loss": 3.7932,
      "step": 432600
    },
    {
      "epoch": 0.07631348882801917,
      "grad_norm": 7.70524263381958,
      "learning_rate": 4.904608359422247e-05,
      "loss": 3.6469,
      "step": 432700
    },
    {
      "epoch": 0.07633112540967575,
      "grad_norm": 7.523650169372559,
      "learning_rate": 4.9045863136951766e-05,
      "loss": 3.7711,
      "step": 432800
    },
    {
      "epoch": 0.07634876199133232,
      "grad_norm": 8.458968162536621,
      "learning_rate": 4.9045642679681055e-05,
      "loss": 3.7417,
      "step": 432900
    },
    {
      "epoch": 0.07636639857298891,
      "grad_norm": 7.056753158569336,
      "learning_rate": 4.904542222241035e-05,
      "loss": 3.6816,
      "step": 433000
    },
    {
      "epoch": 0.07638403515464548,
      "grad_norm": 5.445136547088623,
      "learning_rate": 4.904520176513964e-05,
      "loss": 3.5889,
      "step": 433100
    },
    {
      "epoch": 0.07640167173630207,
      "grad_norm": 9.00703239440918,
      "learning_rate": 4.9044981307868934e-05,
      "loss": 3.596,
      "step": 433200
    },
    {
      "epoch": 0.07641930831795864,
      "grad_norm": 7.6040239334106445,
      "learning_rate": 4.904476085059822e-05,
      "loss": 3.6382,
      "step": 433300
    },
    {
      "epoch": 0.07643694489961522,
      "grad_norm": 6.497304439544678,
      "learning_rate": 4.904454039332752e-05,
      "loss": 3.5942,
      "step": 433400
    },
    {
      "epoch": 0.07645458148127181,
      "grad_norm": 7.646685600280762,
      "learning_rate": 4.904431993605681e-05,
      "loss": 3.8014,
      "step": 433500
    },
    {
      "epoch": 0.07647221806292838,
      "grad_norm": 8.421095848083496,
      "learning_rate": 4.90440994787861e-05,
      "loss": 3.779,
      "step": 433600
    },
    {
      "epoch": 0.07648985464458496,
      "grad_norm": 5.984679222106934,
      "learning_rate": 4.90438790215154e-05,
      "loss": 3.6954,
      "step": 433700
    },
    {
      "epoch": 0.07650749122624154,
      "grad_norm": 7.122715950012207,
      "learning_rate": 4.904365856424469e-05,
      "loss": 3.6687,
      "step": 433800
    },
    {
      "epoch": 0.07652512780789812,
      "grad_norm": 8.545878410339355,
      "learning_rate": 4.904343810697398e-05,
      "loss": 3.6615,
      "step": 433900
    },
    {
      "epoch": 0.0765427643895547,
      "grad_norm": 5.455798149108887,
      "learning_rate": 4.904321764970328e-05,
      "loss": 3.618,
      "step": 434000
    },
    {
      "epoch": 0.07656040097121128,
      "grad_norm": 7.136788845062256,
      "learning_rate": 4.9042997192432567e-05,
      "loss": 3.5871,
      "step": 434100
    },
    {
      "epoch": 0.07657803755286786,
      "grad_norm": 7.912655353546143,
      "learning_rate": 4.904277673516186e-05,
      "loss": 3.663,
      "step": 434200
    },
    {
      "epoch": 0.07659567413452444,
      "grad_norm": 5.595092296600342,
      "learning_rate": 4.904255627789116e-05,
      "loss": 3.6985,
      "step": 434300
    },
    {
      "epoch": 0.07661331071618102,
      "grad_norm": 7.044989109039307,
      "learning_rate": 4.9042335820620446e-05,
      "loss": 3.7365,
      "step": 434400
    },
    {
      "epoch": 0.07663094729783759,
      "grad_norm": 9.475451469421387,
      "learning_rate": 4.904211536334974e-05,
      "loss": 3.6445,
      "step": 434500
    },
    {
      "epoch": 0.07664858387949418,
      "grad_norm": 6.975715160369873,
      "learning_rate": 4.904189490607903e-05,
      "loss": 3.7065,
      "step": 434600
    },
    {
      "epoch": 0.07666622046115075,
      "grad_norm": 8.894719123840332,
      "learning_rate": 4.9041674448808326e-05,
      "loss": 3.7197,
      "step": 434700
    },
    {
      "epoch": 0.07668385704280734,
      "grad_norm": 8.554293632507324,
      "learning_rate": 4.9041453991537614e-05,
      "loss": 3.687,
      "step": 434800
    },
    {
      "epoch": 0.07670149362446392,
      "grad_norm": 8.0712251663208,
      "learning_rate": 4.904123353426691e-05,
      "loss": 3.7277,
      "step": 434900
    },
    {
      "epoch": 0.07671913020612049,
      "grad_norm": 7.080219745635986,
      "learning_rate": 4.90410130769962e-05,
      "loss": 3.6732,
      "step": 435000
    },
    {
      "epoch": 0.07673676678777708,
      "grad_norm": 7.37339448928833,
      "learning_rate": 4.9040792619725494e-05,
      "loss": 3.6882,
      "step": 435100
    },
    {
      "epoch": 0.07675440336943365,
      "grad_norm": 8.379512786865234,
      "learning_rate": 4.904057216245479e-05,
      "loss": 3.6504,
      "step": 435200
    },
    {
      "epoch": 0.07677203995109023,
      "grad_norm": 7.687290191650391,
      "learning_rate": 4.904035170518408e-05,
      "loss": 3.6985,
      "step": 435300
    },
    {
      "epoch": 0.0767896765327468,
      "grad_norm": 8.175966262817383,
      "learning_rate": 4.9040131247913374e-05,
      "loss": 3.8502,
      "step": 435400
    },
    {
      "epoch": 0.07680731311440339,
      "grad_norm": 8.55514144897461,
      "learning_rate": 4.903991079064267e-05,
      "loss": 3.585,
      "step": 435500
    },
    {
      "epoch": 0.07682494969605996,
      "grad_norm": 7.693467140197754,
      "learning_rate": 4.903969033337196e-05,
      "loss": 3.7654,
      "step": 435600
    },
    {
      "epoch": 0.07684258627771655,
      "grad_norm": 6.806702136993408,
      "learning_rate": 4.903946987610125e-05,
      "loss": 3.6994,
      "step": 435700
    },
    {
      "epoch": 0.07686022285937313,
      "grad_norm": 7.643679618835449,
      "learning_rate": 4.903924941883055e-05,
      "loss": 3.5544,
      "step": 435800
    },
    {
      "epoch": 0.0768778594410297,
      "grad_norm": 7.826055526733398,
      "learning_rate": 4.903902896155984e-05,
      "loss": 3.7667,
      "step": 435900
    },
    {
      "epoch": 0.07689549602268629,
      "grad_norm": 6.178545951843262,
      "learning_rate": 4.903880850428913e-05,
      "loss": 3.6438,
      "step": 436000
    },
    {
      "epoch": 0.07691313260434286,
      "grad_norm": 8.067557334899902,
      "learning_rate": 4.903858804701842e-05,
      "loss": 3.7772,
      "step": 436100
    },
    {
      "epoch": 0.07693076918599945,
      "grad_norm": 8.150588035583496,
      "learning_rate": 4.903836758974772e-05,
      "loss": 3.7336,
      "step": 436200
    },
    {
      "epoch": 0.07694840576765602,
      "grad_norm": 6.87881326675415,
      "learning_rate": 4.9038147132477006e-05,
      "loss": 3.6397,
      "step": 436300
    },
    {
      "epoch": 0.0769660423493126,
      "grad_norm": 6.972487926483154,
      "learning_rate": 4.90379266752063e-05,
      "loss": 3.6898,
      "step": 436400
    },
    {
      "epoch": 0.07698367893096919,
      "grad_norm": 6.530811786651611,
      "learning_rate": 4.90377062179356e-05,
      "loss": 3.6439,
      "step": 436500
    },
    {
      "epoch": 0.07700131551262576,
      "grad_norm": 9.585358619689941,
      "learning_rate": 4.9037485760664885e-05,
      "loss": 3.6009,
      "step": 436600
    },
    {
      "epoch": 0.07701895209428235,
      "grad_norm": 9.412237167358398,
      "learning_rate": 4.903726530339418e-05,
      "loss": 3.5761,
      "step": 436700
    },
    {
      "epoch": 0.07703658867593892,
      "grad_norm": 9.616255760192871,
      "learning_rate": 4.9037044846123476e-05,
      "loss": 3.7413,
      "step": 436800
    },
    {
      "epoch": 0.0770542252575955,
      "grad_norm": 8.642836570739746,
      "learning_rate": 4.9036824388852765e-05,
      "loss": 3.7593,
      "step": 436900
    },
    {
      "epoch": 0.07707186183925208,
      "grad_norm": 6.389076232910156,
      "learning_rate": 4.903660393158206e-05,
      "loss": 3.719,
      "step": 437000
    },
    {
      "epoch": 0.07708949842090866,
      "grad_norm": 7.152218341827393,
      "learning_rate": 4.9036383474311356e-05,
      "loss": 3.7132,
      "step": 437100
    },
    {
      "epoch": 0.07710713500256525,
      "grad_norm": 7.074080467224121,
      "learning_rate": 4.9036163017040645e-05,
      "loss": 3.6996,
      "step": 437200
    },
    {
      "epoch": 0.07712477158422182,
      "grad_norm": 7.712960720062256,
      "learning_rate": 4.903594255976994e-05,
      "loss": 3.6894,
      "step": 437300
    },
    {
      "epoch": 0.0771424081658784,
      "grad_norm": 10.995838165283203,
      "learning_rate": 4.903572210249923e-05,
      "loss": 3.6662,
      "step": 437400
    },
    {
      "epoch": 0.07716004474753498,
      "grad_norm": 5.969172954559326,
      "learning_rate": 4.903550164522852e-05,
      "loss": 3.7386,
      "step": 437500
    },
    {
      "epoch": 0.07717768132919156,
      "grad_norm": 7.719086647033691,
      "learning_rate": 4.903528118795781e-05,
      "loss": 3.5991,
      "step": 437600
    },
    {
      "epoch": 0.07719531791084813,
      "grad_norm": 9.501933097839355,
      "learning_rate": 4.903506073068711e-05,
      "loss": 3.6583,
      "step": 437700
    },
    {
      "epoch": 0.07721295449250472,
      "grad_norm": 6.757850170135498,
      "learning_rate": 4.90348402734164e-05,
      "loss": 3.7341,
      "step": 437800
    },
    {
      "epoch": 0.0772305910741613,
      "grad_norm": 7.577383041381836,
      "learning_rate": 4.903461981614569e-05,
      "loss": 3.693,
      "step": 437900
    },
    {
      "epoch": 0.07724822765581787,
      "grad_norm": 7.5706868171691895,
      "learning_rate": 4.903439935887499e-05,
      "loss": 3.6216,
      "step": 438000
    },
    {
      "epoch": 0.07726586423747446,
      "grad_norm": 8.186741828918457,
      "learning_rate": 4.903417890160428e-05,
      "loss": 3.7009,
      "step": 438100
    },
    {
      "epoch": 0.07728350081913103,
      "grad_norm": 6.672252655029297,
      "learning_rate": 4.903395844433357e-05,
      "loss": 3.703,
      "step": 438200
    },
    {
      "epoch": 0.07730113740078762,
      "grad_norm": 7.386938571929932,
      "learning_rate": 4.903373798706287e-05,
      "loss": 3.7652,
      "step": 438300
    },
    {
      "epoch": 0.07731877398244419,
      "grad_norm": 6.607799530029297,
      "learning_rate": 4.9033517529792156e-05,
      "loss": 3.6769,
      "step": 438400
    },
    {
      "epoch": 0.07733641056410077,
      "grad_norm": 7.30051851272583,
      "learning_rate": 4.903329707252145e-05,
      "loss": 3.6625,
      "step": 438500
    },
    {
      "epoch": 0.07735404714575735,
      "grad_norm": 6.177729606628418,
      "learning_rate": 4.903307661525075e-05,
      "loss": 3.6678,
      "step": 438600
    },
    {
      "epoch": 0.07737168372741393,
      "grad_norm": 8.228499412536621,
      "learning_rate": 4.9032856157980036e-05,
      "loss": 3.7923,
      "step": 438700
    },
    {
      "epoch": 0.07738932030907052,
      "grad_norm": 8.893436431884766,
      "learning_rate": 4.903263570070933e-05,
      "loss": 3.8214,
      "step": 438800
    },
    {
      "epoch": 0.07740695689072709,
      "grad_norm": 6.459846019744873,
      "learning_rate": 4.903241524343862e-05,
      "loss": 3.7285,
      "step": 438900
    },
    {
      "epoch": 0.07742459347238367,
      "grad_norm": 6.0148539543151855,
      "learning_rate": 4.903219478616791e-05,
      "loss": 3.5976,
      "step": 439000
    },
    {
      "epoch": 0.07744223005404025,
      "grad_norm": 5.776366710662842,
      "learning_rate": 4.9031974328897204e-05,
      "loss": 3.7097,
      "step": 439100
    },
    {
      "epoch": 0.07745986663569683,
      "grad_norm": 5.257993698120117,
      "learning_rate": 4.90317538716265e-05,
      "loss": 3.6082,
      "step": 439200
    },
    {
      "epoch": 0.0774775032173534,
      "grad_norm": 7.816795825958252,
      "learning_rate": 4.903153341435579e-05,
      "loss": 3.7489,
      "step": 439300
    },
    {
      "epoch": 0.07749513979900999,
      "grad_norm": 6.0918803215026855,
      "learning_rate": 4.9031312957085084e-05,
      "loss": 3.7041,
      "step": 439400
    },
    {
      "epoch": 0.07751277638066657,
      "grad_norm": 8.282465934753418,
      "learning_rate": 4.903109249981438e-05,
      "loss": 3.7187,
      "step": 439500
    },
    {
      "epoch": 0.07753041296232314,
      "grad_norm": 10.361655235290527,
      "learning_rate": 4.903087204254367e-05,
      "loss": 3.7717,
      "step": 439600
    },
    {
      "epoch": 0.07754804954397973,
      "grad_norm": 10.189600944519043,
      "learning_rate": 4.9030651585272964e-05,
      "loss": 3.6631,
      "step": 439700
    },
    {
      "epoch": 0.0775656861256363,
      "grad_norm": 6.768036365509033,
      "learning_rate": 4.903043112800226e-05,
      "loss": 3.7512,
      "step": 439800
    },
    {
      "epoch": 0.07758332270729289,
      "grad_norm": 7.878689765930176,
      "learning_rate": 4.903021067073155e-05,
      "loss": 3.6929,
      "step": 439900
    },
    {
      "epoch": 0.07760095928894946,
      "grad_norm": 7.783135414123535,
      "learning_rate": 4.902999021346084e-05,
      "loss": 3.6543,
      "step": 440000
    },
    {
      "epoch": 0.07761859587060604,
      "grad_norm": 5.3176774978637695,
      "learning_rate": 4.902976975619014e-05,
      "loss": 3.6647,
      "step": 440100
    },
    {
      "epoch": 0.07763623245226263,
      "grad_norm": 5.4988627433776855,
      "learning_rate": 4.902954929891943e-05,
      "loss": 3.6949,
      "step": 440200
    },
    {
      "epoch": 0.0776538690339192,
      "grad_norm": 7.085472106933594,
      "learning_rate": 4.9029328841648716e-05,
      "loss": 3.8765,
      "step": 440300
    },
    {
      "epoch": 0.07767150561557579,
      "grad_norm": 10.797043800354004,
      "learning_rate": 4.902910838437801e-05,
      "loss": 3.7392,
      "step": 440400
    },
    {
      "epoch": 0.07768914219723236,
      "grad_norm": 6.1915740966796875,
      "learning_rate": 4.90288879271073e-05,
      "loss": 3.7209,
      "step": 440500
    },
    {
      "epoch": 0.07770677877888894,
      "grad_norm": 6.895021438598633,
      "learning_rate": 4.9028667469836596e-05,
      "loss": 3.6894,
      "step": 440600
    },
    {
      "epoch": 0.07772441536054552,
      "grad_norm": 8.411437034606934,
      "learning_rate": 4.902844701256589e-05,
      "loss": 3.6543,
      "step": 440700
    },
    {
      "epoch": 0.0777420519422021,
      "grad_norm": 9.966053009033203,
      "learning_rate": 4.902822655529518e-05,
      "loss": 3.6541,
      "step": 440800
    },
    {
      "epoch": 0.07775968852385869,
      "grad_norm": 8.682697296142578,
      "learning_rate": 4.9028006098024475e-05,
      "loss": 3.6747,
      "step": 440900
    },
    {
      "epoch": 0.07777732510551526,
      "grad_norm": 13.486926078796387,
      "learning_rate": 4.902778564075377e-05,
      "loss": 3.5701,
      "step": 441000
    },
    {
      "epoch": 0.07779496168717184,
      "grad_norm": 8.386761665344238,
      "learning_rate": 4.902756518348306e-05,
      "loss": 3.6832,
      "step": 441100
    },
    {
      "epoch": 0.07781259826882841,
      "grad_norm": 7.090724945068359,
      "learning_rate": 4.9027344726212355e-05,
      "loss": 3.7423,
      "step": 441200
    },
    {
      "epoch": 0.077830234850485,
      "grad_norm": 7.926163673400879,
      "learning_rate": 4.902712426894165e-05,
      "loss": 3.7674,
      "step": 441300
    },
    {
      "epoch": 0.07784787143214157,
      "grad_norm": 7.088449478149414,
      "learning_rate": 4.902690381167094e-05,
      "loss": 3.6951,
      "step": 441400
    },
    {
      "epoch": 0.07786550801379816,
      "grad_norm": 9.337469100952148,
      "learning_rate": 4.9026683354400235e-05,
      "loss": 3.6578,
      "step": 441500
    },
    {
      "epoch": 0.07788314459545473,
      "grad_norm": 8.90363597869873,
      "learning_rate": 4.902646289712953e-05,
      "loss": 3.6316,
      "step": 441600
    },
    {
      "epoch": 0.07790078117711131,
      "grad_norm": 10.642297744750977,
      "learning_rate": 4.902624243985882e-05,
      "loss": 3.7459,
      "step": 441700
    },
    {
      "epoch": 0.0779184177587679,
      "grad_norm": 7.919686317443848,
      "learning_rate": 4.902602198258811e-05,
      "loss": 3.6944,
      "step": 441800
    },
    {
      "epoch": 0.07793605434042447,
      "grad_norm": 7.044985294342041,
      "learning_rate": 4.90258015253174e-05,
      "loss": 3.6188,
      "step": 441900
    },
    {
      "epoch": 0.07795369092208106,
      "grad_norm": 6.451050758361816,
      "learning_rate": 4.902558106804669e-05,
      "loss": 3.6759,
      "step": 442000
    },
    {
      "epoch": 0.07797132750373763,
      "grad_norm": 9.14218521118164,
      "learning_rate": 4.902536061077599e-05,
      "loss": 3.7069,
      "step": 442100
    },
    {
      "epoch": 0.07798896408539421,
      "grad_norm": 7.534064769744873,
      "learning_rate": 4.902514015350528e-05,
      "loss": 3.7285,
      "step": 442200
    },
    {
      "epoch": 0.07800660066705078,
      "grad_norm": 7.622120380401611,
      "learning_rate": 4.902491969623457e-05,
      "loss": 3.6804,
      "step": 442300
    },
    {
      "epoch": 0.07802423724870737,
      "grad_norm": 10.077792167663574,
      "learning_rate": 4.902469923896387e-05,
      "loss": 3.7693,
      "step": 442400
    },
    {
      "epoch": 0.07804187383036396,
      "grad_norm": 9.83293342590332,
      "learning_rate": 4.902447878169316e-05,
      "loss": 3.6761,
      "step": 442500
    },
    {
      "epoch": 0.07805951041202053,
      "grad_norm": 7.1075568199157715,
      "learning_rate": 4.902425832442245e-05,
      "loss": 3.6776,
      "step": 442600
    },
    {
      "epoch": 0.07807714699367711,
      "grad_norm": 7.050773620605469,
      "learning_rate": 4.9024037867151746e-05,
      "loss": 3.6684,
      "step": 442700
    },
    {
      "epoch": 0.07809478357533368,
      "grad_norm": 5.309957504272461,
      "learning_rate": 4.902381740988104e-05,
      "loss": 3.7445,
      "step": 442800
    },
    {
      "epoch": 0.07811242015699027,
      "grad_norm": 6.803375244140625,
      "learning_rate": 4.902359695261033e-05,
      "loss": 3.6698,
      "step": 442900
    },
    {
      "epoch": 0.07813005673864684,
      "grad_norm": 10.203985214233398,
      "learning_rate": 4.9023376495339626e-05,
      "loss": 3.6164,
      "step": 443000
    },
    {
      "epoch": 0.07814769332030343,
      "grad_norm": 6.376842021942139,
      "learning_rate": 4.9023156038068915e-05,
      "loss": 3.6156,
      "step": 443100
    },
    {
      "epoch": 0.07816532990196001,
      "grad_norm": 10.2461576461792,
      "learning_rate": 4.902293558079821e-05,
      "loss": 3.7143,
      "step": 443200
    },
    {
      "epoch": 0.07818296648361658,
      "grad_norm": 8.331511497497559,
      "learning_rate": 4.90227151235275e-05,
      "loss": 3.6096,
      "step": 443300
    },
    {
      "epoch": 0.07820060306527317,
      "grad_norm": 8.09865951538086,
      "learning_rate": 4.9022494666256794e-05,
      "loss": 3.6725,
      "step": 443400
    },
    {
      "epoch": 0.07821823964692974,
      "grad_norm": 7.392260551452637,
      "learning_rate": 4.902227420898608e-05,
      "loss": 3.6818,
      "step": 443500
    },
    {
      "epoch": 0.07823587622858633,
      "grad_norm": 6.404531478881836,
      "learning_rate": 4.902205375171538e-05,
      "loss": 3.7007,
      "step": 443600
    },
    {
      "epoch": 0.0782535128102429,
      "grad_norm": 10.115123748779297,
      "learning_rate": 4.9021833294444674e-05,
      "loss": 3.6908,
      "step": 443700
    },
    {
      "epoch": 0.07827114939189948,
      "grad_norm": 9.492060661315918,
      "learning_rate": 4.902161283717396e-05,
      "loss": 3.8203,
      "step": 443800
    },
    {
      "epoch": 0.07828878597355607,
      "grad_norm": 7.9676008224487305,
      "learning_rate": 4.902139237990326e-05,
      "loss": 3.6371,
      "step": 443900
    },
    {
      "epoch": 0.07830642255521264,
      "grad_norm": 7.34025239944458,
      "learning_rate": 4.9021171922632554e-05,
      "loss": 3.6297,
      "step": 444000
    },
    {
      "epoch": 0.07832405913686923,
      "grad_norm": 9.222674369812012,
      "learning_rate": 4.902095146536184e-05,
      "loss": 3.7237,
      "step": 444100
    },
    {
      "epoch": 0.0783416957185258,
      "grad_norm": 6.733476638793945,
      "learning_rate": 4.902073100809114e-05,
      "loss": 3.6488,
      "step": 444200
    },
    {
      "epoch": 0.07835933230018238,
      "grad_norm": 11.475322723388672,
      "learning_rate": 4.902051055082043e-05,
      "loss": 3.7396,
      "step": 444300
    },
    {
      "epoch": 0.07837696888183895,
      "grad_norm": 4.959081172943115,
      "learning_rate": 4.902029009354972e-05,
      "loss": 3.6552,
      "step": 444400
    },
    {
      "epoch": 0.07839460546349554,
      "grad_norm": 8.751216888427734,
      "learning_rate": 4.902006963627902e-05,
      "loss": 3.6883,
      "step": 444500
    },
    {
      "epoch": 0.07841224204515211,
      "grad_norm": 5.879053592681885,
      "learning_rate": 4.9019849179008306e-05,
      "loss": 3.6374,
      "step": 444600
    },
    {
      "epoch": 0.0784298786268087,
      "grad_norm": 7.30397367477417,
      "learning_rate": 4.90196287217376e-05,
      "loss": 3.7139,
      "step": 444700
    },
    {
      "epoch": 0.07844751520846528,
      "grad_norm": 13.683135032653809,
      "learning_rate": 4.901940826446689e-05,
      "loss": 3.6053,
      "step": 444800
    },
    {
      "epoch": 0.07846515179012185,
      "grad_norm": 5.825560092926025,
      "learning_rate": 4.9019187807196186e-05,
      "loss": 3.6874,
      "step": 444900
    },
    {
      "epoch": 0.07848278837177844,
      "grad_norm": 7.518909931182861,
      "learning_rate": 4.9018967349925474e-05,
      "loss": 3.5656,
      "step": 445000
    },
    {
      "epoch": 0.07850042495343501,
      "grad_norm": 6.092891216278076,
      "learning_rate": 4.901874689265477e-05,
      "loss": 3.7205,
      "step": 445100
    },
    {
      "epoch": 0.0785180615350916,
      "grad_norm": 7.131218910217285,
      "learning_rate": 4.9018526435384065e-05,
      "loss": 3.6398,
      "step": 445200
    },
    {
      "epoch": 0.07853569811674817,
      "grad_norm": 6.305461406707764,
      "learning_rate": 4.9018305978113354e-05,
      "loss": 3.7056,
      "step": 445300
    },
    {
      "epoch": 0.07855333469840475,
      "grad_norm": 5.727999687194824,
      "learning_rate": 4.901808552084265e-05,
      "loss": 3.6857,
      "step": 445400
    },
    {
      "epoch": 0.07857097128006134,
      "grad_norm": 5.230605125427246,
      "learning_rate": 4.9017865063571945e-05,
      "loss": 3.702,
      "step": 445500
    },
    {
      "epoch": 0.07858860786171791,
      "grad_norm": 6.703493595123291,
      "learning_rate": 4.9017644606301234e-05,
      "loss": 3.7583,
      "step": 445600
    },
    {
      "epoch": 0.0786062444433745,
      "grad_norm": 6.177393436431885,
      "learning_rate": 4.901742414903053e-05,
      "loss": 3.7489,
      "step": 445700
    },
    {
      "epoch": 0.07862388102503107,
      "grad_norm": 11.684660911560059,
      "learning_rate": 4.9017203691759825e-05,
      "loss": 3.7319,
      "step": 445800
    },
    {
      "epoch": 0.07864151760668765,
      "grad_norm": 10.59794807434082,
      "learning_rate": 4.901698323448911e-05,
      "loss": 3.6715,
      "step": 445900
    },
    {
      "epoch": 0.07865915418834422,
      "grad_norm": 10.46973705291748,
      "learning_rate": 4.901676277721841e-05,
      "loss": 3.6492,
      "step": 446000
    },
    {
      "epoch": 0.07867679077000081,
      "grad_norm": 6.234041690826416,
      "learning_rate": 4.90165423199477e-05,
      "loss": 3.7613,
      "step": 446100
    },
    {
      "epoch": 0.0786944273516574,
      "grad_norm": 6.629697322845459,
      "learning_rate": 4.9016321862676986e-05,
      "loss": 3.7542,
      "step": 446200
    },
    {
      "epoch": 0.07871206393331397,
      "grad_norm": 8.2718505859375,
      "learning_rate": 4.901610140540628e-05,
      "loss": 3.7008,
      "step": 446300
    },
    {
      "epoch": 0.07872970051497055,
      "grad_norm": 8.108490943908691,
      "learning_rate": 4.901588094813558e-05,
      "loss": 3.6593,
      "step": 446400
    },
    {
      "epoch": 0.07874733709662712,
      "grad_norm": 9.929831504821777,
      "learning_rate": 4.901566049086487e-05,
      "loss": 3.7032,
      "step": 446500
    },
    {
      "epoch": 0.07876497367828371,
      "grad_norm": 7.653697490692139,
      "learning_rate": 4.901544003359416e-05,
      "loss": 3.4998,
      "step": 446600
    },
    {
      "epoch": 0.07878261025994028,
      "grad_norm": 10.832070350646973,
      "learning_rate": 4.901521957632346e-05,
      "loss": 3.6809,
      "step": 446700
    },
    {
      "epoch": 0.07880024684159687,
      "grad_norm": 9.951362609863281,
      "learning_rate": 4.901499911905275e-05,
      "loss": 3.4958,
      "step": 446800
    },
    {
      "epoch": 0.07881788342325345,
      "grad_norm": 4.752551078796387,
      "learning_rate": 4.901477866178204e-05,
      "loss": 3.6571,
      "step": 446900
    },
    {
      "epoch": 0.07883552000491002,
      "grad_norm": 6.366255760192871,
      "learning_rate": 4.9014558204511336e-05,
      "loss": 3.5823,
      "step": 447000
    },
    {
      "epoch": 0.07885315658656661,
      "grad_norm": 9.215938568115234,
      "learning_rate": 4.901433774724063e-05,
      "loss": 3.8084,
      "step": 447100
    },
    {
      "epoch": 0.07887079316822318,
      "grad_norm": 6.000089168548584,
      "learning_rate": 4.901411728996992e-05,
      "loss": 3.7103,
      "step": 447200
    },
    {
      "epoch": 0.07888842974987977,
      "grad_norm": 7.028215408325195,
      "learning_rate": 4.9013896832699216e-05,
      "loss": 3.7492,
      "step": 447300
    },
    {
      "epoch": 0.07890606633153634,
      "grad_norm": 6.316380977630615,
      "learning_rate": 4.9013676375428505e-05,
      "loss": 3.717,
      "step": 447400
    },
    {
      "epoch": 0.07892370291319292,
      "grad_norm": 8.098759651184082,
      "learning_rate": 4.90134559181578e-05,
      "loss": 3.6425,
      "step": 447500
    },
    {
      "epoch": 0.0789413394948495,
      "grad_norm": 11.230232238769531,
      "learning_rate": 4.901323546088709e-05,
      "loss": 3.742,
      "step": 447600
    },
    {
      "epoch": 0.07895897607650608,
      "grad_norm": 5.1500325202941895,
      "learning_rate": 4.9013015003616384e-05,
      "loss": 3.6585,
      "step": 447700
    },
    {
      "epoch": 0.07897661265816266,
      "grad_norm": 5.046217918395996,
      "learning_rate": 4.901279454634567e-05,
      "loss": 3.6482,
      "step": 447800
    },
    {
      "epoch": 0.07899424923981924,
      "grad_norm": 9.995467185974121,
      "learning_rate": 4.901257408907497e-05,
      "loss": 3.7254,
      "step": 447900
    },
    {
      "epoch": 0.07901188582147582,
      "grad_norm": 6.866321563720703,
      "learning_rate": 4.9012353631804264e-05,
      "loss": 3.6507,
      "step": 448000
    },
    {
      "epoch": 0.0790295224031324,
      "grad_norm": 8.573179244995117,
      "learning_rate": 4.901213317453355e-05,
      "loss": 3.6547,
      "step": 448100
    },
    {
      "epoch": 0.07904715898478898,
      "grad_norm": 7.5633039474487305,
      "learning_rate": 4.901191271726285e-05,
      "loss": 3.6406,
      "step": 448200
    },
    {
      "epoch": 0.07906479556644555,
      "grad_norm": 9.875642776489258,
      "learning_rate": 4.9011692259992144e-05,
      "loss": 3.6757,
      "step": 448300
    },
    {
      "epoch": 0.07908243214810214,
      "grad_norm": 8.871234893798828,
      "learning_rate": 4.901147180272143e-05,
      "loss": 3.6321,
      "step": 448400
    },
    {
      "epoch": 0.07910006872975872,
      "grad_norm": 8.340600967407227,
      "learning_rate": 4.901125134545073e-05,
      "loss": 3.6543,
      "step": 448500
    },
    {
      "epoch": 0.07911770531141529,
      "grad_norm": 7.9468865394592285,
      "learning_rate": 4.901103088818002e-05,
      "loss": 3.6933,
      "step": 448600
    },
    {
      "epoch": 0.07913534189307188,
      "grad_norm": 7.056306838989258,
      "learning_rate": 4.901081043090931e-05,
      "loss": 3.7121,
      "step": 448700
    },
    {
      "epoch": 0.07915297847472845,
      "grad_norm": 5.569892406463623,
      "learning_rate": 4.901058997363861e-05,
      "loss": 3.6226,
      "step": 448800
    },
    {
      "epoch": 0.07917061505638504,
      "grad_norm": 6.641622066497803,
      "learning_rate": 4.9010369516367896e-05,
      "loss": 3.7236,
      "step": 448900
    },
    {
      "epoch": 0.0791882516380416,
      "grad_norm": 7.713317394256592,
      "learning_rate": 4.9010149059097185e-05,
      "loss": 3.6748,
      "step": 449000
    },
    {
      "epoch": 0.07920588821969819,
      "grad_norm": 7.660445690155029,
      "learning_rate": 4.900992860182648e-05,
      "loss": 3.6194,
      "step": 449100
    },
    {
      "epoch": 0.07922352480135478,
      "grad_norm": 7.586349010467529,
      "learning_rate": 4.9009708144555776e-05,
      "loss": 3.6955,
      "step": 449200
    },
    {
      "epoch": 0.07924116138301135,
      "grad_norm": 6.464510440826416,
      "learning_rate": 4.9009487687285064e-05,
      "loss": 3.746,
      "step": 449300
    },
    {
      "epoch": 0.07925879796466793,
      "grad_norm": 5.323965072631836,
      "learning_rate": 4.900926723001436e-05,
      "loss": 3.6253,
      "step": 449400
    },
    {
      "epoch": 0.0792764345463245,
      "grad_norm": 10.305486679077148,
      "learning_rate": 4.9009046772743655e-05,
      "loss": 3.6537,
      "step": 449500
    },
    {
      "epoch": 0.07929407112798109,
      "grad_norm": 6.6428070068359375,
      "learning_rate": 4.9008826315472944e-05,
      "loss": 3.626,
      "step": 449600
    },
    {
      "epoch": 0.07931170770963766,
      "grad_norm": 9.559377670288086,
      "learning_rate": 4.900860585820224e-05,
      "loss": 3.6443,
      "step": 449700
    },
    {
      "epoch": 0.07932934429129425,
      "grad_norm": 4.657313346862793,
      "learning_rate": 4.9008385400931535e-05,
      "loss": 3.5874,
      "step": 449800
    },
    {
      "epoch": 0.07934698087295083,
      "grad_norm": 7.996322154998779,
      "learning_rate": 4.9008164943660824e-05,
      "loss": 3.5391,
      "step": 449900
    },
    {
      "epoch": 0.0793646174546074,
      "grad_norm": 9.335685729980469,
      "learning_rate": 4.900794448639012e-05,
      "loss": 3.5945,
      "step": 450000
    },
    {
      "epoch": 0.07938225403626399,
      "grad_norm": 6.692124366760254,
      "learning_rate": 4.9007724029119415e-05,
      "loss": 3.6,
      "step": 450100
    },
    {
      "epoch": 0.07939989061792056,
      "grad_norm": 6.902562141418457,
      "learning_rate": 4.90075035718487e-05,
      "loss": 3.7687,
      "step": 450200
    },
    {
      "epoch": 0.07941752719957715,
      "grad_norm": 7.841037750244141,
      "learning_rate": 4.9007283114578e-05,
      "loss": 3.8245,
      "step": 450300
    },
    {
      "epoch": 0.07943516378123372,
      "grad_norm": 8.230574607849121,
      "learning_rate": 4.900706265730729e-05,
      "loss": 3.6086,
      "step": 450400
    },
    {
      "epoch": 0.0794528003628903,
      "grad_norm": 8.412832260131836,
      "learning_rate": 4.9006842200036576e-05,
      "loss": 3.6934,
      "step": 450500
    },
    {
      "epoch": 0.07947043694454688,
      "grad_norm": 7.753962516784668,
      "learning_rate": 4.900662174276587e-05,
      "loss": 3.7499,
      "step": 450600
    },
    {
      "epoch": 0.07948807352620346,
      "grad_norm": 8.99836540222168,
      "learning_rate": 4.900640128549517e-05,
      "loss": 3.708,
      "step": 450700
    },
    {
      "epoch": 0.07950571010786005,
      "grad_norm": 8.29566478729248,
      "learning_rate": 4.9006180828224456e-05,
      "loss": 3.7645,
      "step": 450800
    },
    {
      "epoch": 0.07952334668951662,
      "grad_norm": 7.623349189758301,
      "learning_rate": 4.900596037095375e-05,
      "loss": 3.7054,
      "step": 450900
    },
    {
      "epoch": 0.0795409832711732,
      "grad_norm": 7.671008110046387,
      "learning_rate": 4.900573991368305e-05,
      "loss": 3.7341,
      "step": 451000
    },
    {
      "epoch": 0.07955861985282978,
      "grad_norm": 8.019137382507324,
      "learning_rate": 4.9005519456412335e-05,
      "loss": 3.7379,
      "step": 451100
    },
    {
      "epoch": 0.07957625643448636,
      "grad_norm": 7.894564628601074,
      "learning_rate": 4.900529899914163e-05,
      "loss": 3.6569,
      "step": 451200
    },
    {
      "epoch": 0.07959389301614293,
      "grad_norm": 6.221315860748291,
      "learning_rate": 4.9005078541870926e-05,
      "loss": 3.7109,
      "step": 451300
    },
    {
      "epoch": 0.07961152959779952,
      "grad_norm": 6.0035505294799805,
      "learning_rate": 4.9004858084600215e-05,
      "loss": 3.6806,
      "step": 451400
    },
    {
      "epoch": 0.0796291661794561,
      "grad_norm": 6.0158796310424805,
      "learning_rate": 4.900463762732951e-05,
      "loss": 3.6575,
      "step": 451500
    },
    {
      "epoch": 0.07964680276111268,
      "grad_norm": 5.618982791900635,
      "learning_rate": 4.9004417170058806e-05,
      "loss": 3.6604,
      "step": 451600
    },
    {
      "epoch": 0.07966443934276926,
      "grad_norm": 9.403709411621094,
      "learning_rate": 4.9004196712788095e-05,
      "loss": 3.7417,
      "step": 451700
    },
    {
      "epoch": 0.07968207592442583,
      "grad_norm": 6.4163103103637695,
      "learning_rate": 4.900397625551738e-05,
      "loss": 3.6812,
      "step": 451800
    },
    {
      "epoch": 0.07969971250608242,
      "grad_norm": 7.057097434997559,
      "learning_rate": 4.900375579824668e-05,
      "loss": 3.6661,
      "step": 451900
    },
    {
      "epoch": 0.07971734908773899,
      "grad_norm": 5.2385053634643555,
      "learning_rate": 4.900353534097597e-05,
      "loss": 3.7222,
      "step": 452000
    },
    {
      "epoch": 0.07973498566939557,
      "grad_norm": 5.46359395980835,
      "learning_rate": 4.900331488370526e-05,
      "loss": 3.7447,
      "step": 452100
    },
    {
      "epoch": 0.07975262225105216,
      "grad_norm": 12.607165336608887,
      "learning_rate": 4.900309442643456e-05,
      "loss": 3.7119,
      "step": 452200
    },
    {
      "epoch": 0.07977025883270873,
      "grad_norm": 7.149017333984375,
      "learning_rate": 4.900287396916385e-05,
      "loss": 3.6171,
      "step": 452300
    },
    {
      "epoch": 0.07978789541436532,
      "grad_norm": 7.101261138916016,
      "learning_rate": 4.900265351189314e-05,
      "loss": 3.7223,
      "step": 452400
    },
    {
      "epoch": 0.07980553199602189,
      "grad_norm": 6.696526527404785,
      "learning_rate": 4.900243305462244e-05,
      "loss": 3.6004,
      "step": 452500
    },
    {
      "epoch": 0.07982316857767847,
      "grad_norm": 9.199493408203125,
      "learning_rate": 4.900221259735173e-05,
      "loss": 3.577,
      "step": 452600
    },
    {
      "epoch": 0.07984080515933505,
      "grad_norm": 7.151607990264893,
      "learning_rate": 4.900199214008102e-05,
      "loss": 3.7011,
      "step": 452700
    },
    {
      "epoch": 0.07985844174099163,
      "grad_norm": 6.791898727416992,
      "learning_rate": 4.900177168281032e-05,
      "loss": 3.6969,
      "step": 452800
    },
    {
      "epoch": 0.07987607832264822,
      "grad_norm": 6.591052055358887,
      "learning_rate": 4.9001551225539606e-05,
      "loss": 3.7302,
      "step": 452900
    },
    {
      "epoch": 0.07989371490430479,
      "grad_norm": 7.260658264160156,
      "learning_rate": 4.90013307682689e-05,
      "loss": 3.6424,
      "step": 453000
    },
    {
      "epoch": 0.07991135148596137,
      "grad_norm": 7.611691474914551,
      "learning_rate": 4.90011103109982e-05,
      "loss": 3.6702,
      "step": 453100
    },
    {
      "epoch": 0.07992898806761795,
      "grad_norm": 9.194318771362305,
      "learning_rate": 4.9000889853727486e-05,
      "loss": 3.6804,
      "step": 453200
    },
    {
      "epoch": 0.07994662464927453,
      "grad_norm": 5.874063491821289,
      "learning_rate": 4.9000669396456775e-05,
      "loss": 3.6765,
      "step": 453300
    },
    {
      "epoch": 0.0799642612309311,
      "grad_norm": 6.919520378112793,
      "learning_rate": 4.900044893918607e-05,
      "loss": 3.6444,
      "step": 453400
    },
    {
      "epoch": 0.07998189781258769,
      "grad_norm": 6.824289321899414,
      "learning_rate": 4.900022848191536e-05,
      "loss": 3.6286,
      "step": 453500
    },
    {
      "epoch": 0.07999953439424426,
      "grad_norm": 12.263348579406738,
      "learning_rate": 4.9000008024644654e-05,
      "loss": 3.6698,
      "step": 453600
    },
    {
      "epoch": 0.08001717097590084,
      "grad_norm": 7.191555500030518,
      "learning_rate": 4.899978756737395e-05,
      "loss": 3.6282,
      "step": 453700
    },
    {
      "epoch": 0.08003480755755743,
      "grad_norm": 7.762960433959961,
      "learning_rate": 4.899956711010324e-05,
      "loss": 3.6751,
      "step": 453800
    },
    {
      "epoch": 0.080052444139214,
      "grad_norm": 8.705379486083984,
      "learning_rate": 4.8999346652832534e-05,
      "loss": 3.6835,
      "step": 453900
    },
    {
      "epoch": 0.08007008072087059,
      "grad_norm": 5.611862659454346,
      "learning_rate": 4.899912619556183e-05,
      "loss": 3.7255,
      "step": 454000
    },
    {
      "epoch": 0.08008771730252716,
      "grad_norm": 6.1378326416015625,
      "learning_rate": 4.899890573829112e-05,
      "loss": 3.7159,
      "step": 454100
    },
    {
      "epoch": 0.08010535388418374,
      "grad_norm": 5.892423629760742,
      "learning_rate": 4.8998685281020414e-05,
      "loss": 3.738,
      "step": 454200
    },
    {
      "epoch": 0.08012299046584032,
      "grad_norm": 7.221549034118652,
      "learning_rate": 4.899846482374971e-05,
      "loss": 3.7574,
      "step": 454300
    },
    {
      "epoch": 0.0801406270474969,
      "grad_norm": 10.893157958984375,
      "learning_rate": 4.8998244366479e-05,
      "loss": 3.7879,
      "step": 454400
    },
    {
      "epoch": 0.08015826362915349,
      "grad_norm": 7.762439727783203,
      "learning_rate": 4.899802390920829e-05,
      "loss": 3.7041,
      "step": 454500
    },
    {
      "epoch": 0.08017590021081006,
      "grad_norm": 6.2527079582214355,
      "learning_rate": 4.899780345193758e-05,
      "loss": 3.7208,
      "step": 454600
    },
    {
      "epoch": 0.08019353679246664,
      "grad_norm": 9.565367698669434,
      "learning_rate": 4.899758299466688e-05,
      "loss": 3.6968,
      "step": 454700
    },
    {
      "epoch": 0.08021117337412322,
      "grad_norm": 7.904262065887451,
      "learning_rate": 4.8997362537396166e-05,
      "loss": 3.6074,
      "step": 454800
    },
    {
      "epoch": 0.0802288099557798,
      "grad_norm": 6.515759468078613,
      "learning_rate": 4.899714208012546e-05,
      "loss": 3.7266,
      "step": 454900
    },
    {
      "epoch": 0.08024644653743637,
      "grad_norm": 9.007308959960938,
      "learning_rate": 4.899692162285475e-05,
      "loss": 3.6039,
      "step": 455000
    },
    {
      "epoch": 0.08026408311909296,
      "grad_norm": 6.8334736824035645,
      "learning_rate": 4.8996701165584046e-05,
      "loss": 3.5574,
      "step": 455100
    },
    {
      "epoch": 0.08028171970074954,
      "grad_norm": 6.384785175323486,
      "learning_rate": 4.899648070831334e-05,
      "loss": 3.6419,
      "step": 455200
    },
    {
      "epoch": 0.08029935628240611,
      "grad_norm": 6.590120792388916,
      "learning_rate": 4.899626025104263e-05,
      "loss": 3.6467,
      "step": 455300
    },
    {
      "epoch": 0.0803169928640627,
      "grad_norm": 7.904072284698486,
      "learning_rate": 4.8996039793771925e-05,
      "loss": 3.674,
      "step": 455400
    },
    {
      "epoch": 0.08033462944571927,
      "grad_norm": 7.78420352935791,
      "learning_rate": 4.899581933650122e-05,
      "loss": 3.6352,
      "step": 455500
    },
    {
      "epoch": 0.08035226602737586,
      "grad_norm": 6.201144218444824,
      "learning_rate": 4.899559887923051e-05,
      "loss": 3.6259,
      "step": 455600
    },
    {
      "epoch": 0.08036990260903243,
      "grad_norm": 8.22265625,
      "learning_rate": 4.8995378421959805e-05,
      "loss": 3.6533,
      "step": 455700
    },
    {
      "epoch": 0.08038753919068901,
      "grad_norm": 9.17431640625,
      "learning_rate": 4.89951579646891e-05,
      "loss": 3.6412,
      "step": 455800
    },
    {
      "epoch": 0.0804051757723456,
      "grad_norm": 8.541629791259766,
      "learning_rate": 4.899493750741839e-05,
      "loss": 3.6732,
      "step": 455900
    },
    {
      "epoch": 0.08042281235400217,
      "grad_norm": 7.930307388305664,
      "learning_rate": 4.8994717050147685e-05,
      "loss": 3.6882,
      "step": 456000
    },
    {
      "epoch": 0.08044044893565876,
      "grad_norm": 7.5908002853393555,
      "learning_rate": 4.899449659287697e-05,
      "loss": 3.7036,
      "step": 456100
    },
    {
      "epoch": 0.08045808551731533,
      "grad_norm": 6.217222213745117,
      "learning_rate": 4.899427613560626e-05,
      "loss": 3.5745,
      "step": 456200
    },
    {
      "epoch": 0.08047572209897191,
      "grad_norm": 6.579167366027832,
      "learning_rate": 4.899405567833556e-05,
      "loss": 3.6691,
      "step": 456300
    },
    {
      "epoch": 0.08049335868062849,
      "grad_norm": 7.2604451179504395,
      "learning_rate": 4.899383522106485e-05,
      "loss": 3.5801,
      "step": 456400
    },
    {
      "epoch": 0.08051099526228507,
      "grad_norm": 6.843639850616455,
      "learning_rate": 4.899361476379414e-05,
      "loss": 3.6561,
      "step": 456500
    },
    {
      "epoch": 0.08052863184394164,
      "grad_norm": 7.165009498596191,
      "learning_rate": 4.899339430652344e-05,
      "loss": 3.8033,
      "step": 456600
    },
    {
      "epoch": 0.08054626842559823,
      "grad_norm": 8.49321174621582,
      "learning_rate": 4.899317384925273e-05,
      "loss": 3.6614,
      "step": 456700
    },
    {
      "epoch": 0.08056390500725481,
      "grad_norm": 10.109227180480957,
      "learning_rate": 4.899295339198202e-05,
      "loss": 3.7104,
      "step": 456800
    },
    {
      "epoch": 0.08058154158891138,
      "grad_norm": 5.770642280578613,
      "learning_rate": 4.899273293471132e-05,
      "loss": 3.6507,
      "step": 456900
    },
    {
      "epoch": 0.08059917817056797,
      "grad_norm": 7.293249607086182,
      "learning_rate": 4.899251247744061e-05,
      "loss": 3.6729,
      "step": 457000
    },
    {
      "epoch": 0.08061681475222454,
      "grad_norm": 5.761659145355225,
      "learning_rate": 4.89922920201699e-05,
      "loss": 3.5844,
      "step": 457100
    },
    {
      "epoch": 0.08063445133388113,
      "grad_norm": 5.54684591293335,
      "learning_rate": 4.8992071562899196e-05,
      "loss": 3.5997,
      "step": 457200
    },
    {
      "epoch": 0.0806520879155377,
      "grad_norm": 6.316866874694824,
      "learning_rate": 4.899185110562849e-05,
      "loss": 3.7271,
      "step": 457300
    },
    {
      "epoch": 0.08066972449719428,
      "grad_norm": 5.203205585479736,
      "learning_rate": 4.899163064835778e-05,
      "loss": 3.7102,
      "step": 457400
    },
    {
      "epoch": 0.08068736107885087,
      "grad_norm": 6.6108622550964355,
      "learning_rate": 4.8991410191087076e-05,
      "loss": 3.7355,
      "step": 457500
    },
    {
      "epoch": 0.08070499766050744,
      "grad_norm": 6.09043550491333,
      "learning_rate": 4.8991189733816365e-05,
      "loss": 3.7413,
      "step": 457600
    },
    {
      "epoch": 0.08072263424216403,
      "grad_norm": 9.324723243713379,
      "learning_rate": 4.899096927654566e-05,
      "loss": 3.6582,
      "step": 457700
    },
    {
      "epoch": 0.0807402708238206,
      "grad_norm": 6.882440090179443,
      "learning_rate": 4.899074881927495e-05,
      "loss": 3.8105,
      "step": 457800
    },
    {
      "epoch": 0.08075790740547718,
      "grad_norm": 6.776953220367432,
      "learning_rate": 4.8990528362004244e-05,
      "loss": 3.7055,
      "step": 457900
    },
    {
      "epoch": 0.08077554398713375,
      "grad_norm": 5.646230697631836,
      "learning_rate": 4.899030790473354e-05,
      "loss": 3.6784,
      "step": 458000
    },
    {
      "epoch": 0.08079318056879034,
      "grad_norm": 7.157686233520508,
      "learning_rate": 4.899008744746283e-05,
      "loss": 3.6551,
      "step": 458100
    },
    {
      "epoch": 0.08081081715044693,
      "grad_norm": 10.208516120910645,
      "learning_rate": 4.8989866990192124e-05,
      "loss": 3.7385,
      "step": 458200
    },
    {
      "epoch": 0.0808284537321035,
      "grad_norm": 9.56515121459961,
      "learning_rate": 4.898964653292142e-05,
      "loss": 3.6386,
      "step": 458300
    },
    {
      "epoch": 0.08084609031376008,
      "grad_norm": 6.625887393951416,
      "learning_rate": 4.898942607565071e-05,
      "loss": 3.6757,
      "step": 458400
    },
    {
      "epoch": 0.08086372689541665,
      "grad_norm": 6.041600704193115,
      "learning_rate": 4.8989205618380004e-05,
      "loss": 3.5842,
      "step": 458500
    },
    {
      "epoch": 0.08088136347707324,
      "grad_norm": 6.700634479522705,
      "learning_rate": 4.89889851611093e-05,
      "loss": 3.7216,
      "step": 458600
    },
    {
      "epoch": 0.08089900005872981,
      "grad_norm": 7.422516822814941,
      "learning_rate": 4.898876470383859e-05,
      "loss": 3.6392,
      "step": 458700
    },
    {
      "epoch": 0.0809166366403864,
      "grad_norm": 7.011009693145752,
      "learning_rate": 4.898854424656788e-05,
      "loss": 3.6568,
      "step": 458800
    },
    {
      "epoch": 0.08093427322204298,
      "grad_norm": 7.311017036437988,
      "learning_rate": 4.898832378929717e-05,
      "loss": 3.6884,
      "step": 458900
    },
    {
      "epoch": 0.08095190980369955,
      "grad_norm": 8.863320350646973,
      "learning_rate": 4.898810333202646e-05,
      "loss": 3.6632,
      "step": 459000
    },
    {
      "epoch": 0.08096954638535614,
      "grad_norm": 7.672938346862793,
      "learning_rate": 4.8987882874755756e-05,
      "loss": 3.7222,
      "step": 459100
    },
    {
      "epoch": 0.08098718296701271,
      "grad_norm": 6.518581867218018,
      "learning_rate": 4.898766241748505e-05,
      "loss": 3.6539,
      "step": 459200
    },
    {
      "epoch": 0.0810048195486693,
      "grad_norm": 6.527780055999756,
      "learning_rate": 4.898744196021434e-05,
      "loss": 3.6913,
      "step": 459300
    },
    {
      "epoch": 0.08102245613032587,
      "grad_norm": 6.9496917724609375,
      "learning_rate": 4.8987221502943636e-05,
      "loss": 3.6336,
      "step": 459400
    },
    {
      "epoch": 0.08104009271198245,
      "grad_norm": 10.112728118896484,
      "learning_rate": 4.898700104567293e-05,
      "loss": 3.767,
      "step": 459500
    },
    {
      "epoch": 0.08105772929363902,
      "grad_norm": 8.447321891784668,
      "learning_rate": 4.898678058840222e-05,
      "loss": 3.6601,
      "step": 459600
    },
    {
      "epoch": 0.08107536587529561,
      "grad_norm": 10.577740669250488,
      "learning_rate": 4.8986560131131515e-05,
      "loss": 3.613,
      "step": 459700
    },
    {
      "epoch": 0.0810930024569522,
      "grad_norm": 5.858412265777588,
      "learning_rate": 4.898633967386081e-05,
      "loss": 3.5905,
      "step": 459800
    },
    {
      "epoch": 0.08111063903860877,
      "grad_norm": 9.886750221252441,
      "learning_rate": 4.89861192165901e-05,
      "loss": 3.732,
      "step": 459900
    },
    {
      "epoch": 0.08112827562026535,
      "grad_norm": 7.295095920562744,
      "learning_rate": 4.8985898759319395e-05,
      "loss": 3.7036,
      "step": 460000
    },
    {
      "epoch": 0.08114591220192192,
      "grad_norm": 6.844779968261719,
      "learning_rate": 4.898567830204869e-05,
      "loss": 3.7165,
      "step": 460100
    },
    {
      "epoch": 0.08116354878357851,
      "grad_norm": 7.756472587585449,
      "learning_rate": 4.898545784477798e-05,
      "loss": 3.6807,
      "step": 460200
    },
    {
      "epoch": 0.08118118536523508,
      "grad_norm": 6.590462684631348,
      "learning_rate": 4.8985237387507275e-05,
      "loss": 3.6165,
      "step": 460300
    },
    {
      "epoch": 0.08119882194689167,
      "grad_norm": 6.615135192871094,
      "learning_rate": 4.898501693023656e-05,
      "loss": 3.6991,
      "step": 460400
    },
    {
      "epoch": 0.08121645852854825,
      "grad_norm": 9.257734298706055,
      "learning_rate": 4.898479647296585e-05,
      "loss": 3.6383,
      "step": 460500
    },
    {
      "epoch": 0.08123409511020482,
      "grad_norm": 5.528608322143555,
      "learning_rate": 4.898457601569515e-05,
      "loss": 3.7261,
      "step": 460600
    },
    {
      "epoch": 0.08125173169186141,
      "grad_norm": 4.710591793060303,
      "learning_rate": 4.898435555842444e-05,
      "loss": 3.6523,
      "step": 460700
    },
    {
      "epoch": 0.08126936827351798,
      "grad_norm": 6.282721519470215,
      "learning_rate": 4.898413510115373e-05,
      "loss": 3.5796,
      "step": 460800
    },
    {
      "epoch": 0.08128700485517457,
      "grad_norm": 7.049571990966797,
      "learning_rate": 4.898391464388303e-05,
      "loss": 3.6319,
      "step": 460900
    },
    {
      "epoch": 0.08130464143683114,
      "grad_norm": 7.088080406188965,
      "learning_rate": 4.898369418661232e-05,
      "loss": 3.6931,
      "step": 461000
    },
    {
      "epoch": 0.08132227801848772,
      "grad_norm": 6.828220367431641,
      "learning_rate": 4.898347372934161e-05,
      "loss": 3.748,
      "step": 461100
    },
    {
      "epoch": 0.08133991460014431,
      "grad_norm": 7.280300140380859,
      "learning_rate": 4.898325327207091e-05,
      "loss": 3.7019,
      "step": 461200
    },
    {
      "epoch": 0.08135755118180088,
      "grad_norm": 8.511343002319336,
      "learning_rate": 4.89830328148002e-05,
      "loss": 3.6542,
      "step": 461300
    },
    {
      "epoch": 0.08137518776345747,
      "grad_norm": 6.456700325012207,
      "learning_rate": 4.898281235752949e-05,
      "loss": 3.6421,
      "step": 461400
    },
    {
      "epoch": 0.08139282434511404,
      "grad_norm": 8.33211612701416,
      "learning_rate": 4.8982591900258786e-05,
      "loss": 3.5937,
      "step": 461500
    },
    {
      "epoch": 0.08141046092677062,
      "grad_norm": 6.834408760070801,
      "learning_rate": 4.898237144298808e-05,
      "loss": 3.5859,
      "step": 461600
    },
    {
      "epoch": 0.0814280975084272,
      "grad_norm": 8.693920135498047,
      "learning_rate": 4.898215098571737e-05,
      "loss": 3.7474,
      "step": 461700
    },
    {
      "epoch": 0.08144573409008378,
      "grad_norm": 6.913332462310791,
      "learning_rate": 4.898193052844666e-05,
      "loss": 3.6555,
      "step": 461800
    },
    {
      "epoch": 0.08146337067174036,
      "grad_norm": 5.841396808624268,
      "learning_rate": 4.8981710071175955e-05,
      "loss": 3.6264,
      "step": 461900
    },
    {
      "epoch": 0.08148100725339694,
      "grad_norm": 7.319560527801514,
      "learning_rate": 4.898148961390524e-05,
      "loss": 3.6349,
      "step": 462000
    },
    {
      "epoch": 0.08149864383505352,
      "grad_norm": 8.169614791870117,
      "learning_rate": 4.898126915663454e-05,
      "loss": 3.7792,
      "step": 462100
    },
    {
      "epoch": 0.0815162804167101,
      "grad_norm": 9.477011680603027,
      "learning_rate": 4.8981048699363834e-05,
      "loss": 3.6744,
      "step": 462200
    },
    {
      "epoch": 0.08153391699836668,
      "grad_norm": 10.348965644836426,
      "learning_rate": 4.898082824209312e-05,
      "loss": 3.7911,
      "step": 462300
    },
    {
      "epoch": 0.08155155358002325,
      "grad_norm": 6.557929992675781,
      "learning_rate": 4.898060778482242e-05,
      "loss": 3.6673,
      "step": 462400
    },
    {
      "epoch": 0.08156919016167984,
      "grad_norm": 7.168118476867676,
      "learning_rate": 4.8980387327551714e-05,
      "loss": 3.7102,
      "step": 462500
    },
    {
      "epoch": 0.08158682674333642,
      "grad_norm": 7.068746089935303,
      "learning_rate": 4.8980166870281e-05,
      "loss": 3.6508,
      "step": 462600
    },
    {
      "epoch": 0.08160446332499299,
      "grad_norm": 5.629518508911133,
      "learning_rate": 4.89799464130103e-05,
      "loss": 3.5736,
      "step": 462700
    },
    {
      "epoch": 0.08162209990664958,
      "grad_norm": 7.131881237030029,
      "learning_rate": 4.8979725955739593e-05,
      "loss": 3.6674,
      "step": 462800
    },
    {
      "epoch": 0.08163973648830615,
      "grad_norm": 7.530069351196289,
      "learning_rate": 4.897950549846888e-05,
      "loss": 3.6719,
      "step": 462900
    },
    {
      "epoch": 0.08165737306996274,
      "grad_norm": 10.35403060913086,
      "learning_rate": 4.897928504119818e-05,
      "loss": 3.7197,
      "step": 463000
    },
    {
      "epoch": 0.0816750096516193,
      "grad_norm": 7.0332841873168945,
      "learning_rate": 4.897906458392747e-05,
      "loss": 3.7035,
      "step": 463100
    },
    {
      "epoch": 0.08169264623327589,
      "grad_norm": 7.955877304077148,
      "learning_rate": 4.897884412665676e-05,
      "loss": 3.6178,
      "step": 463200
    },
    {
      "epoch": 0.08171028281493246,
      "grad_norm": 8.419726371765137,
      "learning_rate": 4.897862366938605e-05,
      "loss": 3.6245,
      "step": 463300
    },
    {
      "epoch": 0.08172791939658905,
      "grad_norm": 7.570084095001221,
      "learning_rate": 4.8978403212115346e-05,
      "loss": 3.6851,
      "step": 463400
    },
    {
      "epoch": 0.08174555597824563,
      "grad_norm": 9.540433883666992,
      "learning_rate": 4.8978182754844635e-05,
      "loss": 3.6618,
      "step": 463500
    },
    {
      "epoch": 0.0817631925599022,
      "grad_norm": 10.29986572265625,
      "learning_rate": 4.897796229757393e-05,
      "loss": 3.7006,
      "step": 463600
    },
    {
      "epoch": 0.08178082914155879,
      "grad_norm": 6.841432094573975,
      "learning_rate": 4.8977741840303226e-05,
      "loss": 3.5546,
      "step": 463700
    },
    {
      "epoch": 0.08179846572321536,
      "grad_norm": 9.36317253112793,
      "learning_rate": 4.8977521383032514e-05,
      "loss": 3.6814,
      "step": 463800
    },
    {
      "epoch": 0.08181610230487195,
      "grad_norm": 11.961160659790039,
      "learning_rate": 4.897730092576181e-05,
      "loss": 3.5766,
      "step": 463900
    },
    {
      "epoch": 0.08183373888652852,
      "grad_norm": 9.517953872680664,
      "learning_rate": 4.8977080468491105e-05,
      "loss": 3.6785,
      "step": 464000
    },
    {
      "epoch": 0.0818513754681851,
      "grad_norm": 8.204902648925781,
      "learning_rate": 4.8976860011220394e-05,
      "loss": 3.5876,
      "step": 464100
    },
    {
      "epoch": 0.08186901204984169,
      "grad_norm": 8.089981079101562,
      "learning_rate": 4.897663955394969e-05,
      "loss": 3.674,
      "step": 464200
    },
    {
      "epoch": 0.08188664863149826,
      "grad_norm": 5.6236677169799805,
      "learning_rate": 4.8976419096678985e-05,
      "loss": 3.5976,
      "step": 464300
    },
    {
      "epoch": 0.08190428521315485,
      "grad_norm": 5.778990745544434,
      "learning_rate": 4.8976198639408274e-05,
      "loss": 3.7217,
      "step": 464400
    },
    {
      "epoch": 0.08192192179481142,
      "grad_norm": 6.716907978057861,
      "learning_rate": 4.897597818213757e-05,
      "loss": 3.6536,
      "step": 464500
    },
    {
      "epoch": 0.081939558376468,
      "grad_norm": 5.237999439239502,
      "learning_rate": 4.897575772486686e-05,
      "loss": 3.5587,
      "step": 464600
    },
    {
      "epoch": 0.08195719495812458,
      "grad_norm": 8.591242790222168,
      "learning_rate": 4.897553726759615e-05,
      "loss": 3.7063,
      "step": 464700
    },
    {
      "epoch": 0.08197483153978116,
      "grad_norm": 9.69079875946045,
      "learning_rate": 4.897531681032544e-05,
      "loss": 3.5218,
      "step": 464800
    },
    {
      "epoch": 0.08199246812143775,
      "grad_norm": 9.626258850097656,
      "learning_rate": 4.897509635305474e-05,
      "loss": 3.686,
      "step": 464900
    },
    {
      "epoch": 0.08201010470309432,
      "grad_norm": 9.658127784729004,
      "learning_rate": 4.8974875895784026e-05,
      "loss": 3.5598,
      "step": 465000
    },
    {
      "epoch": 0.0820277412847509,
      "grad_norm": 5.367817401885986,
      "learning_rate": 4.897465543851332e-05,
      "loss": 3.6799,
      "step": 465100
    },
    {
      "epoch": 0.08204537786640748,
      "grad_norm": 5.646162033081055,
      "learning_rate": 4.897443498124262e-05,
      "loss": 3.7158,
      "step": 465200
    },
    {
      "epoch": 0.08206301444806406,
      "grad_norm": 6.055764198303223,
      "learning_rate": 4.8974214523971906e-05,
      "loss": 3.7584,
      "step": 465300
    },
    {
      "epoch": 0.08208065102972063,
      "grad_norm": 6.884361743927002,
      "learning_rate": 4.89739940667012e-05,
      "loss": 3.6791,
      "step": 465400
    },
    {
      "epoch": 0.08209828761137722,
      "grad_norm": 11.485898971557617,
      "learning_rate": 4.8973773609430497e-05,
      "loss": 3.6763,
      "step": 465500
    },
    {
      "epoch": 0.0821159241930338,
      "grad_norm": 7.963928699493408,
      "learning_rate": 4.8973553152159785e-05,
      "loss": 3.7479,
      "step": 465600
    },
    {
      "epoch": 0.08213356077469038,
      "grad_norm": 5.979557037353516,
      "learning_rate": 4.897333269488908e-05,
      "loss": 3.6892,
      "step": 465700
    },
    {
      "epoch": 0.08215119735634696,
      "grad_norm": 8.296419143676758,
      "learning_rate": 4.8973112237618376e-05,
      "loss": 3.587,
      "step": 465800
    },
    {
      "epoch": 0.08216883393800353,
      "grad_norm": 7.262767791748047,
      "learning_rate": 4.8972891780347665e-05,
      "loss": 3.642,
      "step": 465900
    },
    {
      "epoch": 0.08218647051966012,
      "grad_norm": 6.411937713623047,
      "learning_rate": 4.897267132307696e-05,
      "loss": 3.7237,
      "step": 466000
    },
    {
      "epoch": 0.08220410710131669,
      "grad_norm": 7.921780586242676,
      "learning_rate": 4.897245086580625e-05,
      "loss": 3.6776,
      "step": 466100
    },
    {
      "epoch": 0.08222174368297328,
      "grad_norm": 8.795549392700195,
      "learning_rate": 4.897223040853554e-05,
      "loss": 3.6491,
      "step": 466200
    },
    {
      "epoch": 0.08223938026462985,
      "grad_norm": 7.260587215423584,
      "learning_rate": 4.897200995126483e-05,
      "loss": 3.7676,
      "step": 466300
    },
    {
      "epoch": 0.08225701684628643,
      "grad_norm": 5.389345169067383,
      "learning_rate": 4.897178949399413e-05,
      "loss": 3.6891,
      "step": 466400
    },
    {
      "epoch": 0.08227465342794302,
      "grad_norm": 10.021796226501465,
      "learning_rate": 4.897156903672342e-05,
      "loss": 3.7111,
      "step": 466500
    },
    {
      "epoch": 0.08229229000959959,
      "grad_norm": 7.631348609924316,
      "learning_rate": 4.897134857945271e-05,
      "loss": 3.6598,
      "step": 466600
    },
    {
      "epoch": 0.08230992659125617,
      "grad_norm": 8.690006256103516,
      "learning_rate": 4.897112812218201e-05,
      "loss": 3.6873,
      "step": 466700
    },
    {
      "epoch": 0.08232756317291275,
      "grad_norm": 6.927847862243652,
      "learning_rate": 4.89709076649113e-05,
      "loss": 3.6973,
      "step": 466800
    },
    {
      "epoch": 0.08234519975456933,
      "grad_norm": 8.0234956741333,
      "learning_rate": 4.897068720764059e-05,
      "loss": 3.7285,
      "step": 466900
    },
    {
      "epoch": 0.0823628363362259,
      "grad_norm": 8.737454414367676,
      "learning_rate": 4.897046675036989e-05,
      "loss": 3.6236,
      "step": 467000
    },
    {
      "epoch": 0.08238047291788249,
      "grad_norm": 6.471862316131592,
      "learning_rate": 4.897024629309918e-05,
      "loss": 3.6575,
      "step": 467100
    },
    {
      "epoch": 0.08239810949953907,
      "grad_norm": 6.012821197509766,
      "learning_rate": 4.897002583582847e-05,
      "loss": 3.7553,
      "step": 467200
    },
    {
      "epoch": 0.08241574608119565,
      "grad_norm": 8.017693519592285,
      "learning_rate": 4.896980537855777e-05,
      "loss": 3.6395,
      "step": 467300
    },
    {
      "epoch": 0.08243338266285223,
      "grad_norm": 8.511334419250488,
      "learning_rate": 4.8969584921287056e-05,
      "loss": 3.6073,
      "step": 467400
    },
    {
      "epoch": 0.0824510192445088,
      "grad_norm": 9.28030014038086,
      "learning_rate": 4.896936446401635e-05,
      "loss": 3.7319,
      "step": 467500
    },
    {
      "epoch": 0.08246865582616539,
      "grad_norm": 6.850978851318359,
      "learning_rate": 4.896914400674564e-05,
      "loss": 3.6224,
      "step": 467600
    },
    {
      "epoch": 0.08248629240782196,
      "grad_norm": 9.333820343017578,
      "learning_rate": 4.896892354947493e-05,
      "loss": 3.6195,
      "step": 467700
    },
    {
      "epoch": 0.08250392898947854,
      "grad_norm": 5.513814449310303,
      "learning_rate": 4.8968703092204225e-05,
      "loss": 3.7775,
      "step": 467800
    },
    {
      "epoch": 0.08252156557113513,
      "grad_norm": 6.416788101196289,
      "learning_rate": 4.896848263493352e-05,
      "loss": 3.7355,
      "step": 467900
    },
    {
      "epoch": 0.0825392021527917,
      "grad_norm": 6.186037063598633,
      "learning_rate": 4.8968262177662816e-05,
      "loss": 3.7746,
      "step": 468000
    },
    {
      "epoch": 0.08255683873444829,
      "grad_norm": 8.79930591583252,
      "learning_rate": 4.8968041720392104e-05,
      "loss": 3.6454,
      "step": 468100
    },
    {
      "epoch": 0.08257447531610486,
      "grad_norm": 5.7357683181762695,
      "learning_rate": 4.89678212631214e-05,
      "loss": 3.6194,
      "step": 468200
    },
    {
      "epoch": 0.08259211189776144,
      "grad_norm": 7.603967189788818,
      "learning_rate": 4.8967600805850695e-05,
      "loss": 3.5944,
      "step": 468300
    },
    {
      "epoch": 0.08260974847941802,
      "grad_norm": 7.273843288421631,
      "learning_rate": 4.8967380348579984e-05,
      "loss": 3.6031,
      "step": 468400
    },
    {
      "epoch": 0.0826273850610746,
      "grad_norm": 7.227357387542725,
      "learning_rate": 4.896715989130928e-05,
      "loss": 3.6676,
      "step": 468500
    },
    {
      "epoch": 0.08264502164273119,
      "grad_norm": 7.083517551422119,
      "learning_rate": 4.8966939434038575e-05,
      "loss": 3.7026,
      "step": 468600
    },
    {
      "epoch": 0.08266265822438776,
      "grad_norm": 6.854499340057373,
      "learning_rate": 4.8966718976767863e-05,
      "loss": 3.5754,
      "step": 468700
    },
    {
      "epoch": 0.08268029480604434,
      "grad_norm": 7.760843753814697,
      "learning_rate": 4.896649851949716e-05,
      "loss": 3.7076,
      "step": 468800
    },
    {
      "epoch": 0.08269793138770092,
      "grad_norm": 7.061065196990967,
      "learning_rate": 4.896627806222645e-05,
      "loss": 3.6913,
      "step": 468900
    },
    {
      "epoch": 0.0827155679693575,
      "grad_norm": 7.54286003112793,
      "learning_rate": 4.8966057604955736e-05,
      "loss": 3.7185,
      "step": 469000
    },
    {
      "epoch": 0.08273320455101407,
      "grad_norm": 6.685227870941162,
      "learning_rate": 4.896583714768503e-05,
      "loss": 3.5917,
      "step": 469100
    },
    {
      "epoch": 0.08275084113267066,
      "grad_norm": 9.20992660522461,
      "learning_rate": 4.896561669041433e-05,
      "loss": 3.7081,
      "step": 469200
    },
    {
      "epoch": 0.08276847771432723,
      "grad_norm": 5.476212501525879,
      "learning_rate": 4.8965396233143616e-05,
      "loss": 3.6381,
      "step": 469300
    },
    {
      "epoch": 0.08278611429598381,
      "grad_norm": 8.05681324005127,
      "learning_rate": 4.896517577587291e-05,
      "loss": 3.6534,
      "step": 469400
    },
    {
      "epoch": 0.0828037508776404,
      "grad_norm": 7.3948073387146,
      "learning_rate": 4.896495531860221e-05,
      "loss": 3.7357,
      "step": 469500
    },
    {
      "epoch": 0.08282138745929697,
      "grad_norm": 7.410417079925537,
      "learning_rate": 4.8964734861331496e-05,
      "loss": 3.6605,
      "step": 469600
    },
    {
      "epoch": 0.08283902404095356,
      "grad_norm": 7.844762802124023,
      "learning_rate": 4.896451440406079e-05,
      "loss": 3.6716,
      "step": 469700
    },
    {
      "epoch": 0.08285666062261013,
      "grad_norm": 8.710339546203613,
      "learning_rate": 4.8964293946790087e-05,
      "loss": 3.6776,
      "step": 469800
    },
    {
      "epoch": 0.08287429720426671,
      "grad_norm": 6.990035533905029,
      "learning_rate": 4.8964073489519375e-05,
      "loss": 3.5295,
      "step": 469900
    },
    {
      "epoch": 0.08289193378592329,
      "grad_norm": 7.192251205444336,
      "learning_rate": 4.896385303224867e-05,
      "loss": 3.6351,
      "step": 470000
    },
    {
      "epoch": 0.08290957036757987,
      "grad_norm": 5.716277122497559,
      "learning_rate": 4.8963632574977966e-05,
      "loss": 3.7353,
      "step": 470100
    },
    {
      "epoch": 0.08292720694923646,
      "grad_norm": 6.84272575378418,
      "learning_rate": 4.8963412117707255e-05,
      "loss": 3.7064,
      "step": 470200
    },
    {
      "epoch": 0.08294484353089303,
      "grad_norm": 6.2981390953063965,
      "learning_rate": 4.896319166043655e-05,
      "loss": 3.6961,
      "step": 470300
    },
    {
      "epoch": 0.08296248011254961,
      "grad_norm": 8.69148063659668,
      "learning_rate": 4.896297120316584e-05,
      "loss": 3.7153,
      "step": 470400
    },
    {
      "epoch": 0.08298011669420619,
      "grad_norm": 8.391313552856445,
      "learning_rate": 4.896275074589513e-05,
      "loss": 3.6126,
      "step": 470500
    },
    {
      "epoch": 0.08299775327586277,
      "grad_norm": 7.514958381652832,
      "learning_rate": 4.896253028862442e-05,
      "loss": 3.6775,
      "step": 470600
    },
    {
      "epoch": 0.08301538985751934,
      "grad_norm": 9.542119979858398,
      "learning_rate": 4.896230983135372e-05,
      "loss": 3.6652,
      "step": 470700
    },
    {
      "epoch": 0.08303302643917593,
      "grad_norm": 9.202783584594727,
      "learning_rate": 4.896208937408301e-05,
      "loss": 3.6261,
      "step": 470800
    },
    {
      "epoch": 0.08305066302083251,
      "grad_norm": 6.904431343078613,
      "learning_rate": 4.89618689168123e-05,
      "loss": 3.7308,
      "step": 470900
    },
    {
      "epoch": 0.08306829960248908,
      "grad_norm": 9.812596321105957,
      "learning_rate": 4.89616484595416e-05,
      "loss": 3.7914,
      "step": 471000
    },
    {
      "epoch": 0.08308593618414567,
      "grad_norm": 6.5433502197265625,
      "learning_rate": 4.896142800227089e-05,
      "loss": 3.7407,
      "step": 471100
    },
    {
      "epoch": 0.08310357276580224,
      "grad_norm": 5.891529560089111,
      "learning_rate": 4.896120754500018e-05,
      "loss": 3.7141,
      "step": 471200
    },
    {
      "epoch": 0.08312120934745883,
      "grad_norm": 5.854666233062744,
      "learning_rate": 4.896098708772948e-05,
      "loss": 3.7265,
      "step": 471300
    },
    {
      "epoch": 0.0831388459291154,
      "grad_norm": 7.332495212554932,
      "learning_rate": 4.8960766630458767e-05,
      "loss": 3.7758,
      "step": 471400
    },
    {
      "epoch": 0.08315648251077198,
      "grad_norm": 9.325897216796875,
      "learning_rate": 4.896054617318806e-05,
      "loss": 3.6188,
      "step": 471500
    },
    {
      "epoch": 0.08317411909242857,
      "grad_norm": 7.761599063873291,
      "learning_rate": 4.896032571591736e-05,
      "loss": 3.5861,
      "step": 471600
    },
    {
      "epoch": 0.08319175567408514,
      "grad_norm": 7.436484336853027,
      "learning_rate": 4.8960105258646646e-05,
      "loss": 3.6364,
      "step": 471700
    },
    {
      "epoch": 0.08320939225574173,
      "grad_norm": 6.747934818267822,
      "learning_rate": 4.8959884801375935e-05,
      "loss": 3.8097,
      "step": 471800
    },
    {
      "epoch": 0.0832270288373983,
      "grad_norm": 6.93924617767334,
      "learning_rate": 4.895966434410523e-05,
      "loss": 3.7117,
      "step": 471900
    },
    {
      "epoch": 0.08324466541905488,
      "grad_norm": 6.524205684661865,
      "learning_rate": 4.895944388683452e-05,
      "loss": 3.7271,
      "step": 472000
    },
    {
      "epoch": 0.08326230200071146,
      "grad_norm": 7.43046236038208,
      "learning_rate": 4.8959223429563815e-05,
      "loss": 3.7494,
      "step": 472100
    },
    {
      "epoch": 0.08327993858236804,
      "grad_norm": 7.541037559509277,
      "learning_rate": 4.895900297229311e-05,
      "loss": 3.7893,
      "step": 472200
    },
    {
      "epoch": 0.08329757516402461,
      "grad_norm": 5.903703212738037,
      "learning_rate": 4.89587825150224e-05,
      "loss": 3.6276,
      "step": 472300
    },
    {
      "epoch": 0.0833152117456812,
      "grad_norm": 6.4691619873046875,
      "learning_rate": 4.8958562057751694e-05,
      "loss": 3.6784,
      "step": 472400
    },
    {
      "epoch": 0.08333284832733778,
      "grad_norm": 6.6532087326049805,
      "learning_rate": 4.895834160048099e-05,
      "loss": 3.6448,
      "step": 472500
    },
    {
      "epoch": 0.08335048490899435,
      "grad_norm": 8.704421997070312,
      "learning_rate": 4.895812114321028e-05,
      "loss": 3.6505,
      "step": 472600
    },
    {
      "epoch": 0.08336812149065094,
      "grad_norm": 5.853432655334473,
      "learning_rate": 4.8957900685939574e-05,
      "loss": 3.6834,
      "step": 472700
    },
    {
      "epoch": 0.08338575807230751,
      "grad_norm": 7.467911243438721,
      "learning_rate": 4.895768022866887e-05,
      "loss": 3.5917,
      "step": 472800
    },
    {
      "epoch": 0.0834033946539641,
      "grad_norm": 8.280734062194824,
      "learning_rate": 4.895745977139816e-05,
      "loss": 3.6559,
      "step": 472900
    },
    {
      "epoch": 0.08342103123562067,
      "grad_norm": 6.498992919921875,
      "learning_rate": 4.8957239314127453e-05,
      "loss": 3.6268,
      "step": 473000
    },
    {
      "epoch": 0.08343866781727725,
      "grad_norm": 8.402034759521484,
      "learning_rate": 4.895701885685675e-05,
      "loss": 3.6818,
      "step": 473100
    },
    {
      "epoch": 0.08345630439893384,
      "grad_norm": 8.853461265563965,
      "learning_rate": 4.895679839958604e-05,
      "loss": 3.618,
      "step": 473200
    },
    {
      "epoch": 0.08347394098059041,
      "grad_norm": 7.010732173919678,
      "learning_rate": 4.8956577942315326e-05,
      "loss": 3.6833,
      "step": 473300
    },
    {
      "epoch": 0.083491577562247,
      "grad_norm": 5.710505485534668,
      "learning_rate": 4.895635748504462e-05,
      "loss": 3.7318,
      "step": 473400
    },
    {
      "epoch": 0.08350921414390357,
      "grad_norm": 5.630998611450195,
      "learning_rate": 4.895613702777391e-05,
      "loss": 3.6117,
      "step": 473500
    },
    {
      "epoch": 0.08352685072556015,
      "grad_norm": 7.898012638092041,
      "learning_rate": 4.8955916570503206e-05,
      "loss": 3.7099,
      "step": 473600
    },
    {
      "epoch": 0.08354448730721672,
      "grad_norm": 5.427371501922607,
      "learning_rate": 4.89556961132325e-05,
      "loss": 3.6389,
      "step": 473700
    },
    {
      "epoch": 0.08356212388887331,
      "grad_norm": 6.288825988769531,
      "learning_rate": 4.895547565596179e-05,
      "loss": 3.5202,
      "step": 473800
    },
    {
      "epoch": 0.0835797604705299,
      "grad_norm": 7.0995306968688965,
      "learning_rate": 4.8955255198691086e-05,
      "loss": 3.7442,
      "step": 473900
    },
    {
      "epoch": 0.08359739705218647,
      "grad_norm": 8.62704086303711,
      "learning_rate": 4.895503474142038e-05,
      "loss": 3.6921,
      "step": 474000
    },
    {
      "epoch": 0.08361503363384305,
      "grad_norm": 9.808592796325684,
      "learning_rate": 4.895481428414967e-05,
      "loss": 3.6397,
      "step": 474100
    },
    {
      "epoch": 0.08363267021549962,
      "grad_norm": 8.23434066772461,
      "learning_rate": 4.8954593826878965e-05,
      "loss": 3.5089,
      "step": 474200
    },
    {
      "epoch": 0.08365030679715621,
      "grad_norm": 6.981667518615723,
      "learning_rate": 4.895437336960826e-05,
      "loss": 3.628,
      "step": 474300
    },
    {
      "epoch": 0.08366794337881278,
      "grad_norm": 8.053133964538574,
      "learning_rate": 4.895415291233755e-05,
      "loss": 3.6399,
      "step": 474400
    },
    {
      "epoch": 0.08368557996046937,
      "grad_norm": 6.670660972595215,
      "learning_rate": 4.8953932455066845e-05,
      "loss": 3.7003,
      "step": 474500
    },
    {
      "epoch": 0.08370321654212595,
      "grad_norm": 7.208262920379639,
      "learning_rate": 4.8953711997796133e-05,
      "loss": 3.6343,
      "step": 474600
    },
    {
      "epoch": 0.08372085312378252,
      "grad_norm": 6.7383832931518555,
      "learning_rate": 4.895349154052543e-05,
      "loss": 3.6479,
      "step": 474700
    },
    {
      "epoch": 0.08373848970543911,
      "grad_norm": 6.939629554748535,
      "learning_rate": 4.895327108325472e-05,
      "loss": 3.691,
      "step": 474800
    },
    {
      "epoch": 0.08375612628709568,
      "grad_norm": 5.952262878417969,
      "learning_rate": 4.895305062598401e-05,
      "loss": 3.6745,
      "step": 474900
    },
    {
      "epoch": 0.08377376286875227,
      "grad_norm": 8.079789161682129,
      "learning_rate": 4.89528301687133e-05,
      "loss": 3.5885,
      "step": 475000
    },
    {
      "epoch": 0.08379139945040884,
      "grad_norm": 6.644344806671143,
      "learning_rate": 4.89526097114426e-05,
      "loss": 3.6886,
      "step": 475100
    },
    {
      "epoch": 0.08380903603206542,
      "grad_norm": 10.294059753417969,
      "learning_rate": 4.895238925417189e-05,
      "loss": 3.6444,
      "step": 475200
    },
    {
      "epoch": 0.083826672613722,
      "grad_norm": 8.925291061401367,
      "learning_rate": 4.895216879690118e-05,
      "loss": 3.5501,
      "step": 475300
    },
    {
      "epoch": 0.08384430919537858,
      "grad_norm": 10.238365173339844,
      "learning_rate": 4.895194833963048e-05,
      "loss": 3.704,
      "step": 475400
    },
    {
      "epoch": 0.08386194577703517,
      "grad_norm": 5.468502521514893,
      "learning_rate": 4.895172788235977e-05,
      "loss": 3.6851,
      "step": 475500
    },
    {
      "epoch": 0.08387958235869174,
      "grad_norm": 7.529716968536377,
      "learning_rate": 4.895150742508906e-05,
      "loss": 3.717,
      "step": 475600
    },
    {
      "epoch": 0.08389721894034832,
      "grad_norm": 6.137580394744873,
      "learning_rate": 4.8951286967818357e-05,
      "loss": 3.5871,
      "step": 475700
    },
    {
      "epoch": 0.0839148555220049,
      "grad_norm": 6.719723701477051,
      "learning_rate": 4.895106651054765e-05,
      "loss": 3.6567,
      "step": 475800
    },
    {
      "epoch": 0.08393249210366148,
      "grad_norm": 7.228451728820801,
      "learning_rate": 4.895084605327694e-05,
      "loss": 3.7254,
      "step": 475900
    },
    {
      "epoch": 0.08395012868531805,
      "grad_norm": 6.25198221206665,
      "learning_rate": 4.8950625596006236e-05,
      "loss": 3.6307,
      "step": 476000
    },
    {
      "epoch": 0.08396776526697464,
      "grad_norm": 6.005423069000244,
      "learning_rate": 4.8950405138735525e-05,
      "loss": 3.6199,
      "step": 476100
    },
    {
      "epoch": 0.08398540184863122,
      "grad_norm": 9.372342109680176,
      "learning_rate": 4.8950184681464814e-05,
      "loss": 3.6781,
      "step": 476200
    },
    {
      "epoch": 0.0840030384302878,
      "grad_norm": 5.978083610534668,
      "learning_rate": 4.894996422419411e-05,
      "loss": 3.5456,
      "step": 476300
    },
    {
      "epoch": 0.08402067501194438,
      "grad_norm": 8.914100646972656,
      "learning_rate": 4.8949743766923404e-05,
      "loss": 3.7396,
      "step": 476400
    },
    {
      "epoch": 0.08403831159360095,
      "grad_norm": 8.491915702819824,
      "learning_rate": 4.894952330965269e-05,
      "loss": 3.6614,
      "step": 476500
    },
    {
      "epoch": 0.08405594817525754,
      "grad_norm": 5.648612976074219,
      "learning_rate": 4.894930285238199e-05,
      "loss": 3.7253,
      "step": 476600
    },
    {
      "epoch": 0.08407358475691411,
      "grad_norm": 7.851385593414307,
      "learning_rate": 4.8949082395111284e-05,
      "loss": 3.7066,
      "step": 476700
    },
    {
      "epoch": 0.08409122133857069,
      "grad_norm": 7.019824028015137,
      "learning_rate": 4.894886193784057e-05,
      "loss": 3.6298,
      "step": 476800
    },
    {
      "epoch": 0.08410885792022728,
      "grad_norm": 7.988417625427246,
      "learning_rate": 4.894864148056987e-05,
      "loss": 3.7029,
      "step": 476900
    },
    {
      "epoch": 0.08412649450188385,
      "grad_norm": 6.853504657745361,
      "learning_rate": 4.8948421023299164e-05,
      "loss": 3.6565,
      "step": 477000
    },
    {
      "epoch": 0.08414413108354044,
      "grad_norm": 7.762607574462891,
      "learning_rate": 4.894820056602845e-05,
      "loss": 3.7147,
      "step": 477100
    },
    {
      "epoch": 0.08416176766519701,
      "grad_norm": 8.76075553894043,
      "learning_rate": 4.894798010875775e-05,
      "loss": 3.6581,
      "step": 477200
    },
    {
      "epoch": 0.08417940424685359,
      "grad_norm": 10.535209655761719,
      "learning_rate": 4.894775965148704e-05,
      "loss": 3.6452,
      "step": 477300
    },
    {
      "epoch": 0.08419704082851016,
      "grad_norm": 9.826120376586914,
      "learning_rate": 4.894753919421633e-05,
      "loss": 3.6756,
      "step": 477400
    },
    {
      "epoch": 0.08421467741016675,
      "grad_norm": 6.890715599060059,
      "learning_rate": 4.894731873694563e-05,
      "loss": 3.6494,
      "step": 477500
    },
    {
      "epoch": 0.08423231399182333,
      "grad_norm": 6.544672966003418,
      "learning_rate": 4.8947098279674916e-05,
      "loss": 3.7116,
      "step": 477600
    },
    {
      "epoch": 0.0842499505734799,
      "grad_norm": 6.335517406463623,
      "learning_rate": 4.8946877822404205e-05,
      "loss": 3.7773,
      "step": 477700
    },
    {
      "epoch": 0.08426758715513649,
      "grad_norm": 6.827084064483643,
      "learning_rate": 4.89466573651335e-05,
      "loss": 3.5654,
      "step": 477800
    },
    {
      "epoch": 0.08428522373679306,
      "grad_norm": 6.713465213775635,
      "learning_rate": 4.8946436907862796e-05,
      "loss": 3.7837,
      "step": 477900
    },
    {
      "epoch": 0.08430286031844965,
      "grad_norm": 6.270131587982178,
      "learning_rate": 4.8946216450592085e-05,
      "loss": 3.7239,
      "step": 478000
    },
    {
      "epoch": 0.08432049690010622,
      "grad_norm": 9.976588249206543,
      "learning_rate": 4.894599599332138e-05,
      "loss": 3.6236,
      "step": 478100
    },
    {
      "epoch": 0.0843381334817628,
      "grad_norm": 12.321471214294434,
      "learning_rate": 4.8945775536050675e-05,
      "loss": 3.5978,
      "step": 478200
    },
    {
      "epoch": 0.08435577006341938,
      "grad_norm": 5.943332195281982,
      "learning_rate": 4.8945555078779964e-05,
      "loss": 3.6646,
      "step": 478300
    },
    {
      "epoch": 0.08437340664507596,
      "grad_norm": 8.26115608215332,
      "learning_rate": 4.894533462150926e-05,
      "loss": 3.7222,
      "step": 478400
    },
    {
      "epoch": 0.08439104322673255,
      "grad_norm": 7.0668230056762695,
      "learning_rate": 4.8945114164238555e-05,
      "loss": 3.7307,
      "step": 478500
    },
    {
      "epoch": 0.08440867980838912,
      "grad_norm": 7.093237400054932,
      "learning_rate": 4.894489370696785e-05,
      "loss": 3.6523,
      "step": 478600
    },
    {
      "epoch": 0.0844263163900457,
      "grad_norm": 7.425495624542236,
      "learning_rate": 4.894467324969714e-05,
      "loss": 3.7394,
      "step": 478700
    },
    {
      "epoch": 0.08444395297170228,
      "grad_norm": 6.332239627838135,
      "learning_rate": 4.8944452792426435e-05,
      "loss": 3.6908,
      "step": 478800
    },
    {
      "epoch": 0.08446158955335886,
      "grad_norm": 5.88157320022583,
      "learning_rate": 4.8944232335155723e-05,
      "loss": 3.6565,
      "step": 478900
    },
    {
      "epoch": 0.08447922613501543,
      "grad_norm": 7.304112434387207,
      "learning_rate": 4.894401187788501e-05,
      "loss": 3.6956,
      "step": 479000
    },
    {
      "epoch": 0.08449686271667202,
      "grad_norm": 7.944610118865967,
      "learning_rate": 4.894379142061431e-05,
      "loss": 3.6469,
      "step": 479100
    },
    {
      "epoch": 0.0845144992983286,
      "grad_norm": 6.397281169891357,
      "learning_rate": 4.89435709633436e-05,
      "loss": 3.7051,
      "step": 479200
    },
    {
      "epoch": 0.08453213587998518,
      "grad_norm": 7.153572082519531,
      "learning_rate": 4.894335050607289e-05,
      "loss": 3.7044,
      "step": 479300
    },
    {
      "epoch": 0.08454977246164176,
      "grad_norm": 8.254581451416016,
      "learning_rate": 4.894313004880219e-05,
      "loss": 3.6459,
      "step": 479400
    },
    {
      "epoch": 0.08456740904329833,
      "grad_norm": 6.988736629486084,
      "learning_rate": 4.894290959153148e-05,
      "loss": 3.6956,
      "step": 479500
    },
    {
      "epoch": 0.08458504562495492,
      "grad_norm": 9.645042419433594,
      "learning_rate": 4.894268913426077e-05,
      "loss": 3.7161,
      "step": 479600
    },
    {
      "epoch": 0.08460268220661149,
      "grad_norm": 6.343526363372803,
      "learning_rate": 4.894246867699007e-05,
      "loss": 3.6021,
      "step": 479700
    },
    {
      "epoch": 0.08462031878826808,
      "grad_norm": 7.0326337814331055,
      "learning_rate": 4.894224821971936e-05,
      "loss": 3.5966,
      "step": 479800
    },
    {
      "epoch": 0.08463795536992466,
      "grad_norm": 6.2198333740234375,
      "learning_rate": 4.894202776244865e-05,
      "loss": 3.6362,
      "step": 479900
    },
    {
      "epoch": 0.08465559195158123,
      "grad_norm": 7.833825588226318,
      "learning_rate": 4.8941807305177946e-05,
      "loss": 3.5838,
      "step": 480000
    },
    {
      "epoch": 0.08467322853323782,
      "grad_norm": 5.914402484893799,
      "learning_rate": 4.894158684790724e-05,
      "loss": 3.6517,
      "step": 480100
    },
    {
      "epoch": 0.08469086511489439,
      "grad_norm": 7.734200954437256,
      "learning_rate": 4.894136639063653e-05,
      "loss": 3.7639,
      "step": 480200
    },
    {
      "epoch": 0.08470850169655098,
      "grad_norm": 7.721070766448975,
      "learning_rate": 4.8941145933365826e-05,
      "loss": 3.6463,
      "step": 480300
    },
    {
      "epoch": 0.08472613827820755,
      "grad_norm": 7.427560806274414,
      "learning_rate": 4.8940925476095115e-05,
      "loss": 3.6254,
      "step": 480400
    },
    {
      "epoch": 0.08474377485986413,
      "grad_norm": 7.371199607849121,
      "learning_rate": 4.8940705018824403e-05,
      "loss": 3.7334,
      "step": 480500
    },
    {
      "epoch": 0.08476141144152072,
      "grad_norm": 8.364480018615723,
      "learning_rate": 4.89404845615537e-05,
      "loss": 3.5806,
      "step": 480600
    },
    {
      "epoch": 0.08477904802317729,
      "grad_norm": 8.132394790649414,
      "learning_rate": 4.8940264104282994e-05,
      "loss": 3.7212,
      "step": 480700
    },
    {
      "epoch": 0.08479668460483387,
      "grad_norm": 6.732987403869629,
      "learning_rate": 4.894004364701228e-05,
      "loss": 3.6444,
      "step": 480800
    },
    {
      "epoch": 0.08481432118649045,
      "grad_norm": 5.496915817260742,
      "learning_rate": 4.893982318974158e-05,
      "loss": 3.5806,
      "step": 480900
    },
    {
      "epoch": 0.08483195776814703,
      "grad_norm": 7.022021293640137,
      "learning_rate": 4.8939602732470874e-05,
      "loss": 3.6997,
      "step": 481000
    },
    {
      "epoch": 0.0848495943498036,
      "grad_norm": 6.325896739959717,
      "learning_rate": 4.893938227520016e-05,
      "loss": 3.7048,
      "step": 481100
    },
    {
      "epoch": 0.08486723093146019,
      "grad_norm": 10.079315185546875,
      "learning_rate": 4.893916181792946e-05,
      "loss": 3.6617,
      "step": 481200
    },
    {
      "epoch": 0.08488486751311676,
      "grad_norm": 9.189437866210938,
      "learning_rate": 4.8938941360658754e-05,
      "loss": 3.6459,
      "step": 481300
    },
    {
      "epoch": 0.08490250409477335,
      "grad_norm": 9.167560577392578,
      "learning_rate": 4.893872090338804e-05,
      "loss": 3.5664,
      "step": 481400
    },
    {
      "epoch": 0.08492014067642993,
      "grad_norm": 7.101335048675537,
      "learning_rate": 4.893850044611734e-05,
      "loss": 3.6583,
      "step": 481500
    },
    {
      "epoch": 0.0849377772580865,
      "grad_norm": 9.761883735656738,
      "learning_rate": 4.893827998884663e-05,
      "loss": 3.6529,
      "step": 481600
    },
    {
      "epoch": 0.08495541383974309,
      "grad_norm": 5.944740295410156,
      "learning_rate": 4.893805953157592e-05,
      "loss": 3.6658,
      "step": 481700
    },
    {
      "epoch": 0.08497305042139966,
      "grad_norm": 7.047153472900391,
      "learning_rate": 4.893783907430521e-05,
      "loss": 3.5928,
      "step": 481800
    },
    {
      "epoch": 0.08499068700305625,
      "grad_norm": 7.959362506866455,
      "learning_rate": 4.8937618617034506e-05,
      "loss": 3.6917,
      "step": 481900
    },
    {
      "epoch": 0.08500832358471282,
      "grad_norm": 5.125370979309082,
      "learning_rate": 4.8937398159763795e-05,
      "loss": 3.7254,
      "step": 482000
    },
    {
      "epoch": 0.0850259601663694,
      "grad_norm": 8.195343017578125,
      "learning_rate": 4.893717770249309e-05,
      "loss": 3.6836,
      "step": 482100
    },
    {
      "epoch": 0.08504359674802599,
      "grad_norm": 7.740122318267822,
      "learning_rate": 4.8936957245222386e-05,
      "loss": 3.6571,
      "step": 482200
    },
    {
      "epoch": 0.08506123332968256,
      "grad_norm": 7.5704240798950195,
      "learning_rate": 4.8936736787951674e-05,
      "loss": 3.7375,
      "step": 482300
    },
    {
      "epoch": 0.08507886991133914,
      "grad_norm": 6.694551944732666,
      "learning_rate": 4.893651633068097e-05,
      "loss": 3.7312,
      "step": 482400
    },
    {
      "epoch": 0.08509650649299572,
      "grad_norm": 7.297401428222656,
      "learning_rate": 4.8936295873410265e-05,
      "loss": 3.7178,
      "step": 482500
    },
    {
      "epoch": 0.0851141430746523,
      "grad_norm": 7.364553928375244,
      "learning_rate": 4.8936075416139554e-05,
      "loss": 3.6338,
      "step": 482600
    },
    {
      "epoch": 0.08513177965630887,
      "grad_norm": 7.5623650550842285,
      "learning_rate": 4.893585495886885e-05,
      "loss": 3.6925,
      "step": 482700
    },
    {
      "epoch": 0.08514941623796546,
      "grad_norm": 7.712082862854004,
      "learning_rate": 4.8935634501598145e-05,
      "loss": 3.5271,
      "step": 482800
    },
    {
      "epoch": 0.08516705281962204,
      "grad_norm": 6.3478498458862305,
      "learning_rate": 4.8935414044327434e-05,
      "loss": 3.6296,
      "step": 482900
    },
    {
      "epoch": 0.08518468940127862,
      "grad_norm": 8.297115325927734,
      "learning_rate": 4.893519358705673e-05,
      "loss": 3.6721,
      "step": 483000
    },
    {
      "epoch": 0.0852023259829352,
      "grad_norm": 6.434418201446533,
      "learning_rate": 4.8934973129786025e-05,
      "loss": 3.5623,
      "step": 483100
    },
    {
      "epoch": 0.08521996256459177,
      "grad_norm": 8.912853240966797,
      "learning_rate": 4.893475267251531e-05,
      "loss": 3.6162,
      "step": 483200
    },
    {
      "epoch": 0.08523759914624836,
      "grad_norm": 7.992058753967285,
      "learning_rate": 4.89345322152446e-05,
      "loss": 3.6616,
      "step": 483300
    },
    {
      "epoch": 0.08525523572790493,
      "grad_norm": 8.572929382324219,
      "learning_rate": 4.89343117579739e-05,
      "loss": 3.6834,
      "step": 483400
    },
    {
      "epoch": 0.08527287230956151,
      "grad_norm": 7.034485816955566,
      "learning_rate": 4.8934091300703186e-05,
      "loss": 3.5773,
      "step": 483500
    },
    {
      "epoch": 0.0852905088912181,
      "grad_norm": 7.885636806488037,
      "learning_rate": 4.893387084343248e-05,
      "loss": 3.6889,
      "step": 483600
    },
    {
      "epoch": 0.08530814547287467,
      "grad_norm": 12.85715389251709,
      "learning_rate": 4.893365038616178e-05,
      "loss": 3.6898,
      "step": 483700
    },
    {
      "epoch": 0.08532578205453126,
      "grad_norm": 7.888199329376221,
      "learning_rate": 4.8933429928891066e-05,
      "loss": 3.516,
      "step": 483800
    },
    {
      "epoch": 0.08534341863618783,
      "grad_norm": 11.265901565551758,
      "learning_rate": 4.893320947162036e-05,
      "loss": 3.6483,
      "step": 483900
    },
    {
      "epoch": 0.08536105521784441,
      "grad_norm": 7.8756022453308105,
      "learning_rate": 4.893298901434966e-05,
      "loss": 3.6981,
      "step": 484000
    },
    {
      "epoch": 0.08537869179950099,
      "grad_norm": 8.190918922424316,
      "learning_rate": 4.8932768557078945e-05,
      "loss": 3.6632,
      "step": 484100
    },
    {
      "epoch": 0.08539632838115757,
      "grad_norm": 8.14595890045166,
      "learning_rate": 4.893254809980824e-05,
      "loss": 3.6943,
      "step": 484200
    },
    {
      "epoch": 0.08541396496281414,
      "grad_norm": 6.725299835205078,
      "learning_rate": 4.8932327642537536e-05,
      "loss": 3.6475,
      "step": 484300
    },
    {
      "epoch": 0.08543160154447073,
      "grad_norm": 8.715354919433594,
      "learning_rate": 4.8932107185266825e-05,
      "loss": 3.6037,
      "step": 484400
    },
    {
      "epoch": 0.08544923812612731,
      "grad_norm": 7.1554412841796875,
      "learning_rate": 4.893188672799612e-05,
      "loss": 3.7035,
      "step": 484500
    },
    {
      "epoch": 0.08546687470778389,
      "grad_norm": 9.9641752243042,
      "learning_rate": 4.893166627072541e-05,
      "loss": 3.6247,
      "step": 484600
    },
    {
      "epoch": 0.08548451128944047,
      "grad_norm": 6.346101760864258,
      "learning_rate": 4.8931445813454705e-05,
      "loss": 3.6648,
      "step": 484700
    },
    {
      "epoch": 0.08550214787109704,
      "grad_norm": 8.676762580871582,
      "learning_rate": 4.8931225356183993e-05,
      "loss": 3.7136,
      "step": 484800
    },
    {
      "epoch": 0.08551978445275363,
      "grad_norm": 7.3528852462768555,
      "learning_rate": 4.893100489891329e-05,
      "loss": 3.7152,
      "step": 484900
    },
    {
      "epoch": 0.0855374210344102,
      "grad_norm": 7.725150108337402,
      "learning_rate": 4.893078444164258e-05,
      "loss": 3.6969,
      "step": 485000
    },
    {
      "epoch": 0.08555505761606678,
      "grad_norm": 6.2137250900268555,
      "learning_rate": 4.893056398437187e-05,
      "loss": 3.6215,
      "step": 485100
    },
    {
      "epoch": 0.08557269419772337,
      "grad_norm": 9.569990158081055,
      "learning_rate": 4.893034352710117e-05,
      "loss": 3.704,
      "step": 485200
    },
    {
      "epoch": 0.08559033077937994,
      "grad_norm": 7.916459083557129,
      "learning_rate": 4.893012306983046e-05,
      "loss": 3.7792,
      "step": 485300
    },
    {
      "epoch": 0.08560796736103653,
      "grad_norm": 8.669862747192383,
      "learning_rate": 4.892990261255975e-05,
      "loss": 3.7032,
      "step": 485400
    },
    {
      "epoch": 0.0856256039426931,
      "grad_norm": 7.718644142150879,
      "learning_rate": 4.892968215528905e-05,
      "loss": 3.728,
      "step": 485500
    },
    {
      "epoch": 0.08564324052434968,
      "grad_norm": 9.519631385803223,
      "learning_rate": 4.892946169801834e-05,
      "loss": 3.7307,
      "step": 485600
    },
    {
      "epoch": 0.08566087710600626,
      "grad_norm": 8.82819652557373,
      "learning_rate": 4.892924124074763e-05,
      "loss": 3.6928,
      "step": 485700
    },
    {
      "epoch": 0.08567851368766284,
      "grad_norm": 6.634997367858887,
      "learning_rate": 4.892902078347693e-05,
      "loss": 3.5238,
      "step": 485800
    },
    {
      "epoch": 0.08569615026931943,
      "grad_norm": 6.463954448699951,
      "learning_rate": 4.8928800326206216e-05,
      "loss": 3.7214,
      "step": 485900
    },
    {
      "epoch": 0.085713786850976,
      "grad_norm": 5.15905237197876,
      "learning_rate": 4.892857986893551e-05,
      "loss": 3.6288,
      "step": 486000
    },
    {
      "epoch": 0.08573142343263258,
      "grad_norm": 7.081837177276611,
      "learning_rate": 4.89283594116648e-05,
      "loss": 3.612,
      "step": 486100
    },
    {
      "epoch": 0.08574906001428916,
      "grad_norm": 7.4574384689331055,
      "learning_rate": 4.892813895439409e-05,
      "loss": 3.6127,
      "step": 486200
    },
    {
      "epoch": 0.08576669659594574,
      "grad_norm": 9.186725616455078,
      "learning_rate": 4.8927918497123385e-05,
      "loss": 3.6773,
      "step": 486300
    },
    {
      "epoch": 0.08578433317760231,
      "grad_norm": 7.35721492767334,
      "learning_rate": 4.892769803985268e-05,
      "loss": 3.5665,
      "step": 486400
    },
    {
      "epoch": 0.0858019697592589,
      "grad_norm": 6.968008041381836,
      "learning_rate": 4.892747758258197e-05,
      "loss": 3.6255,
      "step": 486500
    },
    {
      "epoch": 0.08581960634091548,
      "grad_norm": 6.541365146636963,
      "learning_rate": 4.8927257125311264e-05,
      "loss": 3.6391,
      "step": 486600
    },
    {
      "epoch": 0.08583724292257205,
      "grad_norm": 8.22774600982666,
      "learning_rate": 4.892703666804056e-05,
      "loss": 3.5846,
      "step": 486700
    },
    {
      "epoch": 0.08585487950422864,
      "grad_norm": 6.234982967376709,
      "learning_rate": 4.892681621076985e-05,
      "loss": 3.6863,
      "step": 486800
    },
    {
      "epoch": 0.08587251608588521,
      "grad_norm": 9.84130573272705,
      "learning_rate": 4.8926595753499144e-05,
      "loss": 3.7467,
      "step": 486900
    },
    {
      "epoch": 0.0858901526675418,
      "grad_norm": 7.2748026847839355,
      "learning_rate": 4.892637529622844e-05,
      "loss": 3.732,
      "step": 487000
    },
    {
      "epoch": 0.08590778924919837,
      "grad_norm": 5.6305742263793945,
      "learning_rate": 4.892615483895773e-05,
      "loss": 3.5798,
      "step": 487100
    },
    {
      "epoch": 0.08592542583085495,
      "grad_norm": 7.167951583862305,
      "learning_rate": 4.8925934381687024e-05,
      "loss": 3.7264,
      "step": 487200
    },
    {
      "epoch": 0.08594306241251153,
      "grad_norm": 5.679258823394775,
      "learning_rate": 4.892571392441632e-05,
      "loss": 3.6713,
      "step": 487300
    },
    {
      "epoch": 0.08596069899416811,
      "grad_norm": 11.379833221435547,
      "learning_rate": 4.892549346714561e-05,
      "loss": 3.7746,
      "step": 487400
    },
    {
      "epoch": 0.0859783355758247,
      "grad_norm": 6.046180248260498,
      "learning_rate": 4.89252730098749e-05,
      "loss": 3.6998,
      "step": 487500
    },
    {
      "epoch": 0.08599597215748127,
      "grad_norm": 7.296496868133545,
      "learning_rate": 4.892505255260419e-05,
      "loss": 3.6494,
      "step": 487600
    },
    {
      "epoch": 0.08601360873913785,
      "grad_norm": 5.864044666290283,
      "learning_rate": 4.892483209533348e-05,
      "loss": 3.5821,
      "step": 487700
    },
    {
      "epoch": 0.08603124532079442,
      "grad_norm": 5.425014972686768,
      "learning_rate": 4.8924611638062776e-05,
      "loss": 3.7239,
      "step": 487800
    },
    {
      "epoch": 0.08604888190245101,
      "grad_norm": 7.618258953094482,
      "learning_rate": 4.892439118079207e-05,
      "loss": 3.6364,
      "step": 487900
    },
    {
      "epoch": 0.08606651848410758,
      "grad_norm": 9.100502014160156,
      "learning_rate": 4.892417072352136e-05,
      "loss": 3.6564,
      "step": 488000
    },
    {
      "epoch": 0.08608415506576417,
      "grad_norm": 6.1848978996276855,
      "learning_rate": 4.8923950266250656e-05,
      "loss": 3.7014,
      "step": 488100
    },
    {
      "epoch": 0.08610179164742075,
      "grad_norm": 7.917967319488525,
      "learning_rate": 4.892372980897995e-05,
      "loss": 3.6145,
      "step": 488200
    },
    {
      "epoch": 0.08611942822907732,
      "grad_norm": 10.031854629516602,
      "learning_rate": 4.892350935170924e-05,
      "loss": 3.7279,
      "step": 488300
    },
    {
      "epoch": 0.08613706481073391,
      "grad_norm": 7.993654251098633,
      "learning_rate": 4.8923288894438535e-05,
      "loss": 3.6699,
      "step": 488400
    },
    {
      "epoch": 0.08615470139239048,
      "grad_norm": 8.651309967041016,
      "learning_rate": 4.892306843716783e-05,
      "loss": 3.6524,
      "step": 488500
    },
    {
      "epoch": 0.08617233797404707,
      "grad_norm": 7.766360759735107,
      "learning_rate": 4.892284797989712e-05,
      "loss": 3.6511,
      "step": 488600
    },
    {
      "epoch": 0.08618997455570364,
      "grad_norm": 8.51415729522705,
      "learning_rate": 4.8922627522626415e-05,
      "loss": 3.7245,
      "step": 488700
    },
    {
      "epoch": 0.08620761113736022,
      "grad_norm": 5.708802700042725,
      "learning_rate": 4.892240706535571e-05,
      "loss": 3.7089,
      "step": 488800
    },
    {
      "epoch": 0.08622524771901681,
      "grad_norm": 7.771138668060303,
      "learning_rate": 4.8922186608085e-05,
      "loss": 3.6812,
      "step": 488900
    },
    {
      "epoch": 0.08624288430067338,
      "grad_norm": 5.4683074951171875,
      "learning_rate": 4.892196615081429e-05,
      "loss": 3.6793,
      "step": 489000
    },
    {
      "epoch": 0.08626052088232997,
      "grad_norm": 9.941940307617188,
      "learning_rate": 4.8921745693543583e-05,
      "loss": 3.7273,
      "step": 489100
    },
    {
      "epoch": 0.08627815746398654,
      "grad_norm": 4.982588768005371,
      "learning_rate": 4.892152523627288e-05,
      "loss": 3.7214,
      "step": 489200
    },
    {
      "epoch": 0.08629579404564312,
      "grad_norm": 5.740220546722412,
      "learning_rate": 4.892130477900217e-05,
      "loss": 3.6638,
      "step": 489300
    },
    {
      "epoch": 0.0863134306272997,
      "grad_norm": 8.74067211151123,
      "learning_rate": 4.892108432173146e-05,
      "loss": 3.7194,
      "step": 489400
    },
    {
      "epoch": 0.08633106720895628,
      "grad_norm": 13.523713111877441,
      "learning_rate": 4.892086386446076e-05,
      "loss": 3.7122,
      "step": 489500
    },
    {
      "epoch": 0.08634870379061287,
      "grad_norm": 8.178247451782227,
      "learning_rate": 4.892064340719005e-05,
      "loss": 3.674,
      "step": 489600
    },
    {
      "epoch": 0.08636634037226944,
      "grad_norm": 7.796494960784912,
      "learning_rate": 4.892042294991934e-05,
      "loss": 3.6695,
      "step": 489700
    },
    {
      "epoch": 0.08638397695392602,
      "grad_norm": 7.678572177886963,
      "learning_rate": 4.892020249264864e-05,
      "loss": 3.7357,
      "step": 489800
    },
    {
      "epoch": 0.0864016135355826,
      "grad_norm": 7.30335807800293,
      "learning_rate": 4.891998203537793e-05,
      "loss": 3.7035,
      "step": 489900
    },
    {
      "epoch": 0.08641925011723918,
      "grad_norm": 10.793689727783203,
      "learning_rate": 4.891976157810722e-05,
      "loss": 3.5818,
      "step": 490000
    },
    {
      "epoch": 0.08643688669889575,
      "grad_norm": 6.083104133605957,
      "learning_rate": 4.891954112083652e-05,
      "loss": 3.6577,
      "step": 490100
    },
    {
      "epoch": 0.08645452328055234,
      "grad_norm": 5.0811944007873535,
      "learning_rate": 4.8919320663565806e-05,
      "loss": 3.506,
      "step": 490200
    },
    {
      "epoch": 0.08647215986220891,
      "grad_norm": 6.275317192077637,
      "learning_rate": 4.89191002062951e-05,
      "loss": 3.6046,
      "step": 490300
    },
    {
      "epoch": 0.0864897964438655,
      "grad_norm": 7.088325500488281,
      "learning_rate": 4.891887974902439e-05,
      "loss": 3.732,
      "step": 490400
    },
    {
      "epoch": 0.08650743302552208,
      "grad_norm": 7.402443885803223,
      "learning_rate": 4.891865929175368e-05,
      "loss": 3.7077,
      "step": 490500
    },
    {
      "epoch": 0.08652506960717865,
      "grad_norm": 7.730866432189941,
      "learning_rate": 4.8918438834482975e-05,
      "loss": 3.601,
      "step": 490600
    },
    {
      "epoch": 0.08654270618883524,
      "grad_norm": 7.751278877258301,
      "learning_rate": 4.891821837721227e-05,
      "loss": 3.6848,
      "step": 490700
    },
    {
      "epoch": 0.08656034277049181,
      "grad_norm": 8.243060111999512,
      "learning_rate": 4.891799791994156e-05,
      "loss": 3.5814,
      "step": 490800
    },
    {
      "epoch": 0.0865779793521484,
      "grad_norm": 6.718184947967529,
      "learning_rate": 4.8917777462670854e-05,
      "loss": 3.7209,
      "step": 490900
    },
    {
      "epoch": 0.08659561593380496,
      "grad_norm": 6.429117679595947,
      "learning_rate": 4.891755700540015e-05,
      "loss": 3.6559,
      "step": 491000
    },
    {
      "epoch": 0.08661325251546155,
      "grad_norm": 8.762999534606934,
      "learning_rate": 4.891733654812944e-05,
      "loss": 3.6246,
      "step": 491100
    },
    {
      "epoch": 0.08663088909711814,
      "grad_norm": 5.701550006866455,
      "learning_rate": 4.8917116090858734e-05,
      "loss": 3.6087,
      "step": 491200
    },
    {
      "epoch": 0.08664852567877471,
      "grad_norm": 8.028752326965332,
      "learning_rate": 4.891689563358803e-05,
      "loss": 3.6725,
      "step": 491300
    },
    {
      "epoch": 0.08666616226043129,
      "grad_norm": 7.568939208984375,
      "learning_rate": 4.891667517631732e-05,
      "loss": 3.7259,
      "step": 491400
    },
    {
      "epoch": 0.08668379884208786,
      "grad_norm": 6.047295570373535,
      "learning_rate": 4.8916454719046614e-05,
      "loss": 3.658,
      "step": 491500
    },
    {
      "epoch": 0.08670143542374445,
      "grad_norm": 6.6995368003845215,
      "learning_rate": 4.891623426177591e-05,
      "loss": 3.7261,
      "step": 491600
    },
    {
      "epoch": 0.08671907200540102,
      "grad_norm": 4.952323913574219,
      "learning_rate": 4.89160138045052e-05,
      "loss": 3.6066,
      "step": 491700
    },
    {
      "epoch": 0.0867367085870576,
      "grad_norm": 6.24501371383667,
      "learning_rate": 4.8915793347234486e-05,
      "loss": 3.6163,
      "step": 491800
    },
    {
      "epoch": 0.08675434516871419,
      "grad_norm": 5.823853492736816,
      "learning_rate": 4.891557288996378e-05,
      "loss": 3.5987,
      "step": 491900
    },
    {
      "epoch": 0.08677198175037076,
      "grad_norm": 6.900102138519287,
      "learning_rate": 4.891535243269307e-05,
      "loss": 3.7232,
      "step": 492000
    },
    {
      "epoch": 0.08678961833202735,
      "grad_norm": 8.56863784790039,
      "learning_rate": 4.8915131975422366e-05,
      "loss": 3.6869,
      "step": 492100
    },
    {
      "epoch": 0.08680725491368392,
      "grad_norm": 6.934925556182861,
      "learning_rate": 4.891491151815166e-05,
      "loss": 3.7361,
      "step": 492200
    },
    {
      "epoch": 0.0868248914953405,
      "grad_norm": 9.452534675598145,
      "learning_rate": 4.891469106088095e-05,
      "loss": 3.6574,
      "step": 492300
    },
    {
      "epoch": 0.08684252807699708,
      "grad_norm": 5.874214172363281,
      "learning_rate": 4.8914470603610246e-05,
      "loss": 3.5958,
      "step": 492400
    },
    {
      "epoch": 0.08686016465865366,
      "grad_norm": 5.889308452606201,
      "learning_rate": 4.891425014633954e-05,
      "loss": 3.6845,
      "step": 492500
    },
    {
      "epoch": 0.08687780124031025,
      "grad_norm": 6.591894149780273,
      "learning_rate": 4.891402968906883e-05,
      "loss": 3.6704,
      "step": 492600
    },
    {
      "epoch": 0.08689543782196682,
      "grad_norm": 6.2644572257995605,
      "learning_rate": 4.8913809231798125e-05,
      "loss": 3.6477,
      "step": 492700
    },
    {
      "epoch": 0.0869130744036234,
      "grad_norm": 6.27340030670166,
      "learning_rate": 4.891358877452742e-05,
      "loss": 3.7002,
      "step": 492800
    },
    {
      "epoch": 0.08693071098527998,
      "grad_norm": 9.904786109924316,
      "learning_rate": 4.891336831725671e-05,
      "loss": 3.6219,
      "step": 492900
    },
    {
      "epoch": 0.08694834756693656,
      "grad_norm": 6.757877349853516,
      "learning_rate": 4.8913147859986005e-05,
      "loss": 3.7334,
      "step": 493000
    },
    {
      "epoch": 0.08696598414859313,
      "grad_norm": 7.006488800048828,
      "learning_rate": 4.89129274027153e-05,
      "loss": 3.6452,
      "step": 493100
    },
    {
      "epoch": 0.08698362073024972,
      "grad_norm": 7.719677925109863,
      "learning_rate": 4.891270694544459e-05,
      "loss": 3.4773,
      "step": 493200
    },
    {
      "epoch": 0.08700125731190629,
      "grad_norm": 9.882872581481934,
      "learning_rate": 4.891248648817388e-05,
      "loss": 3.5575,
      "step": 493300
    },
    {
      "epoch": 0.08701889389356288,
      "grad_norm": 5.61948823928833,
      "learning_rate": 4.891226603090317e-05,
      "loss": 3.6875,
      "step": 493400
    },
    {
      "epoch": 0.08703653047521946,
      "grad_norm": 7.983721733093262,
      "learning_rate": 4.891204557363246e-05,
      "loss": 3.5966,
      "step": 493500
    },
    {
      "epoch": 0.08705416705687603,
      "grad_norm": 8.697920799255371,
      "learning_rate": 4.891182511636176e-05,
      "loss": 3.6308,
      "step": 493600
    },
    {
      "epoch": 0.08707180363853262,
      "grad_norm": 8.140104293823242,
      "learning_rate": 4.891160465909105e-05,
      "loss": 3.6244,
      "step": 493700
    },
    {
      "epoch": 0.08708944022018919,
      "grad_norm": 7.971254825592041,
      "learning_rate": 4.891138420182034e-05,
      "loss": 3.6169,
      "step": 493800
    },
    {
      "epoch": 0.08710707680184578,
      "grad_norm": 6.033226013183594,
      "learning_rate": 4.891116374454964e-05,
      "loss": 3.645,
      "step": 493900
    },
    {
      "epoch": 0.08712471338350235,
      "grad_norm": 8.016345977783203,
      "learning_rate": 4.891094328727893e-05,
      "loss": 3.6514,
      "step": 494000
    },
    {
      "epoch": 0.08714234996515893,
      "grad_norm": 12.857574462890625,
      "learning_rate": 4.891072283000822e-05,
      "loss": 3.6838,
      "step": 494100
    },
    {
      "epoch": 0.08715998654681552,
      "grad_norm": 7.530515670776367,
      "learning_rate": 4.891050237273752e-05,
      "loss": 3.6483,
      "step": 494200
    },
    {
      "epoch": 0.08717762312847209,
      "grad_norm": 9.606423377990723,
      "learning_rate": 4.891028191546681e-05,
      "loss": 3.7147,
      "step": 494300
    },
    {
      "epoch": 0.08719525971012868,
      "grad_norm": 8.927570343017578,
      "learning_rate": 4.89100614581961e-05,
      "loss": 3.5986,
      "step": 494400
    },
    {
      "epoch": 0.08721289629178525,
      "grad_norm": 6.775692939758301,
      "learning_rate": 4.8909841000925396e-05,
      "loss": 3.7866,
      "step": 494500
    },
    {
      "epoch": 0.08723053287344183,
      "grad_norm": 10.282299995422363,
      "learning_rate": 4.8909620543654685e-05,
      "loss": 3.6622,
      "step": 494600
    },
    {
      "epoch": 0.0872481694550984,
      "grad_norm": 7.855335235595703,
      "learning_rate": 4.890940008638398e-05,
      "loss": 3.7165,
      "step": 494700
    },
    {
      "epoch": 0.08726580603675499,
      "grad_norm": 9.204992294311523,
      "learning_rate": 4.890917962911327e-05,
      "loss": 3.6841,
      "step": 494800
    },
    {
      "epoch": 0.08728344261841157,
      "grad_norm": 6.826737403869629,
      "learning_rate": 4.8908959171842565e-05,
      "loss": 3.6471,
      "step": 494900
    },
    {
      "epoch": 0.08730107920006815,
      "grad_norm": 10.655637741088867,
      "learning_rate": 4.8908738714571853e-05,
      "loss": 3.604,
      "step": 495000
    },
    {
      "epoch": 0.08731871578172473,
      "grad_norm": 8.560476303100586,
      "learning_rate": 4.890851825730115e-05,
      "loss": 3.661,
      "step": 495100
    },
    {
      "epoch": 0.0873363523633813,
      "grad_norm": 6.5065016746521,
      "learning_rate": 4.8908297800030444e-05,
      "loss": 3.6559,
      "step": 495200
    },
    {
      "epoch": 0.08735398894503789,
      "grad_norm": 6.8158369064331055,
      "learning_rate": 4.890807734275973e-05,
      "loss": 3.5573,
      "step": 495300
    },
    {
      "epoch": 0.08737162552669446,
      "grad_norm": 7.332538604736328,
      "learning_rate": 4.890785688548903e-05,
      "loss": 3.7032,
      "step": 495400
    },
    {
      "epoch": 0.08738926210835105,
      "grad_norm": 8.941841125488281,
      "learning_rate": 4.8907636428218324e-05,
      "loss": 3.6851,
      "step": 495500
    },
    {
      "epoch": 0.08740689869000763,
      "grad_norm": 7.290349960327148,
      "learning_rate": 4.890741597094761e-05,
      "loss": 3.7284,
      "step": 495600
    },
    {
      "epoch": 0.0874245352716642,
      "grad_norm": 7.975487232208252,
      "learning_rate": 4.890719551367691e-05,
      "loss": 3.5317,
      "step": 495700
    },
    {
      "epoch": 0.08744217185332079,
      "grad_norm": 7.917252063751221,
      "learning_rate": 4.8906975056406204e-05,
      "loss": 3.6209,
      "step": 495800
    },
    {
      "epoch": 0.08745980843497736,
      "grad_norm": 7.269322872161865,
      "learning_rate": 4.890675459913549e-05,
      "loss": 3.5128,
      "step": 495900
    },
    {
      "epoch": 0.08747744501663395,
      "grad_norm": 8.60631275177002,
      "learning_rate": 4.890653414186479e-05,
      "loss": 3.7415,
      "step": 496000
    },
    {
      "epoch": 0.08749508159829052,
      "grad_norm": 8.592804908752441,
      "learning_rate": 4.8906313684594076e-05,
      "loss": 3.6743,
      "step": 496100
    },
    {
      "epoch": 0.0875127181799471,
      "grad_norm": 11.02115249633789,
      "learning_rate": 4.8906093227323365e-05,
      "loss": 3.7077,
      "step": 496200
    },
    {
      "epoch": 0.08753035476160367,
      "grad_norm": 5.987529277801514,
      "learning_rate": 4.890587277005266e-05,
      "loss": 3.5301,
      "step": 496300
    },
    {
      "epoch": 0.08754799134326026,
      "grad_norm": 8.95107650756836,
      "learning_rate": 4.8905652312781956e-05,
      "loss": 3.6607,
      "step": 496400
    },
    {
      "epoch": 0.08756562792491684,
      "grad_norm": 6.52390718460083,
      "learning_rate": 4.8905431855511245e-05,
      "loss": 3.7322,
      "step": 496500
    },
    {
      "epoch": 0.08758326450657342,
      "grad_norm": 7.6787028312683105,
      "learning_rate": 4.890521139824054e-05,
      "loss": 3.7022,
      "step": 496600
    },
    {
      "epoch": 0.08760090108823,
      "grad_norm": 7.210498809814453,
      "learning_rate": 4.8904990940969836e-05,
      "loss": 3.6946,
      "step": 496700
    },
    {
      "epoch": 0.08761853766988657,
      "grad_norm": 8.179675102233887,
      "learning_rate": 4.8904770483699124e-05,
      "loss": 3.7062,
      "step": 496800
    },
    {
      "epoch": 0.08763617425154316,
      "grad_norm": 8.150053977966309,
      "learning_rate": 4.890455002642842e-05,
      "loss": 3.7166,
      "step": 496900
    },
    {
      "epoch": 0.08765381083319973,
      "grad_norm": 9.902524948120117,
      "learning_rate": 4.8904329569157715e-05,
      "loss": 3.629,
      "step": 497000
    },
    {
      "epoch": 0.08767144741485632,
      "grad_norm": 7.476686477661133,
      "learning_rate": 4.8904109111887004e-05,
      "loss": 3.6665,
      "step": 497100
    },
    {
      "epoch": 0.0876890839965129,
      "grad_norm": 9.102267265319824,
      "learning_rate": 4.89038886546163e-05,
      "loss": 3.6616,
      "step": 497200
    },
    {
      "epoch": 0.08770672057816947,
      "grad_norm": 6.137914180755615,
      "learning_rate": 4.8903668197345595e-05,
      "loss": 3.7244,
      "step": 497300
    },
    {
      "epoch": 0.08772435715982606,
      "grad_norm": 7.815012454986572,
      "learning_rate": 4.8903447740074884e-05,
      "loss": 3.7979,
      "step": 497400
    },
    {
      "epoch": 0.08774199374148263,
      "grad_norm": 6.938480854034424,
      "learning_rate": 4.890322728280418e-05,
      "loss": 3.7426,
      "step": 497500
    },
    {
      "epoch": 0.08775963032313921,
      "grad_norm": 7.808218479156494,
      "learning_rate": 4.890300682553347e-05,
      "loss": 3.5994,
      "step": 497600
    },
    {
      "epoch": 0.08777726690479579,
      "grad_norm": 7.6696929931640625,
      "learning_rate": 4.8902786368262757e-05,
      "loss": 3.6635,
      "step": 497700
    },
    {
      "epoch": 0.08779490348645237,
      "grad_norm": 9.528926849365234,
      "learning_rate": 4.890256591099205e-05,
      "loss": 3.7273,
      "step": 497800
    },
    {
      "epoch": 0.08781254006810896,
      "grad_norm": 10.245675086975098,
      "learning_rate": 4.890234545372135e-05,
      "loss": 3.5538,
      "step": 497900
    },
    {
      "epoch": 0.08783017664976553,
      "grad_norm": 6.908057689666748,
      "learning_rate": 4.8902124996450636e-05,
      "loss": 3.6962,
      "step": 498000
    },
    {
      "epoch": 0.08784781323142211,
      "grad_norm": 6.8828840255737305,
      "learning_rate": 4.890190453917993e-05,
      "loss": 3.6256,
      "step": 498100
    },
    {
      "epoch": 0.08786544981307869,
      "grad_norm": 7.613204479217529,
      "learning_rate": 4.890168408190923e-05,
      "loss": 3.6982,
      "step": 498200
    },
    {
      "epoch": 0.08788308639473527,
      "grad_norm": 8.448413848876953,
      "learning_rate": 4.8901463624638516e-05,
      "loss": 3.6092,
      "step": 498300
    },
    {
      "epoch": 0.08790072297639184,
      "grad_norm": 7.863911151885986,
      "learning_rate": 4.890124316736781e-05,
      "loss": 3.768,
      "step": 498400
    },
    {
      "epoch": 0.08791835955804843,
      "grad_norm": 7.085865497589111,
      "learning_rate": 4.890102271009711e-05,
      "loss": 3.6006,
      "step": 498500
    },
    {
      "epoch": 0.08793599613970501,
      "grad_norm": 5.945478916168213,
      "learning_rate": 4.8900802252826395e-05,
      "loss": 3.7802,
      "step": 498600
    },
    {
      "epoch": 0.08795363272136159,
      "grad_norm": 6.212829113006592,
      "learning_rate": 4.890058179555569e-05,
      "loss": 3.6921,
      "step": 498700
    },
    {
      "epoch": 0.08797126930301817,
      "grad_norm": 14.017468452453613,
      "learning_rate": 4.8900361338284986e-05,
      "loss": 3.6466,
      "step": 498800
    },
    {
      "epoch": 0.08798890588467474,
      "grad_norm": 6.482560634613037,
      "learning_rate": 4.8900140881014275e-05,
      "loss": 3.6945,
      "step": 498900
    },
    {
      "epoch": 0.08800654246633133,
      "grad_norm": 7.521191596984863,
      "learning_rate": 4.8899920423743564e-05,
      "loss": 3.6617,
      "step": 499000
    },
    {
      "epoch": 0.0880241790479879,
      "grad_norm": 6.292587757110596,
      "learning_rate": 4.889969996647286e-05,
      "loss": 3.6822,
      "step": 499100
    },
    {
      "epoch": 0.08804181562964448,
      "grad_norm": 5.992237091064453,
      "learning_rate": 4.889947950920215e-05,
      "loss": 3.7005,
      "step": 499200
    },
    {
      "epoch": 0.08805945221130106,
      "grad_norm": 7.66613245010376,
      "learning_rate": 4.889925905193144e-05,
      "loss": 3.6452,
      "step": 499300
    },
    {
      "epoch": 0.08807708879295764,
      "grad_norm": 7.261622905731201,
      "learning_rate": 4.889903859466074e-05,
      "loss": 3.7055,
      "step": 499400
    },
    {
      "epoch": 0.08809472537461423,
      "grad_norm": 8.742507934570312,
      "learning_rate": 4.889881813739003e-05,
      "loss": 3.7484,
      "step": 499500
    },
    {
      "epoch": 0.0881123619562708,
      "grad_norm": 7.3442463874816895,
      "learning_rate": 4.889859768011932e-05,
      "loss": 3.6472,
      "step": 499600
    },
    {
      "epoch": 0.08812999853792738,
      "grad_norm": 9.139487266540527,
      "learning_rate": 4.889837722284862e-05,
      "loss": 3.5922,
      "step": 499700
    },
    {
      "epoch": 0.08814763511958396,
      "grad_norm": 6.835443496704102,
      "learning_rate": 4.889815676557791e-05,
      "loss": 3.6448,
      "step": 499800
    },
    {
      "epoch": 0.08816527170124054,
      "grad_norm": 7.399927139282227,
      "learning_rate": 4.88979363083072e-05,
      "loss": 3.678,
      "step": 499900
    },
    {
      "epoch": 0.08818290828289711,
      "grad_norm": 6.121204853057861,
      "learning_rate": 4.88977158510365e-05,
      "loss": 3.6636,
      "step": 500000
    },
    {
      "epoch": 0.0882005448645537,
      "grad_norm": 7.082039833068848,
      "learning_rate": 4.8897495393765794e-05,
      "loss": 3.7026,
      "step": 500100
    },
    {
      "epoch": 0.08821818144621028,
      "grad_norm": 7.806885242462158,
      "learning_rate": 4.889727493649508e-05,
      "loss": 3.564,
      "step": 500200
    },
    {
      "epoch": 0.08823581802786686,
      "grad_norm": 6.244448184967041,
      "learning_rate": 4.889705447922438e-05,
      "loss": 3.7047,
      "step": 500300
    },
    {
      "epoch": 0.08825345460952344,
      "grad_norm": 8.485760688781738,
      "learning_rate": 4.8896834021953666e-05,
      "loss": 3.6608,
      "step": 500400
    },
    {
      "epoch": 0.08827109119118001,
      "grad_norm": 6.842263698577881,
      "learning_rate": 4.8896613564682955e-05,
      "loss": 3.6496,
      "step": 500500
    },
    {
      "epoch": 0.0882887277728366,
      "grad_norm": 9.42734146118164,
      "learning_rate": 4.889639310741225e-05,
      "loss": 3.5662,
      "step": 500600
    },
    {
      "epoch": 0.08830636435449317,
      "grad_norm": 6.497467517852783,
      "learning_rate": 4.8896172650141546e-05,
      "loss": 3.6469,
      "step": 500700
    },
    {
      "epoch": 0.08832400093614975,
      "grad_norm": 6.011270999908447,
      "learning_rate": 4.8895952192870835e-05,
      "loss": 3.6788,
      "step": 500800
    },
    {
      "epoch": 0.08834163751780634,
      "grad_norm": 8.136298179626465,
      "learning_rate": 4.889573173560013e-05,
      "loss": 3.6853,
      "step": 500900
    },
    {
      "epoch": 0.08835927409946291,
      "grad_norm": 5.0606536865234375,
      "learning_rate": 4.8895511278329426e-05,
      "loss": 3.6118,
      "step": 501000
    },
    {
      "epoch": 0.0883769106811195,
      "grad_norm": 7.976597309112549,
      "learning_rate": 4.8895290821058714e-05,
      "loss": 3.6576,
      "step": 501100
    },
    {
      "epoch": 0.08839454726277607,
      "grad_norm": 6.250396728515625,
      "learning_rate": 4.889507036378801e-05,
      "loss": 3.607,
      "step": 501200
    },
    {
      "epoch": 0.08841218384443265,
      "grad_norm": 7.380758285522461,
      "learning_rate": 4.8894849906517305e-05,
      "loss": 3.6395,
      "step": 501300
    },
    {
      "epoch": 0.08842982042608923,
      "grad_norm": 8.333414077758789,
      "learning_rate": 4.8894629449246594e-05,
      "loss": 3.5888,
      "step": 501400
    },
    {
      "epoch": 0.08844745700774581,
      "grad_norm": 6.899734020233154,
      "learning_rate": 4.889440899197589e-05,
      "loss": 3.6281,
      "step": 501500
    },
    {
      "epoch": 0.0884650935894024,
      "grad_norm": 7.490264892578125,
      "learning_rate": 4.8894188534705185e-05,
      "loss": 3.6057,
      "step": 501600
    },
    {
      "epoch": 0.08848273017105897,
      "grad_norm": 6.98266077041626,
      "learning_rate": 4.8893968077434474e-05,
      "loss": 3.6227,
      "step": 501700
    },
    {
      "epoch": 0.08850036675271555,
      "grad_norm": 6.567863464355469,
      "learning_rate": 4.889374762016376e-05,
      "loss": 3.704,
      "step": 501800
    },
    {
      "epoch": 0.08851800333437213,
      "grad_norm": 6.811932563781738,
      "learning_rate": 4.889352716289306e-05,
      "loss": 3.6068,
      "step": 501900
    },
    {
      "epoch": 0.08853563991602871,
      "grad_norm": 5.739504337310791,
      "learning_rate": 4.8893306705622346e-05,
      "loss": 3.6005,
      "step": 502000
    },
    {
      "epoch": 0.08855327649768528,
      "grad_norm": 9.590052604675293,
      "learning_rate": 4.889308624835164e-05,
      "loss": 3.7058,
      "step": 502100
    },
    {
      "epoch": 0.08857091307934187,
      "grad_norm": 6.213346004486084,
      "learning_rate": 4.889286579108094e-05,
      "loss": 3.6884,
      "step": 502200
    },
    {
      "epoch": 0.08858854966099844,
      "grad_norm": 10.291401863098145,
      "learning_rate": 4.8892645333810226e-05,
      "loss": 3.7239,
      "step": 502300
    },
    {
      "epoch": 0.08860618624265502,
      "grad_norm": 7.456267356872559,
      "learning_rate": 4.889242487653952e-05,
      "loss": 3.5874,
      "step": 502400
    },
    {
      "epoch": 0.08862382282431161,
      "grad_norm": 6.432156085968018,
      "learning_rate": 4.889220441926882e-05,
      "loss": 3.7432,
      "step": 502500
    },
    {
      "epoch": 0.08864145940596818,
      "grad_norm": 6.895060062408447,
      "learning_rate": 4.8891983961998106e-05,
      "loss": 3.6492,
      "step": 502600
    },
    {
      "epoch": 0.08865909598762477,
      "grad_norm": 5.826572418212891,
      "learning_rate": 4.88917635047274e-05,
      "loss": 3.6665,
      "step": 502700
    },
    {
      "epoch": 0.08867673256928134,
      "grad_norm": 6.894649505615234,
      "learning_rate": 4.88915430474567e-05,
      "loss": 3.7048,
      "step": 502800
    },
    {
      "epoch": 0.08869436915093792,
      "grad_norm": 5.977736473083496,
      "learning_rate": 4.8891322590185985e-05,
      "loss": 3.7074,
      "step": 502900
    },
    {
      "epoch": 0.0887120057325945,
      "grad_norm": 5.90456485748291,
      "learning_rate": 4.889110213291528e-05,
      "loss": 3.6556,
      "step": 503000
    },
    {
      "epoch": 0.08872964231425108,
      "grad_norm": 9.133271217346191,
      "learning_rate": 4.8890881675644576e-05,
      "loss": 3.6911,
      "step": 503100
    },
    {
      "epoch": 0.08874727889590767,
      "grad_norm": 6.278512001037598,
      "learning_rate": 4.8890661218373865e-05,
      "loss": 3.4983,
      "step": 503200
    },
    {
      "epoch": 0.08876491547756424,
      "grad_norm": 7.316333293914795,
      "learning_rate": 4.8890440761103154e-05,
      "loss": 3.6453,
      "step": 503300
    },
    {
      "epoch": 0.08878255205922082,
      "grad_norm": 8.474210739135742,
      "learning_rate": 4.889022030383245e-05,
      "loss": 3.5884,
      "step": 503400
    },
    {
      "epoch": 0.0888001886408774,
      "grad_norm": 6.830507755279541,
      "learning_rate": 4.888999984656174e-05,
      "loss": 3.708,
      "step": 503500
    },
    {
      "epoch": 0.08881782522253398,
      "grad_norm": 7.495742321014404,
      "learning_rate": 4.888977938929103e-05,
      "loss": 3.5724,
      "step": 503600
    },
    {
      "epoch": 0.08883546180419055,
      "grad_norm": 7.065596580505371,
      "learning_rate": 4.888955893202033e-05,
      "loss": 3.6923,
      "step": 503700
    },
    {
      "epoch": 0.08885309838584714,
      "grad_norm": 9.35012149810791,
      "learning_rate": 4.888933847474962e-05,
      "loss": 3.6031,
      "step": 503800
    },
    {
      "epoch": 0.08887073496750372,
      "grad_norm": 9.919543266296387,
      "learning_rate": 4.888911801747891e-05,
      "loss": 3.7753,
      "step": 503900
    },
    {
      "epoch": 0.0888883715491603,
      "grad_norm": 7.942517280578613,
      "learning_rate": 4.888889756020821e-05,
      "loss": 3.7618,
      "step": 504000
    },
    {
      "epoch": 0.08890600813081688,
      "grad_norm": 6.929384231567383,
      "learning_rate": 4.88886771029375e-05,
      "loss": 3.607,
      "step": 504100
    },
    {
      "epoch": 0.08892364471247345,
      "grad_norm": 7.796643257141113,
      "learning_rate": 4.888845664566679e-05,
      "loss": 3.681,
      "step": 504200
    },
    {
      "epoch": 0.08894128129413004,
      "grad_norm": 6.75816011428833,
      "learning_rate": 4.888823618839609e-05,
      "loss": 3.6622,
      "step": 504300
    },
    {
      "epoch": 0.08895891787578661,
      "grad_norm": 6.823820114135742,
      "learning_rate": 4.888801573112538e-05,
      "loss": 3.5811,
      "step": 504400
    },
    {
      "epoch": 0.0889765544574432,
      "grad_norm": 7.602556228637695,
      "learning_rate": 4.888779527385467e-05,
      "loss": 3.7667,
      "step": 504500
    },
    {
      "epoch": 0.08899419103909978,
      "grad_norm": 7.72380256652832,
      "learning_rate": 4.888757481658396e-05,
      "loss": 3.6819,
      "step": 504600
    },
    {
      "epoch": 0.08901182762075635,
      "grad_norm": 9.656909942626953,
      "learning_rate": 4.8887354359313256e-05,
      "loss": 3.6525,
      "step": 504700
    },
    {
      "epoch": 0.08902946420241294,
      "grad_norm": 6.551296710968018,
      "learning_rate": 4.8887133902042545e-05,
      "loss": 3.7802,
      "step": 504800
    },
    {
      "epoch": 0.08904710078406951,
      "grad_norm": 6.200597286224365,
      "learning_rate": 4.888691344477184e-05,
      "loss": 3.6589,
      "step": 504900
    },
    {
      "epoch": 0.0890647373657261,
      "grad_norm": 9.895519256591797,
      "learning_rate": 4.888669298750113e-05,
      "loss": 3.599,
      "step": 505000
    },
    {
      "epoch": 0.08908237394738266,
      "grad_norm": 6.884696960449219,
      "learning_rate": 4.8886472530230425e-05,
      "loss": 3.7074,
      "step": 505100
    },
    {
      "epoch": 0.08910001052903925,
      "grad_norm": 6.747698783874512,
      "learning_rate": 4.888625207295972e-05,
      "loss": 3.6867,
      "step": 505200
    },
    {
      "epoch": 0.08911764711069582,
      "grad_norm": 6.473609924316406,
      "learning_rate": 4.888603161568901e-05,
      "loss": 3.6376,
      "step": 505300
    },
    {
      "epoch": 0.08913528369235241,
      "grad_norm": 6.676042079925537,
      "learning_rate": 4.8885811158418304e-05,
      "loss": 3.7004,
      "step": 505400
    },
    {
      "epoch": 0.08915292027400899,
      "grad_norm": 7.805007457733154,
      "learning_rate": 4.88855907011476e-05,
      "loss": 3.6267,
      "step": 505500
    },
    {
      "epoch": 0.08917055685566556,
      "grad_norm": 8.30984878540039,
      "learning_rate": 4.888537024387689e-05,
      "loss": 3.6453,
      "step": 505600
    },
    {
      "epoch": 0.08918819343732215,
      "grad_norm": 6.2087202072143555,
      "learning_rate": 4.8885149786606184e-05,
      "loss": 3.7242,
      "step": 505700
    },
    {
      "epoch": 0.08920583001897872,
      "grad_norm": 8.193327903747559,
      "learning_rate": 4.888492932933548e-05,
      "loss": 3.7675,
      "step": 505800
    },
    {
      "epoch": 0.0892234666006353,
      "grad_norm": 5.737929344177246,
      "learning_rate": 4.888470887206477e-05,
      "loss": 3.7775,
      "step": 505900
    },
    {
      "epoch": 0.08924110318229188,
      "grad_norm": 6.629014492034912,
      "learning_rate": 4.8884488414794064e-05,
      "loss": 3.6906,
      "step": 506000
    },
    {
      "epoch": 0.08925873976394846,
      "grad_norm": 7.612486839294434,
      "learning_rate": 4.888426795752335e-05,
      "loss": 3.5752,
      "step": 506100
    },
    {
      "epoch": 0.08927637634560505,
      "grad_norm": 6.078166961669922,
      "learning_rate": 4.888404750025265e-05,
      "loss": 3.6284,
      "step": 506200
    },
    {
      "epoch": 0.08929401292726162,
      "grad_norm": 7.682644367218018,
      "learning_rate": 4.8883827042981936e-05,
      "loss": 3.7083,
      "step": 506300
    },
    {
      "epoch": 0.0893116495089182,
      "grad_norm": 10.607932090759277,
      "learning_rate": 4.888360658571123e-05,
      "loss": 3.7269,
      "step": 506400
    },
    {
      "epoch": 0.08932928609057478,
      "grad_norm": 8.398478507995605,
      "learning_rate": 4.888338612844052e-05,
      "loss": 3.6211,
      "step": 506500
    },
    {
      "epoch": 0.08934692267223136,
      "grad_norm": 8.325681686401367,
      "learning_rate": 4.8883165671169816e-05,
      "loss": 3.7547,
      "step": 506600
    },
    {
      "epoch": 0.08936455925388793,
      "grad_norm": 12.014626502990723,
      "learning_rate": 4.888294521389911e-05,
      "loss": 3.7487,
      "step": 506700
    },
    {
      "epoch": 0.08938219583554452,
      "grad_norm": 6.499057769775391,
      "learning_rate": 4.88827247566284e-05,
      "loss": 3.6639,
      "step": 506800
    },
    {
      "epoch": 0.0893998324172011,
      "grad_norm": 9.18985652923584,
      "learning_rate": 4.8882504299357696e-05,
      "loss": 3.5738,
      "step": 506900
    },
    {
      "epoch": 0.08941746899885768,
      "grad_norm": 7.429098129272461,
      "learning_rate": 4.888228384208699e-05,
      "loss": 3.6724,
      "step": 507000
    },
    {
      "epoch": 0.08943510558051426,
      "grad_norm": 6.650230884552002,
      "learning_rate": 4.888206338481628e-05,
      "loss": 3.5479,
      "step": 507100
    },
    {
      "epoch": 0.08945274216217083,
      "grad_norm": 7.62190580368042,
      "learning_rate": 4.8881842927545575e-05,
      "loss": 3.8219,
      "step": 507200
    },
    {
      "epoch": 0.08947037874382742,
      "grad_norm": 6.314723491668701,
      "learning_rate": 4.888162247027487e-05,
      "loss": 3.7477,
      "step": 507300
    },
    {
      "epoch": 0.08948801532548399,
      "grad_norm": 6.514613151550293,
      "learning_rate": 4.888140201300416e-05,
      "loss": 3.5903,
      "step": 507400
    },
    {
      "epoch": 0.08950565190714058,
      "grad_norm": 8.727090835571289,
      "learning_rate": 4.8881181555733455e-05,
      "loss": 3.7267,
      "step": 507500
    },
    {
      "epoch": 0.08952328848879716,
      "grad_norm": 7.204858779907227,
      "learning_rate": 4.8880961098462744e-05,
      "loss": 3.6976,
      "step": 507600
    },
    {
      "epoch": 0.08954092507045373,
      "grad_norm": 6.327807426452637,
      "learning_rate": 4.888074064119203e-05,
      "loss": 3.5918,
      "step": 507700
    },
    {
      "epoch": 0.08955856165211032,
      "grad_norm": 6.171817302703857,
      "learning_rate": 4.888052018392133e-05,
      "loss": 3.6371,
      "step": 507800
    },
    {
      "epoch": 0.08957619823376689,
      "grad_norm": 6.054306983947754,
      "learning_rate": 4.888029972665062e-05,
      "loss": 3.6165,
      "step": 507900
    },
    {
      "epoch": 0.08959383481542348,
      "grad_norm": 6.306343078613281,
      "learning_rate": 4.888007926937991e-05,
      "loss": 3.6781,
      "step": 508000
    },
    {
      "epoch": 0.08961147139708005,
      "grad_norm": 7.03799295425415,
      "learning_rate": 4.887985881210921e-05,
      "loss": 3.7103,
      "step": 508100
    },
    {
      "epoch": 0.08962910797873663,
      "grad_norm": 8.391371726989746,
      "learning_rate": 4.88796383548385e-05,
      "loss": 3.7261,
      "step": 508200
    },
    {
      "epoch": 0.0896467445603932,
      "grad_norm": 6.424561023712158,
      "learning_rate": 4.887941789756779e-05,
      "loss": 3.5778,
      "step": 508300
    },
    {
      "epoch": 0.08966438114204979,
      "grad_norm": 7.554312229156494,
      "learning_rate": 4.887919744029709e-05,
      "loss": 3.672,
      "step": 508400
    },
    {
      "epoch": 0.08968201772370638,
      "grad_norm": 7.638091087341309,
      "learning_rate": 4.887897698302638e-05,
      "loss": 3.7002,
      "step": 508500
    },
    {
      "epoch": 0.08969965430536295,
      "grad_norm": 9.20947265625,
      "learning_rate": 4.887875652575567e-05,
      "loss": 3.6213,
      "step": 508600
    },
    {
      "epoch": 0.08971729088701953,
      "grad_norm": 7.032132148742676,
      "learning_rate": 4.887853606848497e-05,
      "loss": 3.581,
      "step": 508700
    },
    {
      "epoch": 0.0897349274686761,
      "grad_norm": 7.394552230834961,
      "learning_rate": 4.887831561121426e-05,
      "loss": 3.5832,
      "step": 508800
    },
    {
      "epoch": 0.08975256405033269,
      "grad_norm": 11.330647468566895,
      "learning_rate": 4.887809515394355e-05,
      "loss": 3.7471,
      "step": 508900
    },
    {
      "epoch": 0.08977020063198926,
      "grad_norm": 10.343018531799316,
      "learning_rate": 4.8877874696672846e-05,
      "loss": 3.7563,
      "step": 509000
    },
    {
      "epoch": 0.08978783721364585,
      "grad_norm": 6.2841362953186035,
      "learning_rate": 4.8877654239402135e-05,
      "loss": 3.61,
      "step": 509100
    },
    {
      "epoch": 0.08980547379530243,
      "grad_norm": 7.422312259674072,
      "learning_rate": 4.8877433782131424e-05,
      "loss": 3.7241,
      "step": 509200
    },
    {
      "epoch": 0.089823110376959,
      "grad_norm": 7.754586219787598,
      "learning_rate": 4.887721332486072e-05,
      "loss": 3.6388,
      "step": 509300
    },
    {
      "epoch": 0.08984074695861559,
      "grad_norm": 6.090063571929932,
      "learning_rate": 4.8876992867590015e-05,
      "loss": 3.722,
      "step": 509400
    },
    {
      "epoch": 0.08985838354027216,
      "grad_norm": 7.080708026885986,
      "learning_rate": 4.88767724103193e-05,
      "loss": 3.64,
      "step": 509500
    },
    {
      "epoch": 0.08987602012192875,
      "grad_norm": 7.741497993469238,
      "learning_rate": 4.88765519530486e-05,
      "loss": 3.6697,
      "step": 509600
    },
    {
      "epoch": 0.08989365670358532,
      "grad_norm": 8.63226318359375,
      "learning_rate": 4.8876331495777894e-05,
      "loss": 3.6778,
      "step": 509700
    },
    {
      "epoch": 0.0899112932852419,
      "grad_norm": 8.049311637878418,
      "learning_rate": 4.887611103850718e-05,
      "loss": 3.702,
      "step": 509800
    },
    {
      "epoch": 0.08992892986689849,
      "grad_norm": 7.258783340454102,
      "learning_rate": 4.887589058123648e-05,
      "loss": 3.5736,
      "step": 509900
    },
    {
      "epoch": 0.08994656644855506,
      "grad_norm": 8.701767921447754,
      "learning_rate": 4.8875670123965774e-05,
      "loss": 3.6692,
      "step": 510000
    },
    {
      "epoch": 0.08996420303021165,
      "grad_norm": 7.0258870124816895,
      "learning_rate": 4.887544966669506e-05,
      "loss": 3.654,
      "step": 510100
    },
    {
      "epoch": 0.08998183961186822,
      "grad_norm": 7.423468112945557,
      "learning_rate": 4.887522920942436e-05,
      "loss": 3.6123,
      "step": 510200
    },
    {
      "epoch": 0.0899994761935248,
      "grad_norm": 11.324702262878418,
      "learning_rate": 4.8875008752153653e-05,
      "loss": 3.6339,
      "step": 510300
    },
    {
      "epoch": 0.09001711277518137,
      "grad_norm": 6.479811191558838,
      "learning_rate": 4.887478829488294e-05,
      "loss": 3.645,
      "step": 510400
    },
    {
      "epoch": 0.09003474935683796,
      "grad_norm": 6.76315975189209,
      "learning_rate": 4.887456783761223e-05,
      "loss": 3.6581,
      "step": 510500
    },
    {
      "epoch": 0.09005238593849454,
      "grad_norm": 10.364388465881348,
      "learning_rate": 4.8874347380341526e-05,
      "loss": 3.6922,
      "step": 510600
    },
    {
      "epoch": 0.09007002252015112,
      "grad_norm": 4.772475719451904,
      "learning_rate": 4.887412692307082e-05,
      "loss": 3.679,
      "step": 510700
    },
    {
      "epoch": 0.0900876591018077,
      "grad_norm": 9.950986862182617,
      "learning_rate": 4.887390646580011e-05,
      "loss": 3.6971,
      "step": 510800
    },
    {
      "epoch": 0.09010529568346427,
      "grad_norm": 8.080774307250977,
      "learning_rate": 4.8873686008529406e-05,
      "loss": 3.6841,
      "step": 510900
    },
    {
      "epoch": 0.09012293226512086,
      "grad_norm": 7.651124000549316,
      "learning_rate": 4.88734655512587e-05,
      "loss": 3.5184,
      "step": 511000
    },
    {
      "epoch": 0.09014056884677743,
      "grad_norm": 11.57952880859375,
      "learning_rate": 4.887324509398799e-05,
      "loss": 3.6633,
      "step": 511100
    },
    {
      "epoch": 0.09015820542843402,
      "grad_norm": 5.692662715911865,
      "learning_rate": 4.8873024636717286e-05,
      "loss": 3.7333,
      "step": 511200
    },
    {
      "epoch": 0.09017584201009059,
      "grad_norm": 8.246713638305664,
      "learning_rate": 4.887280417944658e-05,
      "loss": 3.808,
      "step": 511300
    },
    {
      "epoch": 0.09019347859174717,
      "grad_norm": 7.8104071617126465,
      "learning_rate": 4.887258372217587e-05,
      "loss": 3.6529,
      "step": 511400
    },
    {
      "epoch": 0.09021111517340376,
      "grad_norm": 6.907084941864014,
      "learning_rate": 4.8872363264905165e-05,
      "loss": 3.6674,
      "step": 511500
    },
    {
      "epoch": 0.09022875175506033,
      "grad_norm": 7.191202640533447,
      "learning_rate": 4.887214280763446e-05,
      "loss": 3.5887,
      "step": 511600
    },
    {
      "epoch": 0.09024638833671692,
      "grad_norm": 8.68407154083252,
      "learning_rate": 4.887192235036375e-05,
      "loss": 3.6296,
      "step": 511700
    },
    {
      "epoch": 0.09026402491837349,
      "grad_norm": 6.199825763702393,
      "learning_rate": 4.8871701893093045e-05,
      "loss": 3.8029,
      "step": 511800
    },
    {
      "epoch": 0.09028166150003007,
      "grad_norm": 8.81152057647705,
      "learning_rate": 4.8871481435822334e-05,
      "loss": 3.6033,
      "step": 511900
    },
    {
      "epoch": 0.09029929808168664,
      "grad_norm": 8.822371482849121,
      "learning_rate": 4.887126097855162e-05,
      "loss": 3.673,
      "step": 512000
    },
    {
      "epoch": 0.09031693466334323,
      "grad_norm": 9.573708534240723,
      "learning_rate": 4.887104052128092e-05,
      "loss": 3.6775,
      "step": 512100
    },
    {
      "epoch": 0.09033457124499981,
      "grad_norm": 5.961384296417236,
      "learning_rate": 4.887082006401021e-05,
      "loss": 3.6513,
      "step": 512200
    },
    {
      "epoch": 0.09035220782665639,
      "grad_norm": 8.575728416442871,
      "learning_rate": 4.88705996067395e-05,
      "loss": 3.65,
      "step": 512300
    },
    {
      "epoch": 0.09036984440831297,
      "grad_norm": 9.401471138000488,
      "learning_rate": 4.88703791494688e-05,
      "loss": 3.6558,
      "step": 512400
    },
    {
      "epoch": 0.09038748098996954,
      "grad_norm": 8.493962287902832,
      "learning_rate": 4.887015869219809e-05,
      "loss": 3.6849,
      "step": 512500
    },
    {
      "epoch": 0.09040511757162613,
      "grad_norm": 6.076179027557373,
      "learning_rate": 4.886993823492738e-05,
      "loss": 3.6614,
      "step": 512600
    },
    {
      "epoch": 0.0904227541532827,
      "grad_norm": 6.699894428253174,
      "learning_rate": 4.886971777765668e-05,
      "loss": 3.6241,
      "step": 512700
    },
    {
      "epoch": 0.09044039073493929,
      "grad_norm": 7.049618721008301,
      "learning_rate": 4.886949732038597e-05,
      "loss": 3.6013,
      "step": 512800
    },
    {
      "epoch": 0.09045802731659587,
      "grad_norm": 11.116226196289062,
      "learning_rate": 4.886927686311526e-05,
      "loss": 3.6195,
      "step": 512900
    },
    {
      "epoch": 0.09047566389825244,
      "grad_norm": 6.9150309562683105,
      "learning_rate": 4.8869056405844557e-05,
      "loss": 3.7224,
      "step": 513000
    },
    {
      "epoch": 0.09049330047990903,
      "grad_norm": 8.896926879882812,
      "learning_rate": 4.886883594857385e-05,
      "loss": 3.5779,
      "step": 513100
    },
    {
      "epoch": 0.0905109370615656,
      "grad_norm": 7.170259475708008,
      "learning_rate": 4.886861549130314e-05,
      "loss": 3.6856,
      "step": 513200
    },
    {
      "epoch": 0.09052857364322218,
      "grad_norm": 7.591018199920654,
      "learning_rate": 4.886839503403243e-05,
      "loss": 3.6469,
      "step": 513300
    },
    {
      "epoch": 0.09054621022487876,
      "grad_norm": 9.66507339477539,
      "learning_rate": 4.8868174576761725e-05,
      "loss": 3.5641,
      "step": 513400
    },
    {
      "epoch": 0.09056384680653534,
      "grad_norm": 5.203685760498047,
      "learning_rate": 4.8867954119491014e-05,
      "loss": 3.5779,
      "step": 513500
    },
    {
      "epoch": 0.09058148338819193,
      "grad_norm": 8.155129432678223,
      "learning_rate": 4.886773366222031e-05,
      "loss": 3.6221,
      "step": 513600
    },
    {
      "epoch": 0.0905991199698485,
      "grad_norm": 8.822845458984375,
      "learning_rate": 4.8867513204949605e-05,
      "loss": 3.6646,
      "step": 513700
    },
    {
      "epoch": 0.09061675655150508,
      "grad_norm": 6.698609828948975,
      "learning_rate": 4.886729274767889e-05,
      "loss": 3.5726,
      "step": 513800
    },
    {
      "epoch": 0.09063439313316166,
      "grad_norm": 6.595001697540283,
      "learning_rate": 4.886707229040819e-05,
      "loss": 3.674,
      "step": 513900
    },
    {
      "epoch": 0.09065202971481824,
      "grad_norm": 5.6290202140808105,
      "learning_rate": 4.8866851833137484e-05,
      "loss": 3.5615,
      "step": 514000
    },
    {
      "epoch": 0.09066966629647481,
      "grad_norm": 7.955345630645752,
      "learning_rate": 4.886663137586677e-05,
      "loss": 3.739,
      "step": 514100
    },
    {
      "epoch": 0.0906873028781314,
      "grad_norm": 8.375012397766113,
      "learning_rate": 4.886641091859607e-05,
      "loss": 3.7382,
      "step": 514200
    },
    {
      "epoch": 0.09070493945978797,
      "grad_norm": 7.584303379058838,
      "learning_rate": 4.8866190461325364e-05,
      "loss": 3.585,
      "step": 514300
    },
    {
      "epoch": 0.09072257604144456,
      "grad_norm": 7.428456783294678,
      "learning_rate": 4.886597000405465e-05,
      "loss": 3.5391,
      "step": 514400
    },
    {
      "epoch": 0.09074021262310114,
      "grad_norm": 7.678877830505371,
      "learning_rate": 4.886574954678395e-05,
      "loss": 3.6829,
      "step": 514500
    },
    {
      "epoch": 0.09075784920475771,
      "grad_norm": 8.293274879455566,
      "learning_rate": 4.886552908951324e-05,
      "loss": 3.7691,
      "step": 514600
    },
    {
      "epoch": 0.0907754857864143,
      "grad_norm": 6.155016899108887,
      "learning_rate": 4.886530863224253e-05,
      "loss": 3.7294,
      "step": 514700
    },
    {
      "epoch": 0.09079312236807087,
      "grad_norm": 6.802019119262695,
      "learning_rate": 4.886508817497182e-05,
      "loss": 3.6826,
      "step": 514800
    },
    {
      "epoch": 0.09081075894972745,
      "grad_norm": 8.435467720031738,
      "learning_rate": 4.8864867717701116e-05,
      "loss": 3.6993,
      "step": 514900
    },
    {
      "epoch": 0.09082839553138403,
      "grad_norm": 7.294599533081055,
      "learning_rate": 4.8864647260430405e-05,
      "loss": 3.6339,
      "step": 515000
    },
    {
      "epoch": 0.09084603211304061,
      "grad_norm": 8.409747123718262,
      "learning_rate": 4.88644268031597e-05,
      "loss": 3.6341,
      "step": 515100
    },
    {
      "epoch": 0.0908636686946972,
      "grad_norm": 7.164380073547363,
      "learning_rate": 4.8864206345888996e-05,
      "loss": 3.6205,
      "step": 515200
    },
    {
      "epoch": 0.09088130527635377,
      "grad_norm": 6.011927604675293,
      "learning_rate": 4.8863985888618285e-05,
      "loss": 3.5522,
      "step": 515300
    },
    {
      "epoch": 0.09089894185801035,
      "grad_norm": 6.499483108520508,
      "learning_rate": 4.886376543134758e-05,
      "loss": 3.6225,
      "step": 515400
    },
    {
      "epoch": 0.09091657843966693,
      "grad_norm": 9.870680809020996,
      "learning_rate": 4.8863544974076876e-05,
      "loss": 3.7007,
      "step": 515500
    },
    {
      "epoch": 0.09093421502132351,
      "grad_norm": 6.374599933624268,
      "learning_rate": 4.8863324516806164e-05,
      "loss": 3.703,
      "step": 515600
    },
    {
      "epoch": 0.09095185160298008,
      "grad_norm": 9.949748992919922,
      "learning_rate": 4.886310405953546e-05,
      "loss": 3.6791,
      "step": 515700
    },
    {
      "epoch": 0.09096948818463667,
      "grad_norm": 6.978669166564941,
      "learning_rate": 4.8862883602264755e-05,
      "loss": 3.6577,
      "step": 515800
    },
    {
      "epoch": 0.09098712476629325,
      "grad_norm": 7.140941619873047,
      "learning_rate": 4.8862663144994044e-05,
      "loss": 3.6696,
      "step": 515900
    },
    {
      "epoch": 0.09100476134794983,
      "grad_norm": 8.356186866760254,
      "learning_rate": 4.886244268772334e-05,
      "loss": 3.6731,
      "step": 516000
    },
    {
      "epoch": 0.09102239792960641,
      "grad_norm": 6.74789571762085,
      "learning_rate": 4.886222223045263e-05,
      "loss": 3.6076,
      "step": 516100
    },
    {
      "epoch": 0.09104003451126298,
      "grad_norm": 8.099493980407715,
      "learning_rate": 4.8862001773181924e-05,
      "loss": 3.6821,
      "step": 516200
    },
    {
      "epoch": 0.09105767109291957,
      "grad_norm": 9.355240821838379,
      "learning_rate": 4.886178131591121e-05,
      "loss": 3.6474,
      "step": 516300
    },
    {
      "epoch": 0.09107530767457614,
      "grad_norm": 5.790013790130615,
      "learning_rate": 4.886156085864051e-05,
      "loss": 3.641,
      "step": 516400
    },
    {
      "epoch": 0.09109294425623272,
      "grad_norm": 11.735021591186523,
      "learning_rate": 4.8861340401369796e-05,
      "loss": 3.6634,
      "step": 516500
    },
    {
      "epoch": 0.09111058083788931,
      "grad_norm": 6.427639007568359,
      "learning_rate": 4.886111994409909e-05,
      "loss": 3.7161,
      "step": 516600
    },
    {
      "epoch": 0.09112821741954588,
      "grad_norm": 6.401669025421143,
      "learning_rate": 4.886089948682839e-05,
      "loss": 3.7034,
      "step": 516700
    },
    {
      "epoch": 0.09114585400120247,
      "grad_norm": 6.649649143218994,
      "learning_rate": 4.8860679029557676e-05,
      "loss": 3.6107,
      "step": 516800
    },
    {
      "epoch": 0.09116349058285904,
      "grad_norm": 8.920280456542969,
      "learning_rate": 4.886045857228697e-05,
      "loss": 3.6472,
      "step": 516900
    },
    {
      "epoch": 0.09118112716451562,
      "grad_norm": 8.862852096557617,
      "learning_rate": 4.886023811501627e-05,
      "loss": 3.6136,
      "step": 517000
    },
    {
      "epoch": 0.0911987637461722,
      "grad_norm": 8.58027172088623,
      "learning_rate": 4.8860017657745556e-05,
      "loss": 3.6472,
      "step": 517100
    },
    {
      "epoch": 0.09121640032782878,
      "grad_norm": 7.708022117614746,
      "learning_rate": 4.885979720047485e-05,
      "loss": 3.6425,
      "step": 517200
    },
    {
      "epoch": 0.09123403690948535,
      "grad_norm": 5.876186370849609,
      "learning_rate": 4.8859576743204147e-05,
      "loss": 3.5814,
      "step": 517300
    },
    {
      "epoch": 0.09125167349114194,
      "grad_norm": 7.292956352233887,
      "learning_rate": 4.8859356285933435e-05,
      "loss": 3.5889,
      "step": 517400
    },
    {
      "epoch": 0.09126931007279852,
      "grad_norm": 9.449178695678711,
      "learning_rate": 4.885913582866273e-05,
      "loss": 3.5047,
      "step": 517500
    },
    {
      "epoch": 0.0912869466544551,
      "grad_norm": 7.614042282104492,
      "learning_rate": 4.885891537139202e-05,
      "loss": 3.6584,
      "step": 517600
    },
    {
      "epoch": 0.09130458323611168,
      "grad_norm": 5.869045734405518,
      "learning_rate": 4.885869491412131e-05,
      "loss": 3.6694,
      "step": 517700
    },
    {
      "epoch": 0.09132221981776825,
      "grad_norm": 7.5279622077941895,
      "learning_rate": 4.8858474456850604e-05,
      "loss": 3.7476,
      "step": 517800
    },
    {
      "epoch": 0.09133985639942484,
      "grad_norm": 7.856662750244141,
      "learning_rate": 4.88582539995799e-05,
      "loss": 3.6293,
      "step": 517900
    },
    {
      "epoch": 0.09135749298108141,
      "grad_norm": 7.459348678588867,
      "learning_rate": 4.885803354230919e-05,
      "loss": 3.6859,
      "step": 518000
    },
    {
      "epoch": 0.091375129562738,
      "grad_norm": 6.6123270988464355,
      "learning_rate": 4.885781308503848e-05,
      "loss": 3.6985,
      "step": 518100
    },
    {
      "epoch": 0.09139276614439458,
      "grad_norm": 7.922343730926514,
      "learning_rate": 4.885759262776778e-05,
      "loss": 3.6115,
      "step": 518200
    },
    {
      "epoch": 0.09141040272605115,
      "grad_norm": 7.252674579620361,
      "learning_rate": 4.885737217049707e-05,
      "loss": 3.6611,
      "step": 518300
    },
    {
      "epoch": 0.09142803930770774,
      "grad_norm": 6.360074043273926,
      "learning_rate": 4.885715171322636e-05,
      "loss": 3.7038,
      "step": 518400
    },
    {
      "epoch": 0.09144567588936431,
      "grad_norm": 8.53582763671875,
      "learning_rate": 4.885693125595566e-05,
      "loss": 3.6429,
      "step": 518500
    },
    {
      "epoch": 0.0914633124710209,
      "grad_norm": 6.893160820007324,
      "learning_rate": 4.885671079868495e-05,
      "loss": 3.6505,
      "step": 518600
    },
    {
      "epoch": 0.09148094905267747,
      "grad_norm": 7.191575050354004,
      "learning_rate": 4.885649034141424e-05,
      "loss": 3.7134,
      "step": 518700
    },
    {
      "epoch": 0.09149858563433405,
      "grad_norm": 8.735535621643066,
      "learning_rate": 4.885626988414354e-05,
      "loss": 3.678,
      "step": 518800
    },
    {
      "epoch": 0.09151622221599064,
      "grad_norm": 7.7042694091796875,
      "learning_rate": 4.8856049426872827e-05,
      "loss": 3.684,
      "step": 518900
    },
    {
      "epoch": 0.09153385879764721,
      "grad_norm": 5.531630992889404,
      "learning_rate": 4.885582896960212e-05,
      "loss": 3.6199,
      "step": 519000
    },
    {
      "epoch": 0.0915514953793038,
      "grad_norm": 7.311739921569824,
      "learning_rate": 4.885560851233141e-05,
      "loss": 3.6419,
      "step": 519100
    },
    {
      "epoch": 0.09156913196096036,
      "grad_norm": 7.5766072273254395,
      "learning_rate": 4.88553880550607e-05,
      "loss": 3.6926,
      "step": 519200
    },
    {
      "epoch": 0.09158676854261695,
      "grad_norm": 7.823538303375244,
      "learning_rate": 4.8855167597789995e-05,
      "loss": 3.74,
      "step": 519300
    },
    {
      "epoch": 0.09160440512427352,
      "grad_norm": 11.16775894165039,
      "learning_rate": 4.885494714051929e-05,
      "loss": 3.5563,
      "step": 519400
    },
    {
      "epoch": 0.09162204170593011,
      "grad_norm": 7.254861354827881,
      "learning_rate": 4.885472668324858e-05,
      "loss": 3.6277,
      "step": 519500
    },
    {
      "epoch": 0.09163967828758669,
      "grad_norm": 6.29686164855957,
      "learning_rate": 4.8854506225977875e-05,
      "loss": 3.6223,
      "step": 519600
    },
    {
      "epoch": 0.09165731486924326,
      "grad_norm": 6.532838821411133,
      "learning_rate": 4.885428576870717e-05,
      "loss": 3.6186,
      "step": 519700
    },
    {
      "epoch": 0.09167495145089985,
      "grad_norm": 5.89896821975708,
      "learning_rate": 4.885406531143646e-05,
      "loss": 3.6525,
      "step": 519800
    },
    {
      "epoch": 0.09169258803255642,
      "grad_norm": 8.958627700805664,
      "learning_rate": 4.8853844854165754e-05,
      "loss": 3.6704,
      "step": 519900
    },
    {
      "epoch": 0.091710224614213,
      "grad_norm": 6.676247596740723,
      "learning_rate": 4.885362439689505e-05,
      "loss": 3.659,
      "step": 520000
    },
    {
      "epoch": 0.09172786119586958,
      "grad_norm": 14.304338455200195,
      "learning_rate": 4.885340393962434e-05,
      "loss": 3.7345,
      "step": 520100
    },
    {
      "epoch": 0.09174549777752616,
      "grad_norm": 6.325705051422119,
      "learning_rate": 4.8853183482353634e-05,
      "loss": 3.5327,
      "step": 520200
    },
    {
      "epoch": 0.09176313435918274,
      "grad_norm": 8.317061424255371,
      "learning_rate": 4.885296302508293e-05,
      "loss": 3.6417,
      "step": 520300
    },
    {
      "epoch": 0.09178077094083932,
      "grad_norm": 6.506867408752441,
      "learning_rate": 4.885274256781222e-05,
      "loss": 3.6813,
      "step": 520400
    },
    {
      "epoch": 0.0917984075224959,
      "grad_norm": 8.630816459655762,
      "learning_rate": 4.885252211054151e-05,
      "loss": 3.697,
      "step": 520500
    },
    {
      "epoch": 0.09181604410415248,
      "grad_norm": 7.706911563873291,
      "learning_rate": 4.88523016532708e-05,
      "loss": 3.6721,
      "step": 520600
    },
    {
      "epoch": 0.09183368068580906,
      "grad_norm": 6.9527387619018555,
      "learning_rate": 4.885208119600009e-05,
      "loss": 3.5081,
      "step": 520700
    },
    {
      "epoch": 0.09185131726746563,
      "grad_norm": 9.473078727722168,
      "learning_rate": 4.8851860738729386e-05,
      "loss": 3.5712,
      "step": 520800
    },
    {
      "epoch": 0.09186895384912222,
      "grad_norm": 8.909355163574219,
      "learning_rate": 4.885164028145868e-05,
      "loss": 3.6915,
      "step": 520900
    },
    {
      "epoch": 0.09188659043077879,
      "grad_norm": 8.33442211151123,
      "learning_rate": 4.885141982418797e-05,
      "loss": 3.6776,
      "step": 521000
    },
    {
      "epoch": 0.09190422701243538,
      "grad_norm": 7.183722019195557,
      "learning_rate": 4.8851199366917266e-05,
      "loss": 3.6031,
      "step": 521100
    },
    {
      "epoch": 0.09192186359409196,
      "grad_norm": 9.255875587463379,
      "learning_rate": 4.885097890964656e-05,
      "loss": 3.6469,
      "step": 521200
    },
    {
      "epoch": 0.09193950017574853,
      "grad_norm": 7.691239833831787,
      "learning_rate": 4.885075845237586e-05,
      "loss": 3.6324,
      "step": 521300
    },
    {
      "epoch": 0.09195713675740512,
      "grad_norm": 8.213638305664062,
      "learning_rate": 4.8850537995105146e-05,
      "loss": 3.6045,
      "step": 521400
    },
    {
      "epoch": 0.09197477333906169,
      "grad_norm": 5.347426414489746,
      "learning_rate": 4.885031753783444e-05,
      "loss": 3.6405,
      "step": 521500
    },
    {
      "epoch": 0.09199240992071828,
      "grad_norm": 6.5904927253723145,
      "learning_rate": 4.8850097080563736e-05,
      "loss": 3.6454,
      "step": 521600
    },
    {
      "epoch": 0.09201004650237485,
      "grad_norm": 6.726036548614502,
      "learning_rate": 4.8849876623293025e-05,
      "loss": 3.6354,
      "step": 521700
    },
    {
      "epoch": 0.09202768308403143,
      "grad_norm": 6.891012668609619,
      "learning_rate": 4.884965616602232e-05,
      "loss": 3.7066,
      "step": 521800
    },
    {
      "epoch": 0.09204531966568802,
      "grad_norm": 5.234470844268799,
      "learning_rate": 4.884943570875161e-05,
      "loss": 3.6658,
      "step": 521900
    },
    {
      "epoch": 0.09206295624734459,
      "grad_norm": 7.277344226837158,
      "learning_rate": 4.88492152514809e-05,
      "loss": 3.6754,
      "step": 522000
    },
    {
      "epoch": 0.09208059282900118,
      "grad_norm": 17.15494728088379,
      "learning_rate": 4.8848994794210194e-05,
      "loss": 3.5931,
      "step": 522100
    },
    {
      "epoch": 0.09209822941065775,
      "grad_norm": 7.9119367599487305,
      "learning_rate": 4.884877433693949e-05,
      "loss": 3.5794,
      "step": 522200
    },
    {
      "epoch": 0.09211586599231433,
      "grad_norm": 6.1391377449035645,
      "learning_rate": 4.884855387966878e-05,
      "loss": 3.6396,
      "step": 522300
    },
    {
      "epoch": 0.0921335025739709,
      "grad_norm": 5.832584381103516,
      "learning_rate": 4.884833342239807e-05,
      "loss": 3.6719,
      "step": 522400
    },
    {
      "epoch": 0.09215113915562749,
      "grad_norm": 10.457428932189941,
      "learning_rate": 4.884811296512737e-05,
      "loss": 3.6847,
      "step": 522500
    },
    {
      "epoch": 0.09216877573728408,
      "grad_norm": 10.46524429321289,
      "learning_rate": 4.884789250785666e-05,
      "loss": 3.6743,
      "step": 522600
    },
    {
      "epoch": 0.09218641231894065,
      "grad_norm": 9.617380142211914,
      "learning_rate": 4.884767205058595e-05,
      "loss": 3.7578,
      "step": 522700
    },
    {
      "epoch": 0.09220404890059723,
      "grad_norm": 6.45623779296875,
      "learning_rate": 4.884745159331525e-05,
      "loss": 3.6308,
      "step": 522800
    },
    {
      "epoch": 0.0922216854822538,
      "grad_norm": 8.3037691116333,
      "learning_rate": 4.884723113604454e-05,
      "loss": 3.6823,
      "step": 522900
    },
    {
      "epoch": 0.09223932206391039,
      "grad_norm": 7.301143169403076,
      "learning_rate": 4.884701067877383e-05,
      "loss": 3.6353,
      "step": 523000
    },
    {
      "epoch": 0.09225695864556696,
      "grad_norm": 7.978127956390381,
      "learning_rate": 4.884679022150313e-05,
      "loss": 3.5878,
      "step": 523100
    },
    {
      "epoch": 0.09227459522722355,
      "grad_norm": 5.638370037078857,
      "learning_rate": 4.8846569764232417e-05,
      "loss": 3.7245,
      "step": 523200
    },
    {
      "epoch": 0.09229223180888012,
      "grad_norm": 7.138125419616699,
      "learning_rate": 4.8846349306961705e-05,
      "loss": 3.597,
      "step": 523300
    },
    {
      "epoch": 0.0923098683905367,
      "grad_norm": 8.471790313720703,
      "learning_rate": 4.8846128849691e-05,
      "loss": 3.6239,
      "step": 523400
    },
    {
      "epoch": 0.09232750497219329,
      "grad_norm": 7.423464775085449,
      "learning_rate": 4.884590839242029e-05,
      "loss": 3.6706,
      "step": 523500
    },
    {
      "epoch": 0.09234514155384986,
      "grad_norm": 7.653193950653076,
      "learning_rate": 4.8845687935149585e-05,
      "loss": 3.5694,
      "step": 523600
    },
    {
      "epoch": 0.09236277813550645,
      "grad_norm": 6.467121124267578,
      "learning_rate": 4.884546747787888e-05,
      "loss": 3.6509,
      "step": 523700
    },
    {
      "epoch": 0.09238041471716302,
      "grad_norm": 9.449481964111328,
      "learning_rate": 4.884524702060817e-05,
      "loss": 3.6672,
      "step": 523800
    },
    {
      "epoch": 0.0923980512988196,
      "grad_norm": 10.512592315673828,
      "learning_rate": 4.8845026563337465e-05,
      "loss": 3.7023,
      "step": 523900
    },
    {
      "epoch": 0.09241568788047617,
      "grad_norm": 8.45803165435791,
      "learning_rate": 4.884480610606676e-05,
      "loss": 3.6387,
      "step": 524000
    },
    {
      "epoch": 0.09243332446213276,
      "grad_norm": 6.503298759460449,
      "learning_rate": 4.884458564879605e-05,
      "loss": 3.6323,
      "step": 524100
    },
    {
      "epoch": 0.09245096104378935,
      "grad_norm": 6.674478054046631,
      "learning_rate": 4.8844365191525344e-05,
      "loss": 3.6945,
      "step": 524200
    },
    {
      "epoch": 0.09246859762544592,
      "grad_norm": 8.940278053283691,
      "learning_rate": 4.884414473425464e-05,
      "loss": 3.6305,
      "step": 524300
    },
    {
      "epoch": 0.0924862342071025,
      "grad_norm": 7.280787944793701,
      "learning_rate": 4.884392427698393e-05,
      "loss": 3.621,
      "step": 524400
    },
    {
      "epoch": 0.09250387078875907,
      "grad_norm": 8.578241348266602,
      "learning_rate": 4.8843703819713224e-05,
      "loss": 3.4919,
      "step": 524500
    },
    {
      "epoch": 0.09252150737041566,
      "grad_norm": 6.258188724517822,
      "learning_rate": 4.884348336244252e-05,
      "loss": 3.5973,
      "step": 524600
    },
    {
      "epoch": 0.09253914395207223,
      "grad_norm": 8.134770393371582,
      "learning_rate": 4.884326290517181e-05,
      "loss": 3.6138,
      "step": 524700
    },
    {
      "epoch": 0.09255678053372882,
      "grad_norm": 8.687688827514648,
      "learning_rate": 4.88430424479011e-05,
      "loss": 3.6985,
      "step": 524800
    },
    {
      "epoch": 0.0925744171153854,
      "grad_norm": 10.389161109924316,
      "learning_rate": 4.884282199063039e-05,
      "loss": 3.7068,
      "step": 524900
    },
    {
      "epoch": 0.09259205369704197,
      "grad_norm": 5.930870056152344,
      "learning_rate": 4.884260153335968e-05,
      "loss": 3.7366,
      "step": 525000
    },
    {
      "epoch": 0.09260969027869856,
      "grad_norm": 6.333514213562012,
      "learning_rate": 4.8842381076088976e-05,
      "loss": 3.6465,
      "step": 525100
    },
    {
      "epoch": 0.09262732686035513,
      "grad_norm": 6.296182155609131,
      "learning_rate": 4.884216061881827e-05,
      "loss": 3.621,
      "step": 525200
    },
    {
      "epoch": 0.09264496344201172,
      "grad_norm": 7.060440540313721,
      "learning_rate": 4.884194016154756e-05,
      "loss": 3.6614,
      "step": 525300
    },
    {
      "epoch": 0.09266260002366829,
      "grad_norm": 7.02119779586792,
      "learning_rate": 4.8841719704276856e-05,
      "loss": 3.6023,
      "step": 525400
    },
    {
      "epoch": 0.09268023660532487,
      "grad_norm": 5.193733215332031,
      "learning_rate": 4.884149924700615e-05,
      "loss": 3.7194,
      "step": 525500
    },
    {
      "epoch": 0.09269787318698146,
      "grad_norm": 5.664138317108154,
      "learning_rate": 4.884127878973544e-05,
      "loss": 3.6519,
      "step": 525600
    },
    {
      "epoch": 0.09271550976863803,
      "grad_norm": 7.451827049255371,
      "learning_rate": 4.8841058332464736e-05,
      "loss": 3.737,
      "step": 525700
    },
    {
      "epoch": 0.09273314635029462,
      "grad_norm": 5.475675582885742,
      "learning_rate": 4.884083787519403e-05,
      "loss": 3.6129,
      "step": 525800
    },
    {
      "epoch": 0.09275078293195119,
      "grad_norm": 9.52372932434082,
      "learning_rate": 4.884061741792332e-05,
      "loss": 3.5268,
      "step": 525900
    },
    {
      "epoch": 0.09276841951360777,
      "grad_norm": 6.468312740325928,
      "learning_rate": 4.8840396960652615e-05,
      "loss": 3.6606,
      "step": 526000
    },
    {
      "epoch": 0.09278605609526434,
      "grad_norm": 7.072357177734375,
      "learning_rate": 4.8840176503381904e-05,
      "loss": 3.7102,
      "step": 526100
    },
    {
      "epoch": 0.09280369267692093,
      "grad_norm": 11.09001636505127,
      "learning_rate": 4.88399560461112e-05,
      "loss": 3.7473,
      "step": 526200
    },
    {
      "epoch": 0.0928213292585775,
      "grad_norm": 8.065122604370117,
      "learning_rate": 4.883973558884049e-05,
      "loss": 3.5701,
      "step": 526300
    },
    {
      "epoch": 0.09283896584023409,
      "grad_norm": 9.770968437194824,
      "learning_rate": 4.8839515131569783e-05,
      "loss": 3.718,
      "step": 526400
    },
    {
      "epoch": 0.09285660242189067,
      "grad_norm": 7.583471775054932,
      "learning_rate": 4.883929467429907e-05,
      "loss": 3.6094,
      "step": 526500
    },
    {
      "epoch": 0.09287423900354724,
      "grad_norm": 6.444859027862549,
      "learning_rate": 4.883907421702837e-05,
      "loss": 3.7053,
      "step": 526600
    },
    {
      "epoch": 0.09289187558520383,
      "grad_norm": 7.86371374130249,
      "learning_rate": 4.883885375975766e-05,
      "loss": 3.7484,
      "step": 526700
    },
    {
      "epoch": 0.0929095121668604,
      "grad_norm": 5.992827415466309,
      "learning_rate": 4.883863330248695e-05,
      "loss": 3.6495,
      "step": 526800
    },
    {
      "epoch": 0.09292714874851699,
      "grad_norm": 8.24425220489502,
      "learning_rate": 4.883841284521625e-05,
      "loss": 3.6641,
      "step": 526900
    },
    {
      "epoch": 0.09294478533017356,
      "grad_norm": 8.244068145751953,
      "learning_rate": 4.883819238794554e-05,
      "loss": 3.7273,
      "step": 527000
    },
    {
      "epoch": 0.09296242191183014,
      "grad_norm": 6.696442127227783,
      "learning_rate": 4.883797193067483e-05,
      "loss": 3.6597,
      "step": 527100
    },
    {
      "epoch": 0.09298005849348673,
      "grad_norm": 5.683219909667969,
      "learning_rate": 4.883775147340413e-05,
      "loss": 3.5176,
      "step": 527200
    },
    {
      "epoch": 0.0929976950751433,
      "grad_norm": 6.86220645904541,
      "learning_rate": 4.883753101613342e-05,
      "loss": 3.6668,
      "step": 527300
    },
    {
      "epoch": 0.09301533165679989,
      "grad_norm": 8.866588592529297,
      "learning_rate": 4.883731055886271e-05,
      "loss": 3.6413,
      "step": 527400
    },
    {
      "epoch": 0.09303296823845646,
      "grad_norm": 10.810193061828613,
      "learning_rate": 4.8837090101592007e-05,
      "loss": 3.6965,
      "step": 527500
    },
    {
      "epoch": 0.09305060482011304,
      "grad_norm": 7.9771575927734375,
      "learning_rate": 4.8836869644321295e-05,
      "loss": 3.6372,
      "step": 527600
    },
    {
      "epoch": 0.09306824140176961,
      "grad_norm": 6.591384410858154,
      "learning_rate": 4.8836649187050584e-05,
      "loss": 3.7141,
      "step": 527700
    },
    {
      "epoch": 0.0930858779834262,
      "grad_norm": 7.039266586303711,
      "learning_rate": 4.883642872977988e-05,
      "loss": 3.6336,
      "step": 527800
    },
    {
      "epoch": 0.09310351456508278,
      "grad_norm": 6.28257942199707,
      "learning_rate": 4.8836208272509175e-05,
      "loss": 3.5744,
      "step": 527900
    },
    {
      "epoch": 0.09312115114673936,
      "grad_norm": 8.055347442626953,
      "learning_rate": 4.8835987815238464e-05,
      "loss": 3.609,
      "step": 528000
    },
    {
      "epoch": 0.09313878772839594,
      "grad_norm": 7.738077640533447,
      "learning_rate": 4.883576735796776e-05,
      "loss": 3.552,
      "step": 528100
    },
    {
      "epoch": 0.09315642431005251,
      "grad_norm": 9.460870742797852,
      "learning_rate": 4.8835546900697054e-05,
      "loss": 3.6922,
      "step": 528200
    },
    {
      "epoch": 0.0931740608917091,
      "grad_norm": 8.291383743286133,
      "learning_rate": 4.883532644342634e-05,
      "loss": 3.6675,
      "step": 528300
    },
    {
      "epoch": 0.09319169747336567,
      "grad_norm": 8.710134506225586,
      "learning_rate": 4.883510598615564e-05,
      "loss": 3.7117,
      "step": 528400
    },
    {
      "epoch": 0.09320933405502226,
      "grad_norm": 7.078037261962891,
      "learning_rate": 4.8834885528884934e-05,
      "loss": 3.5748,
      "step": 528500
    },
    {
      "epoch": 0.09322697063667884,
      "grad_norm": 7.474591255187988,
      "learning_rate": 4.883466507161422e-05,
      "loss": 3.5574,
      "step": 528600
    },
    {
      "epoch": 0.09324460721833541,
      "grad_norm": 7.127618312835693,
      "learning_rate": 4.883444461434352e-05,
      "loss": 3.7125,
      "step": 528700
    },
    {
      "epoch": 0.093262243799992,
      "grad_norm": 9.296183586120605,
      "learning_rate": 4.8834224157072814e-05,
      "loss": 3.6287,
      "step": 528800
    },
    {
      "epoch": 0.09327988038164857,
      "grad_norm": 6.36718225479126,
      "learning_rate": 4.88340036998021e-05,
      "loss": 3.6518,
      "step": 528900
    },
    {
      "epoch": 0.09329751696330515,
      "grad_norm": 11.171225547790527,
      "learning_rate": 4.88337832425314e-05,
      "loss": 3.6228,
      "step": 529000
    },
    {
      "epoch": 0.09331515354496173,
      "grad_norm": 6.2640581130981445,
      "learning_rate": 4.8833562785260687e-05,
      "loss": 3.6612,
      "step": 529100
    },
    {
      "epoch": 0.09333279012661831,
      "grad_norm": 8.150104522705078,
      "learning_rate": 4.8833342327989975e-05,
      "loss": 3.6538,
      "step": 529200
    },
    {
      "epoch": 0.09335042670827488,
      "grad_norm": 9.421123504638672,
      "learning_rate": 4.883312187071927e-05,
      "loss": 3.6988,
      "step": 529300
    },
    {
      "epoch": 0.09336806328993147,
      "grad_norm": 7.5389299392700195,
      "learning_rate": 4.8832901413448566e-05,
      "loss": 3.6748,
      "step": 529400
    },
    {
      "epoch": 0.09338569987158805,
      "grad_norm": 7.894505977630615,
      "learning_rate": 4.8832680956177855e-05,
      "loss": 3.6648,
      "step": 529500
    },
    {
      "epoch": 0.09340333645324463,
      "grad_norm": 10.950065612792969,
      "learning_rate": 4.883246049890715e-05,
      "loss": 3.5826,
      "step": 529600
    },
    {
      "epoch": 0.09342097303490121,
      "grad_norm": 6.167078495025635,
      "learning_rate": 4.8832240041636446e-05,
      "loss": 3.6407,
      "step": 529700
    },
    {
      "epoch": 0.09343860961655778,
      "grad_norm": 11.615131378173828,
      "learning_rate": 4.8832019584365735e-05,
      "loss": 3.7845,
      "step": 529800
    },
    {
      "epoch": 0.09345624619821437,
      "grad_norm": 8.193270683288574,
      "learning_rate": 4.883179912709503e-05,
      "loss": 3.6808,
      "step": 529900
    },
    {
      "epoch": 0.09347388277987094,
      "grad_norm": 7.936953544616699,
      "learning_rate": 4.8831578669824325e-05,
      "loss": 3.6849,
      "step": 530000
    },
    {
      "epoch": 0.09349151936152753,
      "grad_norm": 11.50826358795166,
      "learning_rate": 4.8831358212553614e-05,
      "loss": 3.5992,
      "step": 530100
    },
    {
      "epoch": 0.09350915594318411,
      "grad_norm": 7.313963413238525,
      "learning_rate": 4.883113775528291e-05,
      "loss": 3.7089,
      "step": 530200
    },
    {
      "epoch": 0.09352679252484068,
      "grad_norm": 9.304849624633789,
      "learning_rate": 4.8830917298012205e-05,
      "loss": 3.5246,
      "step": 530300
    },
    {
      "epoch": 0.09354442910649727,
      "grad_norm": 6.315911769866943,
      "learning_rate": 4.8830696840741494e-05,
      "loss": 3.662,
      "step": 530400
    },
    {
      "epoch": 0.09356206568815384,
      "grad_norm": 4.987791538238525,
      "learning_rate": 4.883047638347078e-05,
      "loss": 3.6648,
      "step": 530500
    },
    {
      "epoch": 0.09357970226981042,
      "grad_norm": 7.166204929351807,
      "learning_rate": 4.883025592620008e-05,
      "loss": 3.867,
      "step": 530600
    },
    {
      "epoch": 0.093597338851467,
      "grad_norm": 6.923961162567139,
      "learning_rate": 4.883003546892937e-05,
      "loss": 3.6404,
      "step": 530700
    },
    {
      "epoch": 0.09361497543312358,
      "grad_norm": 10.58902645111084,
      "learning_rate": 4.882981501165866e-05,
      "loss": 3.6493,
      "step": 530800
    },
    {
      "epoch": 0.09363261201478017,
      "grad_norm": 5.3699164390563965,
      "learning_rate": 4.882959455438796e-05,
      "loss": 3.6449,
      "step": 530900
    },
    {
      "epoch": 0.09365024859643674,
      "grad_norm": 6.9577531814575195,
      "learning_rate": 4.8829374097117246e-05,
      "loss": 3.5716,
      "step": 531000
    },
    {
      "epoch": 0.09366788517809332,
      "grad_norm": 6.45773983001709,
      "learning_rate": 4.882915363984654e-05,
      "loss": 3.5899,
      "step": 531100
    },
    {
      "epoch": 0.0936855217597499,
      "grad_norm": 6.618577480316162,
      "learning_rate": 4.882893318257584e-05,
      "loss": 3.6296,
      "step": 531200
    },
    {
      "epoch": 0.09370315834140648,
      "grad_norm": 8.827061653137207,
      "learning_rate": 4.8828712725305126e-05,
      "loss": 3.6277,
      "step": 531300
    },
    {
      "epoch": 0.09372079492306305,
      "grad_norm": 8.36644172668457,
      "learning_rate": 4.882849226803442e-05,
      "loss": 3.7507,
      "step": 531400
    },
    {
      "epoch": 0.09373843150471964,
      "grad_norm": 4.992692947387695,
      "learning_rate": 4.882827181076372e-05,
      "loss": 3.5192,
      "step": 531500
    },
    {
      "epoch": 0.09375606808637622,
      "grad_norm": 9.822104454040527,
      "learning_rate": 4.8828051353493006e-05,
      "loss": 3.6911,
      "step": 531600
    },
    {
      "epoch": 0.0937737046680328,
      "grad_norm": 8.834295272827148,
      "learning_rate": 4.88278308962223e-05,
      "loss": 3.594,
      "step": 531700
    },
    {
      "epoch": 0.09379134124968938,
      "grad_norm": 6.813797950744629,
      "learning_rate": 4.8827610438951596e-05,
      "loss": 3.6279,
      "step": 531800
    },
    {
      "epoch": 0.09380897783134595,
      "grad_norm": 7.546360015869141,
      "learning_rate": 4.8827389981680885e-05,
      "loss": 3.6851,
      "step": 531900
    },
    {
      "epoch": 0.09382661441300254,
      "grad_norm": 6.423361778259277,
      "learning_rate": 4.8827169524410174e-05,
      "loss": 3.7708,
      "step": 532000
    },
    {
      "epoch": 0.09384425099465911,
      "grad_norm": 7.507862567901611,
      "learning_rate": 4.882694906713947e-05,
      "loss": 3.6567,
      "step": 532100
    },
    {
      "epoch": 0.0938618875763157,
      "grad_norm": 7.243021488189697,
      "learning_rate": 4.8826728609868765e-05,
      "loss": 3.6827,
      "step": 532200
    },
    {
      "epoch": 0.09387952415797227,
      "grad_norm": 5.400443077087402,
      "learning_rate": 4.8826508152598053e-05,
      "loss": 3.7919,
      "step": 532300
    },
    {
      "epoch": 0.09389716073962885,
      "grad_norm": 8.97709846496582,
      "learning_rate": 4.882628769532735e-05,
      "loss": 3.6687,
      "step": 532400
    },
    {
      "epoch": 0.09391479732128544,
      "grad_norm": 7.542977333068848,
      "learning_rate": 4.8826067238056644e-05,
      "loss": 3.6832,
      "step": 532500
    },
    {
      "epoch": 0.09393243390294201,
      "grad_norm": 5.556171417236328,
      "learning_rate": 4.882584678078593e-05,
      "loss": 3.6701,
      "step": 532600
    },
    {
      "epoch": 0.0939500704845986,
      "grad_norm": 8.569596290588379,
      "learning_rate": 4.882562632351523e-05,
      "loss": 3.7212,
      "step": 532700
    },
    {
      "epoch": 0.09396770706625517,
      "grad_norm": 6.425546646118164,
      "learning_rate": 4.8825405866244524e-05,
      "loss": 3.633,
      "step": 532800
    },
    {
      "epoch": 0.09398534364791175,
      "grad_norm": 7.283482074737549,
      "learning_rate": 4.882518540897381e-05,
      "loss": 3.6512,
      "step": 532900
    },
    {
      "epoch": 0.09400298022956832,
      "grad_norm": 7.6602783203125,
      "learning_rate": 4.882496495170311e-05,
      "loss": 3.6063,
      "step": 533000
    },
    {
      "epoch": 0.09402061681122491,
      "grad_norm": 6.6359639167785645,
      "learning_rate": 4.8824744494432404e-05,
      "loss": 3.6147,
      "step": 533100
    },
    {
      "epoch": 0.0940382533928815,
      "grad_norm": 7.846754550933838,
      "learning_rate": 4.882452403716169e-05,
      "loss": 3.7195,
      "step": 533200
    },
    {
      "epoch": 0.09405588997453806,
      "grad_norm": 8.115888595581055,
      "learning_rate": 4.882430357989098e-05,
      "loss": 3.6045,
      "step": 533300
    },
    {
      "epoch": 0.09407352655619465,
      "grad_norm": 5.579636573791504,
      "learning_rate": 4.8824083122620277e-05,
      "loss": 3.618,
      "step": 533400
    },
    {
      "epoch": 0.09409116313785122,
      "grad_norm": 6.1389055252075195,
      "learning_rate": 4.8823862665349565e-05,
      "loss": 3.6715,
      "step": 533500
    },
    {
      "epoch": 0.09410879971950781,
      "grad_norm": 11.101000785827637,
      "learning_rate": 4.882364220807886e-05,
      "loss": 3.6673,
      "step": 533600
    },
    {
      "epoch": 0.09412643630116438,
      "grad_norm": 8.045672416687012,
      "learning_rate": 4.8823421750808156e-05,
      "loss": 3.6135,
      "step": 533700
    },
    {
      "epoch": 0.09414407288282096,
      "grad_norm": 6.777462005615234,
      "learning_rate": 4.8823201293537445e-05,
      "loss": 3.6788,
      "step": 533800
    },
    {
      "epoch": 0.09416170946447755,
      "grad_norm": 6.954854965209961,
      "learning_rate": 4.882298083626674e-05,
      "loss": 3.5622,
      "step": 533900
    },
    {
      "epoch": 0.09417934604613412,
      "grad_norm": 10.48908805847168,
      "learning_rate": 4.8822760378996036e-05,
      "loss": 3.5858,
      "step": 534000
    },
    {
      "epoch": 0.0941969826277907,
      "grad_norm": 9.905135154724121,
      "learning_rate": 4.8822539921725324e-05,
      "loss": 3.6243,
      "step": 534100
    },
    {
      "epoch": 0.09421461920944728,
      "grad_norm": 7.143589019775391,
      "learning_rate": 4.882231946445462e-05,
      "loss": 3.6185,
      "step": 534200
    },
    {
      "epoch": 0.09423225579110386,
      "grad_norm": 6.122400283813477,
      "learning_rate": 4.8822099007183915e-05,
      "loss": 3.6514,
      "step": 534300
    },
    {
      "epoch": 0.09424989237276044,
      "grad_norm": 9.864103317260742,
      "learning_rate": 4.8821878549913204e-05,
      "loss": 3.5548,
      "step": 534400
    },
    {
      "epoch": 0.09426752895441702,
      "grad_norm": 10.656079292297363,
      "learning_rate": 4.88216580926425e-05,
      "loss": 3.6357,
      "step": 534500
    },
    {
      "epoch": 0.0942851655360736,
      "grad_norm": 6.816460132598877,
      "learning_rate": 4.8821437635371795e-05,
      "loss": 3.6432,
      "step": 534600
    },
    {
      "epoch": 0.09430280211773018,
      "grad_norm": 7.484426498413086,
      "learning_rate": 4.8821217178101084e-05,
      "loss": 3.6411,
      "step": 534700
    },
    {
      "epoch": 0.09432043869938676,
      "grad_norm": 10.007023811340332,
      "learning_rate": 4.882099672083037e-05,
      "loss": 3.6596,
      "step": 534800
    },
    {
      "epoch": 0.09433807528104333,
      "grad_norm": 9.38179874420166,
      "learning_rate": 4.882077626355967e-05,
      "loss": 3.6651,
      "step": 534900
    },
    {
      "epoch": 0.09435571186269992,
      "grad_norm": 6.624889850616455,
      "learning_rate": 4.8820555806288957e-05,
      "loss": 3.5288,
      "step": 535000
    },
    {
      "epoch": 0.09437334844435649,
      "grad_norm": 5.994395732879639,
      "learning_rate": 4.882033534901825e-05,
      "loss": 3.5453,
      "step": 535100
    },
    {
      "epoch": 0.09439098502601308,
      "grad_norm": 15.26416015625,
      "learning_rate": 4.882011489174755e-05,
      "loss": 3.6916,
      "step": 535200
    },
    {
      "epoch": 0.09440862160766965,
      "grad_norm": 9.060850143432617,
      "learning_rate": 4.8819894434476836e-05,
      "loss": 3.6075,
      "step": 535300
    },
    {
      "epoch": 0.09442625818932623,
      "grad_norm": 14.947742462158203,
      "learning_rate": 4.881967397720613e-05,
      "loss": 3.6004,
      "step": 535400
    },
    {
      "epoch": 0.09444389477098282,
      "grad_norm": 5.179296016693115,
      "learning_rate": 4.881945351993543e-05,
      "loss": 3.6065,
      "step": 535500
    },
    {
      "epoch": 0.09446153135263939,
      "grad_norm": 7.543201446533203,
      "learning_rate": 4.8819233062664716e-05,
      "loss": 3.5579,
      "step": 535600
    },
    {
      "epoch": 0.09447916793429598,
      "grad_norm": 8.792612075805664,
      "learning_rate": 4.881901260539401e-05,
      "loss": 3.6386,
      "step": 535700
    },
    {
      "epoch": 0.09449680451595255,
      "grad_norm": 5.734879970550537,
      "learning_rate": 4.881879214812331e-05,
      "loss": 3.6101,
      "step": 535800
    },
    {
      "epoch": 0.09451444109760913,
      "grad_norm": 6.346555709838867,
      "learning_rate": 4.8818571690852595e-05,
      "loss": 3.6028,
      "step": 535900
    },
    {
      "epoch": 0.0945320776792657,
      "grad_norm": 10.426671981811523,
      "learning_rate": 4.881835123358189e-05,
      "loss": 3.6261,
      "step": 536000
    },
    {
      "epoch": 0.09454971426092229,
      "grad_norm": 9.59595012664795,
      "learning_rate": 4.881813077631118e-05,
      "loss": 3.6963,
      "step": 536100
    },
    {
      "epoch": 0.09456735084257888,
      "grad_norm": 6.860207557678223,
      "learning_rate": 4.8817910319040475e-05,
      "loss": 3.5935,
      "step": 536200
    },
    {
      "epoch": 0.09458498742423545,
      "grad_norm": 7.78193473815918,
      "learning_rate": 4.8817689861769764e-05,
      "loss": 3.6779,
      "step": 536300
    },
    {
      "epoch": 0.09460262400589203,
      "grad_norm": 12.312064170837402,
      "learning_rate": 4.881746940449906e-05,
      "loss": 3.619,
      "step": 536400
    },
    {
      "epoch": 0.0946202605875486,
      "grad_norm": 6.574636459350586,
      "learning_rate": 4.881724894722835e-05,
      "loss": 3.5365,
      "step": 536500
    },
    {
      "epoch": 0.09463789716920519,
      "grad_norm": 5.899620056152344,
      "learning_rate": 4.8817028489957643e-05,
      "loss": 3.6695,
      "step": 536600
    },
    {
      "epoch": 0.09465553375086176,
      "grad_norm": 7.986195087432861,
      "learning_rate": 4.881680803268694e-05,
      "loss": 3.5901,
      "step": 536700
    },
    {
      "epoch": 0.09467317033251835,
      "grad_norm": 6.806661605834961,
      "learning_rate": 4.881658757541623e-05,
      "loss": 3.7,
      "step": 536800
    },
    {
      "epoch": 0.09469080691417493,
      "grad_norm": 6.009191036224365,
      "learning_rate": 4.881636711814552e-05,
      "loss": 3.5647,
      "step": 536900
    },
    {
      "epoch": 0.0947084434958315,
      "grad_norm": 5.817234039306641,
      "learning_rate": 4.881614666087482e-05,
      "loss": 3.6553,
      "step": 537000
    },
    {
      "epoch": 0.09472608007748809,
      "grad_norm": 7.765672206878662,
      "learning_rate": 4.881592620360411e-05,
      "loss": 3.5374,
      "step": 537100
    },
    {
      "epoch": 0.09474371665914466,
      "grad_norm": 7.571082592010498,
      "learning_rate": 4.88157057463334e-05,
      "loss": 3.6037,
      "step": 537200
    },
    {
      "epoch": 0.09476135324080125,
      "grad_norm": 10.768656730651855,
      "learning_rate": 4.88154852890627e-05,
      "loss": 3.5724,
      "step": 537300
    },
    {
      "epoch": 0.09477898982245782,
      "grad_norm": 7.328795433044434,
      "learning_rate": 4.881526483179199e-05,
      "loss": 3.6362,
      "step": 537400
    },
    {
      "epoch": 0.0947966264041144,
      "grad_norm": 7.134528160095215,
      "learning_rate": 4.881504437452128e-05,
      "loss": 3.6536,
      "step": 537500
    },
    {
      "epoch": 0.09481426298577099,
      "grad_norm": 5.99350118637085,
      "learning_rate": 4.881482391725057e-05,
      "loss": 3.6873,
      "step": 537600
    },
    {
      "epoch": 0.09483189956742756,
      "grad_norm": 8.195846557617188,
      "learning_rate": 4.881460345997986e-05,
      "loss": 3.584,
      "step": 537700
    },
    {
      "epoch": 0.09484953614908415,
      "grad_norm": 6.657318592071533,
      "learning_rate": 4.8814383002709155e-05,
      "loss": 3.5617,
      "step": 537800
    },
    {
      "epoch": 0.09486717273074072,
      "grad_norm": 9.760422706604004,
      "learning_rate": 4.881416254543845e-05,
      "loss": 3.6487,
      "step": 537900
    },
    {
      "epoch": 0.0948848093123973,
      "grad_norm": 7.5906476974487305,
      "learning_rate": 4.881394208816774e-05,
      "loss": 3.6588,
      "step": 538000
    },
    {
      "epoch": 0.09490244589405387,
      "grad_norm": 6.733633041381836,
      "learning_rate": 4.8813721630897035e-05,
      "loss": 3.638,
      "step": 538100
    },
    {
      "epoch": 0.09492008247571046,
      "grad_norm": 8.399283409118652,
      "learning_rate": 4.881350117362633e-05,
      "loss": 3.7042,
      "step": 538200
    },
    {
      "epoch": 0.09493771905736703,
      "grad_norm": 7.708136081695557,
      "learning_rate": 4.881328071635562e-05,
      "loss": 3.568,
      "step": 538300
    },
    {
      "epoch": 0.09495535563902362,
      "grad_norm": 9.39984130859375,
      "learning_rate": 4.8813060259084914e-05,
      "loss": 3.6721,
      "step": 538400
    },
    {
      "epoch": 0.0949729922206802,
      "grad_norm": 8.208765983581543,
      "learning_rate": 4.881283980181421e-05,
      "loss": 3.7231,
      "step": 538500
    },
    {
      "epoch": 0.09499062880233677,
      "grad_norm": 6.511955738067627,
      "learning_rate": 4.88126193445435e-05,
      "loss": 3.6742,
      "step": 538600
    },
    {
      "epoch": 0.09500826538399336,
      "grad_norm": 6.774454593658447,
      "learning_rate": 4.8812398887272794e-05,
      "loss": 3.6618,
      "step": 538700
    },
    {
      "epoch": 0.09502590196564993,
      "grad_norm": 8.405656814575195,
      "learning_rate": 4.881217843000209e-05,
      "loss": 3.6171,
      "step": 538800
    },
    {
      "epoch": 0.09504353854730652,
      "grad_norm": 10.058125495910645,
      "learning_rate": 4.881195797273138e-05,
      "loss": 3.6873,
      "step": 538900
    },
    {
      "epoch": 0.09506117512896309,
      "grad_norm": 5.982817649841309,
      "learning_rate": 4.8811737515460674e-05,
      "loss": 3.5431,
      "step": 539000
    },
    {
      "epoch": 0.09507881171061967,
      "grad_norm": 5.227137088775635,
      "learning_rate": 4.881151705818996e-05,
      "loss": 3.7216,
      "step": 539100
    },
    {
      "epoch": 0.09509644829227626,
      "grad_norm": 7.120377540588379,
      "learning_rate": 4.881129660091925e-05,
      "loss": 3.692,
      "step": 539200
    },
    {
      "epoch": 0.09511408487393283,
      "grad_norm": 9.409004211425781,
      "learning_rate": 4.8811076143648547e-05,
      "loss": 3.6688,
      "step": 539300
    },
    {
      "epoch": 0.09513172145558942,
      "grad_norm": 8.888625144958496,
      "learning_rate": 4.881085568637784e-05,
      "loss": 3.6568,
      "step": 539400
    },
    {
      "epoch": 0.09514935803724599,
      "grad_norm": 7.720824718475342,
      "learning_rate": 4.881063522910713e-05,
      "loss": 3.5789,
      "step": 539500
    },
    {
      "epoch": 0.09516699461890257,
      "grad_norm": 10.613964080810547,
      "learning_rate": 4.8810414771836426e-05,
      "loss": 3.6446,
      "step": 539600
    },
    {
      "epoch": 0.09518463120055914,
      "grad_norm": 7.168275356292725,
      "learning_rate": 4.881019431456572e-05,
      "loss": 3.5298,
      "step": 539700
    },
    {
      "epoch": 0.09520226778221573,
      "grad_norm": 5.988956928253174,
      "learning_rate": 4.880997385729501e-05,
      "loss": 3.7218,
      "step": 539800
    },
    {
      "epoch": 0.09521990436387232,
      "grad_norm": 8.85808277130127,
      "learning_rate": 4.8809753400024306e-05,
      "loss": 3.7359,
      "step": 539900
    },
    {
      "epoch": 0.09523754094552889,
      "grad_norm": 8.287862777709961,
      "learning_rate": 4.88095329427536e-05,
      "loss": 3.6358,
      "step": 540000
    },
    {
      "epoch": 0.09525517752718547,
      "grad_norm": 9.040033340454102,
      "learning_rate": 4.880931248548289e-05,
      "loss": 3.5603,
      "step": 540100
    },
    {
      "epoch": 0.09527281410884204,
      "grad_norm": 6.733641624450684,
      "learning_rate": 4.8809092028212185e-05,
      "loss": 3.6879,
      "step": 540200
    },
    {
      "epoch": 0.09529045069049863,
      "grad_norm": 6.468383312225342,
      "learning_rate": 4.880887157094148e-05,
      "loss": 3.7045,
      "step": 540300
    },
    {
      "epoch": 0.0953080872721552,
      "grad_norm": 8.816603660583496,
      "learning_rate": 4.880865111367077e-05,
      "loss": 3.628,
      "step": 540400
    },
    {
      "epoch": 0.09532572385381179,
      "grad_norm": 8.18431282043457,
      "learning_rate": 4.880843065640006e-05,
      "loss": 3.6415,
      "step": 540500
    },
    {
      "epoch": 0.09534336043546837,
      "grad_norm": 8.873573303222656,
      "learning_rate": 4.8808210199129354e-05,
      "loss": 3.6783,
      "step": 540600
    },
    {
      "epoch": 0.09536099701712494,
      "grad_norm": 6.379128932952881,
      "learning_rate": 4.880798974185864e-05,
      "loss": 3.6425,
      "step": 540700
    },
    {
      "epoch": 0.09537863359878153,
      "grad_norm": 6.897502899169922,
      "learning_rate": 4.880776928458794e-05,
      "loss": 3.6086,
      "step": 540800
    },
    {
      "epoch": 0.0953962701804381,
      "grad_norm": 10.314983367919922,
      "learning_rate": 4.880754882731723e-05,
      "loss": 3.605,
      "step": 540900
    },
    {
      "epoch": 0.09541390676209469,
      "grad_norm": 7.70831823348999,
      "learning_rate": 4.880732837004652e-05,
      "loss": 3.6583,
      "step": 541000
    },
    {
      "epoch": 0.09543154334375126,
      "grad_norm": 14.792228698730469,
      "learning_rate": 4.880710791277582e-05,
      "loss": 3.6764,
      "step": 541100
    },
    {
      "epoch": 0.09544917992540784,
      "grad_norm": 5.4134650230407715,
      "learning_rate": 4.880688745550511e-05,
      "loss": 3.6395,
      "step": 541200
    },
    {
      "epoch": 0.09546681650706441,
      "grad_norm": 8.282634735107422,
      "learning_rate": 4.88066669982344e-05,
      "loss": 3.6079,
      "step": 541300
    },
    {
      "epoch": 0.095484453088721,
      "grad_norm": 7.171116352081299,
      "learning_rate": 4.88064465409637e-05,
      "loss": 3.6175,
      "step": 541400
    },
    {
      "epoch": 0.09550208967037759,
      "grad_norm": 6.473484992980957,
      "learning_rate": 4.880622608369299e-05,
      "loss": 3.6461,
      "step": 541500
    },
    {
      "epoch": 0.09551972625203416,
      "grad_norm": 9.372001647949219,
      "learning_rate": 4.880600562642228e-05,
      "loss": 3.5255,
      "step": 541600
    },
    {
      "epoch": 0.09553736283369074,
      "grad_norm": 7.2695794105529785,
      "learning_rate": 4.880578516915158e-05,
      "loss": 3.705,
      "step": 541700
    },
    {
      "epoch": 0.09555499941534731,
      "grad_norm": 8.681118965148926,
      "learning_rate": 4.880556471188087e-05,
      "loss": 3.6171,
      "step": 541800
    },
    {
      "epoch": 0.0955726359970039,
      "grad_norm": 6.8442230224609375,
      "learning_rate": 4.880534425461016e-05,
      "loss": 3.6804,
      "step": 541900
    },
    {
      "epoch": 0.09559027257866047,
      "grad_norm": 7.509485244750977,
      "learning_rate": 4.880512379733945e-05,
      "loss": 3.6861,
      "step": 542000
    },
    {
      "epoch": 0.09560790916031706,
      "grad_norm": 8.096780776977539,
      "learning_rate": 4.8804903340068745e-05,
      "loss": 3.6572,
      "step": 542100
    },
    {
      "epoch": 0.09562554574197364,
      "grad_norm": 6.561707973480225,
      "learning_rate": 4.8804682882798034e-05,
      "loss": 3.6663,
      "step": 542200
    },
    {
      "epoch": 0.09564318232363021,
      "grad_norm": 7.601426124572754,
      "learning_rate": 4.880446242552733e-05,
      "loss": 3.6921,
      "step": 542300
    },
    {
      "epoch": 0.0956608189052868,
      "grad_norm": 12.752062797546387,
      "learning_rate": 4.8804241968256625e-05,
      "loss": 3.6819,
      "step": 542400
    },
    {
      "epoch": 0.09567845548694337,
      "grad_norm": 8.299769401550293,
      "learning_rate": 4.8804021510985913e-05,
      "loss": 3.7019,
      "step": 542500
    },
    {
      "epoch": 0.09569609206859996,
      "grad_norm": 9.298048973083496,
      "learning_rate": 4.880380105371521e-05,
      "loss": 3.5596,
      "step": 542600
    },
    {
      "epoch": 0.09571372865025653,
      "grad_norm": 5.889297008514404,
      "learning_rate": 4.8803580596444504e-05,
      "loss": 3.7194,
      "step": 542700
    },
    {
      "epoch": 0.09573136523191311,
      "grad_norm": 8.120408058166504,
      "learning_rate": 4.88033601391738e-05,
      "loss": 3.6553,
      "step": 542800
    },
    {
      "epoch": 0.0957490018135697,
      "grad_norm": 7.359920024871826,
      "learning_rate": 4.880313968190309e-05,
      "loss": 3.6111,
      "step": 542900
    },
    {
      "epoch": 0.09576663839522627,
      "grad_norm": 8.277048110961914,
      "learning_rate": 4.8802919224632384e-05,
      "loss": 3.641,
      "step": 543000
    },
    {
      "epoch": 0.09578427497688285,
      "grad_norm": 6.702645301818848,
      "learning_rate": 4.880269876736168e-05,
      "loss": 3.6314,
      "step": 543100
    },
    {
      "epoch": 0.09580191155853943,
      "grad_norm": 10.057974815368652,
      "learning_rate": 4.880247831009097e-05,
      "loss": 3.7126,
      "step": 543200
    },
    {
      "epoch": 0.09581954814019601,
      "grad_norm": 5.47476863861084,
      "learning_rate": 4.880225785282026e-05,
      "loss": 3.7023,
      "step": 543300
    },
    {
      "epoch": 0.09583718472185258,
      "grad_norm": 6.071082592010498,
      "learning_rate": 4.880203739554955e-05,
      "loss": 3.6372,
      "step": 543400
    },
    {
      "epoch": 0.09585482130350917,
      "grad_norm": 6.412551403045654,
      "learning_rate": 4.880181693827884e-05,
      "loss": 3.6838,
      "step": 543500
    },
    {
      "epoch": 0.09587245788516575,
      "grad_norm": 9.432588577270508,
      "learning_rate": 4.8801596481008136e-05,
      "loss": 3.619,
      "step": 543600
    },
    {
      "epoch": 0.09589009446682233,
      "grad_norm": 8.76122760772705,
      "learning_rate": 4.880137602373743e-05,
      "loss": 3.6837,
      "step": 543700
    },
    {
      "epoch": 0.09590773104847891,
      "grad_norm": 8.982430458068848,
      "learning_rate": 4.880115556646672e-05,
      "loss": 3.7396,
      "step": 543800
    },
    {
      "epoch": 0.09592536763013548,
      "grad_norm": 7.0669941902160645,
      "learning_rate": 4.8800935109196016e-05,
      "loss": 3.6092,
      "step": 543900
    },
    {
      "epoch": 0.09594300421179207,
      "grad_norm": 9.979104042053223,
      "learning_rate": 4.880071465192531e-05,
      "loss": 3.6589,
      "step": 544000
    },
    {
      "epoch": 0.09596064079344864,
      "grad_norm": 7.534481525421143,
      "learning_rate": 4.88004941946546e-05,
      "loss": 3.6995,
      "step": 544100
    },
    {
      "epoch": 0.09597827737510523,
      "grad_norm": 6.045934200286865,
      "learning_rate": 4.8800273737383896e-05,
      "loss": 3.5532,
      "step": 544200
    },
    {
      "epoch": 0.0959959139567618,
      "grad_norm": 5.7283759117126465,
      "learning_rate": 4.880005328011319e-05,
      "loss": 3.6212,
      "step": 544300
    },
    {
      "epoch": 0.09601355053841838,
      "grad_norm": 5.285646915435791,
      "learning_rate": 4.879983282284248e-05,
      "loss": 3.6318,
      "step": 544400
    },
    {
      "epoch": 0.09603118712007497,
      "grad_norm": 9.896377563476562,
      "learning_rate": 4.8799612365571775e-05,
      "loss": 3.8002,
      "step": 544500
    },
    {
      "epoch": 0.09604882370173154,
      "grad_norm": 7.012590408325195,
      "learning_rate": 4.879939190830107e-05,
      "loss": 3.5812,
      "step": 544600
    },
    {
      "epoch": 0.09606646028338812,
      "grad_norm": 6.498575210571289,
      "learning_rate": 4.879917145103036e-05,
      "loss": 3.6782,
      "step": 544700
    },
    {
      "epoch": 0.0960840968650447,
      "grad_norm": 8.374960899353027,
      "learning_rate": 4.879895099375965e-05,
      "loss": 3.6805,
      "step": 544800
    },
    {
      "epoch": 0.09610173344670128,
      "grad_norm": 9.252371788024902,
      "learning_rate": 4.8798730536488944e-05,
      "loss": 3.584,
      "step": 544900
    },
    {
      "epoch": 0.09611937002835785,
      "grad_norm": 7.7115654945373535,
      "learning_rate": 4.879851007921823e-05,
      "loss": 3.6254,
      "step": 545000
    },
    {
      "epoch": 0.09613700661001444,
      "grad_norm": 5.979258060455322,
      "learning_rate": 4.879828962194753e-05,
      "loss": 3.594,
      "step": 545100
    },
    {
      "epoch": 0.09615464319167102,
      "grad_norm": 9.109474182128906,
      "learning_rate": 4.879806916467682e-05,
      "loss": 3.5885,
      "step": 545200
    },
    {
      "epoch": 0.0961722797733276,
      "grad_norm": 6.762512683868408,
      "learning_rate": 4.879784870740611e-05,
      "loss": 3.6332,
      "step": 545300
    },
    {
      "epoch": 0.09618991635498418,
      "grad_norm": 12.642822265625,
      "learning_rate": 4.879762825013541e-05,
      "loss": 3.7215,
      "step": 545400
    },
    {
      "epoch": 0.09620755293664075,
      "grad_norm": 6.235780715942383,
      "learning_rate": 4.87974077928647e-05,
      "loss": 3.6748,
      "step": 545500
    },
    {
      "epoch": 0.09622518951829734,
      "grad_norm": 8.597016334533691,
      "learning_rate": 4.879718733559399e-05,
      "loss": 3.6536,
      "step": 545600
    },
    {
      "epoch": 0.09624282609995391,
      "grad_norm": 6.699336528778076,
      "learning_rate": 4.879696687832329e-05,
      "loss": 3.6949,
      "step": 545700
    },
    {
      "epoch": 0.0962604626816105,
      "grad_norm": 7.157115459442139,
      "learning_rate": 4.879674642105258e-05,
      "loss": 3.5173,
      "step": 545800
    },
    {
      "epoch": 0.09627809926326708,
      "grad_norm": 10.387396812438965,
      "learning_rate": 4.879652596378187e-05,
      "loss": 3.6368,
      "step": 545900
    },
    {
      "epoch": 0.09629573584492365,
      "grad_norm": 5.901274681091309,
      "learning_rate": 4.879630550651117e-05,
      "loss": 3.691,
      "step": 546000
    },
    {
      "epoch": 0.09631337242658024,
      "grad_norm": 6.870750427246094,
      "learning_rate": 4.8796085049240455e-05,
      "loss": 3.603,
      "step": 546100
    },
    {
      "epoch": 0.09633100900823681,
      "grad_norm": 5.411712646484375,
      "learning_rate": 4.879586459196975e-05,
      "loss": 3.687,
      "step": 546200
    },
    {
      "epoch": 0.0963486455898934,
      "grad_norm": 8.418582916259766,
      "learning_rate": 4.879564413469904e-05,
      "loss": 3.572,
      "step": 546300
    },
    {
      "epoch": 0.09636628217154997,
      "grad_norm": 6.259140491485596,
      "learning_rate": 4.8795423677428335e-05,
      "loss": 3.7342,
      "step": 546400
    },
    {
      "epoch": 0.09638391875320655,
      "grad_norm": 7.9023566246032715,
      "learning_rate": 4.8795203220157624e-05,
      "loss": 3.73,
      "step": 546500
    },
    {
      "epoch": 0.09640155533486314,
      "grad_norm": 6.972428321838379,
      "learning_rate": 4.879498276288692e-05,
      "loss": 3.718,
      "step": 546600
    },
    {
      "epoch": 0.09641919191651971,
      "grad_norm": 8.020971298217773,
      "learning_rate": 4.8794762305616215e-05,
      "loss": 3.6296,
      "step": 546700
    },
    {
      "epoch": 0.0964368284981763,
      "grad_norm": 8.023695945739746,
      "learning_rate": 4.87945418483455e-05,
      "loss": 3.6433,
      "step": 546800
    },
    {
      "epoch": 0.09645446507983287,
      "grad_norm": 9.681349754333496,
      "learning_rate": 4.87943213910748e-05,
      "loss": 3.6195,
      "step": 546900
    },
    {
      "epoch": 0.09647210166148945,
      "grad_norm": 5.92701530456543,
      "learning_rate": 4.8794100933804094e-05,
      "loss": 3.6067,
      "step": 547000
    },
    {
      "epoch": 0.09648973824314602,
      "grad_norm": 9.480337142944336,
      "learning_rate": 4.879388047653338e-05,
      "loss": 3.719,
      "step": 547100
    },
    {
      "epoch": 0.09650737482480261,
      "grad_norm": 8.751782417297363,
      "learning_rate": 4.879366001926268e-05,
      "loss": 3.6435,
      "step": 547200
    },
    {
      "epoch": 0.09652501140645918,
      "grad_norm": 8.382174491882324,
      "learning_rate": 4.8793439561991974e-05,
      "loss": 3.5925,
      "step": 547300
    },
    {
      "epoch": 0.09654264798811577,
      "grad_norm": 7.822713375091553,
      "learning_rate": 4.879321910472126e-05,
      "loss": 3.5447,
      "step": 547400
    },
    {
      "epoch": 0.09656028456977235,
      "grad_norm": 9.388981819152832,
      "learning_rate": 4.879299864745056e-05,
      "loss": 3.5981,
      "step": 547500
    },
    {
      "epoch": 0.09657792115142892,
      "grad_norm": 7.486382007598877,
      "learning_rate": 4.879277819017985e-05,
      "loss": 3.6961,
      "step": 547600
    },
    {
      "epoch": 0.09659555773308551,
      "grad_norm": 7.887331962585449,
      "learning_rate": 4.8792557732909135e-05,
      "loss": 3.6533,
      "step": 547700
    },
    {
      "epoch": 0.09661319431474208,
      "grad_norm": 11.65052604675293,
      "learning_rate": 4.879233727563843e-05,
      "loss": 3.6417,
      "step": 547800
    },
    {
      "epoch": 0.09663083089639866,
      "grad_norm": 6.408470153808594,
      "learning_rate": 4.8792116818367726e-05,
      "loss": 3.5743,
      "step": 547900
    },
    {
      "epoch": 0.09664846747805524,
      "grad_norm": 8.157147407531738,
      "learning_rate": 4.8791896361097015e-05,
      "loss": 3.5917,
      "step": 548000
    },
    {
      "epoch": 0.09666610405971182,
      "grad_norm": 6.99595308303833,
      "learning_rate": 4.879167590382631e-05,
      "loss": 3.5869,
      "step": 548100
    },
    {
      "epoch": 0.09668374064136841,
      "grad_norm": 7.035601615905762,
      "learning_rate": 4.8791455446555606e-05,
      "loss": 3.6167,
      "step": 548200
    },
    {
      "epoch": 0.09670137722302498,
      "grad_norm": 6.307426452636719,
      "learning_rate": 4.8791234989284895e-05,
      "loss": 3.6068,
      "step": 548300
    },
    {
      "epoch": 0.09671901380468156,
      "grad_norm": 5.970998764038086,
      "learning_rate": 4.879101453201419e-05,
      "loss": 3.5796,
      "step": 548400
    },
    {
      "epoch": 0.09673665038633814,
      "grad_norm": 8.765302658081055,
      "learning_rate": 4.8790794074743486e-05,
      "loss": 3.6041,
      "step": 548500
    },
    {
      "epoch": 0.09675428696799472,
      "grad_norm": 9.566873550415039,
      "learning_rate": 4.8790573617472774e-05,
      "loss": 3.7144,
      "step": 548600
    },
    {
      "epoch": 0.09677192354965129,
      "grad_norm": 6.12949800491333,
      "learning_rate": 4.879035316020207e-05,
      "loss": 3.6976,
      "step": 548700
    },
    {
      "epoch": 0.09678956013130788,
      "grad_norm": 6.9166131019592285,
      "learning_rate": 4.8790132702931365e-05,
      "loss": 3.6939,
      "step": 548800
    },
    {
      "epoch": 0.09680719671296446,
      "grad_norm": 6.776822566986084,
      "learning_rate": 4.8789912245660654e-05,
      "loss": 3.5151,
      "step": 548900
    },
    {
      "epoch": 0.09682483329462103,
      "grad_norm": 6.507713317871094,
      "learning_rate": 4.878969178838995e-05,
      "loss": 3.6059,
      "step": 549000
    },
    {
      "epoch": 0.09684246987627762,
      "grad_norm": 6.186862468719482,
      "learning_rate": 4.878947133111924e-05,
      "loss": 3.6917,
      "step": 549100
    },
    {
      "epoch": 0.09686010645793419,
      "grad_norm": 7.147612571716309,
      "learning_rate": 4.878925087384853e-05,
      "loss": 3.5949,
      "step": 549200
    },
    {
      "epoch": 0.09687774303959078,
      "grad_norm": 9.56298542022705,
      "learning_rate": 4.878903041657782e-05,
      "loss": 3.6483,
      "step": 549300
    },
    {
      "epoch": 0.09689537962124735,
      "grad_norm": 9.682777404785156,
      "learning_rate": 4.878880995930712e-05,
      "loss": 3.5999,
      "step": 549400
    },
    {
      "epoch": 0.09691301620290393,
      "grad_norm": 6.660022735595703,
      "learning_rate": 4.8788589502036406e-05,
      "loss": 3.5641,
      "step": 549500
    },
    {
      "epoch": 0.09693065278456052,
      "grad_norm": 7.515976905822754,
      "learning_rate": 4.87883690447657e-05,
      "loss": 3.6773,
      "step": 549600
    },
    {
      "epoch": 0.09694828936621709,
      "grad_norm": 10.028413772583008,
      "learning_rate": 4.8788148587495e-05,
      "loss": 3.7327,
      "step": 549700
    },
    {
      "epoch": 0.09696592594787368,
      "grad_norm": 7.605239391326904,
      "learning_rate": 4.8787928130224286e-05,
      "loss": 3.6958,
      "step": 549800
    },
    {
      "epoch": 0.09698356252953025,
      "grad_norm": 7.057785511016846,
      "learning_rate": 4.878770767295358e-05,
      "loss": 3.5801,
      "step": 549900
    },
    {
      "epoch": 0.09700119911118683,
      "grad_norm": 9.131817817687988,
      "learning_rate": 4.878748721568288e-05,
      "loss": 3.6816,
      "step": 550000
    },
    {
      "epoch": 0.0970188356928434,
      "grad_norm": 7.26617956161499,
      "learning_rate": 4.8787266758412166e-05,
      "loss": 3.627,
      "step": 550100
    },
    {
      "epoch": 0.09703647227449999,
      "grad_norm": 6.466357231140137,
      "learning_rate": 4.878704630114146e-05,
      "loss": 3.6815,
      "step": 550200
    },
    {
      "epoch": 0.09705410885615656,
      "grad_norm": 7.275084495544434,
      "learning_rate": 4.878682584387076e-05,
      "loss": 3.581,
      "step": 550300
    },
    {
      "epoch": 0.09707174543781315,
      "grad_norm": 8.626861572265625,
      "learning_rate": 4.8786605386600045e-05,
      "loss": 3.6403,
      "step": 550400
    },
    {
      "epoch": 0.09708938201946973,
      "grad_norm": 8.92979907989502,
      "learning_rate": 4.8786384929329334e-05,
      "loss": 3.5934,
      "step": 550500
    },
    {
      "epoch": 0.0971070186011263,
      "grad_norm": 7.407622337341309,
      "learning_rate": 4.878616447205863e-05,
      "loss": 3.7004,
      "step": 550600
    },
    {
      "epoch": 0.09712465518278289,
      "grad_norm": 6.217518329620361,
      "learning_rate": 4.878594401478792e-05,
      "loss": 3.5806,
      "step": 550700
    },
    {
      "epoch": 0.09714229176443946,
      "grad_norm": 5.948649883270264,
      "learning_rate": 4.8785723557517214e-05,
      "loss": 3.5964,
      "step": 550800
    },
    {
      "epoch": 0.09715992834609605,
      "grad_norm": 6.044308662414551,
      "learning_rate": 4.878550310024651e-05,
      "loss": 3.6455,
      "step": 550900
    },
    {
      "epoch": 0.09717756492775262,
      "grad_norm": 8.360627174377441,
      "learning_rate": 4.87852826429758e-05,
      "loss": 3.7875,
      "step": 551000
    },
    {
      "epoch": 0.0971952015094092,
      "grad_norm": 7.2554121017456055,
      "learning_rate": 4.878506218570509e-05,
      "loss": 3.6093,
      "step": 551100
    },
    {
      "epoch": 0.09721283809106579,
      "grad_norm": 7.865025043487549,
      "learning_rate": 4.878484172843439e-05,
      "loss": 3.6005,
      "step": 551200
    },
    {
      "epoch": 0.09723047467272236,
      "grad_norm": 9.894196510314941,
      "learning_rate": 4.878462127116368e-05,
      "loss": 3.5848,
      "step": 551300
    },
    {
      "epoch": 0.09724811125437895,
      "grad_norm": 7.585621356964111,
      "learning_rate": 4.878440081389297e-05,
      "loss": 3.6131,
      "step": 551400
    },
    {
      "epoch": 0.09726574783603552,
      "grad_norm": 8.636786460876465,
      "learning_rate": 4.878418035662227e-05,
      "loss": 3.5088,
      "step": 551500
    },
    {
      "epoch": 0.0972833844176921,
      "grad_norm": 8.47543716430664,
      "learning_rate": 4.878395989935156e-05,
      "loss": 3.7277,
      "step": 551600
    },
    {
      "epoch": 0.09730102099934868,
      "grad_norm": 5.614193916320801,
      "learning_rate": 4.878373944208085e-05,
      "loss": 3.8229,
      "step": 551700
    },
    {
      "epoch": 0.09731865758100526,
      "grad_norm": 8.946489334106445,
      "learning_rate": 4.878351898481015e-05,
      "loss": 3.6924,
      "step": 551800
    },
    {
      "epoch": 0.09733629416266185,
      "grad_norm": 8.106436729431152,
      "learning_rate": 4.878329852753944e-05,
      "loss": 3.6553,
      "step": 551900
    },
    {
      "epoch": 0.09735393074431842,
      "grad_norm": 7.177933216094971,
      "learning_rate": 4.8783078070268725e-05,
      "loss": 3.5811,
      "step": 552000
    },
    {
      "epoch": 0.097371567325975,
      "grad_norm": 7.648712635040283,
      "learning_rate": 4.878285761299802e-05,
      "loss": 3.6947,
      "step": 552100
    },
    {
      "epoch": 0.09738920390763157,
      "grad_norm": 7.408472061157227,
      "learning_rate": 4.878263715572731e-05,
      "loss": 3.7021,
      "step": 552200
    },
    {
      "epoch": 0.09740684048928816,
      "grad_norm": 7.62238073348999,
      "learning_rate": 4.8782416698456605e-05,
      "loss": 3.6505,
      "step": 552300
    },
    {
      "epoch": 0.09742447707094473,
      "grad_norm": 5.835393905639648,
      "learning_rate": 4.87821962411859e-05,
      "loss": 3.7099,
      "step": 552400
    },
    {
      "epoch": 0.09744211365260132,
      "grad_norm": 6.282369613647461,
      "learning_rate": 4.878197578391519e-05,
      "loss": 3.5709,
      "step": 552500
    },
    {
      "epoch": 0.0974597502342579,
      "grad_norm": 7.45336389541626,
      "learning_rate": 4.8781755326644485e-05,
      "loss": 3.6441,
      "step": 552600
    },
    {
      "epoch": 0.09747738681591447,
      "grad_norm": 9.361502647399902,
      "learning_rate": 4.878153486937378e-05,
      "loss": 3.6923,
      "step": 552700
    },
    {
      "epoch": 0.09749502339757106,
      "grad_norm": 8.630531311035156,
      "learning_rate": 4.878131441210307e-05,
      "loss": 3.6132,
      "step": 552800
    },
    {
      "epoch": 0.09751265997922763,
      "grad_norm": 6.747621536254883,
      "learning_rate": 4.8781093954832364e-05,
      "loss": 3.6608,
      "step": 552900
    },
    {
      "epoch": 0.09753029656088422,
      "grad_norm": 7.744826793670654,
      "learning_rate": 4.878087349756166e-05,
      "loss": 3.6245,
      "step": 553000
    },
    {
      "epoch": 0.09754793314254079,
      "grad_norm": 5.125872611999512,
      "learning_rate": 4.878065304029095e-05,
      "loss": 3.5643,
      "step": 553100
    },
    {
      "epoch": 0.09756556972419737,
      "grad_norm": 11.050756454467773,
      "learning_rate": 4.8780432583020244e-05,
      "loss": 3.6056,
      "step": 553200
    },
    {
      "epoch": 0.09758320630585395,
      "grad_norm": 7.619877815246582,
      "learning_rate": 4.878021212574953e-05,
      "loss": 3.6006,
      "step": 553300
    },
    {
      "epoch": 0.09760084288751053,
      "grad_norm": 8.612778663635254,
      "learning_rate": 4.877999166847883e-05,
      "loss": 3.558,
      "step": 553400
    },
    {
      "epoch": 0.09761847946916712,
      "grad_norm": 7.744881629943848,
      "learning_rate": 4.877977121120812e-05,
      "loss": 3.6314,
      "step": 553500
    },
    {
      "epoch": 0.09763611605082369,
      "grad_norm": 7.437342643737793,
      "learning_rate": 4.877955075393741e-05,
      "loss": 3.6116,
      "step": 553600
    },
    {
      "epoch": 0.09765375263248027,
      "grad_norm": 8.326618194580078,
      "learning_rate": 4.877933029666671e-05,
      "loss": 3.5946,
      "step": 553700
    },
    {
      "epoch": 0.09767138921413684,
      "grad_norm": 8.261777877807617,
      "learning_rate": 4.8779109839395996e-05,
      "loss": 3.4823,
      "step": 553800
    },
    {
      "epoch": 0.09768902579579343,
      "grad_norm": 6.515854358673096,
      "learning_rate": 4.877888938212529e-05,
      "loss": 3.6745,
      "step": 553900
    },
    {
      "epoch": 0.09770666237745,
      "grad_norm": 7.3367180824279785,
      "learning_rate": 4.877866892485459e-05,
      "loss": 3.6797,
      "step": 554000
    },
    {
      "epoch": 0.09772429895910659,
      "grad_norm": 11.368284225463867,
      "learning_rate": 4.8778448467583876e-05,
      "loss": 3.5936,
      "step": 554100
    },
    {
      "epoch": 0.09774193554076317,
      "grad_norm": 7.726979732513428,
      "learning_rate": 4.877822801031317e-05,
      "loss": 3.7086,
      "step": 554200
    },
    {
      "epoch": 0.09775957212241974,
      "grad_norm": 7.4771342277526855,
      "learning_rate": 4.877800755304247e-05,
      "loss": 3.5979,
      "step": 554300
    },
    {
      "epoch": 0.09777720870407633,
      "grad_norm": 6.760197162628174,
      "learning_rate": 4.8777787095771756e-05,
      "loss": 3.6805,
      "step": 554400
    },
    {
      "epoch": 0.0977948452857329,
      "grad_norm": 8.610918045043945,
      "learning_rate": 4.877756663850105e-05,
      "loss": 3.6291,
      "step": 554500
    },
    {
      "epoch": 0.09781248186738949,
      "grad_norm": 7.194014072418213,
      "learning_rate": 4.8777346181230347e-05,
      "loss": 3.5535,
      "step": 554600
    },
    {
      "epoch": 0.09783011844904606,
      "grad_norm": 9.397942543029785,
      "learning_rate": 4.8777125723959635e-05,
      "loss": 3.6111,
      "step": 554700
    },
    {
      "epoch": 0.09784775503070264,
      "grad_norm": 7.255130767822266,
      "learning_rate": 4.8776905266688924e-05,
      "loss": 3.5917,
      "step": 554800
    },
    {
      "epoch": 0.09786539161235923,
      "grad_norm": 7.456818103790283,
      "learning_rate": 4.877668480941822e-05,
      "loss": 3.6024,
      "step": 554900
    },
    {
      "epoch": 0.0978830281940158,
      "grad_norm": 8.741846084594727,
      "learning_rate": 4.877646435214751e-05,
      "loss": 3.6005,
      "step": 555000
    },
    {
      "epoch": 0.09790066477567239,
      "grad_norm": 8.014911651611328,
      "learning_rate": 4.8776243894876804e-05,
      "loss": 3.6521,
      "step": 555100
    },
    {
      "epoch": 0.09791830135732896,
      "grad_norm": 6.022939682006836,
      "learning_rate": 4.87760234376061e-05,
      "loss": 3.6141,
      "step": 555200
    },
    {
      "epoch": 0.09793593793898554,
      "grad_norm": 6.8249735832214355,
      "learning_rate": 4.877580298033539e-05,
      "loss": 3.6576,
      "step": 555300
    },
    {
      "epoch": 0.09795357452064211,
      "grad_norm": 7.955493927001953,
      "learning_rate": 4.877558252306468e-05,
      "loss": 3.742,
      "step": 555400
    },
    {
      "epoch": 0.0979712111022987,
      "grad_norm": 11.569634437561035,
      "learning_rate": 4.877536206579398e-05,
      "loss": 3.6187,
      "step": 555500
    },
    {
      "epoch": 0.09798884768395529,
      "grad_norm": 7.599308967590332,
      "learning_rate": 4.877514160852327e-05,
      "loss": 3.6318,
      "step": 555600
    },
    {
      "epoch": 0.09800648426561186,
      "grad_norm": 7.084339618682861,
      "learning_rate": 4.877492115125256e-05,
      "loss": 3.7058,
      "step": 555700
    },
    {
      "epoch": 0.09802412084726844,
      "grad_norm": 6.903543949127197,
      "learning_rate": 4.877470069398186e-05,
      "loss": 3.6058,
      "step": 555800
    },
    {
      "epoch": 0.09804175742892501,
      "grad_norm": 8.177720069885254,
      "learning_rate": 4.877448023671115e-05,
      "loss": 3.7132,
      "step": 555900
    },
    {
      "epoch": 0.0980593940105816,
      "grad_norm": 6.3717041015625,
      "learning_rate": 4.877425977944044e-05,
      "loss": 3.5922,
      "step": 556000
    },
    {
      "epoch": 0.09807703059223817,
      "grad_norm": 7.116560459136963,
      "learning_rate": 4.877403932216973e-05,
      "loss": 3.6634,
      "step": 556100
    },
    {
      "epoch": 0.09809466717389476,
      "grad_norm": 9.003971099853516,
      "learning_rate": 4.877381886489903e-05,
      "loss": 3.6144,
      "step": 556200
    },
    {
      "epoch": 0.09811230375555133,
      "grad_norm": 5.5091166496276855,
      "learning_rate": 4.8773598407628315e-05,
      "loss": 3.5901,
      "step": 556300
    },
    {
      "epoch": 0.09812994033720791,
      "grad_norm": 7.3985514640808105,
      "learning_rate": 4.877337795035761e-05,
      "loss": 3.6452,
      "step": 556400
    },
    {
      "epoch": 0.0981475769188645,
      "grad_norm": 7.081040859222412,
      "learning_rate": 4.87731574930869e-05,
      "loss": 3.6286,
      "step": 556500
    },
    {
      "epoch": 0.09816521350052107,
      "grad_norm": 6.262103080749512,
      "learning_rate": 4.8772937035816195e-05,
      "loss": 3.6874,
      "step": 556600
    },
    {
      "epoch": 0.09818285008217766,
      "grad_norm": 5.659768581390381,
      "learning_rate": 4.877271657854549e-05,
      "loss": 3.6503,
      "step": 556700
    },
    {
      "epoch": 0.09820048666383423,
      "grad_norm": 8.795173645019531,
      "learning_rate": 4.877249612127478e-05,
      "loss": 3.5879,
      "step": 556800
    },
    {
      "epoch": 0.09821812324549081,
      "grad_norm": 6.153662204742432,
      "learning_rate": 4.8772275664004075e-05,
      "loss": 3.6833,
      "step": 556900
    },
    {
      "epoch": 0.09823575982714738,
      "grad_norm": 9.766984939575195,
      "learning_rate": 4.877205520673337e-05,
      "loss": 3.6088,
      "step": 557000
    },
    {
      "epoch": 0.09825339640880397,
      "grad_norm": 7.124090671539307,
      "learning_rate": 4.877183474946266e-05,
      "loss": 3.7048,
      "step": 557100
    },
    {
      "epoch": 0.09827103299046056,
      "grad_norm": 7.744043827056885,
      "learning_rate": 4.8771614292191954e-05,
      "loss": 3.6756,
      "step": 557200
    },
    {
      "epoch": 0.09828866957211713,
      "grad_norm": 8.594141006469727,
      "learning_rate": 4.877139383492125e-05,
      "loss": 3.6989,
      "step": 557300
    },
    {
      "epoch": 0.09830630615377371,
      "grad_norm": 7.4997172355651855,
      "learning_rate": 4.877117337765054e-05,
      "loss": 3.5704,
      "step": 557400
    },
    {
      "epoch": 0.09832394273543028,
      "grad_norm": 7.1056365966796875,
      "learning_rate": 4.8770952920379834e-05,
      "loss": 3.6315,
      "step": 557500
    },
    {
      "epoch": 0.09834157931708687,
      "grad_norm": 6.262335777282715,
      "learning_rate": 4.877073246310912e-05,
      "loss": 3.6989,
      "step": 557600
    },
    {
      "epoch": 0.09835921589874344,
      "grad_norm": 8.986273765563965,
      "learning_rate": 4.877051200583841e-05,
      "loss": 3.674,
      "step": 557700
    },
    {
      "epoch": 0.09837685248040003,
      "grad_norm": 6.199617385864258,
      "learning_rate": 4.877029154856771e-05,
      "loss": 3.6414,
      "step": 557800
    },
    {
      "epoch": 0.09839448906205661,
      "grad_norm": 7.351140022277832,
      "learning_rate": 4.8770071091297e-05,
      "loss": 3.711,
      "step": 557900
    },
    {
      "epoch": 0.09841212564371318,
      "grad_norm": 7.406363010406494,
      "learning_rate": 4.876985063402629e-05,
      "loss": 3.6401,
      "step": 558000
    },
    {
      "epoch": 0.09842976222536977,
      "grad_norm": 7.552813529968262,
      "learning_rate": 4.8769630176755586e-05,
      "loss": 3.6317,
      "step": 558100
    },
    {
      "epoch": 0.09844739880702634,
      "grad_norm": 6.44188928604126,
      "learning_rate": 4.876940971948488e-05,
      "loss": 3.7241,
      "step": 558200
    },
    {
      "epoch": 0.09846503538868293,
      "grad_norm": 7.819473743438721,
      "learning_rate": 4.876918926221417e-05,
      "loss": 3.6856,
      "step": 558300
    },
    {
      "epoch": 0.0984826719703395,
      "grad_norm": 5.419044494628906,
      "learning_rate": 4.8768968804943466e-05,
      "loss": 3.5578,
      "step": 558400
    },
    {
      "epoch": 0.09850030855199608,
      "grad_norm": 10.634403228759766,
      "learning_rate": 4.876874834767276e-05,
      "loss": 3.6769,
      "step": 558500
    },
    {
      "epoch": 0.09851794513365267,
      "grad_norm": 9.452669143676758,
      "learning_rate": 4.876852789040205e-05,
      "loss": 3.5825,
      "step": 558600
    },
    {
      "epoch": 0.09853558171530924,
      "grad_norm": 7.058658599853516,
      "learning_rate": 4.8768307433131346e-05,
      "loss": 3.6339,
      "step": 558700
    },
    {
      "epoch": 0.09855321829696582,
      "grad_norm": 7.558162689208984,
      "learning_rate": 4.876808697586064e-05,
      "loss": 3.6326,
      "step": 558800
    },
    {
      "epoch": 0.0985708548786224,
      "grad_norm": 7.45266580581665,
      "learning_rate": 4.876786651858993e-05,
      "loss": 3.5572,
      "step": 558900
    },
    {
      "epoch": 0.09858849146027898,
      "grad_norm": 5.845513343811035,
      "learning_rate": 4.8767646061319225e-05,
      "loss": 3.7011,
      "step": 559000
    },
    {
      "epoch": 0.09860612804193555,
      "grad_norm": 7.052828311920166,
      "learning_rate": 4.8767425604048514e-05,
      "loss": 3.606,
      "step": 559100
    },
    {
      "epoch": 0.09862376462359214,
      "grad_norm": 6.546512126922607,
      "learning_rate": 4.87672051467778e-05,
      "loss": 3.6907,
      "step": 559200
    },
    {
      "epoch": 0.09864140120524871,
      "grad_norm": 6.017866611480713,
      "learning_rate": 4.87669846895071e-05,
      "loss": 3.6629,
      "step": 559300
    },
    {
      "epoch": 0.0986590377869053,
      "grad_norm": 10.27976131439209,
      "learning_rate": 4.8766764232236394e-05,
      "loss": 3.6125,
      "step": 559400
    },
    {
      "epoch": 0.09867667436856188,
      "grad_norm": 7.709383964538574,
      "learning_rate": 4.876654377496568e-05,
      "loss": 3.6828,
      "step": 559500
    },
    {
      "epoch": 0.09869431095021845,
      "grad_norm": 11.94737434387207,
      "learning_rate": 4.876632331769498e-05,
      "loss": 3.5402,
      "step": 559600
    },
    {
      "epoch": 0.09871194753187504,
      "grad_norm": 9.777933120727539,
      "learning_rate": 4.876610286042427e-05,
      "loss": 3.6358,
      "step": 559700
    },
    {
      "epoch": 0.09872958411353161,
      "grad_norm": 6.30919075012207,
      "learning_rate": 4.876588240315356e-05,
      "loss": 3.5849,
      "step": 559800
    },
    {
      "epoch": 0.0987472206951882,
      "grad_norm": 6.862438201904297,
      "learning_rate": 4.876566194588286e-05,
      "loss": 3.6221,
      "step": 559900
    },
    {
      "epoch": 0.09876485727684477,
      "grad_norm": 6.052248001098633,
      "learning_rate": 4.876544148861215e-05,
      "loss": 3.6468,
      "step": 560000
    },
    {
      "epoch": 0.09878249385850135,
      "grad_norm": 8.201239585876465,
      "learning_rate": 4.876522103134144e-05,
      "loss": 3.6711,
      "step": 560100
    },
    {
      "epoch": 0.09880013044015794,
      "grad_norm": 8.831830978393555,
      "learning_rate": 4.876500057407074e-05,
      "loss": 3.6614,
      "step": 560200
    },
    {
      "epoch": 0.09881776702181451,
      "grad_norm": 6.421726703643799,
      "learning_rate": 4.876478011680003e-05,
      "loss": 3.6029,
      "step": 560300
    },
    {
      "epoch": 0.0988354036034711,
      "grad_norm": 15.854454040527344,
      "learning_rate": 4.876455965952932e-05,
      "loss": 3.5648,
      "step": 560400
    },
    {
      "epoch": 0.09885304018512767,
      "grad_norm": 10.16082763671875,
      "learning_rate": 4.876433920225861e-05,
      "loss": 3.6325,
      "step": 560500
    },
    {
      "epoch": 0.09887067676678425,
      "grad_norm": 6.024779796600342,
      "learning_rate": 4.8764118744987905e-05,
      "loss": 3.6729,
      "step": 560600
    },
    {
      "epoch": 0.09888831334844082,
      "grad_norm": 8.007024765014648,
      "learning_rate": 4.8763898287717194e-05,
      "loss": 3.6024,
      "step": 560700
    },
    {
      "epoch": 0.09890594993009741,
      "grad_norm": 8.402580261230469,
      "learning_rate": 4.876367783044649e-05,
      "loss": 3.6443,
      "step": 560800
    },
    {
      "epoch": 0.098923586511754,
      "grad_norm": 6.711508750915527,
      "learning_rate": 4.8763457373175785e-05,
      "loss": 3.619,
      "step": 560900
    },
    {
      "epoch": 0.09894122309341057,
      "grad_norm": 8.381183624267578,
      "learning_rate": 4.8763236915905074e-05,
      "loss": 3.6316,
      "step": 561000
    },
    {
      "epoch": 0.09895885967506715,
      "grad_norm": 7.0695881843566895,
      "learning_rate": 4.876301645863437e-05,
      "loss": 3.6255,
      "step": 561100
    },
    {
      "epoch": 0.09897649625672372,
      "grad_norm": 6.5747551918029785,
      "learning_rate": 4.8762796001363665e-05,
      "loss": 3.6995,
      "step": 561200
    },
    {
      "epoch": 0.09899413283838031,
      "grad_norm": 7.174405097961426,
      "learning_rate": 4.876257554409295e-05,
      "loss": 3.6356,
      "step": 561300
    },
    {
      "epoch": 0.09901176942003688,
      "grad_norm": 10.662495613098145,
      "learning_rate": 4.876235508682225e-05,
      "loss": 3.6325,
      "step": 561400
    },
    {
      "epoch": 0.09902940600169347,
      "grad_norm": 8.591843605041504,
      "learning_rate": 4.8762134629551544e-05,
      "loss": 3.6876,
      "step": 561500
    },
    {
      "epoch": 0.09904704258335005,
      "grad_norm": 8.052159309387207,
      "learning_rate": 4.876191417228083e-05,
      "loss": 3.6191,
      "step": 561600
    },
    {
      "epoch": 0.09906467916500662,
      "grad_norm": 7.764841079711914,
      "learning_rate": 4.876169371501013e-05,
      "loss": 3.6527,
      "step": 561700
    },
    {
      "epoch": 0.09908231574666321,
      "grad_norm": 5.872554302215576,
      "learning_rate": 4.8761473257739424e-05,
      "loss": 3.6807,
      "step": 561800
    },
    {
      "epoch": 0.09909995232831978,
      "grad_norm": 6.2356696128845215,
      "learning_rate": 4.876125280046871e-05,
      "loss": 3.6914,
      "step": 561900
    },
    {
      "epoch": 0.09911758890997636,
      "grad_norm": 6.80739164352417,
      "learning_rate": 4.8761032343198e-05,
      "loss": 3.6609,
      "step": 562000
    },
    {
      "epoch": 0.09913522549163294,
      "grad_norm": 7.398368835449219,
      "learning_rate": 4.87608118859273e-05,
      "loss": 3.5529,
      "step": 562100
    },
    {
      "epoch": 0.09915286207328952,
      "grad_norm": 6.556577682495117,
      "learning_rate": 4.8760591428656585e-05,
      "loss": 3.6831,
      "step": 562200
    },
    {
      "epoch": 0.0991704986549461,
      "grad_norm": 8.197029113769531,
      "learning_rate": 4.876037097138588e-05,
      "loss": 3.6664,
      "step": 562300
    },
    {
      "epoch": 0.09918813523660268,
      "grad_norm": 8.246670722961426,
      "learning_rate": 4.8760150514115176e-05,
      "loss": 3.6116,
      "step": 562400
    },
    {
      "epoch": 0.09920577181825926,
      "grad_norm": 6.805668830871582,
      "learning_rate": 4.8759930056844465e-05,
      "loss": 3.6179,
      "step": 562500
    },
    {
      "epoch": 0.09922340839991584,
      "grad_norm": 7.338559627532959,
      "learning_rate": 4.875970959957376e-05,
      "loss": 3.6685,
      "step": 562600
    },
    {
      "epoch": 0.09924104498157242,
      "grad_norm": 9.26170825958252,
      "learning_rate": 4.8759489142303056e-05,
      "loss": 3.5705,
      "step": 562700
    },
    {
      "epoch": 0.09925868156322899,
      "grad_norm": 8.388228416442871,
      "learning_rate": 4.8759268685032345e-05,
      "loss": 3.6112,
      "step": 562800
    },
    {
      "epoch": 0.09927631814488558,
      "grad_norm": 6.7832465171813965,
      "learning_rate": 4.875904822776164e-05,
      "loss": 3.5188,
      "step": 562900
    },
    {
      "epoch": 0.09929395472654215,
      "grad_norm": 9.204217910766602,
      "learning_rate": 4.8758827770490936e-05,
      "loss": 3.6513,
      "step": 563000
    },
    {
      "epoch": 0.09931159130819874,
      "grad_norm": 7.589408874511719,
      "learning_rate": 4.8758607313220224e-05,
      "loss": 3.761,
      "step": 563100
    },
    {
      "epoch": 0.09932922788985532,
      "grad_norm": 11.00493049621582,
      "learning_rate": 4.875838685594952e-05,
      "loss": 3.6648,
      "step": 563200
    },
    {
      "epoch": 0.09934686447151189,
      "grad_norm": 8.760860443115234,
      "learning_rate": 4.875816639867881e-05,
      "loss": 3.6397,
      "step": 563300
    },
    {
      "epoch": 0.09936450105316848,
      "grad_norm": 7.16005802154541,
      "learning_rate": 4.8757945941408104e-05,
      "loss": 3.6372,
      "step": 563400
    },
    {
      "epoch": 0.09938213763482505,
      "grad_norm": 6.491812705993652,
      "learning_rate": 4.875772548413739e-05,
      "loss": 3.5996,
      "step": 563500
    },
    {
      "epoch": 0.09939977421648163,
      "grad_norm": 6.988666534423828,
      "learning_rate": 4.875750502686669e-05,
      "loss": 3.5947,
      "step": 563600
    },
    {
      "epoch": 0.0994174107981382,
      "grad_norm": 8.391753196716309,
      "learning_rate": 4.875728456959598e-05,
      "loss": 3.5298,
      "step": 563700
    },
    {
      "epoch": 0.09943504737979479,
      "grad_norm": 8.141990661621094,
      "learning_rate": 4.875706411232527e-05,
      "loss": 3.654,
      "step": 563800
    },
    {
      "epoch": 0.09945268396145138,
      "grad_norm": 7.444787979125977,
      "learning_rate": 4.875684365505457e-05,
      "loss": 3.6174,
      "step": 563900
    },
    {
      "epoch": 0.09947032054310795,
      "grad_norm": 6.336802005767822,
      "learning_rate": 4.875662319778386e-05,
      "loss": 3.6096,
      "step": 564000
    },
    {
      "epoch": 0.09948795712476453,
      "grad_norm": 8.125182151794434,
      "learning_rate": 4.875640274051315e-05,
      "loss": 3.6745,
      "step": 564100
    },
    {
      "epoch": 0.0995055937064211,
      "grad_norm": 7.997867584228516,
      "learning_rate": 4.875618228324245e-05,
      "loss": 3.7028,
      "step": 564200
    },
    {
      "epoch": 0.09952323028807769,
      "grad_norm": 6.468194961547852,
      "learning_rate": 4.875596182597174e-05,
      "loss": 3.5364,
      "step": 564300
    },
    {
      "epoch": 0.09954086686973426,
      "grad_norm": 5.815331935882568,
      "learning_rate": 4.875574136870103e-05,
      "loss": 3.678,
      "step": 564400
    },
    {
      "epoch": 0.09955850345139085,
      "grad_norm": 9.227461814880371,
      "learning_rate": 4.875552091143033e-05,
      "loss": 3.6786,
      "step": 564500
    },
    {
      "epoch": 0.09957614003304743,
      "grad_norm": 8.969780921936035,
      "learning_rate": 4.875530045415962e-05,
      "loss": 3.5392,
      "step": 564600
    },
    {
      "epoch": 0.099593776614704,
      "grad_norm": 4.921096324920654,
      "learning_rate": 4.875507999688891e-05,
      "loss": 3.652,
      "step": 564700
    },
    {
      "epoch": 0.09961141319636059,
      "grad_norm": 8.112944602966309,
      "learning_rate": 4.87548595396182e-05,
      "loss": 3.6549,
      "step": 564800
    },
    {
      "epoch": 0.09962904977801716,
      "grad_norm": 5.586896896362305,
      "learning_rate": 4.8754639082347495e-05,
      "loss": 3.5746,
      "step": 564900
    },
    {
      "epoch": 0.09964668635967375,
      "grad_norm": 5.991020679473877,
      "learning_rate": 4.8754418625076784e-05,
      "loss": 3.5281,
      "step": 565000
    },
    {
      "epoch": 0.09966432294133032,
      "grad_norm": 12.158223152160645,
      "learning_rate": 4.875419816780608e-05,
      "loss": 3.6832,
      "step": 565100
    },
    {
      "epoch": 0.0996819595229869,
      "grad_norm": 8.733779907226562,
      "learning_rate": 4.8753977710535375e-05,
      "loss": 3.6499,
      "step": 565200
    },
    {
      "epoch": 0.09969959610464348,
      "grad_norm": 6.1073408126831055,
      "learning_rate": 4.8753757253264664e-05,
      "loss": 3.604,
      "step": 565300
    },
    {
      "epoch": 0.09971723268630006,
      "grad_norm": 7.3256635665893555,
      "learning_rate": 4.875353679599396e-05,
      "loss": 3.6972,
      "step": 565400
    },
    {
      "epoch": 0.09973486926795665,
      "grad_norm": 13.145286560058594,
      "learning_rate": 4.8753316338723255e-05,
      "loss": 3.6172,
      "step": 565500
    },
    {
      "epoch": 0.09975250584961322,
      "grad_norm": 9.088397026062012,
      "learning_rate": 4.875309588145254e-05,
      "loss": 3.5299,
      "step": 565600
    },
    {
      "epoch": 0.0997701424312698,
      "grad_norm": 5.821488857269287,
      "learning_rate": 4.875287542418184e-05,
      "loss": 3.6885,
      "step": 565700
    },
    {
      "epoch": 0.09978777901292638,
      "grad_norm": 6.330250263214111,
      "learning_rate": 4.8752654966911134e-05,
      "loss": 3.6599,
      "step": 565800
    },
    {
      "epoch": 0.09980541559458296,
      "grad_norm": 6.85353946685791,
      "learning_rate": 4.875243450964042e-05,
      "loss": 3.5872,
      "step": 565900
    },
    {
      "epoch": 0.09982305217623953,
      "grad_norm": 6.106700897216797,
      "learning_rate": 4.875221405236972e-05,
      "loss": 3.7283,
      "step": 566000
    },
    {
      "epoch": 0.09984068875789612,
      "grad_norm": 7.189019203186035,
      "learning_rate": 4.875199359509901e-05,
      "loss": 3.666,
      "step": 566100
    },
    {
      "epoch": 0.0998583253395527,
      "grad_norm": 8.515417098999023,
      "learning_rate": 4.87517731378283e-05,
      "loss": 3.6831,
      "step": 566200
    },
    {
      "epoch": 0.09987596192120927,
      "grad_norm": 5.596493721008301,
      "learning_rate": 4.875155268055759e-05,
      "loss": 3.6074,
      "step": 566300
    },
    {
      "epoch": 0.09989359850286586,
      "grad_norm": 8.672445297241211,
      "learning_rate": 4.875133222328689e-05,
      "loss": 3.5773,
      "step": 566400
    },
    {
      "epoch": 0.09991123508452243,
      "grad_norm": 6.815729141235352,
      "learning_rate": 4.8751111766016175e-05,
      "loss": 3.6997,
      "step": 566500
    },
    {
      "epoch": 0.09992887166617902,
      "grad_norm": 7.81138277053833,
      "learning_rate": 4.875089130874547e-05,
      "loss": 3.5779,
      "step": 566600
    },
    {
      "epoch": 0.09994650824783559,
      "grad_norm": 6.7010579109191895,
      "learning_rate": 4.8750670851474766e-05,
      "loss": 3.5284,
      "step": 566700
    },
    {
      "epoch": 0.09996414482949217,
      "grad_norm": 9.385978698730469,
      "learning_rate": 4.8750450394204055e-05,
      "loss": 3.5819,
      "step": 566800
    },
    {
      "epoch": 0.09998178141114876,
      "grad_norm": 6.180106163024902,
      "learning_rate": 4.875022993693335e-05,
      "loss": 3.6019,
      "step": 566900
    },
    {
      "epoch": 0.09999941799280533,
      "grad_norm": 6.408753871917725,
      "learning_rate": 4.8750009479662646e-05,
      "loss": 3.7093,
      "step": 567000
    },
    {
      "epoch": 0.10001705457446192,
      "grad_norm": 7.7342352867126465,
      "learning_rate": 4.8749789022391935e-05,
      "loss": 3.6413,
      "step": 567100
    },
    {
      "epoch": 0.10003469115611849,
      "grad_norm": 6.796918869018555,
      "learning_rate": 4.874956856512123e-05,
      "loss": 3.68,
      "step": 567200
    },
    {
      "epoch": 0.10005232773777507,
      "grad_norm": 18.697853088378906,
      "learning_rate": 4.8749348107850526e-05,
      "loss": 3.6997,
      "step": 567300
    },
    {
      "epoch": 0.10006996431943165,
      "grad_norm": 6.154725074768066,
      "learning_rate": 4.8749127650579814e-05,
      "loss": 3.5525,
      "step": 567400
    },
    {
      "epoch": 0.10008760090108823,
      "grad_norm": 14.723370552062988,
      "learning_rate": 4.874890719330911e-05,
      "loss": 3.6395,
      "step": 567500
    },
    {
      "epoch": 0.10010523748274482,
      "grad_norm": 9.341146469116211,
      "learning_rate": 4.87486867360384e-05,
      "loss": 3.6874,
      "step": 567600
    },
    {
      "epoch": 0.10012287406440139,
      "grad_norm": 8.793697357177734,
      "learning_rate": 4.8748466278767694e-05,
      "loss": 3.6607,
      "step": 567700
    },
    {
      "epoch": 0.10014051064605797,
      "grad_norm": 6.535449504852295,
      "learning_rate": 4.874824582149698e-05,
      "loss": 3.7363,
      "step": 567800
    },
    {
      "epoch": 0.10015814722771454,
      "grad_norm": 7.3486809730529785,
      "learning_rate": 4.874802536422628e-05,
      "loss": 3.7084,
      "step": 567900
    },
    {
      "epoch": 0.10017578380937113,
      "grad_norm": 6.206630706787109,
      "learning_rate": 4.874780490695557e-05,
      "loss": 3.5215,
      "step": 568000
    },
    {
      "epoch": 0.1001934203910277,
      "grad_norm": 8.320652961730957,
      "learning_rate": 4.874758444968486e-05,
      "loss": 3.7454,
      "step": 568100
    },
    {
      "epoch": 0.10021105697268429,
      "grad_norm": 10.155586242675781,
      "learning_rate": 4.874736399241416e-05,
      "loss": 3.442,
      "step": 568200
    },
    {
      "epoch": 0.10022869355434086,
      "grad_norm": 8.574193954467773,
      "learning_rate": 4.8747143535143446e-05,
      "loss": 3.5957,
      "step": 568300
    },
    {
      "epoch": 0.10024633013599744,
      "grad_norm": 8.607892990112305,
      "learning_rate": 4.874692307787274e-05,
      "loss": 3.5922,
      "step": 568400
    },
    {
      "epoch": 0.10026396671765403,
      "grad_norm": 6.50797176361084,
      "learning_rate": 4.874670262060204e-05,
      "loss": 3.6076,
      "step": 568500
    },
    {
      "epoch": 0.1002816032993106,
      "grad_norm": 6.50783634185791,
      "learning_rate": 4.8746482163331326e-05,
      "loss": 3.7321,
      "step": 568600
    },
    {
      "epoch": 0.10029923988096719,
      "grad_norm": 8.163576126098633,
      "learning_rate": 4.874626170606062e-05,
      "loss": 3.6762,
      "step": 568700
    },
    {
      "epoch": 0.10031687646262376,
      "grad_norm": 8.455854415893555,
      "learning_rate": 4.874604124878992e-05,
      "loss": 3.604,
      "step": 568800
    },
    {
      "epoch": 0.10033451304428034,
      "grad_norm": 5.743656635284424,
      "learning_rate": 4.8745820791519206e-05,
      "loss": 3.6664,
      "step": 568900
    },
    {
      "epoch": 0.10035214962593692,
      "grad_norm": 6.5741143226623535,
      "learning_rate": 4.87456003342485e-05,
      "loss": 3.6713,
      "step": 569000
    },
    {
      "epoch": 0.1003697862075935,
      "grad_norm": 6.217062473297119,
      "learning_rate": 4.874537987697779e-05,
      "loss": 3.505,
      "step": 569100
    },
    {
      "epoch": 0.10038742278925009,
      "grad_norm": 6.444364070892334,
      "learning_rate": 4.874515941970708e-05,
      "loss": 3.7287,
      "step": 569200
    },
    {
      "epoch": 0.10040505937090666,
      "grad_norm": 6.672305583953857,
      "learning_rate": 4.8744938962436374e-05,
      "loss": 3.6053,
      "step": 569300
    },
    {
      "epoch": 0.10042269595256324,
      "grad_norm": 7.8823113441467285,
      "learning_rate": 4.874471850516567e-05,
      "loss": 3.55,
      "step": 569400
    },
    {
      "epoch": 0.10044033253421981,
      "grad_norm": 6.8703789710998535,
      "learning_rate": 4.874449804789496e-05,
      "loss": 3.7419,
      "step": 569500
    },
    {
      "epoch": 0.1004579691158764,
      "grad_norm": 7.286963939666748,
      "learning_rate": 4.8744277590624254e-05,
      "loss": 3.707,
      "step": 569600
    },
    {
      "epoch": 0.10047560569753297,
      "grad_norm": 6.335760593414307,
      "learning_rate": 4.874405713335355e-05,
      "loss": 3.5813,
      "step": 569700
    },
    {
      "epoch": 0.10049324227918956,
      "grad_norm": 8.540456771850586,
      "learning_rate": 4.874383667608284e-05,
      "loss": 3.6209,
      "step": 569800
    },
    {
      "epoch": 0.10051087886084614,
      "grad_norm": 5.6620354652404785,
      "learning_rate": 4.874361621881213e-05,
      "loss": 3.493,
      "step": 569900
    },
    {
      "epoch": 0.10052851544250271,
      "grad_norm": 6.827848434448242,
      "learning_rate": 4.874339576154143e-05,
      "loss": 3.5779,
      "step": 570000
    },
    {
      "epoch": 0.1005461520241593,
      "grad_norm": 6.808039665222168,
      "learning_rate": 4.874317530427072e-05,
      "loss": 3.573,
      "step": 570100
    },
    {
      "epoch": 0.10056378860581587,
      "grad_norm": 6.312007904052734,
      "learning_rate": 4.874295484700001e-05,
      "loss": 3.6192,
      "step": 570200
    },
    {
      "epoch": 0.10058142518747246,
      "grad_norm": 9.102198600769043,
      "learning_rate": 4.874273438972931e-05,
      "loss": 3.6386,
      "step": 570300
    },
    {
      "epoch": 0.10059906176912903,
      "grad_norm": 8.96749496459961,
      "learning_rate": 4.87425139324586e-05,
      "loss": 3.6689,
      "step": 570400
    },
    {
      "epoch": 0.10061669835078561,
      "grad_norm": 6.859628677368164,
      "learning_rate": 4.8742293475187886e-05,
      "loss": 3.6321,
      "step": 570500
    },
    {
      "epoch": 0.1006343349324422,
      "grad_norm": 7.325776100158691,
      "learning_rate": 4.874207301791718e-05,
      "loss": 3.5296,
      "step": 570600
    },
    {
      "epoch": 0.10065197151409877,
      "grad_norm": 8.71201229095459,
      "learning_rate": 4.874185256064647e-05,
      "loss": 3.5779,
      "step": 570700
    },
    {
      "epoch": 0.10066960809575536,
      "grad_norm": 8.939055442810059,
      "learning_rate": 4.8741632103375765e-05,
      "loss": 3.5997,
      "step": 570800
    },
    {
      "epoch": 0.10068724467741193,
      "grad_norm": 5.603756427764893,
      "learning_rate": 4.874141164610506e-05,
      "loss": 3.6041,
      "step": 570900
    },
    {
      "epoch": 0.10070488125906851,
      "grad_norm": 8.390204429626465,
      "learning_rate": 4.874119118883435e-05,
      "loss": 3.6148,
      "step": 571000
    },
    {
      "epoch": 0.10072251784072508,
      "grad_norm": 6.438600540161133,
      "learning_rate": 4.8740970731563645e-05,
      "loss": 3.5992,
      "step": 571100
    },
    {
      "epoch": 0.10074015442238167,
      "grad_norm": 7.332417964935303,
      "learning_rate": 4.874075027429294e-05,
      "loss": 3.7981,
      "step": 571200
    },
    {
      "epoch": 0.10075779100403824,
      "grad_norm": 9.072246551513672,
      "learning_rate": 4.874052981702223e-05,
      "loss": 3.604,
      "step": 571300
    },
    {
      "epoch": 0.10077542758569483,
      "grad_norm": 9.873640060424805,
      "learning_rate": 4.8740309359751525e-05,
      "loss": 3.6924,
      "step": 571400
    },
    {
      "epoch": 0.10079306416735141,
      "grad_norm": 7.7459397315979,
      "learning_rate": 4.874008890248082e-05,
      "loss": 3.594,
      "step": 571500
    },
    {
      "epoch": 0.10081070074900798,
      "grad_norm": 8.569456100463867,
      "learning_rate": 4.873986844521011e-05,
      "loss": 3.6137,
      "step": 571600
    },
    {
      "epoch": 0.10082833733066457,
      "grad_norm": 6.396213054656982,
      "learning_rate": 4.8739647987939404e-05,
      "loss": 3.6501,
      "step": 571700
    },
    {
      "epoch": 0.10084597391232114,
      "grad_norm": 8.854666709899902,
      "learning_rate": 4.87394275306687e-05,
      "loss": 3.694,
      "step": 571800
    },
    {
      "epoch": 0.10086361049397773,
      "grad_norm": 6.005313873291016,
      "learning_rate": 4.873920707339799e-05,
      "loss": 3.6491,
      "step": 571900
    },
    {
      "epoch": 0.1008812470756343,
      "grad_norm": 5.557247638702393,
      "learning_rate": 4.873898661612728e-05,
      "loss": 3.6619,
      "step": 572000
    },
    {
      "epoch": 0.10089888365729088,
      "grad_norm": 7.154025554656982,
      "learning_rate": 4.873876615885657e-05,
      "loss": 3.6276,
      "step": 572100
    },
    {
      "epoch": 0.10091652023894747,
      "grad_norm": 12.296327590942383,
      "learning_rate": 4.873854570158586e-05,
      "loss": 3.6234,
      "step": 572200
    },
    {
      "epoch": 0.10093415682060404,
      "grad_norm": 7.029384613037109,
      "learning_rate": 4.873832524431516e-05,
      "loss": 3.6001,
      "step": 572300
    },
    {
      "epoch": 0.10095179340226063,
      "grad_norm": 6.27648401260376,
      "learning_rate": 4.873810478704445e-05,
      "loss": 3.6056,
      "step": 572400
    },
    {
      "epoch": 0.1009694299839172,
      "grad_norm": 7.863378047943115,
      "learning_rate": 4.873788432977374e-05,
      "loss": 3.5419,
      "step": 572500
    },
    {
      "epoch": 0.10098706656557378,
      "grad_norm": 6.387692928314209,
      "learning_rate": 4.8737663872503036e-05,
      "loss": 3.6293,
      "step": 572600
    },
    {
      "epoch": 0.10100470314723035,
      "grad_norm": 7.24112606048584,
      "learning_rate": 4.873744341523233e-05,
      "loss": 3.5228,
      "step": 572700
    },
    {
      "epoch": 0.10102233972888694,
      "grad_norm": 8.100357055664062,
      "learning_rate": 4.873722295796162e-05,
      "loss": 3.7259,
      "step": 572800
    },
    {
      "epoch": 0.10103997631054353,
      "grad_norm": 7.778697490692139,
      "learning_rate": 4.8737002500690916e-05,
      "loss": 3.4959,
      "step": 572900
    },
    {
      "epoch": 0.1010576128922001,
      "grad_norm": 10.048491477966309,
      "learning_rate": 4.873678204342021e-05,
      "loss": 3.5504,
      "step": 573000
    },
    {
      "epoch": 0.10107524947385668,
      "grad_norm": 7.151803493499756,
      "learning_rate": 4.87365615861495e-05,
      "loss": 3.5265,
      "step": 573100
    },
    {
      "epoch": 0.10109288605551325,
      "grad_norm": 6.871238708496094,
      "learning_rate": 4.8736341128878796e-05,
      "loss": 3.6688,
      "step": 573200
    },
    {
      "epoch": 0.10111052263716984,
      "grad_norm": 9.459750175476074,
      "learning_rate": 4.8736120671608084e-05,
      "loss": 3.6602,
      "step": 573300
    },
    {
      "epoch": 0.10112815921882641,
      "grad_norm": 7.53466796875,
      "learning_rate": 4.873590021433738e-05,
      "loss": 3.5866,
      "step": 573400
    },
    {
      "epoch": 0.101145795800483,
      "grad_norm": 6.389379501342773,
      "learning_rate": 4.873567975706667e-05,
      "loss": 3.606,
      "step": 573500
    },
    {
      "epoch": 0.10116343238213958,
      "grad_norm": 9.358428955078125,
      "learning_rate": 4.8735459299795964e-05,
      "loss": 3.6245,
      "step": 573600
    },
    {
      "epoch": 0.10118106896379615,
      "grad_norm": 6.410862922668457,
      "learning_rate": 4.873523884252525e-05,
      "loss": 3.6868,
      "step": 573700
    },
    {
      "epoch": 0.10119870554545274,
      "grad_norm": 8.757438659667969,
      "learning_rate": 4.873501838525455e-05,
      "loss": 3.669,
      "step": 573800
    },
    {
      "epoch": 0.10121634212710931,
      "grad_norm": 9.858297348022461,
      "learning_rate": 4.8734797927983843e-05,
      "loss": 3.6027,
      "step": 573900
    },
    {
      "epoch": 0.1012339787087659,
      "grad_norm": 10.402164459228516,
      "learning_rate": 4.873457747071313e-05,
      "loss": 3.6511,
      "step": 574000
    },
    {
      "epoch": 0.10125161529042247,
      "grad_norm": 7.238433837890625,
      "learning_rate": 4.873435701344243e-05,
      "loss": 3.6113,
      "step": 574100
    },
    {
      "epoch": 0.10126925187207905,
      "grad_norm": 10.760266304016113,
      "learning_rate": 4.873413655617172e-05,
      "loss": 3.6558,
      "step": 574200
    },
    {
      "epoch": 0.10128688845373562,
      "grad_norm": 10.253188133239746,
      "learning_rate": 4.873391609890101e-05,
      "loss": 3.6746,
      "step": 574300
    },
    {
      "epoch": 0.10130452503539221,
      "grad_norm": 7.185088634490967,
      "learning_rate": 4.873369564163031e-05,
      "loss": 3.6101,
      "step": 574400
    },
    {
      "epoch": 0.1013221616170488,
      "grad_norm": 9.827237129211426,
      "learning_rate": 4.87334751843596e-05,
      "loss": 3.6934,
      "step": 574500
    },
    {
      "epoch": 0.10133979819870537,
      "grad_norm": 5.763982772827148,
      "learning_rate": 4.87332547270889e-05,
      "loss": 3.6099,
      "step": 574600
    },
    {
      "epoch": 0.10135743478036195,
      "grad_norm": 11.555986404418945,
      "learning_rate": 4.873303426981819e-05,
      "loss": 3.6394,
      "step": 574700
    },
    {
      "epoch": 0.10137507136201852,
      "grad_norm": 9.741105079650879,
      "learning_rate": 4.8732813812547476e-05,
      "loss": 3.7513,
      "step": 574800
    },
    {
      "epoch": 0.10139270794367511,
      "grad_norm": 5.6015095710754395,
      "learning_rate": 4.873259335527677e-05,
      "loss": 3.6812,
      "step": 574900
    },
    {
      "epoch": 0.10141034452533168,
      "grad_norm": 6.693880081176758,
      "learning_rate": 4.873237289800606e-05,
      "loss": 3.6078,
      "step": 575000
    },
    {
      "epoch": 0.10142798110698827,
      "grad_norm": 9.77143383026123,
      "learning_rate": 4.8732152440735355e-05,
      "loss": 3.5745,
      "step": 575100
    },
    {
      "epoch": 0.10144561768864485,
      "grad_norm": 5.748747825622559,
      "learning_rate": 4.873193198346465e-05,
      "loss": 3.7711,
      "step": 575200
    },
    {
      "epoch": 0.10146325427030142,
      "grad_norm": 6.535870552062988,
      "learning_rate": 4.873171152619394e-05,
      "loss": 3.5887,
      "step": 575300
    },
    {
      "epoch": 0.10148089085195801,
      "grad_norm": 7.315760612487793,
      "learning_rate": 4.8731491068923235e-05,
      "loss": 3.6802,
      "step": 575400
    },
    {
      "epoch": 0.10149852743361458,
      "grad_norm": 7.079172134399414,
      "learning_rate": 4.873127061165253e-05,
      "loss": 3.6225,
      "step": 575500
    },
    {
      "epoch": 0.10151616401527117,
      "grad_norm": 5.556293964385986,
      "learning_rate": 4.873105015438182e-05,
      "loss": 3.8158,
      "step": 575600
    },
    {
      "epoch": 0.10153380059692774,
      "grad_norm": 11.23832893371582,
      "learning_rate": 4.8730829697111114e-05,
      "loss": 3.6168,
      "step": 575700
    },
    {
      "epoch": 0.10155143717858432,
      "grad_norm": 8.165277481079102,
      "learning_rate": 4.873060923984041e-05,
      "loss": 3.6296,
      "step": 575800
    },
    {
      "epoch": 0.10156907376024091,
      "grad_norm": 9.569039344787598,
      "learning_rate": 4.87303887825697e-05,
      "loss": 3.539,
      "step": 575900
    },
    {
      "epoch": 0.10158671034189748,
      "grad_norm": 6.755996227264404,
      "learning_rate": 4.8730168325298994e-05,
      "loss": 3.5702,
      "step": 576000
    },
    {
      "epoch": 0.10160434692355406,
      "grad_norm": 7.3895087242126465,
      "learning_rate": 4.872994786802828e-05,
      "loss": 3.5865,
      "step": 576100
    },
    {
      "epoch": 0.10162198350521064,
      "grad_norm": 7.612851619720459,
      "learning_rate": 4.872972741075758e-05,
      "loss": 3.5693,
      "step": 576200
    },
    {
      "epoch": 0.10163962008686722,
      "grad_norm": 6.320690155029297,
      "learning_rate": 4.872950695348687e-05,
      "loss": 3.6499,
      "step": 576300
    },
    {
      "epoch": 0.1016572566685238,
      "grad_norm": 7.581991672515869,
      "learning_rate": 4.872928649621616e-05,
      "loss": 3.7219,
      "step": 576400
    },
    {
      "epoch": 0.10167489325018038,
      "grad_norm": 11.322293281555176,
      "learning_rate": 4.872906603894545e-05,
      "loss": 3.6708,
      "step": 576500
    },
    {
      "epoch": 0.10169252983183696,
      "grad_norm": 7.5752644538879395,
      "learning_rate": 4.8728845581674747e-05,
      "loss": 3.6622,
      "step": 576600
    },
    {
      "epoch": 0.10171016641349354,
      "grad_norm": 8.068681716918945,
      "learning_rate": 4.872862512440404e-05,
      "loss": 3.7045,
      "step": 576700
    },
    {
      "epoch": 0.10172780299515012,
      "grad_norm": 12.098353385925293,
      "learning_rate": 4.872840466713333e-05,
      "loss": 3.6154,
      "step": 576800
    },
    {
      "epoch": 0.10174543957680669,
      "grad_norm": 6.473198890686035,
      "learning_rate": 4.8728184209862626e-05,
      "loss": 3.6071,
      "step": 576900
    },
    {
      "epoch": 0.10176307615846328,
      "grad_norm": 5.736422538757324,
      "learning_rate": 4.872796375259192e-05,
      "loss": 3.6165,
      "step": 577000
    },
    {
      "epoch": 0.10178071274011985,
      "grad_norm": 8.626646041870117,
      "learning_rate": 4.872774329532121e-05,
      "loss": 3.6756,
      "step": 577100
    },
    {
      "epoch": 0.10179834932177644,
      "grad_norm": 8.732068061828613,
      "learning_rate": 4.8727522838050506e-05,
      "loss": 3.6167,
      "step": 577200
    },
    {
      "epoch": 0.101815985903433,
      "grad_norm": 7.603446960449219,
      "learning_rate": 4.87273023807798e-05,
      "loss": 3.7599,
      "step": 577300
    },
    {
      "epoch": 0.10183362248508959,
      "grad_norm": 5.905413627624512,
      "learning_rate": 4.872708192350909e-05,
      "loss": 3.6119,
      "step": 577400
    },
    {
      "epoch": 0.10185125906674618,
      "grad_norm": 8.599793434143066,
      "learning_rate": 4.8726861466238385e-05,
      "loss": 3.6499,
      "step": 577500
    },
    {
      "epoch": 0.10186889564840275,
      "grad_norm": 7.412629127502441,
      "learning_rate": 4.8726641008967674e-05,
      "loss": 3.6329,
      "step": 577600
    },
    {
      "epoch": 0.10188653223005933,
      "grad_norm": 6.165089130401611,
      "learning_rate": 4.872642055169697e-05,
      "loss": 3.6991,
      "step": 577700
    },
    {
      "epoch": 0.1019041688117159,
      "grad_norm": 6.500020980834961,
      "learning_rate": 4.872620009442626e-05,
      "loss": 3.6142,
      "step": 577800
    },
    {
      "epoch": 0.10192180539337249,
      "grad_norm": 5.849531650543213,
      "learning_rate": 4.8725979637155554e-05,
      "loss": 3.532,
      "step": 577900
    },
    {
      "epoch": 0.10193944197502906,
      "grad_norm": 7.829914569854736,
      "learning_rate": 4.872575917988484e-05,
      "loss": 3.5993,
      "step": 578000
    },
    {
      "epoch": 0.10195707855668565,
      "grad_norm": 11.394454002380371,
      "learning_rate": 4.872553872261414e-05,
      "loss": 3.6242,
      "step": 578100
    },
    {
      "epoch": 0.10197471513834223,
      "grad_norm": 10.517884254455566,
      "learning_rate": 4.8725318265343433e-05,
      "loss": 3.5823,
      "step": 578200
    },
    {
      "epoch": 0.1019923517199988,
      "grad_norm": 6.629720687866211,
      "learning_rate": 4.872509780807272e-05,
      "loss": 3.7073,
      "step": 578300
    },
    {
      "epoch": 0.10200998830165539,
      "grad_norm": 5.6592302322387695,
      "learning_rate": 4.872487735080202e-05,
      "loss": 3.5623,
      "step": 578400
    },
    {
      "epoch": 0.10202762488331196,
      "grad_norm": 6.713660717010498,
      "learning_rate": 4.872465689353131e-05,
      "loss": 3.7252,
      "step": 578500
    },
    {
      "epoch": 0.10204526146496855,
      "grad_norm": 7.196137428283691,
      "learning_rate": 4.87244364362606e-05,
      "loss": 3.7824,
      "step": 578600
    },
    {
      "epoch": 0.10206289804662512,
      "grad_norm": 6.47169303894043,
      "learning_rate": 4.87242159789899e-05,
      "loss": 3.6598,
      "step": 578700
    },
    {
      "epoch": 0.1020805346282817,
      "grad_norm": 8.001511573791504,
      "learning_rate": 4.872399552171919e-05,
      "loss": 3.634,
      "step": 578800
    },
    {
      "epoch": 0.10209817120993829,
      "grad_norm": 8.502830505371094,
      "learning_rate": 4.872377506444848e-05,
      "loss": 3.7148,
      "step": 578900
    },
    {
      "epoch": 0.10211580779159486,
      "grad_norm": 6.853275299072266,
      "learning_rate": 4.872355460717778e-05,
      "loss": 3.635,
      "step": 579000
    },
    {
      "epoch": 0.10213344437325145,
      "grad_norm": 6.768678665161133,
      "learning_rate": 4.8723334149907066e-05,
      "loss": 3.6499,
      "step": 579100
    },
    {
      "epoch": 0.10215108095490802,
      "grad_norm": 6.634003162384033,
      "learning_rate": 4.8723113692636354e-05,
      "loss": 3.633,
      "step": 579200
    },
    {
      "epoch": 0.1021687175365646,
      "grad_norm": 6.726072311401367,
      "learning_rate": 4.872289323536565e-05,
      "loss": 3.6749,
      "step": 579300
    },
    {
      "epoch": 0.10218635411822118,
      "grad_norm": 9.959053993225098,
      "learning_rate": 4.8722672778094945e-05,
      "loss": 3.6293,
      "step": 579400
    },
    {
      "epoch": 0.10220399069987776,
      "grad_norm": 6.663880348205566,
      "learning_rate": 4.8722452320824234e-05,
      "loss": 3.6156,
      "step": 579500
    },
    {
      "epoch": 0.10222162728153435,
      "grad_norm": 11.571663856506348,
      "learning_rate": 4.872223186355353e-05,
      "loss": 3.695,
      "step": 579600
    },
    {
      "epoch": 0.10223926386319092,
      "grad_norm": 6.587597846984863,
      "learning_rate": 4.8722011406282825e-05,
      "loss": 3.6772,
      "step": 579700
    },
    {
      "epoch": 0.1022569004448475,
      "grad_norm": 10.974270820617676,
      "learning_rate": 4.8721790949012113e-05,
      "loss": 3.6487,
      "step": 579800
    },
    {
      "epoch": 0.10227453702650408,
      "grad_norm": 5.76105260848999,
      "learning_rate": 4.872157049174141e-05,
      "loss": 3.67,
      "step": 579900
    },
    {
      "epoch": 0.10229217360816066,
      "grad_norm": 7.280901908874512,
      "learning_rate": 4.8721350034470704e-05,
      "loss": 3.7081,
      "step": 580000
    },
    {
      "epoch": 0.10230981018981723,
      "grad_norm": 6.475538730621338,
      "learning_rate": 4.872112957719999e-05,
      "loss": 3.6069,
      "step": 580100
    },
    {
      "epoch": 0.10232744677147382,
      "grad_norm": 8.501315116882324,
      "learning_rate": 4.872090911992929e-05,
      "loss": 3.5404,
      "step": 580200
    },
    {
      "epoch": 0.10234508335313039,
      "grad_norm": 7.228046894073486,
      "learning_rate": 4.8720688662658584e-05,
      "loss": 3.6665,
      "step": 580300
    },
    {
      "epoch": 0.10236271993478697,
      "grad_norm": 6.1106085777282715,
      "learning_rate": 4.872046820538787e-05,
      "loss": 3.6509,
      "step": 580400
    },
    {
      "epoch": 0.10238035651644356,
      "grad_norm": 7.269428253173828,
      "learning_rate": 4.872024774811717e-05,
      "loss": 3.7204,
      "step": 580500
    },
    {
      "epoch": 0.10239799309810013,
      "grad_norm": 7.767906665802002,
      "learning_rate": 4.872002729084646e-05,
      "loss": 3.6119,
      "step": 580600
    },
    {
      "epoch": 0.10241562967975672,
      "grad_norm": 7.078530788421631,
      "learning_rate": 4.8719806833575746e-05,
      "loss": 3.6696,
      "step": 580700
    },
    {
      "epoch": 0.10243326626141329,
      "grad_norm": 8.656145095825195,
      "learning_rate": 4.871958637630504e-05,
      "loss": 3.6374,
      "step": 580800
    },
    {
      "epoch": 0.10245090284306987,
      "grad_norm": 5.58525276184082,
      "learning_rate": 4.8719365919034337e-05,
      "loss": 3.6176,
      "step": 580900
    },
    {
      "epoch": 0.10246853942472645,
      "grad_norm": 8.091761589050293,
      "learning_rate": 4.8719145461763625e-05,
      "loss": 3.5841,
      "step": 581000
    },
    {
      "epoch": 0.10248617600638303,
      "grad_norm": 8.295327186584473,
      "learning_rate": 4.871892500449292e-05,
      "loss": 3.5323,
      "step": 581100
    },
    {
      "epoch": 0.10250381258803962,
      "grad_norm": 8.54151725769043,
      "learning_rate": 4.8718704547222216e-05,
      "loss": 3.6806,
      "step": 581200
    },
    {
      "epoch": 0.10252144916969619,
      "grad_norm": 7.677383899688721,
      "learning_rate": 4.8718484089951505e-05,
      "loss": 3.6643,
      "step": 581300
    },
    {
      "epoch": 0.10253908575135277,
      "grad_norm": 9.40875244140625,
      "learning_rate": 4.87182636326808e-05,
      "loss": 3.5915,
      "step": 581400
    },
    {
      "epoch": 0.10255672233300935,
      "grad_norm": 5.731922626495361,
      "learning_rate": 4.8718043175410096e-05,
      "loss": 3.6398,
      "step": 581500
    },
    {
      "epoch": 0.10257435891466593,
      "grad_norm": 8.587109565734863,
      "learning_rate": 4.8717822718139384e-05,
      "loss": 3.5772,
      "step": 581600
    },
    {
      "epoch": 0.1025919954963225,
      "grad_norm": 6.9583916664123535,
      "learning_rate": 4.871760226086868e-05,
      "loss": 3.596,
      "step": 581700
    },
    {
      "epoch": 0.10260963207797909,
      "grad_norm": 9.471800804138184,
      "learning_rate": 4.8717381803597975e-05,
      "loss": 3.704,
      "step": 581800
    },
    {
      "epoch": 0.10262726865963567,
      "grad_norm": 7.628434181213379,
      "learning_rate": 4.8717161346327264e-05,
      "loss": 3.6856,
      "step": 581900
    },
    {
      "epoch": 0.10264490524129224,
      "grad_norm": 7.080699443817139,
      "learning_rate": 4.871694088905655e-05,
      "loss": 3.6558,
      "step": 582000
    },
    {
      "epoch": 0.10266254182294883,
      "grad_norm": 7.69987154006958,
      "learning_rate": 4.871672043178585e-05,
      "loss": 3.6336,
      "step": 582100
    },
    {
      "epoch": 0.1026801784046054,
      "grad_norm": 7.366456508636475,
      "learning_rate": 4.871649997451514e-05,
      "loss": 3.8063,
      "step": 582200
    },
    {
      "epoch": 0.10269781498626199,
      "grad_norm": 8.886734008789062,
      "learning_rate": 4.871627951724443e-05,
      "loss": 3.6743,
      "step": 582300
    },
    {
      "epoch": 0.10271545156791856,
      "grad_norm": 7.877499580383301,
      "learning_rate": 4.871605905997373e-05,
      "loss": 3.6517,
      "step": 582400
    },
    {
      "epoch": 0.10273308814957514,
      "grad_norm": 6.674941539764404,
      "learning_rate": 4.8715838602703017e-05,
      "loss": 3.6232,
      "step": 582500
    },
    {
      "epoch": 0.10275072473123173,
      "grad_norm": 7.971896171569824,
      "learning_rate": 4.871561814543231e-05,
      "loss": 3.5652,
      "step": 582600
    },
    {
      "epoch": 0.1027683613128883,
      "grad_norm": 7.385692596435547,
      "learning_rate": 4.871539768816161e-05,
      "loss": 3.5742,
      "step": 582700
    },
    {
      "epoch": 0.10278599789454489,
      "grad_norm": 8.042642593383789,
      "learning_rate": 4.8715177230890896e-05,
      "loss": 3.58,
      "step": 582800
    },
    {
      "epoch": 0.10280363447620146,
      "grad_norm": 8.704183578491211,
      "learning_rate": 4.871495677362019e-05,
      "loss": 3.613,
      "step": 582900
    },
    {
      "epoch": 0.10282127105785804,
      "grad_norm": 7.420389175415039,
      "learning_rate": 4.871473631634949e-05,
      "loss": 3.5915,
      "step": 583000
    },
    {
      "epoch": 0.10283890763951462,
      "grad_norm": 6.962486743927002,
      "learning_rate": 4.8714515859078776e-05,
      "loss": 3.6062,
      "step": 583100
    },
    {
      "epoch": 0.1028565442211712,
      "grad_norm": 8.140429496765137,
      "learning_rate": 4.871429540180807e-05,
      "loss": 3.583,
      "step": 583200
    },
    {
      "epoch": 0.10287418080282777,
      "grad_norm": 6.922443389892578,
      "learning_rate": 4.871407494453737e-05,
      "loss": 3.598,
      "step": 583300
    },
    {
      "epoch": 0.10289181738448436,
      "grad_norm": 6.66746711730957,
      "learning_rate": 4.8713854487266655e-05,
      "loss": 3.5689,
      "step": 583400
    },
    {
      "epoch": 0.10290945396614094,
      "grad_norm": 8.409470558166504,
      "learning_rate": 4.8713634029995944e-05,
      "loss": 3.5545,
      "step": 583500
    },
    {
      "epoch": 0.10292709054779751,
      "grad_norm": 7.184690475463867,
      "learning_rate": 4.871341357272524e-05,
      "loss": 3.542,
      "step": 583600
    },
    {
      "epoch": 0.1029447271294541,
      "grad_norm": 7.226692199707031,
      "learning_rate": 4.871319311545453e-05,
      "loss": 3.5907,
      "step": 583700
    },
    {
      "epoch": 0.10296236371111067,
      "grad_norm": 7.92956018447876,
      "learning_rate": 4.8712972658183824e-05,
      "loss": 3.6995,
      "step": 583800
    },
    {
      "epoch": 0.10298000029276726,
      "grad_norm": 7.708108901977539,
      "learning_rate": 4.871275220091312e-05,
      "loss": 3.6298,
      "step": 583900
    },
    {
      "epoch": 0.10299763687442383,
      "grad_norm": 7.386821269989014,
      "learning_rate": 4.871253174364241e-05,
      "loss": 3.5687,
      "step": 584000
    },
    {
      "epoch": 0.10301527345608041,
      "grad_norm": 8.845340728759766,
      "learning_rate": 4.8712311286371703e-05,
      "loss": 3.6079,
      "step": 584100
    },
    {
      "epoch": 0.103032910037737,
      "grad_norm": 6.644858360290527,
      "learning_rate": 4.8712090829101e-05,
      "loss": 3.7371,
      "step": 584200
    },
    {
      "epoch": 0.10305054661939357,
      "grad_norm": 7.37863302230835,
      "learning_rate": 4.871187037183029e-05,
      "loss": 3.5263,
      "step": 584300
    },
    {
      "epoch": 0.10306818320105016,
      "grad_norm": 6.412471294403076,
      "learning_rate": 4.871164991455958e-05,
      "loss": 3.4652,
      "step": 584400
    },
    {
      "epoch": 0.10308581978270673,
      "grad_norm": 8.509477615356445,
      "learning_rate": 4.871142945728888e-05,
      "loss": 3.6509,
      "step": 584500
    },
    {
      "epoch": 0.10310345636436331,
      "grad_norm": 8.671660423278809,
      "learning_rate": 4.871120900001817e-05,
      "loss": 3.7175,
      "step": 584600
    },
    {
      "epoch": 0.10312109294601988,
      "grad_norm": 8.908832550048828,
      "learning_rate": 4.871098854274746e-05,
      "loss": 3.5948,
      "step": 584700
    },
    {
      "epoch": 0.10313872952767647,
      "grad_norm": 5.824886798858643,
      "learning_rate": 4.871076808547675e-05,
      "loss": 3.6216,
      "step": 584800
    },
    {
      "epoch": 0.10315636610933306,
      "grad_norm": 5.7887797355651855,
      "learning_rate": 4.871054762820605e-05,
      "loss": 3.6296,
      "step": 584900
    },
    {
      "epoch": 0.10317400269098963,
      "grad_norm": 6.769430637359619,
      "learning_rate": 4.8710327170935336e-05,
      "loss": 3.6596,
      "step": 585000
    },
    {
      "epoch": 0.10319163927264621,
      "grad_norm": 8.580361366271973,
      "learning_rate": 4.871010671366463e-05,
      "loss": 3.5523,
      "step": 585100
    },
    {
      "epoch": 0.10320927585430278,
      "grad_norm": 7.149285793304443,
      "learning_rate": 4.8709886256393926e-05,
      "loss": 3.6233,
      "step": 585200
    },
    {
      "epoch": 0.10322691243595937,
      "grad_norm": 7.0707855224609375,
      "learning_rate": 4.8709665799123215e-05,
      "loss": 3.5696,
      "step": 585300
    },
    {
      "epoch": 0.10324454901761594,
      "grad_norm": 10.684657096862793,
      "learning_rate": 4.870944534185251e-05,
      "loss": 3.6519,
      "step": 585400
    },
    {
      "epoch": 0.10326218559927253,
      "grad_norm": 5.843988418579102,
      "learning_rate": 4.8709224884581806e-05,
      "loss": 3.6836,
      "step": 585500
    },
    {
      "epoch": 125.55746140651802,
      "grad_norm": 4.725229263305664,
      "learning_rate": 1.3400062500000001e-05,
      "loss": 4.3876,
      "step": 585600
    },
    {
      "epoch": 125.57890222984562,
      "grad_norm": 4.948629856109619,
      "learning_rate": 1.33938125e-05,
      "loss": 4.3475,
      "step": 585700
    },
    {
      "epoch": 125.60034305317325,
      "grad_norm": 5.149175643920898,
      "learning_rate": 1.3387562500000001e-05,
      "loss": 4.4181,
      "step": 585800
    },
    {
      "epoch": 125.62178387650086,
      "grad_norm": 5.536757469177246,
      "learning_rate": 1.3381312500000001e-05,
      "loss": 4.3648,
      "step": 585900
    },
    {
      "epoch": 125.64322469982848,
      "grad_norm": 4.662549018859863,
      "learning_rate": 1.3375062500000003e-05,
      "loss": 4.3849,
      "step": 586000
    },
    {
      "epoch": 125.66466552315609,
      "grad_norm": 5.745605945587158,
      "learning_rate": 1.3368812499999999e-05,
      "loss": 4.4339,
      "step": 586100
    },
    {
      "epoch": 125.68610634648371,
      "grad_norm": 4.632571220397949,
      "learning_rate": 1.33625625e-05,
      "loss": 4.3472,
      "step": 586200
    },
    {
      "epoch": 125.70754716981132,
      "grad_norm": 5.449338436126709,
      "learning_rate": 1.33563125e-05,
      "loss": 4.3487,
      "step": 586300
    },
    {
      "epoch": 125.72898799313894,
      "grad_norm": 4.955025672912598,
      "learning_rate": 1.3350062500000002e-05,
      "loss": 4.4066,
      "step": 586400
    },
    {
      "epoch": 125.75042881646655,
      "grad_norm": 4.782766342163086,
      "learning_rate": 1.33438125e-05,
      "loss": 4.3149,
      "step": 586500
    },
    {
      "epoch": 125.77186963979416,
      "grad_norm": 5.563626766204834,
      "learning_rate": 1.33375625e-05,
      "loss": 4.3213,
      "step": 586600
    },
    {
      "epoch": 125.79331046312178,
      "grad_norm": 5.371592998504639,
      "learning_rate": 1.3331312500000001e-05,
      "loss": 4.3656,
      "step": 586700
    },
    {
      "epoch": 125.8147512864494,
      "grad_norm": 4.570016384124756,
      "learning_rate": 1.3325062500000001e-05,
      "loss": 4.3858,
      "step": 586800
    },
    {
      "epoch": 125.83619210977702,
      "grad_norm": 4.913315773010254,
      "learning_rate": 1.33188125e-05,
      "loss": 4.3345,
      "step": 586900
    },
    {
      "epoch": 125.85763293310463,
      "grad_norm": 5.417346000671387,
      "learning_rate": 1.3312562500000001e-05,
      "loss": 4.3546,
      "step": 587000
    },
    {
      "epoch": 125.87907375643225,
      "grad_norm": 4.5788469314575195,
      "learning_rate": 1.3306312500000001e-05,
      "loss": 4.4299,
      "step": 587100
    },
    {
      "epoch": 125.90051457975986,
      "grad_norm": 6.158863544464111,
      "learning_rate": 1.3300062500000002e-05,
      "loss": 4.3615,
      "step": 587200
    },
    {
      "epoch": 125.92195540308748,
      "grad_norm": 5.313174724578857,
      "learning_rate": 1.3293812499999999e-05,
      "loss": 4.3519,
      "step": 587300
    },
    {
      "epoch": 125.94339622641509,
      "grad_norm": 4.573226451873779,
      "learning_rate": 1.32875625e-05,
      "loss": 4.3145,
      "step": 587400
    },
    {
      "epoch": 125.96483704974271,
      "grad_norm": 5.061871528625488,
      "learning_rate": 1.32813125e-05,
      "loss": 4.3637,
      "step": 587500
    },
    {
      "epoch": 125.98627787307032,
      "grad_norm": 4.6913042068481445,
      "learning_rate": 1.3275062500000002e-05,
      "loss": 4.3099,
      "step": 587600
    },
    {
      "epoch": 126.00771869639794,
      "grad_norm": 4.902107238769531,
      "learning_rate": 1.32688125e-05,
      "loss": 4.3471,
      "step": 587700
    },
    {
      "epoch": 126.02915951972555,
      "grad_norm": 4.8603596687316895,
      "learning_rate": 1.32625625e-05,
      "loss": 4.3541,
      "step": 587800
    },
    {
      "epoch": 126.05060034305318,
      "grad_norm": 4.829841613769531,
      "learning_rate": 1.3256312500000001e-05,
      "loss": 4.3841,
      "step": 587900
    },
    {
      "epoch": 126.07204116638079,
      "grad_norm": 5.018275260925293,
      "learning_rate": 1.3250062500000001e-05,
      "loss": 4.2956,
      "step": 588000
    },
    {
      "epoch": 126.09348198970841,
      "grad_norm": 5.0734992027282715,
      "learning_rate": 1.32438125e-05,
      "loss": 4.3718,
      "step": 588100
    },
    {
      "epoch": 126.11492281303602,
      "grad_norm": 5.317062854766846,
      "learning_rate": 1.3237562500000001e-05,
      "loss": 4.3762,
      "step": 588200
    },
    {
      "epoch": 126.13636363636364,
      "grad_norm": 4.8915276527404785,
      "learning_rate": 1.32313125e-05,
      "loss": 4.3607,
      "step": 588300
    },
    {
      "epoch": 126.15780445969125,
      "grad_norm": 4.595896244049072,
      "learning_rate": 1.3225062500000002e-05,
      "loss": 4.3108,
      "step": 588400
    },
    {
      "epoch": 126.17924528301887,
      "grad_norm": 4.982741832733154,
      "learning_rate": 1.32188125e-05,
      "loss": 4.3394,
      "step": 588500
    },
    {
      "epoch": 126.20068610634648,
      "grad_norm": 5.038116931915283,
      "learning_rate": 1.32125625e-05,
      "loss": 4.2954,
      "step": 588600
    },
    {
      "epoch": 126.2221269296741,
      "grad_norm": 4.903191089630127,
      "learning_rate": 1.3206312500000002e-05,
      "loss": 4.3108,
      "step": 588700
    },
    {
      "epoch": 126.24356775300171,
      "grad_norm": 5.249936580657959,
      "learning_rate": 1.3200062500000002e-05,
      "loss": 4.3435,
      "step": 588800
    },
    {
      "epoch": 126.26500857632934,
      "grad_norm": 6.261125087738037,
      "learning_rate": 1.31938125e-05,
      "loss": 4.3942,
      "step": 588900
    },
    {
      "epoch": 126.28644939965695,
      "grad_norm": 4.989310264587402,
      "learning_rate": 1.31875625e-05,
      "loss": 4.2943,
      "step": 589000
    },
    {
      "epoch": 126.30789022298457,
      "grad_norm": 4.668510437011719,
      "learning_rate": 1.3181312500000001e-05,
      "loss": 4.301,
      "step": 589100
    },
    {
      "epoch": 126.32933104631218,
      "grad_norm": 5.124311923980713,
      "learning_rate": 1.3175062500000001e-05,
      "loss": 4.3193,
      "step": 589200
    },
    {
      "epoch": 126.3507718696398,
      "grad_norm": 5.05914306640625,
      "learning_rate": 1.31688125e-05,
      "loss": 4.3906,
      "step": 589300
    },
    {
      "epoch": 126.37221269296741,
      "grad_norm": 4.985738754272461,
      "learning_rate": 1.31625625e-05,
      "loss": 4.3136,
      "step": 589400
    },
    {
      "epoch": 126.39365351629503,
      "grad_norm": 5.526599884033203,
      "learning_rate": 1.31563125e-05,
      "loss": 4.2688,
      "step": 589500
    },
    {
      "epoch": 126.41509433962264,
      "grad_norm": 5.145612716674805,
      "learning_rate": 1.3150062500000002e-05,
      "loss": 4.3332,
      "step": 589600
    },
    {
      "epoch": 126.43653516295026,
      "grad_norm": 4.884579181671143,
      "learning_rate": 1.31438125e-05,
      "loss": 4.3333,
      "step": 589700
    },
    {
      "epoch": 126.45797598627787,
      "grad_norm": 5.892326354980469,
      "learning_rate": 1.31375625e-05,
      "loss": 4.3228,
      "step": 589800
    },
    {
      "epoch": 126.47941680960548,
      "grad_norm": 5.814121723175049,
      "learning_rate": 1.3131312500000002e-05,
      "loss": 4.3817,
      "step": 589900
    },
    {
      "epoch": 126.5008576329331,
      "grad_norm": 5.824431419372559,
      "learning_rate": 1.3125062500000002e-05,
      "loss": 4.3215,
      "step": 590000
    },
    {
      "epoch": 126.52229845626071,
      "grad_norm": 4.772730350494385,
      "learning_rate": 1.31188125e-05,
      "loss": 4.3114,
      "step": 590100
    },
    {
      "epoch": 126.54373927958834,
      "grad_norm": 4.9102373123168945,
      "learning_rate": 1.31125625e-05,
      "loss": 4.3212,
      "step": 590200
    },
    {
      "epoch": 126.56518010291595,
      "grad_norm": 4.827760696411133,
      "learning_rate": 1.3106312500000001e-05,
      "loss": 4.2465,
      "step": 590300
    },
    {
      "epoch": 126.58662092624357,
      "grad_norm": 5.735234260559082,
      "learning_rate": 1.3100062500000001e-05,
      "loss": 4.3427,
      "step": 590400
    },
    {
      "epoch": 126.60806174957118,
      "grad_norm": 4.8419694900512695,
      "learning_rate": 1.3093812499999999e-05,
      "loss": 4.3354,
      "step": 590500
    },
    {
      "epoch": 126.6295025728988,
      "grad_norm": 5.1284050941467285,
      "learning_rate": 1.30875625e-05,
      "loss": 4.3327,
      "step": 590600
    },
    {
      "epoch": 126.65094339622641,
      "grad_norm": 4.980939865112305,
      "learning_rate": 1.30813125e-05,
      "loss": 4.3694,
      "step": 590700
    },
    {
      "epoch": 126.67238421955403,
      "grad_norm": 5.353923320770264,
      "learning_rate": 1.3075062500000002e-05,
      "loss": 4.3055,
      "step": 590800
    },
    {
      "epoch": 126.69382504288164,
      "grad_norm": 5.416377067565918,
      "learning_rate": 1.30688125e-05,
      "loss": 4.395,
      "step": 590900
    },
    {
      "epoch": 126.71526586620926,
      "grad_norm": 5.027549743652344,
      "learning_rate": 1.30625625e-05,
      "loss": 4.2329,
      "step": 591000
    },
    {
      "epoch": 126.73670668953687,
      "grad_norm": 5.026271820068359,
      "learning_rate": 1.3056312500000002e-05,
      "loss": 4.3023,
      "step": 591100
    },
    {
      "epoch": 126.7581475128645,
      "grad_norm": 5.360419273376465,
      "learning_rate": 1.3050062500000001e-05,
      "loss": 4.2712,
      "step": 591200
    },
    {
      "epoch": 126.7795883361921,
      "grad_norm": 4.756561756134033,
      "learning_rate": 1.30438125e-05,
      "loss": 4.2538,
      "step": 591300
    },
    {
      "epoch": 126.80102915951973,
      "grad_norm": 5.101385116577148,
      "learning_rate": 1.30375625e-05,
      "loss": 4.2628,
      "step": 591400
    },
    {
      "epoch": 126.82246998284734,
      "grad_norm": 5.28092098236084,
      "learning_rate": 1.3031312500000001e-05,
      "loss": 4.3098,
      "step": 591500
    },
    {
      "epoch": 126.84391080617496,
      "grad_norm": 4.962138652801514,
      "learning_rate": 1.30250625e-05,
      "loss": 4.3634,
      "step": 591600
    },
    {
      "epoch": 126.86535162950257,
      "grad_norm": 5.114508152008057,
      "learning_rate": 1.3018812499999999e-05,
      "loss": 4.2755,
      "step": 591700
    },
    {
      "epoch": 126.88679245283019,
      "grad_norm": 4.727991580963135,
      "learning_rate": 1.30125625e-05,
      "loss": 4.294,
      "step": 591800
    },
    {
      "epoch": 126.9082332761578,
      "grad_norm": 4.914166450500488,
      "learning_rate": 1.30063125e-05,
      "loss": 4.3327,
      "step": 591900
    },
    {
      "epoch": 126.92967409948542,
      "grad_norm": 5.095897197723389,
      "learning_rate": 1.3000062500000002e-05,
      "loss": 4.32,
      "step": 592000
    },
    {
      "epoch": 126.95111492281303,
      "grad_norm": 5.422811031341553,
      "learning_rate": 1.29938125e-05,
      "loss": 4.3043,
      "step": 592100
    },
    {
      "epoch": 126.97255574614066,
      "grad_norm": 4.910032272338867,
      "learning_rate": 1.29875625e-05,
      "loss": 4.2908,
      "step": 592200
    },
    {
      "epoch": 126.99399656946827,
      "grad_norm": 5.310182094573975,
      "learning_rate": 1.2981312500000001e-05,
      "loss": 4.2779,
      "step": 592300
    },
    {
      "epoch": 127.01543739279589,
      "grad_norm": 5.453465461730957,
      "learning_rate": 1.2975062500000001e-05,
      "loss": 4.3208,
      "step": 592400
    },
    {
      "epoch": 127.0368782161235,
      "grad_norm": 4.832089900970459,
      "learning_rate": 1.29688125e-05,
      "loss": 4.2808,
      "step": 592500
    },
    {
      "epoch": 127.05831903945112,
      "grad_norm": 5.1623735427856445,
      "learning_rate": 1.29625625e-05,
      "loss": 4.263,
      "step": 592600
    },
    {
      "epoch": 127.07975986277873,
      "grad_norm": 4.76866340637207,
      "learning_rate": 1.29563125e-05,
      "loss": 4.2802,
      "step": 592700
    },
    {
      "epoch": 127.10120068610635,
      "grad_norm": 4.817444801330566,
      "learning_rate": 1.29500625e-05,
      "loss": 4.2271,
      "step": 592800
    },
    {
      "epoch": 127.12264150943396,
      "grad_norm": 5.903348445892334,
      "learning_rate": 1.2943812499999999e-05,
      "loss": 4.292,
      "step": 592900
    },
    {
      "epoch": 127.14408233276158,
      "grad_norm": 5.483699321746826,
      "learning_rate": 1.29375625e-05,
      "loss": 4.2544,
      "step": 593000
    },
    {
      "epoch": 127.1655231560892,
      "grad_norm": 5.711763858795166,
      "learning_rate": 1.29313125e-05,
      "loss": 4.2926,
      "step": 593100
    },
    {
      "epoch": 127.18696397941682,
      "grad_norm": 4.907310962677002,
      "learning_rate": 1.2925062500000002e-05,
      "loss": 4.2341,
      "step": 593200
    },
    {
      "epoch": 127.20840480274443,
      "grad_norm": 4.766937255859375,
      "learning_rate": 1.29188125e-05,
      "loss": 4.2688,
      "step": 593300
    },
    {
      "epoch": 127.22984562607203,
      "grad_norm": 5.682747840881348,
      "learning_rate": 1.29125625e-05,
      "loss": 4.283,
      "step": 593400
    },
    {
      "epoch": 127.25128644939966,
      "grad_norm": 5.5081987380981445,
      "learning_rate": 1.2906312500000001e-05,
      "loss": 4.2585,
      "step": 593500
    },
    {
      "epoch": 127.27272727272727,
      "grad_norm": 4.835540771484375,
      "learning_rate": 1.2900062500000001e-05,
      "loss": 4.306,
      "step": 593600
    },
    {
      "epoch": 127.29416809605489,
      "grad_norm": 5.2193145751953125,
      "learning_rate": 1.28938125e-05,
      "loss": 4.3434,
      "step": 593700
    },
    {
      "epoch": 127.3156089193825,
      "grad_norm": 5.0072760581970215,
      "learning_rate": 1.2887562499999999e-05,
      "loss": 4.2765,
      "step": 593800
    },
    {
      "epoch": 127.33704974271012,
      "grad_norm": 4.990513801574707,
      "learning_rate": 1.28813125e-05,
      "loss": 4.3108,
      "step": 593900
    },
    {
      "epoch": 127.35849056603773,
      "grad_norm": 5.112508773803711,
      "learning_rate": 1.28750625e-05,
      "loss": 4.2491,
      "step": 594000
    },
    {
      "epoch": 127.37993138936535,
      "grad_norm": 4.7005085945129395,
      "learning_rate": 1.2868812499999999e-05,
      "loss": 4.2297,
      "step": 594100
    },
    {
      "epoch": 127.40137221269296,
      "grad_norm": 4.999324321746826,
      "learning_rate": 1.28625625e-05,
      "loss": 4.2848,
      "step": 594200
    },
    {
      "epoch": 127.42281303602059,
      "grad_norm": 4.748147964477539,
      "learning_rate": 1.28563125e-05,
      "loss": 4.2735,
      "step": 594300
    },
    {
      "epoch": 127.4442538593482,
      "grad_norm": 5.29844331741333,
      "learning_rate": 1.2850062500000002e-05,
      "loss": 4.2518,
      "step": 594400
    },
    {
      "epoch": 127.46569468267582,
      "grad_norm": 4.6038947105407715,
      "learning_rate": 1.28438125e-05,
      "loss": 4.3173,
      "step": 594500
    },
    {
      "epoch": 127.48713550600343,
      "grad_norm": 4.865420818328857,
      "learning_rate": 1.28375625e-05,
      "loss": 4.2762,
      "step": 594600
    },
    {
      "epoch": 127.50857632933105,
      "grad_norm": 5.261907577514648,
      "learning_rate": 1.2831312500000001e-05,
      "loss": 4.2897,
      "step": 594700
    },
    {
      "epoch": 127.53001715265866,
      "grad_norm": 5.56648588180542,
      "learning_rate": 1.2825062500000001e-05,
      "loss": 4.2913,
      "step": 594800
    },
    {
      "epoch": 127.55145797598628,
      "grad_norm": 4.8475341796875,
      "learning_rate": 1.2818812500000003e-05,
      "loss": 4.1994,
      "step": 594900
    },
    {
      "epoch": 127.57289879931389,
      "grad_norm": 5.422364711761475,
      "learning_rate": 1.28125625e-05,
      "loss": 4.2729,
      "step": 595000
    },
    {
      "epoch": 127.59433962264151,
      "grad_norm": 5.1686787605285645,
      "learning_rate": 1.28063125e-05,
      "loss": 4.2855,
      "step": 595100
    },
    {
      "epoch": 127.61578044596912,
      "grad_norm": 5.074495792388916,
      "learning_rate": 1.2800062500000002e-05,
      "loss": 4.2287,
      "step": 595200
    },
    {
      "epoch": 127.63722126929675,
      "grad_norm": 5.078681945800781,
      "learning_rate": 1.2793812500000002e-05,
      "loss": 4.2124,
      "step": 595300
    },
    {
      "epoch": 127.65866209262435,
      "grad_norm": 6.355652809143066,
      "learning_rate": 1.27875625e-05,
      "loss": 4.2224,
      "step": 595400
    },
    {
      "epoch": 127.68010291595198,
      "grad_norm": 5.020149230957031,
      "learning_rate": 1.27813125e-05,
      "loss": 4.2414,
      "step": 595500
    },
    {
      "epoch": 127.70154373927959,
      "grad_norm": 5.293422222137451,
      "learning_rate": 1.2775062500000001e-05,
      "loss": 4.1951,
      "step": 595600
    },
    {
      "epoch": 127.72298456260721,
      "grad_norm": 5.094793319702148,
      "learning_rate": 1.2768812500000001e-05,
      "loss": 4.214,
      "step": 595700
    },
    {
      "epoch": 127.74442538593482,
      "grad_norm": 5.98081111907959,
      "learning_rate": 1.27625625e-05,
      "loss": 4.1831,
      "step": 595800
    },
    {
      "epoch": 127.76586620926244,
      "grad_norm": 5.3974223136901855,
      "learning_rate": 1.2756312500000001e-05,
      "loss": 4.2095,
      "step": 595900
    },
    {
      "epoch": 127.78730703259005,
      "grad_norm": 5.365213871002197,
      "learning_rate": 1.2750062500000001e-05,
      "loss": 4.1938,
      "step": 596000
    },
    {
      "epoch": 127.80874785591767,
      "grad_norm": 5.396967887878418,
      "learning_rate": 1.2743812500000002e-05,
      "loss": 4.2021,
      "step": 596100
    },
    {
      "epoch": 127.83018867924528,
      "grad_norm": 5.7968902587890625,
      "learning_rate": 1.27375625e-05,
      "loss": 4.2662,
      "step": 596200
    },
    {
      "epoch": 127.8516295025729,
      "grad_norm": 5.580892562866211,
      "learning_rate": 1.27313125e-05,
      "loss": 4.2437,
      "step": 596300
    },
    {
      "epoch": 127.87307032590051,
      "grad_norm": 5.236348628997803,
      "learning_rate": 1.2725062500000002e-05,
      "loss": 4.281,
      "step": 596400
    },
    {
      "epoch": 127.89451114922814,
      "grad_norm": 5.455384731292725,
      "learning_rate": 1.2718812500000002e-05,
      "loss": 4.2886,
      "step": 596500
    },
    {
      "epoch": 127.91595197255575,
      "grad_norm": 4.965966701507568,
      "learning_rate": 1.27125625e-05,
      "loss": 4.2335,
      "step": 596600
    },
    {
      "epoch": 127.93739279588337,
      "grad_norm": 5.113708972930908,
      "learning_rate": 1.27063125e-05,
      "loss": 4.2817,
      "step": 596700
    },
    {
      "epoch": 127.95883361921098,
      "grad_norm": 5.880431175231934,
      "learning_rate": 1.2700062500000001e-05,
      "loss": 4.1997,
      "step": 596800
    },
    {
      "epoch": 127.98027444253859,
      "grad_norm": 5.392854690551758,
      "learning_rate": 1.2693812500000001e-05,
      "loss": 4.1735,
      "step": 596900
    },
    {
      "epoch": 128.0017152658662,
      "grad_norm": 5.116530418395996,
      "learning_rate": 1.26875625e-05,
      "loss": 4.2098,
      "step": 597000
    },
    {
      "epoch": 128.02315608919383,
      "grad_norm": 4.8187575340271,
      "learning_rate": 1.2681312500000001e-05,
      "loss": 4.1822,
      "step": 597100
    },
    {
      "epoch": 128.04459691252143,
      "grad_norm": 5.52436637878418,
      "learning_rate": 1.26750625e-05,
      "loss": 4.1995,
      "step": 597200
    },
    {
      "epoch": 128.06603773584905,
      "grad_norm": 5.64387845993042,
      "learning_rate": 1.2668812500000002e-05,
      "loss": 4.1678,
      "step": 597300
    },
    {
      "epoch": 128.08747855917667,
      "grad_norm": 5.441429615020752,
      "learning_rate": 1.26625625e-05,
      "loss": 4.2784,
      "step": 597400
    },
    {
      "epoch": 128.1089193825043,
      "grad_norm": 5.428428649902344,
      "learning_rate": 1.26563125e-05,
      "loss": 4.1154,
      "step": 597500
    },
    {
      "epoch": 128.1303602058319,
      "grad_norm": 5.351032733917236,
      "learning_rate": 1.2650062500000002e-05,
      "loss": 4.2013,
      "step": 597600
    },
    {
      "epoch": 128.15180102915951,
      "grad_norm": 5.170368671417236,
      "learning_rate": 1.2643812500000002e-05,
      "loss": 4.2399,
      "step": 597700
    },
    {
      "epoch": 128.17324185248714,
      "grad_norm": 5.058557510375977,
      "learning_rate": 1.26375625e-05,
      "loss": 4.2037,
      "step": 597800
    },
    {
      "epoch": 128.19468267581476,
      "grad_norm": 5.045764923095703,
      "learning_rate": 1.26313125e-05,
      "loss": 4.175,
      "step": 597900
    },
    {
      "epoch": 128.21612349914236,
      "grad_norm": 5.1129560470581055,
      "learning_rate": 1.2625062500000001e-05,
      "loss": 4.1644,
      "step": 598000
    },
    {
      "epoch": 128.23756432246998,
      "grad_norm": 5.661579608917236,
      "learning_rate": 1.2618812500000001e-05,
      "loss": 4.1825,
      "step": 598100
    },
    {
      "epoch": 128.2590051457976,
      "grad_norm": 5.203364372253418,
      "learning_rate": 1.26125625e-05,
      "loss": 4.2511,
      "step": 598200
    },
    {
      "epoch": 128.28044596912522,
      "grad_norm": 5.22987174987793,
      "learning_rate": 1.26063125e-05,
      "loss": 4.198,
      "step": 598300
    },
    {
      "epoch": 128.30188679245282,
      "grad_norm": 5.263072967529297,
      "learning_rate": 1.26000625e-05,
      "loss": 4.2119,
      "step": 598400
    },
    {
      "epoch": 128.32332761578044,
      "grad_norm": 5.447597980499268,
      "learning_rate": 1.2593812500000002e-05,
      "loss": 4.2495,
      "step": 598500
    },
    {
      "epoch": 128.34476843910807,
      "grad_norm": 5.872044086456299,
      "learning_rate": 1.25875625e-05,
      "loss": 4.2192,
      "step": 598600
    },
    {
      "epoch": 128.3662092624357,
      "grad_norm": 5.6402692794799805,
      "learning_rate": 1.25813125e-05,
      "loss": 4.1516,
      "step": 598700
    },
    {
      "epoch": 128.38765008576328,
      "grad_norm": 5.730019569396973,
      "learning_rate": 1.2575062500000002e-05,
      "loss": 4.2332,
      "step": 598800
    },
    {
      "epoch": 128.4090909090909,
      "grad_norm": 5.519023418426514,
      "learning_rate": 1.2568812500000002e-05,
      "loss": 4.1708,
      "step": 598900
    },
    {
      "epoch": 128.43053173241853,
      "grad_norm": 6.209645748138428,
      "learning_rate": 1.25625625e-05,
      "loss": 4.2053,
      "step": 599000
    },
    {
      "epoch": 128.45197255574615,
      "grad_norm": 5.950794219970703,
      "learning_rate": 1.25563125e-05,
      "loss": 4.1872,
      "step": 599100
    },
    {
      "epoch": 128.47341337907375,
      "grad_norm": 6.73830509185791,
      "learning_rate": 1.2550062500000001e-05,
      "loss": 4.1377,
      "step": 599200
    },
    {
      "epoch": 128.49485420240137,
      "grad_norm": 5.4969611167907715,
      "learning_rate": 1.2543812500000001e-05,
      "loss": 4.1843,
      "step": 599300
    },
    {
      "epoch": 128.516295025729,
      "grad_norm": 5.6459221839904785,
      "learning_rate": 1.2537562499999999e-05,
      "loss": 4.2395,
      "step": 599400
    },
    {
      "epoch": 128.53773584905662,
      "grad_norm": 6.018577575683594,
      "learning_rate": 1.25313125e-05,
      "loss": 4.2075,
      "step": 599500
    },
    {
      "epoch": 128.5591766723842,
      "grad_norm": 6.880694389343262,
      "learning_rate": 1.25250625e-05,
      "loss": 4.1612,
      "step": 599600
    },
    {
      "epoch": 128.58061749571183,
      "grad_norm": 5.614744663238525,
      "learning_rate": 1.2518812500000002e-05,
      "loss": 4.2555,
      "step": 599700
    },
    {
      "epoch": 128.60205831903946,
      "grad_norm": 5.539393901824951,
      "learning_rate": 1.25125625e-05,
      "loss": 4.1876,
      "step": 599800
    },
    {
      "epoch": 128.62349914236708,
      "grad_norm": 5.46809196472168,
      "learning_rate": 1.25063125e-05,
      "loss": 4.2331,
      "step": 599900
    },
    {
      "epoch": 128.64493996569468,
      "grad_norm": 6.946106433868408,
      "learning_rate": 1.2500062500000002e-05,
      "loss": 4.1966,
      "step": 600000
    },
    {
      "epoch": 128.6663807890223,
      "grad_norm": 6.512420654296875,
      "learning_rate": 1.24938125e-05,
      "loss": 4.184,
      "step": 600100
    },
    {
      "epoch": 128.68782161234992,
      "grad_norm": 5.189788341522217,
      "learning_rate": 1.2487562500000001e-05,
      "loss": 4.2283,
      "step": 600200
    },
    {
      "epoch": 128.70926243567752,
      "grad_norm": 5.523454189300537,
      "learning_rate": 1.2481312500000001e-05,
      "loss": 4.1642,
      "step": 600300
    },
    {
      "epoch": 128.73070325900514,
      "grad_norm": 6.330836772918701,
      "learning_rate": 1.2475062500000001e-05,
      "loss": 4.1536,
      "step": 600400
    },
    {
      "epoch": 128.75214408233276,
      "grad_norm": 5.798956394195557,
      "learning_rate": 1.24688125e-05,
      "loss": 4.2505,
      "step": 600500
    },
    {
      "epoch": 128.77358490566039,
      "grad_norm": 5.248087406158447,
      "learning_rate": 1.24625625e-05,
      "loss": 4.1478,
      "step": 600600
    },
    {
      "epoch": 128.79502572898798,
      "grad_norm": 5.4958319664001465,
      "learning_rate": 1.24563125e-05,
      "loss": 4.2063,
      "step": 600700
    },
    {
      "epoch": 128.8164665523156,
      "grad_norm": 5.343441963195801,
      "learning_rate": 1.24500625e-05,
      "loss": 4.2327,
      "step": 600800
    },
    {
      "epoch": 128.83790737564323,
      "grad_norm": 5.764435291290283,
      "learning_rate": 1.24438125e-05,
      "loss": 4.1395,
      "step": 600900
    },
    {
      "epoch": 128.85934819897085,
      "grad_norm": 5.511600017547607,
      "learning_rate": 1.2437562500000002e-05,
      "loss": 4.1964,
      "step": 601000
    },
    {
      "epoch": 128.88078902229844,
      "grad_norm": 5.618407249450684,
      "learning_rate": 1.24313125e-05,
      "loss": 4.1686,
      "step": 601100
    },
    {
      "epoch": 128.90222984562607,
      "grad_norm": 4.933440685272217,
      "learning_rate": 1.2425062500000001e-05,
      "loss": 4.2318,
      "step": 601200
    },
    {
      "epoch": 128.9236706689537,
      "grad_norm": 5.453413486480713,
      "learning_rate": 1.24188125e-05,
      "loss": 4.1573,
      "step": 601300
    },
    {
      "epoch": 128.9451114922813,
      "grad_norm": 5.898644924163818,
      "learning_rate": 1.2412562500000001e-05,
      "loss": 4.1621,
      "step": 601400
    },
    {
      "epoch": 128.9665523156089,
      "grad_norm": 5.388498783111572,
      "learning_rate": 1.2406312500000001e-05,
      "loss": 4.1865,
      "step": 601500
    },
    {
      "epoch": 128.98799313893653,
      "grad_norm": 6.383260250091553,
      "learning_rate": 1.24000625e-05,
      "loss": 4.2031,
      "step": 601600
    },
    {
      "epoch": 129.00943396226415,
      "grad_norm": 5.4855732917785645,
      "learning_rate": 1.23938125e-05,
      "loss": 4.1204,
      "step": 601700
    },
    {
      "epoch": 129.03087478559178,
      "grad_norm": 5.621053218841553,
      "learning_rate": 1.23875625e-05,
      "loss": 4.18,
      "step": 601800
    },
    {
      "epoch": 129.05231560891937,
      "grad_norm": 5.243719577789307,
      "learning_rate": 1.23813125e-05,
      "loss": 4.178,
      "step": 601900
    },
    {
      "epoch": 129.073756432247,
      "grad_norm": 4.817695140838623,
      "learning_rate": 1.23750625e-05,
      "loss": 4.1872,
      "step": 602000
    },
    {
      "epoch": 129.09519725557462,
      "grad_norm": 5.6964569091796875,
      "learning_rate": 1.23688125e-05,
      "loss": 4.1468,
      "step": 602100
    },
    {
      "epoch": 129.11663807890224,
      "grad_norm": 5.759354114532471,
      "learning_rate": 1.2362562500000002e-05,
      "loss": 4.166,
      "step": 602200
    },
    {
      "epoch": 129.13807890222984,
      "grad_norm": 5.887294292449951,
      "learning_rate": 1.23563125e-05,
      "loss": 4.1428,
      "step": 602300
    },
    {
      "epoch": 129.15951972555746,
      "grad_norm": 5.8495306968688965,
      "learning_rate": 1.2350062500000001e-05,
      "loss": 4.128,
      "step": 602400
    },
    {
      "epoch": 129.18096054888508,
      "grad_norm": 5.321633338928223,
      "learning_rate": 1.23438125e-05,
      "loss": 4.1381,
      "step": 602500
    },
    {
      "epoch": 129.2024013722127,
      "grad_norm": 5.4319376945495605,
      "learning_rate": 1.2337562500000001e-05,
      "loss": 4.1818,
      "step": 602600
    },
    {
      "epoch": 129.2238421955403,
      "grad_norm": 5.581693172454834,
      "learning_rate": 1.2331312500000001e-05,
      "loss": 4.1662,
      "step": 602700
    },
    {
      "epoch": 129.24528301886792,
      "grad_norm": 5.8254618644714355,
      "learning_rate": 1.23250625e-05,
      "loss": 4.1398,
      "step": 602800
    },
    {
      "epoch": 129.26672384219555,
      "grad_norm": 5.79688835144043,
      "learning_rate": 1.23188125e-05,
      "loss": 4.2006,
      "step": 602900
    },
    {
      "epoch": 129.28816466552317,
      "grad_norm": 5.643237113952637,
      "learning_rate": 1.23125625e-05,
      "loss": 4.1907,
      "step": 603000
    },
    {
      "epoch": 129.30960548885076,
      "grad_norm": 5.093372344970703,
      "learning_rate": 1.23063125e-05,
      "loss": 4.1408,
      "step": 603100
    },
    {
      "epoch": 129.3310463121784,
      "grad_norm": 5.393284320831299,
      "learning_rate": 1.23000625e-05,
      "loss": 4.1396,
      "step": 603200
    },
    {
      "epoch": 129.352487135506,
      "grad_norm": 6.112338542938232,
      "learning_rate": 1.22938125e-05,
      "loss": 4.1065,
      "step": 603300
    },
    {
      "epoch": 129.37392795883363,
      "grad_norm": 5.596655368804932,
      "learning_rate": 1.2287562500000002e-05,
      "loss": 4.1674,
      "step": 603400
    },
    {
      "epoch": 129.39536878216123,
      "grad_norm": 5.180272102355957,
      "learning_rate": 1.22813125e-05,
      "loss": 4.1249,
      "step": 603500
    },
    {
      "epoch": 129.41680960548885,
      "grad_norm": 5.524545669555664,
      "learning_rate": 1.2275062500000001e-05,
      "loss": 4.149,
      "step": 603600
    },
    {
      "epoch": 129.43825042881647,
      "grad_norm": 5.531404495239258,
      "learning_rate": 1.22688125e-05,
      "loss": 4.1107,
      "step": 603700
    },
    {
      "epoch": 129.45969125214407,
      "grad_norm": 6.108120918273926,
      "learning_rate": 1.2262562500000001e-05,
      "loss": 4.1496,
      "step": 603800
    },
    {
      "epoch": 129.4811320754717,
      "grad_norm": 5.592749118804932,
      "learning_rate": 1.22563125e-05,
      "loss": 4.0933,
      "step": 603900
    },
    {
      "epoch": 129.50257289879931,
      "grad_norm": 5.679654121398926,
      "learning_rate": 1.22500625e-05,
      "loss": 4.118,
      "step": 604000
    },
    {
      "epoch": 129.52401372212694,
      "grad_norm": 5.809651851654053,
      "learning_rate": 1.22438125e-05,
      "loss": 4.1352,
      "step": 604100
    },
    {
      "epoch": 129.54545454545453,
      "grad_norm": 5.258325576782227,
      "learning_rate": 1.22375625e-05,
      "loss": 4.1515,
      "step": 604200
    },
    {
      "epoch": 129.56689536878216,
      "grad_norm": 5.851169109344482,
      "learning_rate": 1.22313125e-05,
      "loss": 4.1477,
      "step": 604300
    },
    {
      "epoch": 129.58833619210978,
      "grad_norm": 5.048301696777344,
      "learning_rate": 1.22250625e-05,
      "loss": 4.1754,
      "step": 604400
    },
    {
      "epoch": 129.6097770154374,
      "grad_norm": 5.695531368255615,
      "learning_rate": 1.22188125e-05,
      "loss": 4.1611,
      "step": 604500
    },
    {
      "epoch": 129.631217838765,
      "grad_norm": 5.64902925491333,
      "learning_rate": 1.2212562500000001e-05,
      "loss": 4.1561,
      "step": 604600
    },
    {
      "epoch": 129.65265866209262,
      "grad_norm": 5.556628227233887,
      "learning_rate": 1.22063125e-05,
      "loss": 4.1336,
      "step": 604700
    },
    {
      "epoch": 129.67409948542024,
      "grad_norm": 5.520710468292236,
      "learning_rate": 1.2200062500000001e-05,
      "loss": 4.1161,
      "step": 604800
    },
    {
      "epoch": 129.69554030874787,
      "grad_norm": 5.3261494636535645,
      "learning_rate": 1.21938125e-05,
      "loss": 4.1585,
      "step": 604900
    },
    {
      "epoch": 129.71698113207546,
      "grad_norm": 5.926516532897949,
      "learning_rate": 1.21875625e-05,
      "loss": 4.1513,
      "step": 605000
    },
    {
      "epoch": 129.73842195540308,
      "grad_norm": 5.0584259033203125,
      "learning_rate": 1.21813125e-05,
      "loss": 4.1608,
      "step": 605100
    },
    {
      "epoch": 129.7598627787307,
      "grad_norm": 5.982754707336426,
      "learning_rate": 1.21750625e-05,
      "loss": 4.1088,
      "step": 605200
    },
    {
      "epoch": 129.78130360205833,
      "grad_norm": 5.333752155303955,
      "learning_rate": 1.21688125e-05,
      "loss": 4.1946,
      "step": 605300
    },
    {
      "epoch": 129.80274442538592,
      "grad_norm": 5.173734664916992,
      "learning_rate": 1.21625625e-05,
      "loss": 4.139,
      "step": 605400
    },
    {
      "epoch": 129.82418524871355,
      "grad_norm": 5.236318111419678,
      "learning_rate": 1.21563125e-05,
      "loss": 4.1439,
      "step": 605500
    },
    {
      "epoch": 129.84562607204117,
      "grad_norm": 5.375393390655518,
      "learning_rate": 1.21500625e-05,
      "loss": 4.0757,
      "step": 605600
    },
    {
      "epoch": 129.8670668953688,
      "grad_norm": 6.381308555603027,
      "learning_rate": 1.21438125e-05,
      "loss": 4.1191,
      "step": 605700
    },
    {
      "epoch": 129.8885077186964,
      "grad_norm": 6.283232688903809,
      "learning_rate": 1.2137562500000001e-05,
      "loss": 4.1039,
      "step": 605800
    },
    {
      "epoch": 129.909948542024,
      "grad_norm": 6.004438877105713,
      "learning_rate": 1.21313125e-05,
      "loss": 4.1153,
      "step": 605900
    },
    {
      "epoch": 129.93138936535163,
      "grad_norm": 6.435779094696045,
      "learning_rate": 1.2125062500000001e-05,
      "loss": 4.1074,
      "step": 606000
    },
    {
      "epoch": 129.95283018867926,
      "grad_norm": 5.387287139892578,
      "learning_rate": 1.21188125e-05,
      "loss": 4.0642,
      "step": 606100
    },
    {
      "epoch": 129.97427101200685,
      "grad_norm": 5.34620475769043,
      "learning_rate": 1.21125625e-05,
      "loss": 4.1501,
      "step": 606200
    },
    {
      "epoch": 129.99571183533448,
      "grad_norm": 5.90120792388916,
      "learning_rate": 1.21063125e-05,
      "loss": 4.1483,
      "step": 606300
    },
    {
      "epoch": 130.0171526586621,
      "grad_norm": 5.483157634735107,
      "learning_rate": 1.21000625e-05,
      "loss": 4.143,
      "step": 606400
    },
    {
      "epoch": 130.03859348198972,
      "grad_norm": 5.694818019866943,
      "learning_rate": 1.20938125e-05,
      "loss": 4.074,
      "step": 606500
    },
    {
      "epoch": 130.06003430531732,
      "grad_norm": 5.471097469329834,
      "learning_rate": 1.20875625e-05,
      "loss": 4.0892,
      "step": 606600
    },
    {
      "epoch": 130.08147512864494,
      "grad_norm": 5.642873287200928,
      "learning_rate": 1.20813125e-05,
      "loss": 4.1754,
      "step": 606700
    },
    {
      "epoch": 130.10291595197256,
      "grad_norm": 6.212222099304199,
      "learning_rate": 1.2075062500000001e-05,
      "loss": 4.111,
      "step": 606800
    },
    {
      "epoch": 130.12435677530019,
      "grad_norm": 5.390522003173828,
      "learning_rate": 1.20688125e-05,
      "loss": 4.0703,
      "step": 606900
    },
    {
      "epoch": 130.14579759862778,
      "grad_norm": 6.34865140914917,
      "learning_rate": 1.2062562500000001e-05,
      "loss": 4.0958,
      "step": 607000
    },
    {
      "epoch": 130.1672384219554,
      "grad_norm": 6.592705726623535,
      "learning_rate": 1.20563125e-05,
      "loss": 4.0688,
      "step": 607100
    },
    {
      "epoch": 130.18867924528303,
      "grad_norm": 5.750351428985596,
      "learning_rate": 1.20500625e-05,
      "loss": 4.1287,
      "step": 607200
    },
    {
      "epoch": 130.21012006861062,
      "grad_norm": 5.527034759521484,
      "learning_rate": 1.20438125e-05,
      "loss": 4.1174,
      "step": 607300
    },
    {
      "epoch": 130.23156089193824,
      "grad_norm": 5.756742000579834,
      "learning_rate": 1.20375625e-05,
      "loss": 4.0971,
      "step": 607400
    },
    {
      "epoch": 130.25300171526587,
      "grad_norm": 6.280481338500977,
      "learning_rate": 1.2031312500000002e-05,
      "loss": 4.1198,
      "step": 607500
    },
    {
      "epoch": 130.2744425385935,
      "grad_norm": 5.583868026733398,
      "learning_rate": 1.20250625e-05,
      "loss": 4.1119,
      "step": 607600
    },
    {
      "epoch": 130.29588336192108,
      "grad_norm": 5.56512975692749,
      "learning_rate": 1.2018812500000002e-05,
      "loss": 4.0895,
      "step": 607700
    },
    {
      "epoch": 130.3173241852487,
      "grad_norm": 5.421658039093018,
      "learning_rate": 1.20125625e-05,
      "loss": 4.1358,
      "step": 607800
    },
    {
      "epoch": 130.33876500857633,
      "grad_norm": 5.981625556945801,
      "learning_rate": 1.2006312500000001e-05,
      "loss": 4.0612,
      "step": 607900
    },
    {
      "epoch": 130.36020583190395,
      "grad_norm": 5.965569019317627,
      "learning_rate": 1.2000062500000001e-05,
      "loss": 4.0728,
      "step": 608000
    },
    {
      "epoch": 130.38164665523155,
      "grad_norm": 5.576311111450195,
      "learning_rate": 1.1993812500000001e-05,
      "loss": 4.1361,
      "step": 608100
    },
    {
      "epoch": 130.40308747855917,
      "grad_norm": 6.390936851501465,
      "learning_rate": 1.1987562500000001e-05,
      "loss": 4.1074,
      "step": 608200
    },
    {
      "epoch": 130.4245283018868,
      "grad_norm": 5.623229026794434,
      "learning_rate": 1.19813125e-05,
      "loss": 4.106,
      "step": 608300
    },
    {
      "epoch": 130.44596912521442,
      "grad_norm": 5.227471351623535,
      "learning_rate": 1.19750625e-05,
      "loss": 4.0447,
      "step": 608400
    },
    {
      "epoch": 130.467409948542,
      "grad_norm": 5.249897003173828,
      "learning_rate": 1.19688125e-05,
      "loss": 4.0875,
      "step": 608500
    },
    {
      "epoch": 130.48885077186964,
      "grad_norm": 5.5257487297058105,
      "learning_rate": 1.19625625e-05,
      "loss": 4.1589,
      "step": 608600
    },
    {
      "epoch": 130.51029159519726,
      "grad_norm": 5.7643537521362305,
      "learning_rate": 1.1956312500000002e-05,
      "loss": 4.0489,
      "step": 608700
    },
    {
      "epoch": 130.53173241852488,
      "grad_norm": 5.631106853485107,
      "learning_rate": 1.19500625e-05,
      "loss": 4.113,
      "step": 608800
    },
    {
      "epoch": 130.55317324185248,
      "grad_norm": 5.638521671295166,
      "learning_rate": 1.1943812500000002e-05,
      "loss": 4.0916,
      "step": 608900
    },
    {
      "epoch": 130.5746140651801,
      "grad_norm": 6.136349678039551,
      "learning_rate": 1.19375625e-05,
      "loss": 4.0148,
      "step": 609000
    },
    {
      "epoch": 130.59605488850772,
      "grad_norm": 5.433111190795898,
      "learning_rate": 1.1931312500000001e-05,
      "loss": 4.1004,
      "step": 609100
    },
    {
      "epoch": 130.61749571183535,
      "grad_norm": 5.376852035522461,
      "learning_rate": 1.1925062500000001e-05,
      "loss": 4.0584,
      "step": 609200
    },
    {
      "epoch": 130.63893653516294,
      "grad_norm": 5.657209396362305,
      "learning_rate": 1.1918812500000001e-05,
      "loss": 4.1135,
      "step": 609300
    },
    {
      "epoch": 130.66037735849056,
      "grad_norm": 5.6543402671813965,
      "learning_rate": 1.1912562500000001e-05,
      "loss": 4.1243,
      "step": 609400
    },
    {
      "epoch": 130.6818181818182,
      "grad_norm": 5.861546039581299,
      "learning_rate": 1.19063125e-05,
      "loss": 4.088,
      "step": 609500
    },
    {
      "epoch": 130.7032590051458,
      "grad_norm": 6.328167915344238,
      "learning_rate": 1.19000625e-05,
      "loss": 4.0651,
      "step": 609600
    },
    {
      "epoch": 130.7246998284734,
      "grad_norm": 5.486414909362793,
      "learning_rate": 1.18938125e-05,
      "loss": 4.0589,
      "step": 609700
    },
    {
      "epoch": 130.74614065180103,
      "grad_norm": 5.617335319519043,
      "learning_rate": 1.18875625e-05,
      "loss": 4.0825,
      "step": 609800
    },
    {
      "epoch": 130.76758147512865,
      "grad_norm": 5.784517288208008,
      "learning_rate": 1.1881312500000002e-05,
      "loss": 4.1047,
      "step": 609900
    },
    {
      "epoch": 130.78902229845627,
      "grad_norm": 5.64169454574585,
      "learning_rate": 1.18750625e-05,
      "loss": 4.0821,
      "step": 610000
    },
    {
      "epoch": 130.81046312178387,
      "grad_norm": 5.330316543579102,
      "learning_rate": 1.1868812500000001e-05,
      "loss": 4.0973,
      "step": 610100
    },
    {
      "epoch": 130.8319039451115,
      "grad_norm": 5.913486957550049,
      "learning_rate": 1.18625625e-05,
      "loss": 4.0864,
      "step": 610200
    },
    {
      "epoch": 130.85334476843911,
      "grad_norm": 5.703406810760498,
      "learning_rate": 1.1856312500000001e-05,
      "loss": 4.0632,
      "step": 610300
    },
    {
      "epoch": 130.87478559176674,
      "grad_norm": 5.949947834014893,
      "learning_rate": 1.1850062500000001e-05,
      "loss": 4.1046,
      "step": 610400
    },
    {
      "epoch": 130.89622641509433,
      "grad_norm": 6.010239124298096,
      "learning_rate": 1.1843812500000001e-05,
      "loss": 4.0992,
      "step": 610500
    },
    {
      "epoch": 130.91766723842196,
      "grad_norm": 5.491071701049805,
      "learning_rate": 1.18375625e-05,
      "loss": 4.0746,
      "step": 610600
    },
    {
      "epoch": 130.93910806174958,
      "grad_norm": 6.324003219604492,
      "learning_rate": 1.18313125e-05,
      "loss": 4.1456,
      "step": 610700
    },
    {
      "epoch": 130.96054888507717,
      "grad_norm": 6.298465728759766,
      "learning_rate": 1.18250625e-05,
      "loss": 4.0599,
      "step": 610800
    },
    {
      "epoch": 130.9819897084048,
      "grad_norm": 5.765684127807617,
      "learning_rate": 1.18188125e-05,
      "loss": 4.1089,
      "step": 610900
    },
    {
      "epoch": 131.00343053173242,
      "grad_norm": 5.316961765289307,
      "learning_rate": 1.18125625e-05,
      "loss": 4.1077,
      "step": 611000
    },
    {
      "epoch": 131.02487135506004,
      "grad_norm": 5.241873741149902,
      "learning_rate": 1.1806312500000002e-05,
      "loss": 4.0884,
      "step": 611100
    },
    {
      "epoch": 131.04631217838764,
      "grad_norm": 5.218694686889648,
      "learning_rate": 1.18000625e-05,
      "loss": 4.0648,
      "step": 611200
    },
    {
      "epoch": 131.06775300171526,
      "grad_norm": 5.792762279510498,
      "learning_rate": 1.1793812500000001e-05,
      "loss": 4.0531,
      "step": 611300
    },
    {
      "epoch": 131.08919382504288,
      "grad_norm": 5.979757308959961,
      "learning_rate": 1.17875625e-05,
      "loss": 3.9868,
      "step": 611400
    },
    {
      "epoch": 131.1106346483705,
      "grad_norm": 6.534023761749268,
      "learning_rate": 1.1781312500000001e-05,
      "loss": 4.0474,
      "step": 611500
    },
    {
      "epoch": 131.1320754716981,
      "grad_norm": 6.041891574859619,
      "learning_rate": 1.1775062500000001e-05,
      "loss": 4.0249,
      "step": 611600
    },
    {
      "epoch": 131.15351629502572,
      "grad_norm": 6.429842472076416,
      "learning_rate": 1.17688125e-05,
      "loss": 4.0377,
      "step": 611700
    },
    {
      "epoch": 131.17495711835335,
      "grad_norm": 5.795620441436768,
      "learning_rate": 1.17625625e-05,
      "loss": 4.0575,
      "step": 611800
    },
    {
      "epoch": 131.19639794168097,
      "grad_norm": 5.625301361083984,
      "learning_rate": 1.17563125e-05,
      "loss": 4.0674,
      "step": 611900
    },
    {
      "epoch": 131.21783876500857,
      "grad_norm": 6.048238754272461,
      "learning_rate": 1.17500625e-05,
      "loss": 4.0297,
      "step": 612000
    },
    {
      "epoch": 131.2392795883362,
      "grad_norm": 5.533015251159668,
      "learning_rate": 1.1743812500000002e-05,
      "loss": 4.0759,
      "step": 612100
    },
    {
      "epoch": 131.2607204116638,
      "grad_norm": 6.134433269500732,
      "learning_rate": 1.17375625e-05,
      "loss": 4.081,
      "step": 612200
    },
    {
      "epoch": 131.28216123499143,
      "grad_norm": 6.0759196281433105,
      "learning_rate": 1.1731312500000002e-05,
      "loss": 4.0993,
      "step": 612300
    },
    {
      "epoch": 131.30360205831903,
      "grad_norm": 5.41015625,
      "learning_rate": 1.17250625e-05,
      "loss": 4.0625,
      "step": 612400
    },
    {
      "epoch": 131.32504288164665,
      "grad_norm": 5.922178268432617,
      "learning_rate": 1.1718812500000001e-05,
      "loss": 4.0874,
      "step": 612500
    },
    {
      "epoch": 131.34648370497428,
      "grad_norm": 6.206899642944336,
      "learning_rate": 1.1712562500000001e-05,
      "loss": 4.0467,
      "step": 612600
    },
    {
      "epoch": 131.3679245283019,
      "grad_norm": 5.659335136413574,
      "learning_rate": 1.1706312500000001e-05,
      "loss": 4.0734,
      "step": 612700
    },
    {
      "epoch": 131.3893653516295,
      "grad_norm": 5.6892619132995605,
      "learning_rate": 1.17000625e-05,
      "loss": 4.0556,
      "step": 612800
    },
    {
      "epoch": 131.41080617495712,
      "grad_norm": 5.600351333618164,
      "learning_rate": 1.16938125e-05,
      "loss": 3.9995,
      "step": 612900
    },
    {
      "epoch": 131.43224699828474,
      "grad_norm": 5.198005676269531,
      "learning_rate": 1.16875625e-05,
      "loss": 4.0485,
      "step": 613000
    },
    {
      "epoch": 131.45368782161236,
      "grad_norm": 9.193877220153809,
      "learning_rate": 1.16813125e-05,
      "loss": 4.0893,
      "step": 613100
    },
    {
      "epoch": 131.47512864493996,
      "grad_norm": 6.491858005523682,
      "learning_rate": 1.16750625e-05,
      "loss": 4.0558,
      "step": 613200
    },
    {
      "epoch": 131.49656946826758,
      "grad_norm": 5.965433120727539,
      "learning_rate": 1.1668812500000002e-05,
      "loss": 4.0561,
      "step": 613300
    },
    {
      "epoch": 131.5180102915952,
      "grad_norm": 5.678478240966797,
      "learning_rate": 1.16625625e-05,
      "loss": 4.0099,
      "step": 613400
    },
    {
      "epoch": 131.53945111492283,
      "grad_norm": 5.956496715545654,
      "learning_rate": 1.1656312500000001e-05,
      "loss": 4.1015,
      "step": 613500
    },
    {
      "epoch": 131.56089193825042,
      "grad_norm": 5.859246730804443,
      "learning_rate": 1.16500625e-05,
      "loss": 4.037,
      "step": 613600
    },
    {
      "epoch": 131.58233276157804,
      "grad_norm": 5.444637775421143,
      "learning_rate": 1.1643812500000001e-05,
      "loss": 4.0731,
      "step": 613700
    },
    {
      "epoch": 131.60377358490567,
      "grad_norm": 6.077742576599121,
      "learning_rate": 1.1637562500000001e-05,
      "loss": 4.0181,
      "step": 613800
    },
    {
      "epoch": 131.62521440823326,
      "grad_norm": 5.181943416595459,
      "learning_rate": 1.16313125e-05,
      "loss": 4.0378,
      "step": 613900
    },
    {
      "epoch": 131.64665523156089,
      "grad_norm": 5.637723922729492,
      "learning_rate": 1.16250625e-05,
      "loss": 4.1406,
      "step": 614000
    },
    {
      "epoch": 131.6680960548885,
      "grad_norm": 5.469040870666504,
      "learning_rate": 1.16188125e-05,
      "loss": 4.0446,
      "step": 614100
    },
    {
      "epoch": 131.68953687821613,
      "grad_norm": 6.101040840148926,
      "learning_rate": 1.16125625e-05,
      "loss": 4.0537,
      "step": 614200
    },
    {
      "epoch": 131.71097770154373,
      "grad_norm": 5.589128494262695,
      "learning_rate": 1.16063125e-05,
      "loss": 4.1135,
      "step": 614300
    },
    {
      "epoch": 131.73241852487135,
      "grad_norm": 6.039915084838867,
      "learning_rate": 1.16000625e-05,
      "loss": 4.0926,
      "step": 614400
    },
    {
      "epoch": 131.75385934819897,
      "grad_norm": 5.849669933319092,
      "learning_rate": 1.1593812500000002e-05,
      "loss": 4.0731,
      "step": 614500
    },
    {
      "epoch": 131.7753001715266,
      "grad_norm": 5.693417549133301,
      "learning_rate": 1.15875625e-05,
      "loss": 4.0427,
      "step": 614600
    },
    {
      "epoch": 131.7967409948542,
      "grad_norm": 5.460636615753174,
      "learning_rate": 1.1581312500000001e-05,
      "loss": 3.9851,
      "step": 614700
    },
    {
      "epoch": 131.8181818181818,
      "grad_norm": 6.099348068237305,
      "learning_rate": 1.15750625e-05,
      "loss": 4.0169,
      "step": 614800
    },
    {
      "epoch": 131.83962264150944,
      "grad_norm": 5.802774906158447,
      "learning_rate": 1.1568812500000001e-05,
      "loss": 4.0647,
      "step": 614900
    },
    {
      "epoch": 131.86106346483706,
      "grad_norm": 6.3611345291137695,
      "learning_rate": 1.15625625e-05,
      "loss": 4.0262,
      "step": 615000
    },
    {
      "epoch": 131.88250428816465,
      "grad_norm": 5.732442378997803,
      "learning_rate": 1.15563125e-05,
      "loss": 4.021,
      "step": 615100
    },
    {
      "epoch": 131.90394511149228,
      "grad_norm": 5.659514427185059,
      "learning_rate": 1.15500625e-05,
      "loss": 4.1052,
      "step": 615200
    },
    {
      "epoch": 131.9253859348199,
      "grad_norm": 5.6892991065979,
      "learning_rate": 1.15438125e-05,
      "loss": 4.0678,
      "step": 615300
    },
    {
      "epoch": 131.94682675814752,
      "grad_norm": 5.869893550872803,
      "learning_rate": 1.15375625e-05,
      "loss": 4.004,
      "step": 615400
    },
    {
      "epoch": 131.96826758147512,
      "grad_norm": 6.212049961090088,
      "learning_rate": 1.15313125e-05,
      "loss": 3.9725,
      "step": 615500
    },
    {
      "epoch": 131.98970840480274,
      "grad_norm": 5.637662887573242,
      "learning_rate": 1.15250625e-05,
      "loss": 4.0327,
      "step": 615600
    },
    {
      "epoch": 132.01114922813036,
      "grad_norm": 5.511212348937988,
      "learning_rate": 1.1518812500000001e-05,
      "loss": 4.0214,
      "step": 615700
    },
    {
      "epoch": 132.032590051458,
      "grad_norm": 5.447386741638184,
      "learning_rate": 1.15125625e-05,
      "loss": 3.9751,
      "step": 615800
    },
    {
      "epoch": 132.05403087478558,
      "grad_norm": 5.834105014801025,
      "learning_rate": 1.1506312500000001e-05,
      "loss": 4.0328,
      "step": 615900
    },
    {
      "epoch": 132.0754716981132,
      "grad_norm": 5.3738579750061035,
      "learning_rate": 1.15000625e-05,
      "loss": 3.9853,
      "step": 616000
    },
    {
      "epoch": 132.09691252144083,
      "grad_norm": 6.275941848754883,
      "learning_rate": 1.14938125e-05,
      "loss": 4.0063,
      "step": 616100
    },
    {
      "epoch": 132.11835334476845,
      "grad_norm": 5.691192626953125,
      "learning_rate": 1.14875625e-05,
      "loss": 4.0646,
      "step": 616200
    },
    {
      "epoch": 132.13979416809605,
      "grad_norm": 5.854719638824463,
      "learning_rate": 1.14813125e-05,
      "loss": 3.9849,
      "step": 616300
    },
    {
      "epoch": 132.16123499142367,
      "grad_norm": 5.831467628479004,
      "learning_rate": 1.14750625e-05,
      "loss": 3.9484,
      "step": 616400
    },
    {
      "epoch": 132.1826758147513,
      "grad_norm": 5.595592021942139,
      "learning_rate": 1.14688125e-05,
      "loss": 4.0863,
      "step": 616500
    },
    {
      "epoch": 132.20411663807892,
      "grad_norm": 5.940971851348877,
      "learning_rate": 1.14625625e-05,
      "loss": 3.9517,
      "step": 616600
    },
    {
      "epoch": 132.2255574614065,
      "grad_norm": 5.861093997955322,
      "learning_rate": 1.14563125e-05,
      "loss": 4.0237,
      "step": 616700
    },
    {
      "epoch": 132.24699828473413,
      "grad_norm": 6.330018997192383,
      "learning_rate": 1.14500625e-05,
      "loss": 4.0321,
      "step": 616800
    },
    {
      "epoch": 132.26843910806176,
      "grad_norm": 5.860313892364502,
      "learning_rate": 1.1443812500000001e-05,
      "loss": 4.0245,
      "step": 616900
    },
    {
      "epoch": 132.28987993138938,
      "grad_norm": 5.919654369354248,
      "learning_rate": 1.14375625e-05,
      "loss": 4.0378,
      "step": 617000
    },
    {
      "epoch": 132.31132075471697,
      "grad_norm": 6.646875858306885,
      "learning_rate": 1.1431312500000001e-05,
      "loss": 4.0121,
      "step": 617100
    },
    {
      "epoch": 132.3327615780446,
      "grad_norm": 6.163206100463867,
      "learning_rate": 1.14250625e-05,
      "loss": 3.993,
      "step": 617200
    },
    {
      "epoch": 132.35420240137222,
      "grad_norm": 5.996459007263184,
      "learning_rate": 1.14188125e-05,
      "loss": 4.0099,
      "step": 617300
    },
    {
      "epoch": 132.37564322469981,
      "grad_norm": 5.817009449005127,
      "learning_rate": 1.14125625e-05,
      "loss": 4.0836,
      "step": 617400
    },
    {
      "epoch": 132.39708404802744,
      "grad_norm": 5.348058700561523,
      "learning_rate": 1.14063125e-05,
      "loss": 4.0008,
      "step": 617500
    },
    {
      "epoch": 132.41852487135506,
      "grad_norm": 6.138765811920166,
      "learning_rate": 1.14000625e-05,
      "loss": 4.0374,
      "step": 617600
    },
    {
      "epoch": 132.43996569468268,
      "grad_norm": 5.241931915283203,
      "learning_rate": 1.13938125e-05,
      "loss": 3.9847,
      "step": 617700
    },
    {
      "epoch": 132.46140651801028,
      "grad_norm": 6.382328033447266,
      "learning_rate": 1.13875625e-05,
      "loss": 4.0224,
      "step": 617800
    },
    {
      "epoch": 132.4828473413379,
      "grad_norm": 6.470994472503662,
      "learning_rate": 1.1381312500000001e-05,
      "loss": 4.0292,
      "step": 617900
    },
    {
      "epoch": 132.50428816466552,
      "grad_norm": 6.012767791748047,
      "learning_rate": 1.13750625e-05,
      "loss": 4.0064,
      "step": 618000
    },
    {
      "epoch": 132.52572898799315,
      "grad_norm": 5.632625102996826,
      "learning_rate": 1.1368812500000001e-05,
      "loss": 3.9993,
      "step": 618100
    },
    {
      "epoch": 132.54716981132074,
      "grad_norm": 6.089929103851318,
      "learning_rate": 1.13625625e-05,
      "loss": 3.9666,
      "step": 618200
    },
    {
      "epoch": 132.56861063464837,
      "grad_norm": 5.66009521484375,
      "learning_rate": 1.1356312500000001e-05,
      "loss": 4.0474,
      "step": 618300
    },
    {
      "epoch": 132.590051457976,
      "grad_norm": 5.4783430099487305,
      "learning_rate": 1.13500625e-05,
      "loss": 4.0477,
      "step": 618400
    },
    {
      "epoch": 132.6114922813036,
      "grad_norm": 7.429322242736816,
      "learning_rate": 1.13438125e-05,
      "loss": 4.0075,
      "step": 618500
    },
    {
      "epoch": 132.6329331046312,
      "grad_norm": 5.755763053894043,
      "learning_rate": 1.13375625e-05,
      "loss": 4.0358,
      "step": 618600
    },
    {
      "epoch": 132.65437392795883,
      "grad_norm": 5.708045482635498,
      "learning_rate": 1.13313125e-05,
      "loss": 4.0251,
      "step": 618700
    },
    {
      "epoch": 132.67581475128645,
      "grad_norm": 5.768935203552246,
      "learning_rate": 1.13250625e-05,
      "loss": 4.0098,
      "step": 618800
    },
    {
      "epoch": 132.69725557461408,
      "grad_norm": 5.658116340637207,
      "learning_rate": 1.13188125e-05,
      "loss": 4.0069,
      "step": 618900
    },
    {
      "epoch": 132.71869639794167,
      "grad_norm": 5.671741485595703,
      "learning_rate": 1.13125625e-05,
      "loss": 4.0178,
      "step": 619000
    },
    {
      "epoch": 132.7401372212693,
      "grad_norm": 7.266000270843506,
      "learning_rate": 1.1306312500000001e-05,
      "loss": 4.0386,
      "step": 619100
    },
    {
      "epoch": 132.76157804459692,
      "grad_norm": 6.27689266204834,
      "learning_rate": 1.13000625e-05,
      "loss": 4.0585,
      "step": 619200
    },
    {
      "epoch": 132.78301886792454,
      "grad_norm": 6.398798942565918,
      "learning_rate": 1.1293812500000001e-05,
      "loss": 4.0283,
      "step": 619300
    },
    {
      "epoch": 132.80445969125213,
      "grad_norm": 5.325928688049316,
      "learning_rate": 1.12875625e-05,
      "loss": 4.0203,
      "step": 619400
    },
    {
      "epoch": 132.82590051457976,
      "grad_norm": 5.841249465942383,
      "learning_rate": 1.12813125e-05,
      "loss": 3.9856,
      "step": 619500
    },
    {
      "epoch": 132.84734133790738,
      "grad_norm": 5.411748886108398,
      "learning_rate": 1.12750625e-05,
      "loss": 3.9493,
      "step": 619600
    },
    {
      "epoch": 132.868782161235,
      "grad_norm": 5.441104888916016,
      "learning_rate": 1.12688125e-05,
      "loss": 4.033,
      "step": 619700
    },
    {
      "epoch": 132.8902229845626,
      "grad_norm": 6.571525573730469,
      "learning_rate": 1.12625625e-05,
      "loss": 3.9705,
      "step": 619800
    },
    {
      "epoch": 132.91166380789022,
      "grad_norm": 5.627660751342773,
      "learning_rate": 1.12563125e-05,
      "loss": 4.0235,
      "step": 619900
    },
    {
      "epoch": 132.93310463121784,
      "grad_norm": 6.006900787353516,
      "learning_rate": 1.1250062500000002e-05,
      "loss": 3.9792,
      "step": 620000
    },
    {
      "epoch": 132.95454545454547,
      "grad_norm": 6.388037204742432,
      "learning_rate": 1.12438125e-05,
      "loss": 4.0965,
      "step": 620100
    },
    {
      "epoch": 132.97598627787306,
      "grad_norm": 6.682453155517578,
      "learning_rate": 1.1237562500000001e-05,
      "loss": 4.0005,
      "step": 620200
    },
    {
      "epoch": 132.99742710120069,
      "grad_norm": 6.094842433929443,
      "learning_rate": 1.1231312500000001e-05,
      "loss": 4.0062,
      "step": 620300
    },
    {
      "epoch": 133.0188679245283,
      "grad_norm": 5.540489673614502,
      "learning_rate": 1.1225062500000001e-05,
      "loss": 3.9908,
      "step": 620400
    },
    {
      "epoch": 133.04030874785593,
      "grad_norm": 5.811652183532715,
      "learning_rate": 1.1218812500000001e-05,
      "loss": 4.0367,
      "step": 620500
    },
    {
      "epoch": 133.06174957118353,
      "grad_norm": 5.63046407699585,
      "learning_rate": 1.12125625e-05,
      "loss": 4.0248,
      "step": 620600
    },
    {
      "epoch": 133.08319039451115,
      "grad_norm": 6.371542453765869,
      "learning_rate": 1.12063125e-05,
      "loss": 3.9429,
      "step": 620700
    },
    {
      "epoch": 133.10463121783877,
      "grad_norm": 6.923498153686523,
      "learning_rate": 1.12000625e-05,
      "loss": 4.0335,
      "step": 620800
    },
    {
      "epoch": 133.12607204116637,
      "grad_norm": 5.470812797546387,
      "learning_rate": 1.11938125e-05,
      "loss": 4.0027,
      "step": 620900
    },
    {
      "epoch": 133.147512864494,
      "grad_norm": 6.294493675231934,
      "learning_rate": 1.1187562500000002e-05,
      "loss": 4.0317,
      "step": 621000
    },
    {
      "epoch": 133.1689536878216,
      "grad_norm": 6.737648963928223,
      "learning_rate": 1.11813125e-05,
      "loss": 3.9511,
      "step": 621100
    },
    {
      "epoch": 133.19039451114924,
      "grad_norm": 5.877333641052246,
      "learning_rate": 1.1175062500000002e-05,
      "loss": 4.0042,
      "step": 621200
    },
    {
      "epoch": 133.21183533447683,
      "grad_norm": 6.41827917098999,
      "learning_rate": 1.11688125e-05,
      "loss": 4.0004,
      "step": 621300
    },
    {
      "epoch": 133.23327615780445,
      "grad_norm": 6.22472620010376,
      "learning_rate": 1.1162562500000001e-05,
      "loss": 4.033,
      "step": 621400
    },
    {
      "epoch": 133.25471698113208,
      "grad_norm": 6.198545932769775,
      "learning_rate": 1.1156312500000001e-05,
      "loss": 3.9976,
      "step": 621500
    },
    {
      "epoch": 133.2761578044597,
      "grad_norm": 6.350452899932861,
      "learning_rate": 1.1150062500000001e-05,
      "loss": 3.9539,
      "step": 621600
    },
    {
      "epoch": 133.2975986277873,
      "grad_norm": 5.8063645362854,
      "learning_rate": 1.11438125e-05,
      "loss": 4.0333,
      "step": 621700
    },
    {
      "epoch": 133.31903945111492,
      "grad_norm": 6.000504493713379,
      "learning_rate": 1.11375625e-05,
      "loss": 4.0073,
      "step": 621800
    },
    {
      "epoch": 133.34048027444254,
      "grad_norm": 5.887099742889404,
      "learning_rate": 1.11313125e-05,
      "loss": 3.9713,
      "step": 621900
    },
    {
      "epoch": 133.36192109777016,
      "grad_norm": 5.785624027252197,
      "learning_rate": 1.11250625e-05,
      "loss": 3.9909,
      "step": 622000
    },
    {
      "epoch": 133.38336192109776,
      "grad_norm": 5.9127020835876465,
      "learning_rate": 1.11188125e-05,
      "loss": 4.0311,
      "step": 622100
    },
    {
      "epoch": 133.40480274442538,
      "grad_norm": 5.987598419189453,
      "learning_rate": 1.1112562500000002e-05,
      "loss": 3.9833,
      "step": 622200
    },
    {
      "epoch": 133.426243567753,
      "grad_norm": 5.7098236083984375,
      "learning_rate": 1.11063125e-05,
      "loss": 3.9517,
      "step": 622300
    },
    {
      "epoch": 133.44768439108063,
      "grad_norm": 7.366232395172119,
      "learning_rate": 1.1100062500000001e-05,
      "loss": 4.0091,
      "step": 622400
    },
    {
      "epoch": 133.46912521440822,
      "grad_norm": 5.531813144683838,
      "learning_rate": 1.10938125e-05,
      "loss": 3.9878,
      "step": 622500
    },
    {
      "epoch": 133.49056603773585,
      "grad_norm": 5.890875816345215,
      "learning_rate": 1.1087562500000001e-05,
      "loss": 3.8988,
      "step": 622600
    },
    {
      "epoch": 133.51200686106347,
      "grad_norm": 5.938722133636475,
      "learning_rate": 1.1081312500000001e-05,
      "loss": 3.9778,
      "step": 622700
    },
    {
      "epoch": 133.5334476843911,
      "grad_norm": 5.795599937438965,
      "learning_rate": 1.10750625e-05,
      "loss": 3.9576,
      "step": 622800
    },
    {
      "epoch": 133.5548885077187,
      "grad_norm": 5.364099502563477,
      "learning_rate": 1.10688125e-05,
      "loss": 3.9333,
      "step": 622900
    },
    {
      "epoch": 133.5763293310463,
      "grad_norm": 6.515252113342285,
      "learning_rate": 1.10625625e-05,
      "loss": 3.9894,
      "step": 623000
    },
    {
      "epoch": 133.59777015437393,
      "grad_norm": 5.959033489227295,
      "learning_rate": 1.10563125e-05,
      "loss": 4.0105,
      "step": 623100
    },
    {
      "epoch": 133.61921097770156,
      "grad_norm": 6.868414402008057,
      "learning_rate": 1.10500625e-05,
      "loss": 4.0063,
      "step": 623200
    },
    {
      "epoch": 133.64065180102915,
      "grad_norm": 6.538573741912842,
      "learning_rate": 1.10438125e-05,
      "loss": 3.9644,
      "step": 623300
    },
    {
      "epoch": 133.66209262435677,
      "grad_norm": 6.382094860076904,
      "learning_rate": 1.1037562500000002e-05,
      "loss": 3.9582,
      "step": 623400
    },
    {
      "epoch": 133.6835334476844,
      "grad_norm": 6.030552387237549,
      "learning_rate": 1.10313125e-05,
      "loss": 4.0103,
      "step": 623500
    },
    {
      "epoch": 133.70497427101202,
      "grad_norm": 5.849234104156494,
      "learning_rate": 1.1025062500000001e-05,
      "loss": 3.9732,
      "step": 623600
    },
    {
      "epoch": 133.72641509433961,
      "grad_norm": 5.930737495422363,
      "learning_rate": 1.1018812500000001e-05,
      "loss": 3.9367,
      "step": 623700
    },
    {
      "epoch": 133.74785591766724,
      "grad_norm": 5.475716590881348,
      "learning_rate": 1.1012562500000001e-05,
      "loss": 3.906,
      "step": 623800
    },
    {
      "epoch": 133.76929674099486,
      "grad_norm": 5.753244876861572,
      "learning_rate": 1.1006312500000001e-05,
      "loss": 3.9712,
      "step": 623900
    },
    {
      "epoch": 133.79073756432248,
      "grad_norm": 5.866084098815918,
      "learning_rate": 1.10000625e-05,
      "loss": 3.9363,
      "step": 624000
    },
    {
      "epoch": 133.81217838765008,
      "grad_norm": 6.324174880981445,
      "learning_rate": 1.09938125e-05,
      "loss": 3.9069,
      "step": 624100
    },
    {
      "epoch": 133.8336192109777,
      "grad_norm": 6.302420616149902,
      "learning_rate": 1.09875625e-05,
      "loss": 3.9963,
      "step": 624200
    },
    {
      "epoch": 133.85506003430532,
      "grad_norm": 5.967585563659668,
      "learning_rate": 1.09813125e-05,
      "loss": 3.9561,
      "step": 624300
    },
    {
      "epoch": 133.87650085763292,
      "grad_norm": 5.840359687805176,
      "learning_rate": 1.0975062500000002e-05,
      "loss": 3.9366,
      "step": 624400
    },
    {
      "epoch": 133.89794168096054,
      "grad_norm": 5.490501880645752,
      "learning_rate": 1.09688125e-05,
      "loss": 3.9242,
      "step": 624500
    },
    {
      "epoch": 133.91938250428817,
      "grad_norm": 6.066132545471191,
      "learning_rate": 1.0962562500000001e-05,
      "loss": 3.9833,
      "step": 624600
    },
    {
      "epoch": 133.9408233276158,
      "grad_norm": 5.864774227142334,
      "learning_rate": 1.09563125e-05,
      "loss": 3.9627,
      "step": 624700
    },
    {
      "epoch": 133.96226415094338,
      "grad_norm": 6.036484718322754,
      "learning_rate": 1.0950062500000001e-05,
      "loss": 3.9755,
      "step": 624800
    },
    {
      "epoch": 133.983704974271,
      "grad_norm": 5.704857349395752,
      "learning_rate": 1.0943812500000001e-05,
      "loss": 3.9741,
      "step": 624900
    },
    {
      "epoch": 134.00514579759863,
      "grad_norm": 5.9872612953186035,
      "learning_rate": 1.0937562500000001e-05,
      "loss": 3.9584,
      "step": 625000
    },
    {
      "epoch": 134.02658662092625,
      "grad_norm": 6.4450907707214355,
      "learning_rate": 1.09313125e-05,
      "loss": 3.9845,
      "step": 625100
    },
    {
      "epoch": 134.04802744425385,
      "grad_norm": 5.959571838378906,
      "learning_rate": 1.09250625e-05,
      "loss": 3.9098,
      "step": 625200
    },
    {
      "epoch": 134.06946826758147,
      "grad_norm": 6.031853199005127,
      "learning_rate": 1.09188125e-05,
      "loss": 4.0041,
      "step": 625300
    },
    {
      "epoch": 134.0909090909091,
      "grad_norm": 6.163605690002441,
      "learning_rate": 1.09125625e-05,
      "loss": 3.9877,
      "step": 625400
    },
    {
      "epoch": 134.11234991423672,
      "grad_norm": 5.7803635597229,
      "learning_rate": 1.09063125e-05,
      "loss": 3.924,
      "step": 625500
    },
    {
      "epoch": 134.1337907375643,
      "grad_norm": 6.011051654815674,
      "learning_rate": 1.0900062500000002e-05,
      "loss": 3.915,
      "step": 625600
    },
    {
      "epoch": 134.15523156089193,
      "grad_norm": 6.072082042694092,
      "learning_rate": 1.08938125e-05,
      "loss": 3.9011,
      "step": 625700
    },
    {
      "epoch": 134.17667238421956,
      "grad_norm": 6.490401268005371,
      "learning_rate": 1.0887562500000001e-05,
      "loss": 3.9181,
      "step": 625800
    },
    {
      "epoch": 134.19811320754718,
      "grad_norm": 6.036462783813477,
      "learning_rate": 1.08813125e-05,
      "loss": 3.9422,
      "step": 625900
    },
    {
      "epoch": 134.21955403087478,
      "grad_norm": 5.893373489379883,
      "learning_rate": 1.0875062500000001e-05,
      "loss": 3.9088,
      "step": 626000
    },
    {
      "epoch": 134.2409948542024,
      "grad_norm": 6.804598808288574,
      "learning_rate": 1.0868812500000001e-05,
      "loss": 4.0,
      "step": 626100
    },
    {
      "epoch": 134.26243567753002,
      "grad_norm": 6.097489356994629,
      "learning_rate": 1.08625625e-05,
      "loss": 4.0162,
      "step": 626200
    },
    {
      "epoch": 134.28387650085764,
      "grad_norm": 5.773850440979004,
      "learning_rate": 1.08563125e-05,
      "loss": 3.9556,
      "step": 626300
    },
    {
      "epoch": 134.30531732418524,
      "grad_norm": 6.2277326583862305,
      "learning_rate": 1.08500625e-05,
      "loss": 4.0047,
      "step": 626400
    },
    {
      "epoch": 134.32675814751286,
      "grad_norm": 6.031414985656738,
      "learning_rate": 1.08438125e-05,
      "loss": 3.9848,
      "step": 626500
    },
    {
      "epoch": 134.34819897084049,
      "grad_norm": 6.44569730758667,
      "learning_rate": 1.08375625e-05,
      "loss": 3.9816,
      "step": 626600
    },
    {
      "epoch": 134.3696397941681,
      "grad_norm": 6.357348442077637,
      "learning_rate": 1.08313125e-05,
      "loss": 3.9204,
      "step": 626700
    },
    {
      "epoch": 134.3910806174957,
      "grad_norm": 6.099053859710693,
      "learning_rate": 1.0825062500000002e-05,
      "loss": 3.9012,
      "step": 626800
    },
    {
      "epoch": 134.41252144082333,
      "grad_norm": 6.2468132972717285,
      "learning_rate": 1.08188125e-05,
      "loss": 3.9844,
      "step": 626900
    },
    {
      "epoch": 134.43396226415095,
      "grad_norm": 6.359466552734375,
      "learning_rate": 1.0812562500000001e-05,
      "loss": 3.9175,
      "step": 627000
    },
    {
      "epoch": 134.45540308747857,
      "grad_norm": 6.296996593475342,
      "learning_rate": 1.08063125e-05,
      "loss": 3.9529,
      "step": 627100
    },
    {
      "epoch": 134.47684391080617,
      "grad_norm": 6.192467212677002,
      "learning_rate": 1.0800062500000001e-05,
      "loss": 3.9162,
      "step": 627200
    },
    {
      "epoch": 134.4982847341338,
      "grad_norm": 6.158371448516846,
      "learning_rate": 1.07938125e-05,
      "loss": 3.9846,
      "step": 627300
    },
    {
      "epoch": 134.5197255574614,
      "grad_norm": 6.872250080108643,
      "learning_rate": 1.07875625e-05,
      "loss": 3.9003,
      "step": 627400
    },
    {
      "epoch": 134.54116638078904,
      "grad_norm": 5.9407639503479,
      "learning_rate": 1.07813125e-05,
      "loss": 3.9417,
      "step": 627500
    },
    {
      "epoch": 134.56260720411663,
      "grad_norm": 5.970428466796875,
      "learning_rate": 1.07750625e-05,
      "loss": 3.8999,
      "step": 627600
    },
    {
      "epoch": 134.58404802744425,
      "grad_norm": 6.406899452209473,
      "learning_rate": 1.07688125e-05,
      "loss": 3.9541,
      "step": 627700
    },
    {
      "epoch": 134.60548885077188,
      "grad_norm": 6.51137113571167,
      "learning_rate": 1.07625625e-05,
      "loss": 3.9683,
      "step": 627800
    },
    {
      "epoch": 134.62692967409947,
      "grad_norm": 7.0258893966674805,
      "learning_rate": 1.07563125e-05,
      "loss": 3.9684,
      "step": 627900
    },
    {
      "epoch": 134.6483704974271,
      "grad_norm": 6.419306755065918,
      "learning_rate": 1.0750062500000001e-05,
      "loss": 3.9123,
      "step": 628000
    },
    {
      "epoch": 134.66981132075472,
      "grad_norm": 7.969782829284668,
      "learning_rate": 1.07438125e-05,
      "loss": 3.958,
      "step": 628100
    },
    {
      "epoch": 134.69125214408234,
      "grad_norm": 6.54944372177124,
      "learning_rate": 1.0737562500000001e-05,
      "loss": 3.9458,
      "step": 628200
    },
    {
      "epoch": 134.71269296740994,
      "grad_norm": 5.9901556968688965,
      "learning_rate": 1.07313125e-05,
      "loss": 3.9573,
      "step": 628300
    },
    {
      "epoch": 134.73413379073756,
      "grad_norm": 6.683938980102539,
      "learning_rate": 1.07250625e-05,
      "loss": 3.9195,
      "step": 628400
    },
    {
      "epoch": 134.75557461406518,
      "grad_norm": 5.892124176025391,
      "learning_rate": 1.07188125e-05,
      "loss": 3.9568,
      "step": 628500
    },
    {
      "epoch": 134.7770154373928,
      "grad_norm": 6.084361553192139,
      "learning_rate": 1.07125625e-05,
      "loss": 3.9417,
      "step": 628600
    },
    {
      "epoch": 134.7984562607204,
      "grad_norm": 5.65487003326416,
      "learning_rate": 1.07063125e-05,
      "loss": 3.9813,
      "step": 628700
    },
    {
      "epoch": 134.81989708404802,
      "grad_norm": 5.869820594787598,
      "learning_rate": 1.07000625e-05,
      "loss": 3.8509,
      "step": 628800
    },
    {
      "epoch": 134.84133790737565,
      "grad_norm": 5.574516773223877,
      "learning_rate": 1.06938125e-05,
      "loss": 3.9276,
      "step": 628900
    },
    {
      "epoch": 134.86277873070327,
      "grad_norm": 5.651790142059326,
      "learning_rate": 1.06875625e-05,
      "loss": 3.9733,
      "step": 629000
    },
    {
      "epoch": 134.88421955403086,
      "grad_norm": 6.102755546569824,
      "learning_rate": 1.06813125e-05,
      "loss": 3.9483,
      "step": 629100
    },
    {
      "epoch": 134.9056603773585,
      "grad_norm": 5.850857734680176,
      "learning_rate": 1.0675062500000001e-05,
      "loss": 3.9831,
      "step": 629200
    },
    {
      "epoch": 134.9271012006861,
      "grad_norm": 6.738445281982422,
      "learning_rate": 1.06688125e-05,
      "loss": 3.8981,
      "step": 629300
    },
    {
      "epoch": 134.94854202401373,
      "grad_norm": 5.76391077041626,
      "learning_rate": 1.0662562500000001e-05,
      "loss": 3.9814,
      "step": 629400
    },
    {
      "epoch": 134.96998284734133,
      "grad_norm": 5.9782395362854,
      "learning_rate": 1.06563125e-05,
      "loss": 3.9444,
      "step": 629500
    },
    {
      "epoch": 134.99142367066895,
      "grad_norm": 6.664828300476074,
      "learning_rate": 1.06500625e-05,
      "loss": 3.92,
      "step": 629600
    },
    {
      "epoch": 135.01286449399657,
      "grad_norm": 5.453591346740723,
      "learning_rate": 1.06438125e-05,
      "loss": 3.971,
      "step": 629700
    },
    {
      "epoch": 135.0343053173242,
      "grad_norm": 7.010724067687988,
      "learning_rate": 1.06375625e-05,
      "loss": 3.9738,
      "step": 629800
    },
    {
      "epoch": 135.0557461406518,
      "grad_norm": 5.670107364654541,
      "learning_rate": 1.06313125e-05,
      "loss": 3.9463,
      "step": 629900
    },
    {
      "epoch": 135.07718696397941,
      "grad_norm": 6.350174903869629,
      "learning_rate": 1.06250625e-05,
      "loss": 3.9615,
      "step": 630000
    },
    {
      "epoch": 135.09862778730704,
      "grad_norm": 6.485413074493408,
      "learning_rate": 1.06188125e-05,
      "loss": 3.9518,
      "step": 630100
    },
    {
      "epoch": 135.12006861063466,
      "grad_norm": 5.7929606437683105,
      "learning_rate": 1.0612562500000001e-05,
      "loss": 3.8657,
      "step": 630200
    },
    {
      "epoch": 135.14150943396226,
      "grad_norm": 5.935987949371338,
      "learning_rate": 1.06063125e-05,
      "loss": 3.9176,
      "step": 630300
    },
    {
      "epoch": 135.16295025728988,
      "grad_norm": 5.829982757568359,
      "learning_rate": 1.0600062500000001e-05,
      "loss": 3.9816,
      "step": 630400
    },
    {
      "epoch": 135.1843910806175,
      "grad_norm": 5.701885223388672,
      "learning_rate": 1.05938125e-05,
      "loss": 3.8755,
      "step": 630500
    },
    {
      "epoch": 135.20583190394512,
      "grad_norm": 5.652773857116699,
      "learning_rate": 1.05875625e-05,
      "loss": 3.9266,
      "step": 630600
    },
    {
      "epoch": 135.22727272727272,
      "grad_norm": 6.371731758117676,
      "learning_rate": 1.05813125e-05,
      "loss": 3.8737,
      "step": 630700
    },
    {
      "epoch": 135.24871355060034,
      "grad_norm": 7.102472305297852,
      "learning_rate": 1.05750625e-05,
      "loss": 3.8916,
      "step": 630800
    },
    {
      "epoch": 135.27015437392797,
      "grad_norm": 6.313286781311035,
      "learning_rate": 1.05688125e-05,
      "loss": 3.9197,
      "step": 630900
    },
    {
      "epoch": 135.29159519725556,
      "grad_norm": 6.422595977783203,
      "learning_rate": 1.05625625e-05,
      "loss": 3.9247,
      "step": 631000
    },
    {
      "epoch": 135.31303602058318,
      "grad_norm": 7.169744968414307,
      "learning_rate": 1.05563125e-05,
      "loss": 3.9339,
      "step": 631100
    },
    {
      "epoch": 135.3344768439108,
      "grad_norm": 5.944256782531738,
      "learning_rate": 1.05500625e-05,
      "loss": 3.9394,
      "step": 631200
    },
    {
      "epoch": 135.35591766723843,
      "grad_norm": 5.541204929351807,
      "learning_rate": 1.05438125e-05,
      "loss": 3.9209,
      "step": 631300
    },
    {
      "epoch": 135.37735849056602,
      "grad_norm": 6.589775562286377,
      "learning_rate": 1.0537562500000001e-05,
      "loss": 3.8953,
      "step": 631400
    },
    {
      "epoch": 135.39879931389365,
      "grad_norm": 6.767045497894287,
      "learning_rate": 1.05313125e-05,
      "loss": 3.8942,
      "step": 631500
    },
    {
      "epoch": 135.42024013722127,
      "grad_norm": 6.460496425628662,
      "learning_rate": 1.0525062500000001e-05,
      "loss": 3.874,
      "step": 631600
    },
    {
      "epoch": 135.4416809605489,
      "grad_norm": 6.338187217712402,
      "learning_rate": 1.05188125e-05,
      "loss": 3.901,
      "step": 631700
    },
    {
      "epoch": 135.4631217838765,
      "grad_norm": 5.684137344360352,
      "learning_rate": 1.05125625e-05,
      "loss": 3.8648,
      "step": 631800
    },
    {
      "epoch": 135.4845626072041,
      "grad_norm": 6.018472194671631,
      "learning_rate": 1.05063125e-05,
      "loss": 3.8746,
      "step": 631900
    },
    {
      "epoch": 135.50600343053173,
      "grad_norm": 5.812869071960449,
      "learning_rate": 1.05000625e-05,
      "loss": 3.9114,
      "step": 632000
    },
    {
      "epoch": 135.52744425385936,
      "grad_norm": 5.794755935668945,
      "learning_rate": 1.04938125e-05,
      "loss": 3.9543,
      "step": 632100
    },
    {
      "epoch": 135.54888507718695,
      "grad_norm": 6.418732166290283,
      "learning_rate": 1.04875625e-05,
      "loss": 3.9395,
      "step": 632200
    },
    {
      "epoch": 135.57032590051458,
      "grad_norm": 6.566527843475342,
      "learning_rate": 1.04813125e-05,
      "loss": 3.913,
      "step": 632300
    },
    {
      "epoch": 135.5917667238422,
      "grad_norm": 6.156482219696045,
      "learning_rate": 1.04750625e-05,
      "loss": 3.9313,
      "step": 632400
    },
    {
      "epoch": 135.61320754716982,
      "grad_norm": 6.447478771209717,
      "learning_rate": 1.0468812500000001e-05,
      "loss": 3.8912,
      "step": 632500
    },
    {
      "epoch": 135.63464837049742,
      "grad_norm": 6.5473222732543945,
      "learning_rate": 1.0462562500000001e-05,
      "loss": 3.942,
      "step": 632600
    },
    {
      "epoch": 135.65608919382504,
      "grad_norm": 6.073878765106201,
      "learning_rate": 1.0456312500000001e-05,
      "loss": 3.9437,
      "step": 632700
    },
    {
      "epoch": 135.67753001715266,
      "grad_norm": 5.7927937507629395,
      "learning_rate": 1.0450062500000001e-05,
      "loss": 3.9198,
      "step": 632800
    },
    {
      "epoch": 135.69897084048029,
      "grad_norm": 6.034496307373047,
      "learning_rate": 1.04438125e-05,
      "loss": 3.8891,
      "step": 632900
    },
    {
      "epoch": 135.72041166380788,
      "grad_norm": 6.652186393737793,
      "learning_rate": 1.04375625e-05,
      "loss": 3.9045,
      "step": 633000
    },
    {
      "epoch": 135.7418524871355,
      "grad_norm": 6.470897197723389,
      "learning_rate": 1.04313125e-05,
      "loss": 3.8775,
      "step": 633100
    },
    {
      "epoch": 135.76329331046313,
      "grad_norm": 6.707894802093506,
      "learning_rate": 1.04250625e-05,
      "loss": 3.8998,
      "step": 633200
    },
    {
      "epoch": 135.78473413379075,
      "grad_norm": 6.264459133148193,
      "learning_rate": 1.0418812500000002e-05,
      "loss": 3.8954,
      "step": 633300
    },
    {
      "epoch": 135.80617495711834,
      "grad_norm": 6.1061530113220215,
      "learning_rate": 1.04125625e-05,
      "loss": 3.9203,
      "step": 633400
    },
    {
      "epoch": 135.82761578044597,
      "grad_norm": 5.44601583480835,
      "learning_rate": 1.0406312500000002e-05,
      "loss": 3.9417,
      "step": 633500
    },
    {
      "epoch": 135.8490566037736,
      "grad_norm": 6.0685858726501465,
      "learning_rate": 1.04000625e-05,
      "loss": 3.9318,
      "step": 633600
    },
    {
      "epoch": 135.8704974271012,
      "grad_norm": 5.849567890167236,
      "learning_rate": 1.0393812500000001e-05,
      "loss": 3.8971,
      "step": 633700
    },
    {
      "epoch": 135.8919382504288,
      "grad_norm": 5.6233954429626465,
      "learning_rate": 1.0387562500000001e-05,
      "loss": 3.8788,
      "step": 633800
    },
    {
      "epoch": 135.91337907375643,
      "grad_norm": 6.893620014190674,
      "learning_rate": 1.0381312500000001e-05,
      "loss": 3.8927,
      "step": 633900
    },
    {
      "epoch": 135.93481989708405,
      "grad_norm": 5.726417064666748,
      "learning_rate": 1.03750625e-05,
      "loss": 3.9382,
      "step": 634000
    },
    {
      "epoch": 135.95626072041168,
      "grad_norm": 6.03634786605835,
      "learning_rate": 1.03688125e-05,
      "loss": 3.9006,
      "step": 634100
    },
    {
      "epoch": 135.97770154373927,
      "grad_norm": 6.562643051147461,
      "learning_rate": 1.03625625e-05,
      "loss": 3.963,
      "step": 634200
    },
    {
      "epoch": 135.9991423670669,
      "grad_norm": 6.75887393951416,
      "learning_rate": 1.03563125e-05,
      "loss": 3.9446,
      "step": 634300
    },
    {
      "epoch": 136.02058319039452,
      "grad_norm": 6.020671367645264,
      "learning_rate": 1.03500625e-05,
      "loss": 3.8574,
      "step": 634400
    },
    {
      "epoch": 136.0420240137221,
      "grad_norm": 6.061811447143555,
      "learning_rate": 1.0343812500000002e-05,
      "loss": 3.8716,
      "step": 634500
    },
    {
      "epoch": 136.06346483704974,
      "grad_norm": 6.442028045654297,
      "learning_rate": 1.03375625e-05,
      "loss": 3.9139,
      "step": 634600
    },
    {
      "epoch": 136.08490566037736,
      "grad_norm": 6.15325403213501,
      "learning_rate": 1.0331312500000001e-05,
      "loss": 3.9009,
      "step": 634700
    },
    {
      "epoch": 136.10634648370498,
      "grad_norm": 6.825167179107666,
      "learning_rate": 1.03250625e-05,
      "loss": 3.8599,
      "step": 634800
    },
    {
      "epoch": 136.12778730703258,
      "grad_norm": 5.654301166534424,
      "learning_rate": 1.0318812500000001e-05,
      "loss": 3.9432,
      "step": 634900
    },
    {
      "epoch": 136.1492281303602,
      "grad_norm": 6.5829548835754395,
      "learning_rate": 1.0312562500000001e-05,
      "loss": 3.91,
      "step": 635000
    },
    {
      "epoch": 136.17066895368782,
      "grad_norm": 6.1117682456970215,
      "learning_rate": 1.03063125e-05,
      "loss": 3.8645,
      "step": 635100
    },
    {
      "epoch": 136.19210977701545,
      "grad_norm": 6.457761287689209,
      "learning_rate": 1.03000625e-05,
      "loss": 3.8989,
      "step": 635200
    },
    {
      "epoch": 136.21355060034304,
      "grad_norm": 6.494921684265137,
      "learning_rate": 1.02938125e-05,
      "loss": 3.9396,
      "step": 635300
    },
    {
      "epoch": 136.23499142367066,
      "grad_norm": 6.454315185546875,
      "learning_rate": 1.02875625e-05,
      "loss": 3.8482,
      "step": 635400
    },
    {
      "epoch": 136.2564322469983,
      "grad_norm": 5.787158012390137,
      "learning_rate": 1.02813125e-05,
      "loss": 3.8632,
      "step": 635500
    },
    {
      "epoch": 136.2778730703259,
      "grad_norm": 6.163175582885742,
      "learning_rate": 1.02750625e-05,
      "loss": 3.8792,
      "step": 635600
    },
    {
      "epoch": 136.2993138936535,
      "grad_norm": 6.238912105560303,
      "learning_rate": 1.0268812500000002e-05,
      "loss": 3.8594,
      "step": 635700
    },
    {
      "epoch": 136.32075471698113,
      "grad_norm": 5.733357906341553,
      "learning_rate": 1.02625625e-05,
      "loss": 3.8956,
      "step": 635800
    },
    {
      "epoch": 136.34219554030875,
      "grad_norm": 5.9189348220825195,
      "learning_rate": 1.0256312500000001e-05,
      "loss": 3.8532,
      "step": 635900
    },
    {
      "epoch": 136.36363636363637,
      "grad_norm": 6.379335403442383,
      "learning_rate": 1.0250062500000001e-05,
      "loss": 3.9061,
      "step": 636000
    },
    {
      "epoch": 136.38507718696397,
      "grad_norm": 5.734580993652344,
      "learning_rate": 1.0243812500000001e-05,
      "loss": 3.8853,
      "step": 636100
    },
    {
      "epoch": 136.4065180102916,
      "grad_norm": 6.187445640563965,
      "learning_rate": 1.02375625e-05,
      "loss": 3.9019,
      "step": 636200
    },
    {
      "epoch": 136.42795883361921,
      "grad_norm": 5.761856555938721,
      "learning_rate": 1.02313125e-05,
      "loss": 3.9048,
      "step": 636300
    },
    {
      "epoch": 136.44939965694684,
      "grad_norm": 5.9935688972473145,
      "learning_rate": 1.02250625e-05,
      "loss": 3.8657,
      "step": 636400
    },
    {
      "epoch": 136.47084048027443,
      "grad_norm": 6.645451545715332,
      "learning_rate": 1.02188125e-05,
      "loss": 3.8862,
      "step": 636500
    },
    {
      "epoch": 136.49228130360206,
      "grad_norm": 6.097968578338623,
      "learning_rate": 1.02125625e-05,
      "loss": 3.8967,
      "step": 636600
    },
    {
      "epoch": 136.51372212692968,
      "grad_norm": 6.101292133331299,
      "learning_rate": 1.0206312500000002e-05,
      "loss": 3.8862,
      "step": 636700
    },
    {
      "epoch": 136.5351629502573,
      "grad_norm": 6.560171127319336,
      "learning_rate": 1.02000625e-05,
      "loss": 3.8926,
      "step": 636800
    },
    {
      "epoch": 136.5566037735849,
      "grad_norm": 6.457268238067627,
      "learning_rate": 1.0193812500000001e-05,
      "loss": 3.9152,
      "step": 636900
    },
    {
      "epoch": 136.57804459691252,
      "grad_norm": 5.336522102355957,
      "learning_rate": 1.01875625e-05,
      "loss": 3.8897,
      "step": 637000
    },
    {
      "epoch": 136.59948542024014,
      "grad_norm": 6.244125843048096,
      "learning_rate": 1.0181312500000001e-05,
      "loss": 3.8926,
      "step": 637100
    },
    {
      "epoch": 136.62092624356777,
      "grad_norm": 6.45878791809082,
      "learning_rate": 1.0175062500000001e-05,
      "loss": 3.8757,
      "step": 637200
    },
    {
      "epoch": 136.64236706689536,
      "grad_norm": 7.254731178283691,
      "learning_rate": 1.01688125e-05,
      "loss": 3.8892,
      "step": 637300
    },
    {
      "epoch": 136.66380789022298,
      "grad_norm": 6.31600284576416,
      "learning_rate": 1.01625625e-05,
      "loss": 3.8518,
      "step": 637400
    },
    {
      "epoch": 136.6852487135506,
      "grad_norm": 5.6716437339782715,
      "learning_rate": 1.01563125e-05,
      "loss": 3.9022,
      "step": 637500
    },
    {
      "epoch": 136.70668953687823,
      "grad_norm": 5.8493146896362305,
      "learning_rate": 1.01500625e-05,
      "loss": 3.9405,
      "step": 637600
    },
    {
      "epoch": 136.72813036020582,
      "grad_norm": 5.759970188140869,
      "learning_rate": 1.01438125e-05,
      "loss": 3.8666,
      "step": 637700
    },
    {
      "epoch": 136.74957118353345,
      "grad_norm": 5.766616344451904,
      "learning_rate": 1.01375625e-05,
      "loss": 3.8588,
      "step": 637800
    },
    {
      "epoch": 136.77101200686107,
      "grad_norm": 6.673479080200195,
      "learning_rate": 1.0131312500000002e-05,
      "loss": 3.9188,
      "step": 637900
    },
    {
      "epoch": 136.79245283018867,
      "grad_norm": 6.787637233734131,
      "learning_rate": 1.01250625e-05,
      "loss": 3.8854,
      "step": 638000
    },
    {
      "epoch": 136.8138936535163,
      "grad_norm": 6.597579479217529,
      "learning_rate": 1.0118812500000001e-05,
      "loss": 3.8429,
      "step": 638100
    },
    {
      "epoch": 136.8353344768439,
      "grad_norm": 6.866049766540527,
      "learning_rate": 1.01125625e-05,
      "loss": 3.9677,
      "step": 638200
    },
    {
      "epoch": 136.85677530017153,
      "grad_norm": 6.542141914367676,
      "learning_rate": 1.0106312500000001e-05,
      "loss": 3.9399,
      "step": 638300
    },
    {
      "epoch": 136.87821612349913,
      "grad_norm": 5.642463684082031,
      "learning_rate": 1.01000625e-05,
      "loss": 3.8653,
      "step": 638400
    },
    {
      "epoch": 136.89965694682675,
      "grad_norm": 6.383871555328369,
      "learning_rate": 1.00938125e-05,
      "loss": 3.8612,
      "step": 638500
    },
    {
      "epoch": 136.92109777015438,
      "grad_norm": 5.966736793518066,
      "learning_rate": 1.00875625e-05,
      "loss": 3.8999,
      "step": 638600
    },
    {
      "epoch": 136.942538593482,
      "grad_norm": 5.992005825042725,
      "learning_rate": 1.00813125e-05,
      "loss": 3.8561,
      "step": 638700
    },
    {
      "epoch": 136.9639794168096,
      "grad_norm": 6.2366838455200195,
      "learning_rate": 1.00750625e-05,
      "loss": 3.8802,
      "step": 638800
    },
    {
      "epoch": 136.98542024013722,
      "grad_norm": 6.302860736846924,
      "learning_rate": 1.00688125e-05,
      "loss": 3.9065,
      "step": 638900
    },
    {
      "epoch": 137.00686106346484,
      "grad_norm": 6.19704532623291,
      "learning_rate": 1.00625625e-05,
      "loss": 3.917,
      "step": 639000
    },
    {
      "epoch": 137.02830188679246,
      "grad_norm": 5.741844654083252,
      "learning_rate": 1.0056312500000001e-05,
      "loss": 3.8468,
      "step": 639100
    },
    {
      "epoch": 137.04974271012006,
      "grad_norm": 6.523345470428467,
      "learning_rate": 1.00500625e-05,
      "loss": 3.8241,
      "step": 639200
    },
    {
      "epoch": 137.07118353344768,
      "grad_norm": 6.108356475830078,
      "learning_rate": 1.0043812500000001e-05,
      "loss": 3.8896,
      "step": 639300
    },
    {
      "epoch": 137.0926243567753,
      "grad_norm": 5.751429557800293,
      "learning_rate": 1.00375625e-05,
      "loss": 3.87,
      "step": 639400
    },
    {
      "epoch": 137.11406518010293,
      "grad_norm": 5.653985977172852,
      "learning_rate": 1.0031312500000001e-05,
      "loss": 3.8654,
      "step": 639500
    },
    {
      "epoch": 137.13550600343052,
      "grad_norm": 6.018007278442383,
      "learning_rate": 1.00250625e-05,
      "loss": 3.8712,
      "step": 639600
    },
    {
      "epoch": 137.15694682675814,
      "grad_norm": 6.0693888664245605,
      "learning_rate": 1.00188125e-05,
      "loss": 3.8861,
      "step": 639700
    },
    {
      "epoch": 137.17838765008577,
      "grad_norm": 6.201247692108154,
      "learning_rate": 1.00125625e-05,
      "loss": 3.8446,
      "step": 639800
    },
    {
      "epoch": 137.1998284734134,
      "grad_norm": 6.659412860870361,
      "learning_rate": 1.00063125e-05,
      "loss": 3.8692,
      "step": 639900
    },
    {
      "epoch": 137.22126929674099,
      "grad_norm": 6.068946361541748,
      "learning_rate": 1.00000625e-05,
      "loss": 3.9203,
      "step": 640000
    },
    {
      "epoch": 137.2427101200686,
      "grad_norm": 8.308708190917969,
      "learning_rate": 9.9938125e-06,
      "loss": 3.9118,
      "step": 640100
    },
    {
      "epoch": 137.26415094339623,
      "grad_norm": 6.490332126617432,
      "learning_rate": 9.9875625e-06,
      "loss": 3.8291,
      "step": 640200
    },
    {
      "epoch": 137.28559176672385,
      "grad_norm": 5.5991387367248535,
      "learning_rate": 9.981312500000001e-06,
      "loss": 3.8899,
      "step": 640300
    },
    {
      "epoch": 137.30703259005145,
      "grad_norm": 6.840065002441406,
      "learning_rate": 9.9750625e-06,
      "loss": 3.8319,
      "step": 640400
    },
    {
      "epoch": 137.32847341337907,
      "grad_norm": 6.247708320617676,
      "learning_rate": 9.968812500000001e-06,
      "loss": 3.8731,
      "step": 640500
    },
    {
      "epoch": 137.3499142367067,
      "grad_norm": 6.274624824523926,
      "learning_rate": 9.9625625e-06,
      "loss": 3.9066,
      "step": 640600
    },
    {
      "epoch": 137.37135506003432,
      "grad_norm": 6.835042476654053,
      "learning_rate": 9.9563125e-06,
      "loss": 3.8463,
      "step": 640700
    },
    {
      "epoch": 137.3927958833619,
      "grad_norm": 7.031444072723389,
      "learning_rate": 9.9500625e-06,
      "loss": 3.885,
      "step": 640800
    },
    {
      "epoch": 137.41423670668954,
      "grad_norm": 6.087455749511719,
      "learning_rate": 9.9438125e-06,
      "loss": 3.8234,
      "step": 640900
    },
    {
      "epoch": 137.43567753001716,
      "grad_norm": 7.500072479248047,
      "learning_rate": 9.9375625e-06,
      "loss": 3.8834,
      "step": 641000
    },
    {
      "epoch": 137.45711835334478,
      "grad_norm": 6.617897033691406,
      "learning_rate": 9.9313125e-06,
      "loss": 3.8574,
      "step": 641100
    },
    {
      "epoch": 137.47855917667238,
      "grad_norm": 6.954105377197266,
      "learning_rate": 9.9250625e-06,
      "loss": 3.8464,
      "step": 641200
    },
    {
      "epoch": 137.5,
      "grad_norm": 5.987370491027832,
      "learning_rate": 9.9188125e-06,
      "loss": 3.8719,
      "step": 641300
    },
    {
      "epoch": 137.52144082332762,
      "grad_norm": 5.79050874710083,
      "learning_rate": 9.9125625e-06,
      "loss": 3.8781,
      "step": 641400
    },
    {
      "epoch": 137.54288164665522,
      "grad_norm": 6.02201509475708,
      "learning_rate": 9.906312500000001e-06,
      "loss": 3.9068,
      "step": 641500
    },
    {
      "epoch": 137.56432246998284,
      "grad_norm": 5.83216667175293,
      "learning_rate": 9.9000625e-06,
      "loss": 3.8666,
      "step": 641600
    },
    {
      "epoch": 137.58576329331046,
      "grad_norm": 5.967967510223389,
      "learning_rate": 9.893812500000001e-06,
      "loss": 3.8092,
      "step": 641700
    },
    {
      "epoch": 137.6072041166381,
      "grad_norm": 6.036029815673828,
      "learning_rate": 9.8875625e-06,
      "loss": 3.8688,
      "step": 641800
    },
    {
      "epoch": 137.62864493996568,
      "grad_norm": 6.094457626342773,
      "learning_rate": 9.8813125e-06,
      "loss": 3.8836,
      "step": 641900
    },
    {
      "epoch": 137.6500857632933,
      "grad_norm": 6.483053207397461,
      "learning_rate": 9.8750625e-06,
      "loss": 3.8473,
      "step": 642000
    },
    {
      "epoch": 137.67152658662093,
      "grad_norm": 6.251136302947998,
      "learning_rate": 9.8688125e-06,
      "loss": 3.9105,
      "step": 642100
    },
    {
      "epoch": 137.69296740994855,
      "grad_norm": 6.71176290512085,
      "learning_rate": 9.8625625e-06,
      "loss": 3.8296,
      "step": 642200
    },
    {
      "epoch": 137.71440823327615,
      "grad_norm": 5.9311113357543945,
      "learning_rate": 9.8563125e-06,
      "loss": 3.8589,
      "step": 642300
    },
    {
      "epoch": 137.73584905660377,
      "grad_norm": 6.564871311187744,
      "learning_rate": 9.8500625e-06,
      "loss": 3.904,
      "step": 642400
    },
    {
      "epoch": 137.7572898799314,
      "grad_norm": 6.576669692993164,
      "learning_rate": 9.843812500000001e-06,
      "loss": 3.887,
      "step": 642500
    },
    {
      "epoch": 137.77873070325901,
      "grad_norm": 6.503785610198975,
      "learning_rate": 9.8375625e-06,
      "loss": 3.7917,
      "step": 642600
    },
    {
      "epoch": 137.8001715265866,
      "grad_norm": 5.868405342102051,
      "learning_rate": 9.831312500000001e-06,
      "loss": 3.8302,
      "step": 642700
    },
    {
      "epoch": 137.82161234991423,
      "grad_norm": 6.586078643798828,
      "learning_rate": 9.8250625e-06,
      "loss": 3.8678,
      "step": 642800
    },
    {
      "epoch": 137.84305317324186,
      "grad_norm": 6.4611639976501465,
      "learning_rate": 9.8188125e-06,
      "loss": 3.8403,
      "step": 642900
    },
    {
      "epoch": 137.86449399656948,
      "grad_norm": 6.044192314147949,
      "learning_rate": 9.8125625e-06,
      "loss": 3.7804,
      "step": 643000
    },
    {
      "epoch": 137.88593481989707,
      "grad_norm": 6.903033256530762,
      "learning_rate": 9.8063125e-06,
      "loss": 3.8659,
      "step": 643100
    },
    {
      "epoch": 137.9073756432247,
      "grad_norm": 6.12156343460083,
      "learning_rate": 9.8000625e-06,
      "loss": 3.8465,
      "step": 643200
    },
    {
      "epoch": 137.92881646655232,
      "grad_norm": 6.193262577056885,
      "learning_rate": 9.7938125e-06,
      "loss": 3.8497,
      "step": 643300
    },
    {
      "epoch": 137.95025728987994,
      "grad_norm": 6.311185359954834,
      "learning_rate": 9.7875625e-06,
      "loss": 3.9196,
      "step": 643400
    },
    {
      "epoch": 137.97169811320754,
      "grad_norm": 6.388875961303711,
      "learning_rate": 9.7813125e-06,
      "loss": 3.82,
      "step": 643500
    },
    {
      "epoch": 137.99313893653516,
      "grad_norm": 6.533657073974609,
      "learning_rate": 9.7750625e-06,
      "loss": 3.8764,
      "step": 643600
    },
    {
      "epoch": 138.01457975986278,
      "grad_norm": 5.913424015045166,
      "learning_rate": 9.768812500000001e-06,
      "loss": 3.8745,
      "step": 643700
    },
    {
      "epoch": 138.0360205831904,
      "grad_norm": 5.999622821807861,
      "learning_rate": 9.7625625e-06,
      "loss": 3.8389,
      "step": 643800
    },
    {
      "epoch": 138.057461406518,
      "grad_norm": 5.941911697387695,
      "learning_rate": 9.756312500000001e-06,
      "loss": 3.8371,
      "step": 643900
    },
    {
      "epoch": 138.07890222984562,
      "grad_norm": 6.516711235046387,
      "learning_rate": 9.750062499999999e-06,
      "loss": 3.778,
      "step": 644000
    },
    {
      "epoch": 138.10034305317325,
      "grad_norm": 6.58361291885376,
      "learning_rate": 9.7438125e-06,
      "loss": 3.8399,
      "step": 644100
    },
    {
      "epoch": 138.12178387650087,
      "grad_norm": 6.0175371170043945,
      "learning_rate": 9.7375625e-06,
      "loss": 3.8881,
      "step": 644200
    },
    {
      "epoch": 138.14322469982847,
      "grad_norm": 6.770781517028809,
      "learning_rate": 9.7313125e-06,
      "loss": 3.8602,
      "step": 644300
    },
    {
      "epoch": 138.1646655231561,
      "grad_norm": 5.9503655433654785,
      "learning_rate": 9.7250625e-06,
      "loss": 3.8576,
      "step": 644400
    },
    {
      "epoch": 138.1861063464837,
      "grad_norm": 5.870382785797119,
      "learning_rate": 9.7188125e-06,
      "loss": 3.9113,
      "step": 644500
    },
    {
      "epoch": 138.20754716981133,
      "grad_norm": 5.967496395111084,
      "learning_rate": 9.7125625e-06,
      "loss": 3.7771,
      "step": 644600
    },
    {
      "epoch": 138.22898799313893,
      "grad_norm": 6.720696926116943,
      "learning_rate": 9.7063125e-06,
      "loss": 3.8782,
      "step": 644700
    },
    {
      "epoch": 138.25042881646655,
      "grad_norm": 6.1006245613098145,
      "learning_rate": 9.7000625e-06,
      "loss": 3.8162,
      "step": 644800
    },
    {
      "epoch": 138.27186963979418,
      "grad_norm": 6.3705830574035645,
      "learning_rate": 9.693812500000001e-06,
      "loss": 3.8213,
      "step": 644900
    },
    {
      "epoch": 138.29331046312177,
      "grad_norm": 6.807765483856201,
      "learning_rate": 9.687562500000001e-06,
      "loss": 3.8671,
      "step": 645000
    },
    {
      "epoch": 138.3147512864494,
      "grad_norm": 6.467195510864258,
      "learning_rate": 9.6813125e-06,
      "loss": 3.7994,
      "step": 645100
    },
    {
      "epoch": 138.33619210977702,
      "grad_norm": 5.916468143463135,
      "learning_rate": 9.6750625e-06,
      "loss": 3.8373,
      "step": 645200
    },
    {
      "epoch": 138.35763293310464,
      "grad_norm": 6.371642589569092,
      "learning_rate": 9.6688125e-06,
      "loss": 3.8568,
      "step": 645300
    },
    {
      "epoch": 138.37907375643223,
      "grad_norm": 6.4161529541015625,
      "learning_rate": 9.6625625e-06,
      "loss": 3.8521,
      "step": 645400
    },
    {
      "epoch": 138.40051457975986,
      "grad_norm": 6.455230236053467,
      "learning_rate": 9.6563125e-06,
      "loss": 3.8144,
      "step": 645500
    },
    {
      "epoch": 138.42195540308748,
      "grad_norm": 6.383815765380859,
      "learning_rate": 9.650062500000002e-06,
      "loss": 3.8021,
      "step": 645600
    },
    {
      "epoch": 138.4433962264151,
      "grad_norm": 6.137114524841309,
      "learning_rate": 9.6438125e-06,
      "loss": 3.8515,
      "step": 645700
    },
    {
      "epoch": 138.4648370497427,
      "grad_norm": 6.395249843597412,
      "learning_rate": 9.637562500000001e-06,
      "loss": 3.8145,
      "step": 645800
    },
    {
      "epoch": 138.48627787307032,
      "grad_norm": 5.136014461517334,
      "learning_rate": 9.6313125e-06,
      "loss": 3.8185,
      "step": 645900
    },
    {
      "epoch": 138.50771869639794,
      "grad_norm": 6.202571392059326,
      "learning_rate": 9.625062500000001e-06,
      "loss": 3.8545,
      "step": 646000
    },
    {
      "epoch": 138.52915951972557,
      "grad_norm": 7.159219264984131,
      "learning_rate": 9.618812500000001e-06,
      "loss": 3.882,
      "step": 646100
    },
    {
      "epoch": 138.55060034305316,
      "grad_norm": 6.276491641998291,
      "learning_rate": 9.612562500000001e-06,
      "loss": 3.8725,
      "step": 646200
    },
    {
      "epoch": 138.57204116638079,
      "grad_norm": 7.162270545959473,
      "learning_rate": 9.6063125e-06,
      "loss": 3.826,
      "step": 646300
    },
    {
      "epoch": 138.5934819897084,
      "grad_norm": 6.17242431640625,
      "learning_rate": 9.6000625e-06,
      "loss": 3.8902,
      "step": 646400
    },
    {
      "epoch": 138.61492281303603,
      "grad_norm": 6.196813583374023,
      "learning_rate": 9.5938125e-06,
      "loss": 3.787,
      "step": 646500
    },
    {
      "epoch": 138.63636363636363,
      "grad_norm": 6.060848236083984,
      "learning_rate": 9.5875625e-06,
      "loss": 3.8101,
      "step": 646600
    },
    {
      "epoch": 138.65780445969125,
      "grad_norm": 6.323190689086914,
      "learning_rate": 9.5813125e-06,
      "loss": 3.8738,
      "step": 646700
    },
    {
      "epoch": 138.67924528301887,
      "grad_norm": 6.041979789733887,
      "learning_rate": 9.575062500000002e-06,
      "loss": 3.8897,
      "step": 646800
    },
    {
      "epoch": 138.7006861063465,
      "grad_norm": 6.494446754455566,
      "learning_rate": 9.5688125e-06,
      "loss": 3.7697,
      "step": 646900
    },
    {
      "epoch": 138.7221269296741,
      "grad_norm": 5.972496032714844,
      "learning_rate": 9.562562500000001e-06,
      "loss": 3.8219,
      "step": 647000
    },
    {
      "epoch": 138.7435677530017,
      "grad_norm": 5.773841381072998,
      "learning_rate": 9.556312500000001e-06,
      "loss": 3.8456,
      "step": 647100
    },
    {
      "epoch": 138.76500857632934,
      "grad_norm": 6.817348957061768,
      "learning_rate": 9.550062500000001e-06,
      "loss": 3.8316,
      "step": 647200
    },
    {
      "epoch": 138.78644939965696,
      "grad_norm": 6.3626389503479,
      "learning_rate": 9.543812500000001e-06,
      "loss": 3.8558,
      "step": 647300
    },
    {
      "epoch": 138.80789022298455,
      "grad_norm": 6.698154926300049,
      "learning_rate": 9.5375625e-06,
      "loss": 3.8468,
      "step": 647400
    },
    {
      "epoch": 138.82933104631218,
      "grad_norm": 6.265098571777344,
      "learning_rate": 9.5313125e-06,
      "loss": 3.8343,
      "step": 647500
    },
    {
      "epoch": 138.8507718696398,
      "grad_norm": 6.657649993896484,
      "learning_rate": 9.5250625e-06,
      "loss": 3.849,
      "step": 647600
    },
    {
      "epoch": 138.87221269296742,
      "grad_norm": 5.693792819976807,
      "learning_rate": 9.5188125e-06,
      "loss": 3.8257,
      "step": 647700
    },
    {
      "epoch": 138.89365351629502,
      "grad_norm": 6.7025251388549805,
      "learning_rate": 9.512562500000002e-06,
      "loss": 3.8456,
      "step": 647800
    },
    {
      "epoch": 138.91509433962264,
      "grad_norm": 5.925719261169434,
      "learning_rate": 9.5063125e-06,
      "loss": 3.8529,
      "step": 647900
    },
    {
      "epoch": 138.93653516295026,
      "grad_norm": 5.9392619132995605,
      "learning_rate": 9.500062500000002e-06,
      "loss": 3.8167,
      "step": 648000
    },
    {
      "epoch": 138.9579759862779,
      "grad_norm": 6.637460708618164,
      "learning_rate": 9.4938125e-06,
      "loss": 3.8331,
      "step": 648100
    },
    {
      "epoch": 138.97941680960548,
      "grad_norm": 6.002130508422852,
      "learning_rate": 9.487562500000001e-06,
      "loss": 3.8633,
      "step": 648200
    },
    {
      "epoch": 139.0008576329331,
      "grad_norm": 6.171891212463379,
      "learning_rate": 9.481312500000001e-06,
      "loss": 3.838,
      "step": 648300
    },
    {
      "epoch": 139.02229845626073,
      "grad_norm": 5.783202648162842,
      "learning_rate": 9.475062500000001e-06,
      "loss": 3.8019,
      "step": 648400
    },
    {
      "epoch": 139.04373927958832,
      "grad_norm": 6.426947593688965,
      "learning_rate": 9.4688125e-06,
      "loss": 3.8326,
      "step": 648500
    },
    {
      "epoch": 139.06518010291595,
      "grad_norm": 6.342541694641113,
      "learning_rate": 9.4625625e-06,
      "loss": 3.7909,
      "step": 648600
    },
    {
      "epoch": 139.08662092624357,
      "grad_norm": 5.979938507080078,
      "learning_rate": 9.4563125e-06,
      "loss": 3.8341,
      "step": 648700
    },
    {
      "epoch": 139.1080617495712,
      "grad_norm": 6.444791793823242,
      "learning_rate": 9.4500625e-06,
      "loss": 3.8334,
      "step": 648800
    },
    {
      "epoch": 139.1295025728988,
      "grad_norm": 6.350848197937012,
      "learning_rate": 9.4438125e-06,
      "loss": 3.8223,
      "step": 648900
    },
    {
      "epoch": 139.1509433962264,
      "grad_norm": 6.4620137214660645,
      "learning_rate": 9.437562500000002e-06,
      "loss": 3.8156,
      "step": 649000
    },
    {
      "epoch": 139.17238421955403,
      "grad_norm": 6.283349990844727,
      "learning_rate": 9.4313125e-06,
      "loss": 3.8058,
      "step": 649100
    },
    {
      "epoch": 139.19382504288166,
      "grad_norm": 6.186037540435791,
      "learning_rate": 9.425062500000001e-06,
      "loss": 3.768,
      "step": 649200
    },
    {
      "epoch": 139.21526586620925,
      "grad_norm": 7.121926784515381,
      "learning_rate": 9.4188125e-06,
      "loss": 3.8582,
      "step": 649300
    },
    {
      "epoch": 139.23670668953687,
      "grad_norm": 6.510516166687012,
      "learning_rate": 9.412562500000001e-06,
      "loss": 3.8373,
      "step": 649400
    },
    {
      "epoch": 139.2581475128645,
      "grad_norm": 6.091357707977295,
      "learning_rate": 9.406312500000001e-06,
      "loss": 3.7956,
      "step": 649500
    },
    {
      "epoch": 139.27958833619212,
      "grad_norm": 6.024596214294434,
      "learning_rate": 9.4000625e-06,
      "loss": 3.7919,
      "step": 649600
    },
    {
      "epoch": 139.30102915951971,
      "grad_norm": 6.247907638549805,
      "learning_rate": 9.3938125e-06,
      "loss": 3.8383,
      "step": 649700
    },
    {
      "epoch": 139.32246998284734,
      "grad_norm": 5.737754821777344,
      "learning_rate": 9.3875625e-06,
      "loss": 3.8311,
      "step": 649800
    },
    {
      "epoch": 139.34391080617496,
      "grad_norm": 6.4796142578125,
      "learning_rate": 9.3813125e-06,
      "loss": 3.8409,
      "step": 649900
    },
    {
      "epoch": 139.36535162950258,
      "grad_norm": 6.156490325927734,
      "learning_rate": 9.3750625e-06,
      "loss": 3.8014,
      "step": 650000
    },
    {
      "epoch": 139.38679245283018,
      "grad_norm": 6.14211893081665,
      "learning_rate": 9.3688125e-06,
      "loss": 3.8109,
      "step": 650100
    },
    {
      "epoch": 139.4082332761578,
      "grad_norm": 6.387867450714111,
      "learning_rate": 9.362562500000002e-06,
      "loss": 3.8835,
      "step": 650200
    },
    {
      "epoch": 139.42967409948542,
      "grad_norm": 5.897659778594971,
      "learning_rate": 9.3563125e-06,
      "loss": 3.8034,
      "step": 650300
    },
    {
      "epoch": 139.45111492281305,
      "grad_norm": 6.118872165679932,
      "learning_rate": 9.350062500000001e-06,
      "loss": 3.8215,
      "step": 650400
    },
    {
      "epoch": 139.47255574614064,
      "grad_norm": 6.344561576843262,
      "learning_rate": 9.3438125e-06,
      "loss": 3.8146,
      "step": 650500
    },
    {
      "epoch": 139.49399656946827,
      "grad_norm": 5.697045803070068,
      "learning_rate": 9.337562500000001e-06,
      "loss": 3.7746,
      "step": 650600
    },
    {
      "epoch": 139.5154373927959,
      "grad_norm": 8.20432186126709,
      "learning_rate": 9.3313125e-06,
      "loss": 3.8819,
      "step": 650700
    },
    {
      "epoch": 139.5368782161235,
      "grad_norm": 6.30307674407959,
      "learning_rate": 9.3250625e-06,
      "loss": 3.8323,
      "step": 650800
    },
    {
      "epoch": 139.5583190394511,
      "grad_norm": 7.752411842346191,
      "learning_rate": 9.3188125e-06,
      "loss": 3.8202,
      "step": 650900
    },
    {
      "epoch": 139.57975986277873,
      "grad_norm": 6.294713020324707,
      "learning_rate": 9.3125625e-06,
      "loss": 3.8929,
      "step": 651000
    },
    {
      "epoch": 139.60120068610635,
      "grad_norm": 6.28486442565918,
      "learning_rate": 9.3063125e-06,
      "loss": 3.8157,
      "step": 651100
    },
    {
      "epoch": 139.62264150943398,
      "grad_norm": 6.5210089683532715,
      "learning_rate": 9.3000625e-06,
      "loss": 3.7895,
      "step": 651200
    },
    {
      "epoch": 139.64408233276157,
      "grad_norm": 7.302844047546387,
      "learning_rate": 9.2938125e-06,
      "loss": 3.8104,
      "step": 651300
    },
    {
      "epoch": 139.6655231560892,
      "grad_norm": 6.064615249633789,
      "learning_rate": 9.287562500000001e-06,
      "loss": 3.816,
      "step": 651400
    },
    {
      "epoch": 139.68696397941682,
      "grad_norm": 6.4439520835876465,
      "learning_rate": 9.2813125e-06,
      "loss": 3.8097,
      "step": 651500
    },
    {
      "epoch": 139.70840480274444,
      "grad_norm": 6.476833820343018,
      "learning_rate": 9.275062500000001e-06,
      "loss": 3.8169,
      "step": 651600
    },
    {
      "epoch": 139.72984562607203,
      "grad_norm": 6.04712438583374,
      "learning_rate": 9.2688125e-06,
      "loss": 3.7839,
      "step": 651700
    },
    {
      "epoch": 139.75128644939966,
      "grad_norm": 6.081193447113037,
      "learning_rate": 9.2625625e-06,
      "loss": 3.7941,
      "step": 651800
    },
    {
      "epoch": 139.77272727272728,
      "grad_norm": 6.265553951263428,
      "learning_rate": 9.2563125e-06,
      "loss": 3.8474,
      "step": 651900
    },
    {
      "epoch": 139.79416809605488,
      "grad_norm": 5.893064022064209,
      "learning_rate": 9.2500625e-06,
      "loss": 3.8395,
      "step": 652000
    },
    {
      "epoch": 139.8156089193825,
      "grad_norm": 6.10775089263916,
      "learning_rate": 9.2438125e-06,
      "loss": 3.747,
      "step": 652100
    },
    {
      "epoch": 139.83704974271012,
      "grad_norm": 6.047421455383301,
      "learning_rate": 9.2375625e-06,
      "loss": 3.7666,
      "step": 652200
    },
    {
      "epoch": 139.85849056603774,
      "grad_norm": 6.847489356994629,
      "learning_rate": 9.2313125e-06,
      "loss": 3.8461,
      "step": 652300
    },
    {
      "epoch": 139.87993138936534,
      "grad_norm": 6.618931770324707,
      "learning_rate": 9.2250625e-06,
      "loss": 3.7909,
      "step": 652400
    },
    {
      "epoch": 139.90137221269296,
      "grad_norm": 6.338653564453125,
      "learning_rate": 9.2188125e-06,
      "loss": 3.7727,
      "step": 652500
    },
    {
      "epoch": 139.92281303602059,
      "grad_norm": 6.411098003387451,
      "learning_rate": 9.212562500000001e-06,
      "loss": 3.8288,
      "step": 652600
    },
    {
      "epoch": 139.9442538593482,
      "grad_norm": 6.2961344718933105,
      "learning_rate": 9.2063125e-06,
      "loss": 3.8322,
      "step": 652700
    },
    {
      "epoch": 139.9656946826758,
      "grad_norm": 6.500593662261963,
      "learning_rate": 9.200062500000001e-06,
      "loss": 3.8566,
      "step": 652800
    },
    {
      "epoch": 139.98713550600343,
      "grad_norm": 6.466996669769287,
      "learning_rate": 9.1938125e-06,
      "loss": 3.8379,
      "step": 652900
    },
    {
      "epoch": 140.00857632933105,
      "grad_norm": 6.9095306396484375,
      "learning_rate": 9.1875625e-06,
      "loss": 3.7956,
      "step": 653000
    },
    {
      "epoch": 140.03001715265867,
      "grad_norm": 6.281257152557373,
      "learning_rate": 9.1813125e-06,
      "loss": 3.79,
      "step": 653100
    },
    {
      "epoch": 140.05145797598627,
      "grad_norm": 6.182285308837891,
      "learning_rate": 9.1750625e-06,
      "loss": 3.7584,
      "step": 653200
    },
    {
      "epoch": 140.0728987993139,
      "grad_norm": 6.186845302581787,
      "learning_rate": 9.1688125e-06,
      "loss": 3.8327,
      "step": 653300
    },
    {
      "epoch": 140.0943396226415,
      "grad_norm": 6.2438249588012695,
      "learning_rate": 9.1625625e-06,
      "loss": 3.8427,
      "step": 653400
    },
    {
      "epoch": 140.11578044596914,
      "grad_norm": 6.151648998260498,
      "learning_rate": 9.1563125e-06,
      "loss": 3.7719,
      "step": 653500
    },
    {
      "epoch": 140.13722126929673,
      "grad_norm": 6.897914409637451,
      "learning_rate": 9.150062500000001e-06,
      "loss": 3.7868,
      "step": 653600
    },
    {
      "epoch": 140.15866209262435,
      "grad_norm": 6.472265720367432,
      "learning_rate": 9.1438125e-06,
      "loss": 3.7957,
      "step": 653700
    },
    {
      "epoch": 140.18010291595198,
      "grad_norm": 6.086777687072754,
      "learning_rate": 9.137562500000001e-06,
      "loss": 3.8116,
      "step": 653800
    },
    {
      "epoch": 140.2015437392796,
      "grad_norm": 6.270190238952637,
      "learning_rate": 9.1313125e-06,
      "loss": 3.7911,
      "step": 653900
    },
    {
      "epoch": 140.2229845626072,
      "grad_norm": 6.613190174102783,
      "learning_rate": 9.125062500000001e-06,
      "loss": 3.82,
      "step": 654000
    },
    {
      "epoch": 140.24442538593482,
      "grad_norm": 6.486826419830322,
      "learning_rate": 9.1188125e-06,
      "loss": 3.8179,
      "step": 654100
    },
    {
      "epoch": 140.26586620926244,
      "grad_norm": 5.610054969787598,
      "learning_rate": 9.1125625e-06,
      "loss": 3.797,
      "step": 654200
    },
    {
      "epoch": 140.28730703259006,
      "grad_norm": 6.06287956237793,
      "learning_rate": 9.1063125e-06,
      "loss": 3.8268,
      "step": 654300
    },
    {
      "epoch": 140.30874785591766,
      "grad_norm": 6.582794189453125,
      "learning_rate": 9.1000625e-06,
      "loss": 3.8085,
      "step": 654400
    },
    {
      "epoch": 140.33018867924528,
      "grad_norm": 7.067599296569824,
      "learning_rate": 9.0938125e-06,
      "loss": 3.7649,
      "step": 654500
    },
    {
      "epoch": 140.3516295025729,
      "grad_norm": 6.686309814453125,
      "learning_rate": 9.0875625e-06,
      "loss": 3.8004,
      "step": 654600
    },
    {
      "epoch": 140.37307032590053,
      "grad_norm": 6.716557025909424,
      "learning_rate": 9.0813125e-06,
      "loss": 3.7939,
      "step": 654700
    },
    {
      "epoch": 140.39451114922812,
      "grad_norm": 5.788333892822266,
      "learning_rate": 9.075062500000001e-06,
      "loss": 3.8378,
      "step": 654800
    },
    {
      "epoch": 140.41595197255575,
      "grad_norm": 6.301366806030273,
      "learning_rate": 9.0688125e-06,
      "loss": 3.7911,
      "step": 654900
    },
    {
      "epoch": 140.43739279588337,
      "grad_norm": 5.869117736816406,
      "learning_rate": 9.062562500000001e-06,
      "loss": 3.7396,
      "step": 655000
    },
    {
      "epoch": 140.45883361921096,
      "grad_norm": 6.277225494384766,
      "learning_rate": 9.0563125e-06,
      "loss": 3.7964,
      "step": 655100
    },
    {
      "epoch": 140.4802744425386,
      "grad_norm": 6.503134727478027,
      "learning_rate": 9.0500625e-06,
      "loss": 3.7637,
      "step": 655200
    },
    {
      "epoch": 140.5017152658662,
      "grad_norm": 6.902827262878418,
      "learning_rate": 9.0438125e-06,
      "loss": 3.7954,
      "step": 655300
    },
    {
      "epoch": 140.52315608919383,
      "grad_norm": 6.702945232391357,
      "learning_rate": 9.0375625e-06,
      "loss": 3.8575,
      "step": 655400
    },
    {
      "epoch": 140.54459691252143,
      "grad_norm": 5.7653374671936035,
      "learning_rate": 9.0313125e-06,
      "loss": 3.7743,
      "step": 655500
    },
    {
      "epoch": 140.56603773584905,
      "grad_norm": 6.240996837615967,
      "learning_rate": 9.0250625e-06,
      "loss": 3.8163,
      "step": 655600
    },
    {
      "epoch": 140.58747855917667,
      "grad_norm": 6.503358364105225,
      "learning_rate": 9.0188125e-06,
      "loss": 3.7462,
      "step": 655700
    },
    {
      "epoch": 140.6089193825043,
      "grad_norm": 7.002255439758301,
      "learning_rate": 9.0125625e-06,
      "loss": 3.8171,
      "step": 655800
    },
    {
      "epoch": 140.6303602058319,
      "grad_norm": 6.491053104400635,
      "learning_rate": 9.0063125e-06,
      "loss": 3.7727,
      "step": 655900
    },
    {
      "epoch": 140.65180102915951,
      "grad_norm": 6.178665637969971,
      "learning_rate": 9.000062500000001e-06,
      "loss": 3.8136,
      "step": 656000
    },
    {
      "epoch": 140.67324185248714,
      "grad_norm": 6.30403995513916,
      "learning_rate": 8.9938125e-06,
      "loss": 3.8187,
      "step": 656100
    },
    {
      "epoch": 140.69468267581476,
      "grad_norm": 6.615748882293701,
      "learning_rate": 8.987562500000001e-06,
      "loss": 3.8778,
      "step": 656200
    },
    {
      "epoch": 140.71612349914236,
      "grad_norm": 6.396128177642822,
      "learning_rate": 8.981312499999999e-06,
      "loss": 3.763,
      "step": 656300
    },
    {
      "epoch": 140.73756432246998,
      "grad_norm": 6.334092617034912,
      "learning_rate": 8.9750625e-06,
      "loss": 3.709,
      "step": 656400
    },
    {
      "epoch": 140.7590051457976,
      "grad_norm": 6.673690319061279,
      "learning_rate": 8.9688125e-06,
      "loss": 3.7727,
      "step": 656500
    },
    {
      "epoch": 140.78044596912522,
      "grad_norm": 6.303407192230225,
      "learning_rate": 8.9625625e-06,
      "loss": 3.8034,
      "step": 656600
    },
    {
      "epoch": 140.80188679245282,
      "grad_norm": 6.408837795257568,
      "learning_rate": 8.9563125e-06,
      "loss": 3.8141,
      "step": 656700
    },
    {
      "epoch": 140.82332761578044,
      "grad_norm": 6.268865585327148,
      "learning_rate": 8.9500625e-06,
      "loss": 3.7804,
      "step": 656800
    },
    {
      "epoch": 140.84476843910807,
      "grad_norm": 6.818911075592041,
      "learning_rate": 8.9438125e-06,
      "loss": 3.7871,
      "step": 656900
    },
    {
      "epoch": 140.8662092624357,
      "grad_norm": 6.095311641693115,
      "learning_rate": 8.9375625e-06,
      "loss": 3.7697,
      "step": 657000
    },
    {
      "epoch": 140.88765008576328,
      "grad_norm": 6.792342662811279,
      "learning_rate": 8.9313125e-06,
      "loss": 3.8057,
      "step": 657100
    },
    {
      "epoch": 140.9090909090909,
      "grad_norm": 6.697936058044434,
      "learning_rate": 8.925062500000001e-06,
      "loss": 3.7968,
      "step": 657200
    },
    {
      "epoch": 140.93053173241853,
      "grad_norm": 6.779050350189209,
      "learning_rate": 8.9188125e-06,
      "loss": 3.789,
      "step": 657300
    },
    {
      "epoch": 140.95197255574615,
      "grad_norm": 7.063510417938232,
      "learning_rate": 8.9125625e-06,
      "loss": 3.8352,
      "step": 657400
    },
    {
      "epoch": 140.97341337907375,
      "grad_norm": 6.361499786376953,
      "learning_rate": 8.9063125e-06,
      "loss": 3.7737,
      "step": 657500
    },
    {
      "epoch": 140.99485420240137,
      "grad_norm": 7.589278697967529,
      "learning_rate": 8.9000625e-06,
      "loss": 3.8251,
      "step": 657600
    },
    {
      "epoch": 141.016295025729,
      "grad_norm": 6.642117977142334,
      "learning_rate": 8.8938125e-06,
      "loss": 3.768,
      "step": 657700
    },
    {
      "epoch": 141.03773584905662,
      "grad_norm": 6.387856483459473,
      "learning_rate": 8.8875625e-06,
      "loss": 3.7462,
      "step": 657800
    },
    {
      "epoch": 141.0591766723842,
      "grad_norm": 6.525427341461182,
      "learning_rate": 8.881312500000002e-06,
      "loss": 3.8265,
      "step": 657900
    },
    {
      "epoch": 141.08061749571183,
      "grad_norm": 6.088174343109131,
      "learning_rate": 8.8750625e-06,
      "loss": 3.7862,
      "step": 658000
    },
    {
      "epoch": 141.10205831903946,
      "grad_norm": 6.281473159790039,
      "learning_rate": 8.868812500000001e-06,
      "loss": 3.766,
      "step": 658100
    },
    {
      "epoch": 141.12349914236708,
      "grad_norm": 7.208044052124023,
      "learning_rate": 8.8625625e-06,
      "loss": 3.8046,
      "step": 658200
    },
    {
      "epoch": 141.14493996569468,
      "grad_norm": 6.208065032958984,
      "learning_rate": 8.856312500000001e-06,
      "loss": 3.7842,
      "step": 658300
    },
    {
      "epoch": 141.1663807890223,
      "grad_norm": 6.298615455627441,
      "learning_rate": 8.850062500000001e-06,
      "loss": 3.7364,
      "step": 658400
    },
    {
      "epoch": 141.18782161234992,
      "grad_norm": 5.745283603668213,
      "learning_rate": 8.8438125e-06,
      "loss": 3.7467,
      "step": 658500
    },
    {
      "epoch": 141.20926243567752,
      "grad_norm": 6.556151390075684,
      "learning_rate": 8.8375625e-06,
      "loss": 3.8116,
      "step": 658600
    },
    {
      "epoch": 141.23070325900514,
      "grad_norm": 6.796318531036377,
      "learning_rate": 8.8313125e-06,
      "loss": 3.7434,
      "step": 658700
    },
    {
      "epoch": 141.25214408233276,
      "grad_norm": 5.96743631362915,
      "learning_rate": 8.8250625e-06,
      "loss": 3.7782,
      "step": 658800
    },
    {
      "epoch": 141.27358490566039,
      "grad_norm": 6.171128749847412,
      "learning_rate": 8.8188125e-06,
      "loss": 3.7628,
      "step": 658900
    },
    {
      "epoch": 141.29502572898798,
      "grad_norm": 6.045928955078125,
      "learning_rate": 8.8125625e-06,
      "loss": 3.7693,
      "step": 659000
    },
    {
      "epoch": 141.3164665523156,
      "grad_norm": 5.865624904632568,
      "learning_rate": 8.806312500000002e-06,
      "loss": 3.7907,
      "step": 659100
    },
    {
      "epoch": 141.33790737564323,
      "grad_norm": 5.859588146209717,
      "learning_rate": 8.8000625e-06,
      "loss": 3.7478,
      "step": 659200
    },
    {
      "epoch": 141.35934819897085,
      "grad_norm": 6.539630889892578,
      "learning_rate": 8.793812500000001e-06,
      "loss": 3.8094,
      "step": 659300
    },
    {
      "epoch": 141.38078902229844,
      "grad_norm": 7.257053375244141,
      "learning_rate": 8.787562500000001e-06,
      "loss": 3.8039,
      "step": 659400
    },
    {
      "epoch": 141.40222984562607,
      "grad_norm": 5.915034294128418,
      "learning_rate": 8.781312500000001e-06,
      "loss": 3.8192,
      "step": 659500
    },
    {
      "epoch": 141.4236706689537,
      "grad_norm": 6.739136219024658,
      "learning_rate": 8.7750625e-06,
      "loss": 3.7688,
      "step": 659600
    },
    {
      "epoch": 141.4451114922813,
      "grad_norm": 6.659821033477783,
      "learning_rate": 8.7688125e-06,
      "loss": 3.7877,
      "step": 659700
    },
    {
      "epoch": 141.4665523156089,
      "grad_norm": 6.725977420806885,
      "learning_rate": 8.7625625e-06,
      "loss": 3.8211,
      "step": 659800
    },
    {
      "epoch": 141.48799313893653,
      "grad_norm": 6.555480003356934,
      "learning_rate": 8.7563125e-06,
      "loss": 3.7801,
      "step": 659900
    },
    {
      "epoch": 141.50943396226415,
      "grad_norm": 6.422644138336182,
      "learning_rate": 8.7500625e-06,
      "loss": 3.7732,
      "step": 660000
    },
    {
      "epoch": 141.53087478559178,
      "grad_norm": 6.879598617553711,
      "learning_rate": 8.743812500000002e-06,
      "loss": 3.7794,
      "step": 660100
    },
    {
      "epoch": 141.55231560891937,
      "grad_norm": 6.222362995147705,
      "learning_rate": 8.7375625e-06,
      "loss": 3.7845,
      "step": 660200
    },
    {
      "epoch": 141.573756432247,
      "grad_norm": 6.2778825759887695,
      "learning_rate": 8.731312500000001e-06,
      "loss": 3.741,
      "step": 660300
    },
    {
      "epoch": 141.59519725557462,
      "grad_norm": 7.119344711303711,
      "learning_rate": 8.7250625e-06,
      "loss": 3.7522,
      "step": 660400
    },
    {
      "epoch": 141.61663807890224,
      "grad_norm": 6.003873825073242,
      "learning_rate": 8.718812500000001e-06,
      "loss": 3.7239,
      "step": 660500
    },
    {
      "epoch": 141.63807890222984,
      "grad_norm": 6.8338165283203125,
      "learning_rate": 8.712562500000001e-06,
      "loss": 3.7984,
      "step": 660600
    },
    {
      "epoch": 141.65951972555746,
      "grad_norm": 6.559757709503174,
      "learning_rate": 8.706312500000001e-06,
      "loss": 3.7602,
      "step": 660700
    },
    {
      "epoch": 141.68096054888508,
      "grad_norm": 6.598486423492432,
      "learning_rate": 8.7000625e-06,
      "loss": 3.7438,
      "step": 660800
    },
    {
      "epoch": 141.7024013722127,
      "grad_norm": 7.4230756759643555,
      "learning_rate": 8.6938125e-06,
      "loss": 3.7661,
      "step": 660900
    },
    {
      "epoch": 141.7238421955403,
      "grad_norm": 6.701887607574463,
      "learning_rate": 8.6875625e-06,
      "loss": 3.782,
      "step": 661000
    },
    {
      "epoch": 141.74528301886792,
      "grad_norm": 6.644582271575928,
      "learning_rate": 8.6813125e-06,
      "loss": 3.7924,
      "step": 661100
    },
    {
      "epoch": 141.76672384219555,
      "grad_norm": 6.477502346038818,
      "learning_rate": 8.6750625e-06,
      "loss": 3.7411,
      "step": 661200
    },
    {
      "epoch": 141.78816466552317,
      "grad_norm": 6.655189037322998,
      "learning_rate": 8.668812500000002e-06,
      "loss": 3.7339,
      "step": 661300
    },
    {
      "epoch": 141.80960548885076,
      "grad_norm": 6.695984840393066,
      "learning_rate": 8.6625625e-06,
      "loss": 3.7754,
      "step": 661400
    },
    {
      "epoch": 141.8310463121784,
      "grad_norm": 6.6159749031066895,
      "learning_rate": 8.656312500000001e-06,
      "loss": 3.8571,
      "step": 661500
    },
    {
      "epoch": 141.852487135506,
      "grad_norm": 9.241941452026367,
      "learning_rate": 8.6500625e-06,
      "loss": 3.7763,
      "step": 661600
    },
    {
      "epoch": 141.87392795883363,
      "grad_norm": 6.3361358642578125,
      "learning_rate": 8.643812500000001e-06,
      "loss": 3.7201,
      "step": 661700
    },
    {
      "epoch": 141.89536878216123,
      "grad_norm": 6.760210037231445,
      "learning_rate": 8.637562500000001e-06,
      "loss": 3.8019,
      "step": 661800
    },
    {
      "epoch": 141.91680960548885,
      "grad_norm": 6.229978561401367,
      "learning_rate": 8.6313125e-06,
      "loss": 3.7591,
      "step": 661900
    },
    {
      "epoch": 141.93825042881647,
      "grad_norm": 6.166163444519043,
      "learning_rate": 8.6250625e-06,
      "loss": 3.7906,
      "step": 662000
    },
    {
      "epoch": 141.95969125214407,
      "grad_norm": 6.611548900604248,
      "learning_rate": 8.6188125e-06,
      "loss": 3.8359,
      "step": 662100
    },
    {
      "epoch": 141.9811320754717,
      "grad_norm": 6.679568767547607,
      "learning_rate": 8.6125625e-06,
      "loss": 3.787,
      "step": 662200
    },
    {
      "epoch": 142.00257289879931,
      "grad_norm": 6.42621374130249,
      "learning_rate": 8.6063125e-06,
      "loss": 3.7881,
      "step": 662300
    },
    {
      "epoch": 142.02401372212694,
      "grad_norm": 5.866964817047119,
      "learning_rate": 8.6000625e-06,
      "loss": 3.7669,
      "step": 662400
    },
    {
      "epoch": 142.04545454545453,
      "grad_norm": 6.738622665405273,
      "learning_rate": 8.593812500000002e-06,
      "loss": 3.8096,
      "step": 662500
    },
    {
      "epoch": 142.06689536878216,
      "grad_norm": 6.687463760375977,
      "learning_rate": 8.5875625e-06,
      "loss": 3.7187,
      "step": 662600
    },
    {
      "epoch": 142.08833619210978,
      "grad_norm": 6.347267150878906,
      "learning_rate": 8.581312500000001e-06,
      "loss": 3.7265,
      "step": 662700
    },
    {
      "epoch": 142.1097770154374,
      "grad_norm": 6.470943927764893,
      "learning_rate": 8.5750625e-06,
      "loss": 3.7616,
      "step": 662800
    },
    {
      "epoch": 142.131217838765,
      "grad_norm": 6.070991516113281,
      "learning_rate": 8.568812500000001e-06,
      "loss": 3.8036,
      "step": 662900
    },
    {
      "epoch": 142.15265866209262,
      "grad_norm": 6.647085189819336,
      "learning_rate": 8.5625625e-06,
      "loss": 3.7517,
      "step": 663000
    },
    {
      "epoch": 142.17409948542024,
      "grad_norm": 6.493407726287842,
      "learning_rate": 8.5563125e-06,
      "loss": 3.6849,
      "step": 663100
    },
    {
      "epoch": 142.19554030874787,
      "grad_norm": 7.907402038574219,
      "learning_rate": 8.5500625e-06,
      "loss": 3.7519,
      "step": 663200
    },
    {
      "epoch": 142.21698113207546,
      "grad_norm": 6.770328998565674,
      "learning_rate": 8.5438125e-06,
      "loss": 3.7592,
      "step": 663300
    },
    {
      "epoch": 142.23842195540308,
      "grad_norm": 6.2297587394714355,
      "learning_rate": 8.5375625e-06,
      "loss": 3.7546,
      "step": 663400
    },
    {
      "epoch": 142.2598627787307,
      "grad_norm": 6.650481700897217,
      "learning_rate": 8.5313125e-06,
      "loss": 3.846,
      "step": 663500
    },
    {
      "epoch": 142.28130360205833,
      "grad_norm": 7.006616592407227,
      "learning_rate": 8.5250625e-06,
      "loss": 3.7866,
      "step": 663600
    },
    {
      "epoch": 142.30274442538592,
      "grad_norm": 7.4883294105529785,
      "learning_rate": 8.518812500000001e-06,
      "loss": 3.7607,
      "step": 663700
    },
    {
      "epoch": 142.32418524871355,
      "grad_norm": 7.011424541473389,
      "learning_rate": 8.5125625e-06,
      "loss": 3.7362,
      "step": 663800
    },
    {
      "epoch": 142.34562607204117,
      "grad_norm": 6.29816198348999,
      "learning_rate": 8.506312500000001e-06,
      "loss": 3.7946,
      "step": 663900
    },
    {
      "epoch": 142.3670668953688,
      "grad_norm": 6.1410112380981445,
      "learning_rate": 8.5000625e-06,
      "loss": 3.707,
      "step": 664000
    },
    {
      "epoch": 142.3885077186964,
      "grad_norm": 6.6969828605651855,
      "learning_rate": 8.4938125e-06,
      "loss": 3.7322,
      "step": 664100
    },
    {
      "epoch": 142.409948542024,
      "grad_norm": 6.292760372161865,
      "learning_rate": 8.4875625e-06,
      "loss": 3.7702,
      "step": 664200
    },
    {
      "epoch": 142.43138936535163,
      "grad_norm": 6.253159523010254,
      "learning_rate": 8.4813125e-06,
      "loss": 3.7372,
      "step": 664300
    },
    {
      "epoch": 142.45283018867926,
      "grad_norm": 5.962240695953369,
      "learning_rate": 8.4750625e-06,
      "loss": 3.7653,
      "step": 664400
    },
    {
      "epoch": 142.47427101200685,
      "grad_norm": 6.140214920043945,
      "learning_rate": 8.4688125e-06,
      "loss": 3.7445,
      "step": 664500
    },
    {
      "epoch": 142.49571183533448,
      "grad_norm": 6.2093915939331055,
      "learning_rate": 8.4625625e-06,
      "loss": 3.6684,
      "step": 664600
    },
    {
      "epoch": 142.5171526586621,
      "grad_norm": 6.765385150909424,
      "learning_rate": 8.4563125e-06,
      "loss": 3.7758,
      "step": 664700
    },
    {
      "epoch": 142.53859348198972,
      "grad_norm": 6.2019877433776855,
      "learning_rate": 8.4500625e-06,
      "loss": 3.7333,
      "step": 664800
    },
    {
      "epoch": 142.56003430531732,
      "grad_norm": 6.712227821350098,
      "learning_rate": 8.443812500000001e-06,
      "loss": 3.7706,
      "step": 664900
    },
    {
      "epoch": 142.58147512864494,
      "grad_norm": 6.867129802703857,
      "learning_rate": 8.4375625e-06,
      "loss": 3.8457,
      "step": 665000
    },
    {
      "epoch": 142.60291595197256,
      "grad_norm": 6.487653732299805,
      "learning_rate": 8.431312500000001e-06,
      "loss": 3.779,
      "step": 665100
    },
    {
      "epoch": 142.62435677530019,
      "grad_norm": 5.871138095855713,
      "learning_rate": 8.4250625e-06,
      "loss": 3.7425,
      "step": 665200
    },
    {
      "epoch": 142.64579759862778,
      "grad_norm": 5.345970153808594,
      "learning_rate": 8.4188125e-06,
      "loss": 3.7306,
      "step": 665300
    },
    {
      "epoch": 142.6672384219554,
      "grad_norm": 6.586262226104736,
      "learning_rate": 8.4125625e-06,
      "loss": 3.7592,
      "step": 665400
    },
    {
      "epoch": 142.68867924528303,
      "grad_norm": 5.881659984588623,
      "learning_rate": 8.4063125e-06,
      "loss": 3.7622,
      "step": 665500
    },
    {
      "epoch": 142.71012006861062,
      "grad_norm": 6.726282596588135,
      "learning_rate": 8.4000625e-06,
      "loss": 3.8072,
      "step": 665600
    },
    {
      "epoch": 142.73156089193824,
      "grad_norm": 6.86177921295166,
      "learning_rate": 8.3938125e-06,
      "loss": 3.7458,
      "step": 665700
    },
    {
      "epoch": 142.75300171526587,
      "grad_norm": 6.83796501159668,
      "learning_rate": 8.3875625e-06,
      "loss": 3.7589,
      "step": 665800
    },
    {
      "epoch": 142.7744425385935,
      "grad_norm": 6.868029594421387,
      "learning_rate": 8.381312500000001e-06,
      "loss": 3.7613,
      "step": 665900
    },
    {
      "epoch": 142.79588336192108,
      "grad_norm": 6.2246599197387695,
      "learning_rate": 8.3750625e-06,
      "loss": 3.7562,
      "step": 666000
    },
    {
      "epoch": 142.8173241852487,
      "grad_norm": 6.98009729385376,
      "learning_rate": 8.368812500000001e-06,
      "loss": 3.7754,
      "step": 666100
    },
    {
      "epoch": 142.83876500857633,
      "grad_norm": 6.198972702026367,
      "learning_rate": 8.3625625e-06,
      "loss": 3.7084,
      "step": 666200
    },
    {
      "epoch": 142.86020583190395,
      "grad_norm": 6.527207374572754,
      "learning_rate": 8.3563125e-06,
      "loss": 3.75,
      "step": 666300
    },
    {
      "epoch": 142.88164665523155,
      "grad_norm": 6.626411437988281,
      "learning_rate": 8.3500625e-06,
      "loss": 3.8179,
      "step": 666400
    },
    {
      "epoch": 142.90308747855917,
      "grad_norm": 5.908041954040527,
      "learning_rate": 8.3438125e-06,
      "loss": 3.7493,
      "step": 666500
    },
    {
      "epoch": 142.9245283018868,
      "grad_norm": 5.839380264282227,
      "learning_rate": 8.3375625e-06,
      "loss": 3.7577,
      "step": 666600
    },
    {
      "epoch": 142.94596912521442,
      "grad_norm": 7.408921718597412,
      "learning_rate": 8.3313125e-06,
      "loss": 3.7587,
      "step": 666700
    },
    {
      "epoch": 142.967409948542,
      "grad_norm": 6.70302677154541,
      "learning_rate": 8.3250625e-06,
      "loss": 3.7554,
      "step": 666800
    },
    {
      "epoch": 142.98885077186964,
      "grad_norm": 7.0115966796875,
      "learning_rate": 8.3188125e-06,
      "loss": 3.7942,
      "step": 666900
    },
    {
      "epoch": 143.01029159519726,
      "grad_norm": 6.5129899978637695,
      "learning_rate": 8.3125625e-06,
      "loss": 3.7394,
      "step": 667000
    },
    {
      "epoch": 143.03173241852488,
      "grad_norm": 7.067324638366699,
      "learning_rate": 8.306312500000001e-06,
      "loss": 3.7766,
      "step": 667100
    },
    {
      "epoch": 143.05317324185248,
      "grad_norm": 6.00688362121582,
      "learning_rate": 8.3000625e-06,
      "loss": 3.8013,
      "step": 667200
    },
    {
      "epoch": 143.0746140651801,
      "grad_norm": 6.289604663848877,
      "learning_rate": 8.293812500000001e-06,
      "loss": 3.7503,
      "step": 667300
    },
    {
      "epoch": 143.09605488850772,
      "grad_norm": 6.818353176116943,
      "learning_rate": 8.287562499999999e-06,
      "loss": 3.7584,
      "step": 667400
    },
    {
      "epoch": 143.11749571183535,
      "grad_norm": 6.370419025421143,
      "learning_rate": 8.2813125e-06,
      "loss": 3.728,
      "step": 667500
    },
    {
      "epoch": 143.13893653516294,
      "grad_norm": 6.648532390594482,
      "learning_rate": 8.2750625e-06,
      "loss": 3.7181,
      "step": 667600
    },
    {
      "epoch": 143.16037735849056,
      "grad_norm": 6.750278472900391,
      "learning_rate": 8.2688125e-06,
      "loss": 3.7591,
      "step": 667700
    },
    {
      "epoch": 143.1818181818182,
      "grad_norm": 6.541488170623779,
      "learning_rate": 8.2625625e-06,
      "loss": 3.7653,
      "step": 667800
    },
    {
      "epoch": 143.2032590051458,
      "grad_norm": 6.824886798858643,
      "learning_rate": 8.2563125e-06,
      "loss": 3.7739,
      "step": 667900
    },
    {
      "epoch": 143.2246998284734,
      "grad_norm": 6.615970611572266,
      "learning_rate": 8.2500625e-06,
      "loss": 3.7119,
      "step": 668000
    },
    {
      "epoch": 143.24614065180103,
      "grad_norm": 6.484884738922119,
      "learning_rate": 8.2438125e-06,
      "loss": 3.7514,
      "step": 668100
    },
    {
      "epoch": 143.26758147512865,
      "grad_norm": 6.0115437507629395,
      "learning_rate": 8.2375625e-06,
      "loss": 3.7758,
      "step": 668200
    },
    {
      "epoch": 143.28902229845627,
      "grad_norm": 6.837680816650391,
      "learning_rate": 8.231312500000001e-06,
      "loss": 3.7756,
      "step": 668300
    },
    {
      "epoch": 143.31046312178387,
      "grad_norm": 6.260519504547119,
      "learning_rate": 8.2250625e-06,
      "loss": 3.6799,
      "step": 668400
    },
    {
      "epoch": 143.3319039451115,
      "grad_norm": 6.003363132476807,
      "learning_rate": 8.218812500000001e-06,
      "loss": 3.7185,
      "step": 668500
    },
    {
      "epoch": 143.35334476843911,
      "grad_norm": 6.23573112487793,
      "learning_rate": 8.212562499999999e-06,
      "loss": 3.7411,
      "step": 668600
    },
    {
      "epoch": 143.37478559176674,
      "grad_norm": 6.637829303741455,
      "learning_rate": 8.2063125e-06,
      "loss": 3.7338,
      "step": 668700
    },
    {
      "epoch": 143.39622641509433,
      "grad_norm": 7.128600597381592,
      "learning_rate": 8.2000625e-06,
      "loss": 3.7646,
      "step": 668800
    },
    {
      "epoch": 143.41766723842196,
      "grad_norm": 6.502882480621338,
      "learning_rate": 8.1938125e-06,
      "loss": 3.7176,
      "step": 668900
    },
    {
      "epoch": 143.43910806174958,
      "grad_norm": 6.559331893920898,
      "learning_rate": 8.1875625e-06,
      "loss": 3.7251,
      "step": 669000
    },
    {
      "epoch": 143.46054888507717,
      "grad_norm": 6.012581825256348,
      "learning_rate": 8.1813125e-06,
      "loss": 3.7237,
      "step": 669100
    },
    {
      "epoch": 143.4819897084048,
      "grad_norm": 6.18349027633667,
      "learning_rate": 8.1750625e-06,
      "loss": 3.7442,
      "step": 669200
    },
    {
      "epoch": 143.50343053173242,
      "grad_norm": 6.028260231018066,
      "learning_rate": 8.1688125e-06,
      "loss": 3.7391,
      "step": 669300
    },
    {
      "epoch": 143.52487135506004,
      "grad_norm": 6.439328193664551,
      "learning_rate": 8.1625625e-06,
      "loss": 3.6989,
      "step": 669400
    },
    {
      "epoch": 143.54631217838764,
      "grad_norm": 7.696290969848633,
      "learning_rate": 8.156312500000001e-06,
      "loss": 3.7068,
      "step": 669500
    },
    {
      "epoch": 143.56775300171526,
      "grad_norm": 6.561961650848389,
      "learning_rate": 8.1500625e-06,
      "loss": 3.7315,
      "step": 669600
    },
    {
      "epoch": 143.58919382504288,
      "grad_norm": 7.262970447540283,
      "learning_rate": 8.1438125e-06,
      "loss": 3.781,
      "step": 669700
    },
    {
      "epoch": 143.6106346483705,
      "grad_norm": 6.499212741851807,
      "learning_rate": 8.137562499999999e-06,
      "loss": 3.7338,
      "step": 669800
    },
    {
      "epoch": 143.6320754716981,
      "grad_norm": 6.5900421142578125,
      "learning_rate": 8.1313125e-06,
      "loss": 3.7521,
      "step": 669900
    },
    {
      "epoch": 143.65351629502572,
      "grad_norm": 6.49364709854126,
      "learning_rate": 8.1250625e-06,
      "loss": 3.7666,
      "step": 670000
    },
    {
      "epoch": 143.67495711835335,
      "grad_norm": 6.755640029907227,
      "learning_rate": 8.1188125e-06,
      "loss": 3.7635,
      "step": 670100
    },
    {
      "epoch": 143.69639794168097,
      "grad_norm": 6.259525299072266,
      "learning_rate": 8.112562500000002e-06,
      "loss": 3.7332,
      "step": 670200
    },
    {
      "epoch": 143.71783876500857,
      "grad_norm": 5.997583866119385,
      "learning_rate": 8.1063125e-06,
      "loss": 3.7565,
      "step": 670300
    },
    {
      "epoch": 143.7392795883362,
      "grad_norm": 6.272647380828857,
      "learning_rate": 8.100062500000001e-06,
      "loss": 3.6909,
      "step": 670400
    },
    {
      "epoch": 143.7607204116638,
      "grad_norm": 6.179378986358643,
      "learning_rate": 8.0938125e-06,
      "loss": 3.8009,
      "step": 670500
    },
    {
      "epoch": 143.78216123499143,
      "grad_norm": 6.553849697113037,
      "learning_rate": 8.087562500000001e-06,
      "loss": 3.7398,
      "step": 670600
    },
    {
      "epoch": 143.80360205831903,
      "grad_norm": 6.878076553344727,
      "learning_rate": 8.081312500000001e-06,
      "loss": 3.7872,
      "step": 670700
    },
    {
      "epoch": 143.82504288164665,
      "grad_norm": 6.799681663513184,
      "learning_rate": 8.0750625e-06,
      "loss": 3.7151,
      "step": 670800
    },
    {
      "epoch": 143.84648370497428,
      "grad_norm": 7.744922637939453,
      "learning_rate": 8.0688125e-06,
      "loss": 3.7597,
      "step": 670900
    },
    {
      "epoch": 143.8679245283019,
      "grad_norm": 5.8732757568359375,
      "learning_rate": 8.0625625e-06,
      "loss": 3.7344,
      "step": 671000
    },
    {
      "epoch": 143.8893653516295,
      "grad_norm": 6.3764448165893555,
      "learning_rate": 8.0563125e-06,
      "loss": 3.6992,
      "step": 671100
    },
    {
      "epoch": 143.91080617495712,
      "grad_norm": 6.234516620635986,
      "learning_rate": 8.050062500000002e-06,
      "loss": 3.7564,
      "step": 671200
    },
    {
      "epoch": 143.93224699828474,
      "grad_norm": 6.909518718719482,
      "learning_rate": 8.0438125e-06,
      "loss": 3.7599,
      "step": 671300
    },
    {
      "epoch": 143.95368782161236,
      "grad_norm": 7.123536109924316,
      "learning_rate": 8.037562500000002e-06,
      "loss": 3.6585,
      "step": 671400
    },
    {
      "epoch": 143.97512864493996,
      "grad_norm": 6.68703031539917,
      "learning_rate": 8.0313125e-06,
      "loss": 3.7442,
      "step": 671500
    },
    {
      "epoch": 143.99656946826758,
      "grad_norm": 6.069364547729492,
      "learning_rate": 8.025062500000001e-06,
      "loss": 3.7603,
      "step": 671600
    },
    {
      "epoch": 144.0180102915952,
      "grad_norm": 6.667585849761963,
      "learning_rate": 8.018812500000001e-06,
      "loss": 3.7247,
      "step": 671700
    },
    {
      "epoch": 144.03945111492283,
      "grad_norm": 6.309157371520996,
      "learning_rate": 8.012562500000001e-06,
      "loss": 3.7519,
      "step": 671800
    },
    {
      "epoch": 144.06089193825042,
      "grad_norm": 6.1284990310668945,
      "learning_rate": 8.0063125e-06,
      "loss": 3.7253,
      "step": 671900
    },
    {
      "epoch": 144.08233276157804,
      "grad_norm": 6.345468521118164,
      "learning_rate": 8.0000625e-06,
      "loss": 3.7625,
      "step": 672000
    },
    {
      "epoch": 144.10377358490567,
      "grad_norm": 6.446476459503174,
      "learning_rate": 7.9938125e-06,
      "loss": 3.7843,
      "step": 672100
    },
    {
      "epoch": 144.12521440823326,
      "grad_norm": 6.479752540588379,
      "learning_rate": 7.9875625e-06,
      "loss": 3.7044,
      "step": 672200
    },
    {
      "epoch": 144.14665523156089,
      "grad_norm": 6.940110683441162,
      "learning_rate": 7.9813125e-06,
      "loss": 3.7216,
      "step": 672300
    },
    {
      "epoch": 144.1680960548885,
      "grad_norm": 6.198185443878174,
      "learning_rate": 7.975062500000002e-06,
      "loss": 3.7121,
      "step": 672400
    },
    {
      "epoch": 144.18953687821613,
      "grad_norm": 6.532151222229004,
      "learning_rate": 7.9688125e-06,
      "loss": 3.7312,
      "step": 672500
    },
    {
      "epoch": 144.21097770154373,
      "grad_norm": 6.210446834564209,
      "learning_rate": 7.962562500000001e-06,
      "loss": 3.7612,
      "step": 672600
    },
    {
      "epoch": 144.23241852487135,
      "grad_norm": 7.011137008666992,
      "learning_rate": 7.9563125e-06,
      "loss": 3.7016,
      "step": 672700
    },
    {
      "epoch": 144.25385934819897,
      "grad_norm": 6.384747505187988,
      "learning_rate": 7.950062500000001e-06,
      "loss": 3.7414,
      "step": 672800
    },
    {
      "epoch": 144.2753001715266,
      "grad_norm": 6.73204231262207,
      "learning_rate": 7.943812500000001e-06,
      "loss": 3.7199,
      "step": 672900
    },
    {
      "epoch": 144.2967409948542,
      "grad_norm": 5.893293380737305,
      "learning_rate": 7.9375625e-06,
      "loss": 3.7511,
      "step": 673000
    },
    {
      "epoch": 144.3181818181818,
      "grad_norm": 6.503104209899902,
      "learning_rate": 7.9313125e-06,
      "loss": 3.6946,
      "step": 673100
    },
    {
      "epoch": 144.33962264150944,
      "grad_norm": 6.895331859588623,
      "learning_rate": 7.9250625e-06,
      "loss": 3.7018,
      "step": 673200
    },
    {
      "epoch": 144.36106346483706,
      "grad_norm": 6.741488933563232,
      "learning_rate": 7.9188125e-06,
      "loss": 3.7109,
      "step": 673300
    },
    {
      "epoch": 144.38250428816465,
      "grad_norm": 6.491826057434082,
      "learning_rate": 7.9125625e-06,
      "loss": 3.6859,
      "step": 673400
    },
    {
      "epoch": 144.40394511149228,
      "grad_norm": 6.357956409454346,
      "learning_rate": 7.9063125e-06,
      "loss": 3.7531,
      "step": 673500
    },
    {
      "epoch": 144.4253859348199,
      "grad_norm": 7.971099853515625,
      "learning_rate": 7.900062500000002e-06,
      "loss": 3.7776,
      "step": 673600
    },
    {
      "epoch": 144.44682675814752,
      "grad_norm": 6.4213643074035645,
      "learning_rate": 7.8938125e-06,
      "loss": 3.7098,
      "step": 673700
    },
    {
      "epoch": 144.46826758147512,
      "grad_norm": 6.126220703125,
      "learning_rate": 7.887562500000001e-06,
      "loss": 3.7205,
      "step": 673800
    },
    {
      "epoch": 144.48970840480274,
      "grad_norm": 6.761229515075684,
      "learning_rate": 7.8813125e-06,
      "loss": 3.751,
      "step": 673900
    },
    {
      "epoch": 144.51114922813036,
      "grad_norm": 6.461826801300049,
      "learning_rate": 7.875062500000001e-06,
      "loss": 3.6808,
      "step": 674000
    },
    {
      "epoch": 144.532590051458,
      "grad_norm": 6.632471561431885,
      "learning_rate": 7.8688125e-06,
      "loss": 3.7126,
      "step": 674100
    },
    {
      "epoch": 144.55403087478558,
      "grad_norm": 7.128600597381592,
      "learning_rate": 7.8625625e-06,
      "loss": 3.739,
      "step": 674200
    },
    {
      "epoch": 144.5754716981132,
      "grad_norm": 6.538464546203613,
      "learning_rate": 7.8563125e-06,
      "loss": 3.7457,
      "step": 674300
    },
    {
      "epoch": 144.59691252144083,
      "grad_norm": 7.333620548248291,
      "learning_rate": 7.8500625e-06,
      "loss": 3.6759,
      "step": 674400
    },
    {
      "epoch": 144.61835334476845,
      "grad_norm": 6.765096187591553,
      "learning_rate": 7.8438125e-06,
      "loss": 3.7387,
      "step": 674500
    },
    {
      "epoch": 144.63979416809605,
      "grad_norm": 7.038923263549805,
      "learning_rate": 7.8375625e-06,
      "loss": 3.6946,
      "step": 674600
    },
    {
      "epoch": 144.66123499142367,
      "grad_norm": 6.598473072052002,
      "learning_rate": 7.8313125e-06,
      "loss": 3.7206,
      "step": 674700
    },
    {
      "epoch": 144.6826758147513,
      "grad_norm": 6.328092575073242,
      "learning_rate": 7.825062500000001e-06,
      "loss": 3.7436,
      "step": 674800
    },
    {
      "epoch": 144.70411663807892,
      "grad_norm": 6.664746284484863,
      "learning_rate": 7.8188125e-06,
      "loss": 3.7004,
      "step": 674900
    },
    {
      "epoch": 144.7255574614065,
      "grad_norm": 7.304104328155518,
      "learning_rate": 7.812562500000001e-06,
      "loss": 3.7187,
      "step": 675000
    },
    {
      "epoch": 144.74699828473413,
      "grad_norm": 6.562466144561768,
      "learning_rate": 7.8063125e-06,
      "loss": 3.7365,
      "step": 675100
    },
    {
      "epoch": 144.76843910806176,
      "grad_norm": 6.999959468841553,
      "learning_rate": 7.8000625e-06,
      "loss": 3.7518,
      "step": 675200
    },
    {
      "epoch": 144.78987993138938,
      "grad_norm": 6.9071784019470215,
      "learning_rate": 7.7938125e-06,
      "loss": 3.7279,
      "step": 675300
    },
    {
      "epoch": 144.81132075471697,
      "grad_norm": 6.731721878051758,
      "learning_rate": 7.7875625e-06,
      "loss": 3.7192,
      "step": 675400
    },
    {
      "epoch": 144.8327615780446,
      "grad_norm": 6.341362953186035,
      "learning_rate": 7.7813125e-06,
      "loss": 3.7169,
      "step": 675500
    },
    {
      "epoch": 144.85420240137222,
      "grad_norm": 6.203519344329834,
      "learning_rate": 7.7750625e-06,
      "loss": 3.7076,
      "step": 675600
    },
    {
      "epoch": 144.87564322469981,
      "grad_norm": 5.935932159423828,
      "learning_rate": 7.7688125e-06,
      "loss": 3.729,
      "step": 675700
    },
    {
      "epoch": 144.89708404802744,
      "grad_norm": 6.268857002258301,
      "learning_rate": 7.7625625e-06,
      "loss": 3.7136,
      "step": 675800
    },
    {
      "epoch": 144.91852487135506,
      "grad_norm": 6.800405979156494,
      "learning_rate": 7.7563125e-06,
      "loss": 3.7636,
      "step": 675900
    },
    {
      "epoch": 144.93996569468268,
      "grad_norm": 6.24332857131958,
      "learning_rate": 7.750062500000001e-06,
      "loss": 3.7347,
      "step": 676000
    },
    {
      "epoch": 144.96140651801028,
      "grad_norm": 6.006579399108887,
      "learning_rate": 7.7438125e-06,
      "loss": 3.6969,
      "step": 676100
    },
    {
      "epoch": 144.9828473413379,
      "grad_norm": 6.885811805725098,
      "learning_rate": 7.737562500000001e-06,
      "loss": 3.7371,
      "step": 676200
    },
    {
      "epoch": 145.00428816466552,
      "grad_norm": 7.0753560066223145,
      "learning_rate": 7.7313125e-06,
      "loss": 3.7272,
      "step": 676300
    },
    {
      "epoch": 145.02572898799315,
      "grad_norm": 7.192377090454102,
      "learning_rate": 7.7250625e-06,
      "loss": 3.6807,
      "step": 676400
    },
    {
      "epoch": 145.04716981132074,
      "grad_norm": 6.410168647766113,
      "learning_rate": 7.7188125e-06,
      "loss": 3.75,
      "step": 676500
    },
    {
      "epoch": 145.06861063464837,
      "grad_norm": 7.1343817710876465,
      "learning_rate": 7.7125625e-06,
      "loss": 3.7879,
      "step": 676600
    },
    {
      "epoch": 145.090051457976,
      "grad_norm": 6.332779884338379,
      "learning_rate": 7.7063125e-06,
      "loss": 3.6932,
      "step": 676700
    },
    {
      "epoch": 145.1114922813036,
      "grad_norm": 6.985702037811279,
      "learning_rate": 7.7000625e-06,
      "loss": 3.7189,
      "step": 676800
    },
    {
      "epoch": 145.1329331046312,
      "grad_norm": 6.8734025955200195,
      "learning_rate": 7.6938125e-06,
      "loss": 3.7492,
      "step": 676900
    },
    {
      "epoch": 145.15437392795883,
      "grad_norm": 6.68843412399292,
      "learning_rate": 7.687562500000001e-06,
      "loss": 3.6592,
      "step": 677000
    },
    {
      "epoch": 145.17581475128645,
      "grad_norm": 6.874237060546875,
      "learning_rate": 7.6813125e-06,
      "loss": 3.7486,
      "step": 677100
    },
    {
      "epoch": 145.19725557461408,
      "grad_norm": 6.832094192504883,
      "learning_rate": 7.675062500000001e-06,
      "loss": 3.7276,
      "step": 677200
    },
    {
      "epoch": 145.21869639794167,
      "grad_norm": 6.332457065582275,
      "learning_rate": 7.6688125e-06,
      "loss": 3.6749,
      "step": 677300
    },
    {
      "epoch": 145.2401372212693,
      "grad_norm": 6.30726432800293,
      "learning_rate": 7.662562500000001e-06,
      "loss": 3.7179,
      "step": 677400
    },
    {
      "epoch": 145.26157804459692,
      "grad_norm": 6.548726558685303,
      "learning_rate": 7.6563125e-06,
      "loss": 3.6765,
      "step": 677500
    },
    {
      "epoch": 145.28301886792454,
      "grad_norm": 6.975574493408203,
      "learning_rate": 7.6500625e-06,
      "loss": 3.6461,
      "step": 677600
    },
    {
      "epoch": 145.30445969125213,
      "grad_norm": 7.388523578643799,
      "learning_rate": 7.6438125e-06,
      "loss": 3.6971,
      "step": 677700
    },
    {
      "epoch": 145.32590051457976,
      "grad_norm": 6.569766521453857,
      "learning_rate": 7.6375625e-06,
      "loss": 3.8011,
      "step": 677800
    },
    {
      "epoch": 145.34734133790738,
      "grad_norm": 6.321416854858398,
      "learning_rate": 7.6313125e-06,
      "loss": 3.7261,
      "step": 677900
    },
    {
      "epoch": 145.368782161235,
      "grad_norm": 6.517329216003418,
      "learning_rate": 7.625062500000001e-06,
      "loss": 3.6543,
      "step": 678000
    },
    {
      "epoch": 145.3902229845626,
      "grad_norm": 6.175588130950928,
      "learning_rate": 7.6188125e-06,
      "loss": 3.6905,
      "step": 678100
    },
    {
      "epoch": 145.41166380789022,
      "grad_norm": 6.309090614318848,
      "learning_rate": 7.6125625000000005e-06,
      "loss": 3.7002,
      "step": 678200
    },
    {
      "epoch": 145.43310463121784,
      "grad_norm": 7.761034965515137,
      "learning_rate": 7.6063124999999995e-06,
      "loss": 3.7143,
      "step": 678300
    },
    {
      "epoch": 145.45454545454547,
      "grad_norm": 6.381799697875977,
      "learning_rate": 7.6000625e-06,
      "loss": 3.6778,
      "step": 678400
    },
    {
      "epoch": 145.47598627787306,
      "grad_norm": 7.456116676330566,
      "learning_rate": 7.5938125e-06,
      "loss": 3.7182,
      "step": 678500
    },
    {
      "epoch": 145.49742710120069,
      "grad_norm": 6.39198112487793,
      "learning_rate": 7.587562500000001e-06,
      "loss": 3.6314,
      "step": 678600
    },
    {
      "epoch": 145.5188679245283,
      "grad_norm": 6.210982322692871,
      "learning_rate": 7.5813125e-06,
      "loss": 3.702,
      "step": 678700
    },
    {
      "epoch": 145.54030874785593,
      "grad_norm": 6.701704502105713,
      "learning_rate": 7.5750625000000005e-06,
      "loss": 3.7455,
      "step": 678800
    },
    {
      "epoch": 145.56174957118353,
      "grad_norm": 6.990242958068848,
      "learning_rate": 7.5688124999999995e-06,
      "loss": 3.6755,
      "step": 678900
    },
    {
      "epoch": 145.58319039451115,
      "grad_norm": 8.135247230529785,
      "learning_rate": 7.5625625e-06,
      "loss": 3.7165,
      "step": 679000
    },
    {
      "epoch": 145.60463121783877,
      "grad_norm": 6.6466569900512695,
      "learning_rate": 7.5563125e-06,
      "loss": 3.6999,
      "step": 679100
    },
    {
      "epoch": 145.62607204116637,
      "grad_norm": 7.050286293029785,
      "learning_rate": 7.550062500000001e-06,
      "loss": 3.6662,
      "step": 679200
    },
    {
      "epoch": 145.647512864494,
      "grad_norm": 6.4685773849487305,
      "learning_rate": 7.5438125e-06,
      "loss": 3.7733,
      "step": 679300
    },
    {
      "epoch": 145.6689536878216,
      "grad_norm": 6.251474857330322,
      "learning_rate": 7.5375625e-06,
      "loss": 3.6947,
      "step": 679400
    },
    {
      "epoch": 145.69039451114924,
      "grad_norm": 6.4207892417907715,
      "learning_rate": 7.5313125e-06,
      "loss": 3.7053,
      "step": 679500
    },
    {
      "epoch": 145.71183533447683,
      "grad_norm": 6.7692461013793945,
      "learning_rate": 7.525062500000001e-06,
      "loss": 3.6844,
      "step": 679600
    },
    {
      "epoch": 145.73327615780445,
      "grad_norm": 6.738043308258057,
      "learning_rate": 7.5188125e-06,
      "loss": 3.7143,
      "step": 679700
    },
    {
      "epoch": 145.75471698113208,
      "grad_norm": 6.658436298370361,
      "learning_rate": 7.512562500000001e-06,
      "loss": 3.7317,
      "step": 679800
    },
    {
      "epoch": 145.7761578044597,
      "grad_norm": 6.296742916107178,
      "learning_rate": 7.5063125e-06,
      "loss": 3.6971,
      "step": 679900
    },
    {
      "epoch": 145.7975986277873,
      "grad_norm": 7.146112442016602,
      "learning_rate": 7.5000625e-06,
      "loss": 3.696,
      "step": 680000
    },
    {
      "epoch": 145.81903945111492,
      "grad_norm": 6.5293049812316895,
      "learning_rate": 7.4938125e-06,
      "loss": 3.695,
      "step": 680100
    },
    {
      "epoch": 145.84048027444254,
      "grad_norm": 7.100734233856201,
      "learning_rate": 7.487562500000001e-06,
      "loss": 3.7463,
      "step": 680200
    },
    {
      "epoch": 145.86192109777016,
      "grad_norm": 6.828013896942139,
      "learning_rate": 7.4813125e-06,
      "loss": 3.7312,
      "step": 680300
    },
    {
      "epoch": 145.88336192109776,
      "grad_norm": 6.441692352294922,
      "learning_rate": 7.4750625000000006e-06,
      "loss": 3.7069,
      "step": 680400
    },
    {
      "epoch": 145.90480274442538,
      "grad_norm": 7.127501964569092,
      "learning_rate": 7.4688124999999996e-06,
      "loss": 3.696,
      "step": 680500
    },
    {
      "epoch": 145.926243567753,
      "grad_norm": 6.526202201843262,
      "learning_rate": 7.4625625e-06,
      "loss": 3.8088,
      "step": 680600
    },
    {
      "epoch": 145.94768439108063,
      "grad_norm": 6.023708343505859,
      "learning_rate": 7.4563125e-06,
      "loss": 3.7148,
      "step": 680700
    },
    {
      "epoch": 145.96912521440822,
      "grad_norm": 6.28714656829834,
      "learning_rate": 7.450062500000001e-06,
      "loss": 3.7436,
      "step": 680800
    },
    {
      "epoch": 145.99056603773585,
      "grad_norm": 6.040224552154541,
      "learning_rate": 7.4438125e-06,
      "loss": 3.7475,
      "step": 680900
    },
    {
      "epoch": 146.01200686106347,
      "grad_norm": 6.367061614990234,
      "learning_rate": 7.4375625000000005e-06,
      "loss": 3.697,
      "step": 681000
    },
    {
      "epoch": 146.0334476843911,
      "grad_norm": 6.6818766593933105,
      "learning_rate": 7.4313124999999995e-06,
      "loss": 3.6926,
      "step": 681100
    },
    {
      "epoch": 146.0548885077187,
      "grad_norm": 6.735650062561035,
      "learning_rate": 7.4250625e-06,
      "loss": 3.6596,
      "step": 681200
    },
    {
      "epoch": 146.0763293310463,
      "grad_norm": 5.866845607757568,
      "learning_rate": 7.4188125e-06,
      "loss": 3.7432,
      "step": 681300
    },
    {
      "epoch": 146.09777015437393,
      "grad_norm": 6.363959312438965,
      "learning_rate": 7.412562500000001e-06,
      "loss": 3.6856,
      "step": 681400
    },
    {
      "epoch": 146.11921097770156,
      "grad_norm": 6.6849751472473145,
      "learning_rate": 7.4063125e-06,
      "loss": 3.6997,
      "step": 681500
    },
    {
      "epoch": 146.14065180102915,
      "grad_norm": 6.647244930267334,
      "learning_rate": 7.4000625000000004e-06,
      "loss": 3.6598,
      "step": 681600
    },
    {
      "epoch": 146.16209262435677,
      "grad_norm": 6.536392688751221,
      "learning_rate": 7.3938124999999994e-06,
      "loss": 3.7121,
      "step": 681700
    },
    {
      "epoch": 146.1835334476844,
      "grad_norm": 6.542807102203369,
      "learning_rate": 7.3875625e-06,
      "loss": 3.7044,
      "step": 681800
    },
    {
      "epoch": 146.20497427101202,
      "grad_norm": 6.245061874389648,
      "learning_rate": 7.3813125e-06,
      "loss": 3.6646,
      "step": 681900
    },
    {
      "epoch": 146.22641509433961,
      "grad_norm": 6.430360794067383,
      "learning_rate": 7.375062500000001e-06,
      "loss": 3.7106,
      "step": 682000
    },
    {
      "epoch": 146.24785591766724,
      "grad_norm": 7.386494159698486,
      "learning_rate": 7.3688125e-06,
      "loss": 3.7138,
      "step": 682100
    },
    {
      "epoch": 146.26929674099486,
      "grad_norm": 6.373210430145264,
      "learning_rate": 7.3625625e-06,
      "loss": 3.7363,
      "step": 682200
    },
    {
      "epoch": 146.29073756432248,
      "grad_norm": 6.593141078948975,
      "learning_rate": 7.356312499999999e-06,
      "loss": 3.6905,
      "step": 682300
    },
    {
      "epoch": 146.31217838765008,
      "grad_norm": 6.182401657104492,
      "learning_rate": 7.3500625e-06,
      "loss": 3.7031,
      "step": 682400
    },
    {
      "epoch": 146.3336192109777,
      "grad_norm": 7.077778339385986,
      "learning_rate": 7.343812500000001e-06,
      "loss": 3.723,
      "step": 682500
    },
    {
      "epoch": 146.35506003430532,
      "grad_norm": 7.224523067474365,
      "learning_rate": 7.337562500000001e-06,
      "loss": 3.6629,
      "step": 682600
    },
    {
      "epoch": 146.37650085763292,
      "grad_norm": 6.671813488006592,
      "learning_rate": 7.331312500000001e-06,
      "loss": 3.6459,
      "step": 682700
    },
    {
      "epoch": 146.39794168096054,
      "grad_norm": 6.775936603546143,
      "learning_rate": 7.3250625e-06,
      "loss": 3.7276,
      "step": 682800
    },
    {
      "epoch": 146.41938250428817,
      "grad_norm": 7.755044937133789,
      "learning_rate": 7.318812500000001e-06,
      "loss": 3.6978,
      "step": 682900
    },
    {
      "epoch": 146.4408233276158,
      "grad_norm": 7.412604331970215,
      "learning_rate": 7.3125625e-06,
      "loss": 3.6864,
      "step": 683000
    },
    {
      "epoch": 146.46226415094338,
      "grad_norm": 6.444636344909668,
      "learning_rate": 7.306312500000001e-06,
      "loss": 3.7096,
      "step": 683100
    },
    {
      "epoch": 146.483704974271,
      "grad_norm": 6.8352952003479,
      "learning_rate": 7.3000625000000005e-06,
      "loss": 3.6845,
      "step": 683200
    },
    {
      "epoch": 146.50514579759863,
      "grad_norm": 6.343637943267822,
      "learning_rate": 7.293812500000001e-06,
      "loss": 3.7248,
      "step": 683300
    },
    {
      "epoch": 146.52658662092625,
      "grad_norm": 6.894742012023926,
      "learning_rate": 7.2875625e-06,
      "loss": 3.7224,
      "step": 683400
    },
    {
      "epoch": 146.54802744425385,
      "grad_norm": 6.5486955642700195,
      "learning_rate": 7.281312500000001e-06,
      "loss": 3.6448,
      "step": 683500
    },
    {
      "epoch": 146.56946826758147,
      "grad_norm": 7.167681694030762,
      "learning_rate": 7.2750625e-06,
      "loss": 3.7122,
      "step": 683600
    },
    {
      "epoch": 146.5909090909091,
      "grad_norm": 6.8300251960754395,
      "learning_rate": 7.268812500000001e-06,
      "loss": 3.7259,
      "step": 683700
    },
    {
      "epoch": 146.61234991423672,
      "grad_norm": 6.896383762359619,
      "learning_rate": 7.2625625000000005e-06,
      "loss": 3.7248,
      "step": 683800
    },
    {
      "epoch": 146.6337907375643,
      "grad_norm": 6.084974765777588,
      "learning_rate": 7.256312500000001e-06,
      "loss": 3.6872,
      "step": 683900
    },
    {
      "epoch": 146.65523156089193,
      "grad_norm": 7.025745868682861,
      "learning_rate": 7.2500625e-06,
      "loss": 3.7075,
      "step": 684000
    },
    {
      "epoch": 146.67667238421956,
      "grad_norm": 6.0279669761657715,
      "learning_rate": 7.243812500000001e-06,
      "loss": 3.6323,
      "step": 684100
    },
    {
      "epoch": 146.69811320754718,
      "grad_norm": 6.163763046264648,
      "learning_rate": 7.2375625e-06,
      "loss": 3.6887,
      "step": 684200
    },
    {
      "epoch": 146.71955403087478,
      "grad_norm": 7.959088325500488,
      "learning_rate": 7.2313125000000006e-06,
      "loss": 3.7211,
      "step": 684300
    },
    {
      "epoch": 146.7409948542024,
      "grad_norm": 6.503664970397949,
      "learning_rate": 7.2250625e-06,
      "loss": 3.6672,
      "step": 684400
    },
    {
      "epoch": 146.76243567753002,
      "grad_norm": 6.3016438484191895,
      "learning_rate": 7.218812500000001e-06,
      "loss": 3.6619,
      "step": 684500
    },
    {
      "epoch": 146.78387650085764,
      "grad_norm": 6.912335395812988,
      "learning_rate": 7.2125625e-06,
      "loss": 3.6588,
      "step": 684600
    },
    {
      "epoch": 146.80531732418524,
      "grad_norm": 6.392665386199951,
      "learning_rate": 7.206312500000001e-06,
      "loss": 3.6967,
      "step": 684700
    },
    {
      "epoch": 146.82675814751286,
      "grad_norm": 6.319997310638428,
      "learning_rate": 7.2000625e-06,
      "loss": 3.7496,
      "step": 684800
    },
    {
      "epoch": 146.84819897084049,
      "grad_norm": 6.478763580322266,
      "learning_rate": 7.1938125000000005e-06,
      "loss": 3.6971,
      "step": 684900
    },
    {
      "epoch": 146.8696397941681,
      "grad_norm": 6.528867244720459,
      "learning_rate": 7.1875625e-06,
      "loss": 3.678,
      "step": 685000
    },
    {
      "epoch": 146.8910806174957,
      "grad_norm": 6.831228733062744,
      "learning_rate": 7.181312500000001e-06,
      "loss": 3.7452,
      "step": 685100
    },
    {
      "epoch": 146.91252144082333,
      "grad_norm": 6.662437438964844,
      "learning_rate": 7.1750625e-06,
      "loss": 3.6754,
      "step": 685200
    },
    {
      "epoch": 146.93396226415095,
      "grad_norm": 6.662660121917725,
      "learning_rate": 7.168812500000001e-06,
      "loss": 3.6893,
      "step": 685300
    },
    {
      "epoch": 146.95540308747857,
      "grad_norm": 6.246044635772705,
      "learning_rate": 7.162562500000001e-06,
      "loss": 3.7128,
      "step": 685400
    },
    {
      "epoch": 146.97684391080617,
      "grad_norm": 7.007647514343262,
      "learning_rate": 7.1563125000000004e-06,
      "loss": 3.7573,
      "step": 685500
    },
    {
      "epoch": 146.9982847341338,
      "grad_norm": 7.116260051727295,
      "learning_rate": 7.1500625e-06,
      "loss": 3.6632,
      "step": 685600
    },
    {
      "epoch": 147.0197255574614,
      "grad_norm": 6.636363983154297,
      "learning_rate": 7.143812500000001e-06,
      "loss": 3.689,
      "step": 685700
    },
    {
      "epoch": 147.04116638078904,
      "grad_norm": 6.442625999450684,
      "learning_rate": 7.1375625e-06,
      "loss": 3.7274,
      "step": 685800
    },
    {
      "epoch": 147.06260720411663,
      "grad_norm": 6.2261247634887695,
      "learning_rate": 7.131312500000001e-06,
      "loss": 3.681,
      "step": 685900
    },
    {
      "epoch": 147.08404802744425,
      "grad_norm": 6.787309169769287,
      "learning_rate": 7.1250625000000005e-06,
      "loss": 3.656,
      "step": 686000
    },
    {
      "epoch": 147.10548885077188,
      "grad_norm": 6.761825084686279,
      "learning_rate": 7.118812500000001e-06,
      "loss": 3.6444,
      "step": 686100
    },
    {
      "epoch": 147.12692967409947,
      "grad_norm": 6.115318298339844,
      "learning_rate": 7.1125625e-06,
      "loss": 3.6306,
      "step": 686200
    },
    {
      "epoch": 147.1483704974271,
      "grad_norm": 6.72755241394043,
      "learning_rate": 7.106312500000001e-06,
      "loss": 3.7102,
      "step": 686300
    },
    {
      "epoch": 147.16981132075472,
      "grad_norm": 6.636256694793701,
      "learning_rate": 7.1000625e-06,
      "loss": 3.7276,
      "step": 686400
    },
    {
      "epoch": 147.19125214408234,
      "grad_norm": 6.705650329589844,
      "learning_rate": 7.093812500000001e-06,
      "loss": 3.7225,
      "step": 686500
    },
    {
      "epoch": 147.21269296740994,
      "grad_norm": 6.813177585601807,
      "learning_rate": 7.0875625000000005e-06,
      "loss": 3.7221,
      "step": 686600
    },
    {
      "epoch": 147.23413379073756,
      "grad_norm": 6.259734153747559,
      "learning_rate": 7.081312500000001e-06,
      "loss": 3.705,
      "step": 686700
    },
    {
      "epoch": 147.25557461406518,
      "grad_norm": 6.1685285568237305,
      "learning_rate": 7.0750625e-06,
      "loss": 3.7012,
      "step": 686800
    },
    {
      "epoch": 147.2770154373928,
      "grad_norm": 6.394306182861328,
      "learning_rate": 7.068812500000001e-06,
      "loss": 3.7076,
      "step": 686900
    },
    {
      "epoch": 147.2984562607204,
      "grad_norm": 7.556168079376221,
      "learning_rate": 7.0625625e-06,
      "loss": 3.6463,
      "step": 687000
    },
    {
      "epoch": 147.31989708404802,
      "grad_norm": 6.859193801879883,
      "learning_rate": 7.0563125000000006e-06,
      "loss": 3.6739,
      "step": 687100
    },
    {
      "epoch": 147.34133790737565,
      "grad_norm": 6.1697163581848145,
      "learning_rate": 7.0500625e-06,
      "loss": 3.6628,
      "step": 687200
    },
    {
      "epoch": 147.36277873070327,
      "grad_norm": 6.208736419677734,
      "learning_rate": 7.043812500000001e-06,
      "loss": 3.6688,
      "step": 687300
    },
    {
      "epoch": 147.38421955403086,
      "grad_norm": 6.910383701324463,
      "learning_rate": 7.0375625e-06,
      "loss": 3.6305,
      "step": 687400
    },
    {
      "epoch": 147.4056603773585,
      "grad_norm": 6.659983158111572,
      "learning_rate": 7.031312500000001e-06,
      "loss": 3.691,
      "step": 687500
    },
    {
      "epoch": 147.4271012006861,
      "grad_norm": 6.86824369430542,
      "learning_rate": 7.0250625e-06,
      "loss": 3.6925,
      "step": 687600
    },
    {
      "epoch": 147.44854202401373,
      "grad_norm": 6.5888447761535645,
      "learning_rate": 7.0188125000000005e-06,
      "loss": 3.6537,
      "step": 687700
    },
    {
      "epoch": 147.46998284734133,
      "grad_norm": 6.751401901245117,
      "learning_rate": 7.0125625e-06,
      "loss": 3.6616,
      "step": 687800
    },
    {
      "epoch": 147.49142367066895,
      "grad_norm": 7.645360946655273,
      "learning_rate": 7.006312500000001e-06,
      "loss": 3.6182,
      "step": 687900
    },
    {
      "epoch": 147.51286449399657,
      "grad_norm": 6.695041656494141,
      "learning_rate": 7.0000625e-06,
      "loss": 3.7241,
      "step": 688000
    },
    {
      "epoch": 147.5343053173242,
      "grad_norm": 6.856454849243164,
      "learning_rate": 6.993812500000001e-06,
      "loss": 3.6721,
      "step": 688100
    },
    {
      "epoch": 147.5557461406518,
      "grad_norm": 6.898955345153809,
      "learning_rate": 6.9875625e-06,
      "loss": 3.7511,
      "step": 688200
    },
    {
      "epoch": 147.57718696397941,
      "grad_norm": 6.074383735656738,
      "learning_rate": 6.9813125e-06,
      "loss": 3.6836,
      "step": 688300
    },
    {
      "epoch": 147.59862778730704,
      "grad_norm": 6.752013206481934,
      "learning_rate": 6.9750625e-06,
      "loss": 3.6731,
      "step": 688400
    },
    {
      "epoch": 147.62006861063466,
      "grad_norm": 6.927596092224121,
      "learning_rate": 6.968812500000001e-06,
      "loss": 3.7534,
      "step": 688500
    },
    {
      "epoch": 147.64150943396226,
      "grad_norm": 6.598309516906738,
      "learning_rate": 6.9625625e-06,
      "loss": 3.7443,
      "step": 688600
    },
    {
      "epoch": 147.66295025728988,
      "grad_norm": 6.458530902862549,
      "learning_rate": 6.956312500000001e-06,
      "loss": 3.7252,
      "step": 688700
    },
    {
      "epoch": 147.6843910806175,
      "grad_norm": 6.7175984382629395,
      "learning_rate": 6.9500625e-06,
      "loss": 3.6927,
      "step": 688800
    },
    {
      "epoch": 147.70583190394512,
      "grad_norm": 7.572147369384766,
      "learning_rate": 6.9438125e-06,
      "loss": 3.6374,
      "step": 688900
    },
    {
      "epoch": 147.72727272727272,
      "grad_norm": 6.391861438751221,
      "learning_rate": 6.9375625e-06,
      "loss": 3.6872,
      "step": 689000
    },
    {
      "epoch": 147.74871355060034,
      "grad_norm": 6.975728511810303,
      "learning_rate": 6.931312500000001e-06,
      "loss": 3.6423,
      "step": 689100
    },
    {
      "epoch": 147.77015437392797,
      "grad_norm": 7.2870025634765625,
      "learning_rate": 6.9250625e-06,
      "loss": 3.6751,
      "step": 689200
    },
    {
      "epoch": 147.79159519725556,
      "grad_norm": 7.368049621582031,
      "learning_rate": 6.918812500000001e-06,
      "loss": 3.7293,
      "step": 689300
    },
    {
      "epoch": 147.81303602058318,
      "grad_norm": 6.332057952880859,
      "learning_rate": 6.9125625e-06,
      "loss": 3.6239,
      "step": 689400
    },
    {
      "epoch": 147.8344768439108,
      "grad_norm": 6.146596908569336,
      "learning_rate": 6.9063125e-06,
      "loss": 3.6681,
      "step": 689500
    },
    {
      "epoch": 147.85591766723843,
      "grad_norm": 6.634731769561768,
      "learning_rate": 6.9000625e-06,
      "loss": 3.7581,
      "step": 689600
    },
    {
      "epoch": 147.87735849056602,
      "grad_norm": 6.82610559463501,
      "learning_rate": 6.893812500000001e-06,
      "loss": 3.6999,
      "step": 689700
    },
    {
      "epoch": 147.89879931389365,
      "grad_norm": 6.6477580070495605,
      "learning_rate": 6.8875625e-06,
      "loss": 3.667,
      "step": 689800
    },
    {
      "epoch": 147.92024013722127,
      "grad_norm": 6.122439384460449,
      "learning_rate": 6.8813125000000005e-06,
      "loss": 3.6735,
      "step": 689900
    },
    {
      "epoch": 147.9416809605489,
      "grad_norm": 7.2349772453308105,
      "learning_rate": 6.8750624999999995e-06,
      "loss": 3.6493,
      "step": 690000
    },
    {
      "epoch": 147.9631217838765,
      "grad_norm": 6.453047275543213,
      "learning_rate": 6.8688125e-06,
      "loss": 3.6308,
      "step": 690100
    },
    {
      "epoch": 147.9845626072041,
      "grad_norm": 7.051427841186523,
      "learning_rate": 6.8625625e-06,
      "loss": 3.6913,
      "step": 690200
    },
    {
      "epoch": 148.00600343053173,
      "grad_norm": 7.187465667724609,
      "learning_rate": 6.856312500000001e-06,
      "loss": 3.6666,
      "step": 690300
    },
    {
      "epoch": 148.02744425385936,
      "grad_norm": 6.230503559112549,
      "learning_rate": 6.8500625e-06,
      "loss": 3.6524,
      "step": 690400
    },
    {
      "epoch": 148.04888507718695,
      "grad_norm": 6.995238304138184,
      "learning_rate": 6.8438125000000005e-06,
      "loss": 3.6492,
      "step": 690500
    },
    {
      "epoch": 148.07032590051458,
      "grad_norm": 6.635960578918457,
      "learning_rate": 6.8375624999999995e-06,
      "loss": 3.6638,
      "step": 690600
    },
    {
      "epoch": 148.0917667238422,
      "grad_norm": 6.600986003875732,
      "learning_rate": 6.8313125e-06,
      "loss": 3.7203,
      "step": 690700
    },
    {
      "epoch": 148.11320754716982,
      "grad_norm": 7.317533016204834,
      "learning_rate": 6.8250625e-06,
      "loss": 3.6532,
      "step": 690800
    },
    {
      "epoch": 148.13464837049742,
      "grad_norm": 7.005056381225586,
      "learning_rate": 6.818812500000001e-06,
      "loss": 3.6498,
      "step": 690900
    },
    {
      "epoch": 148.15608919382504,
      "grad_norm": 6.503084659576416,
      "learning_rate": 6.8125625e-06,
      "loss": 3.6661,
      "step": 691000
    },
    {
      "epoch": 148.17753001715266,
      "grad_norm": 6.547377586364746,
      "learning_rate": 6.8063125e-06,
      "loss": 3.6542,
      "step": 691100
    },
    {
      "epoch": 148.19897084048029,
      "grad_norm": 7.072615146636963,
      "learning_rate": 6.8000625e-06,
      "loss": 3.6629,
      "step": 691200
    },
    {
      "epoch": 148.22041166380788,
      "grad_norm": 6.779899597167969,
      "learning_rate": 6.7938125e-06,
      "loss": 3.7022,
      "step": 691300
    },
    {
      "epoch": 148.2418524871355,
      "grad_norm": 6.619811534881592,
      "learning_rate": 6.7875625e-06,
      "loss": 3.6705,
      "step": 691400
    },
    {
      "epoch": 148.26329331046313,
      "grad_norm": 6.483672142028809,
      "learning_rate": 6.781312500000001e-06,
      "loss": 3.6684,
      "step": 691500
    },
    {
      "epoch": 148.28473413379075,
      "grad_norm": 6.698637008666992,
      "learning_rate": 6.7750625e-06,
      "loss": 3.6567,
      "step": 691600
    },
    {
      "epoch": 148.30617495711834,
      "grad_norm": 6.887923240661621,
      "learning_rate": 6.7688125e-06,
      "loss": 3.68,
      "step": 691700
    },
    {
      "epoch": 148.32761578044597,
      "grad_norm": 6.082623481750488,
      "learning_rate": 6.7625625e-06,
      "loss": 3.6414,
      "step": 691800
    },
    {
      "epoch": 148.3490566037736,
      "grad_norm": 6.660476207733154,
      "learning_rate": 6.756312500000001e-06,
      "loss": 3.6453,
      "step": 691900
    },
    {
      "epoch": 148.3704974271012,
      "grad_norm": 7.424510955810547,
      "learning_rate": 6.7500625e-06,
      "loss": 3.7196,
      "step": 692000
    },
    {
      "epoch": 148.3919382504288,
      "grad_norm": 6.873418807983398,
      "learning_rate": 6.743812500000001e-06,
      "loss": 3.6593,
      "step": 692100
    },
    {
      "epoch": 148.41337907375643,
      "grad_norm": 6.435853958129883,
      "learning_rate": 6.7375625e-06,
      "loss": 3.665,
      "step": 692200
    },
    {
      "epoch": 148.43481989708405,
      "grad_norm": 6.871062278747559,
      "learning_rate": 6.7313125e-06,
      "loss": 3.6773,
      "step": 692300
    },
    {
      "epoch": 148.45626072041168,
      "grad_norm": 6.899366855621338,
      "learning_rate": 6.7250625e-06,
      "loss": 3.6761,
      "step": 692400
    },
    {
      "epoch": 148.47770154373927,
      "grad_norm": 6.69762659072876,
      "learning_rate": 6.718812500000001e-06,
      "loss": 3.642,
      "step": 692500
    },
    {
      "epoch": 148.4991423670669,
      "grad_norm": 6.304242134094238,
      "learning_rate": 6.7125625e-06,
      "loss": 3.6713,
      "step": 692600
    },
    {
      "epoch": 148.52058319039452,
      "grad_norm": 6.322568416595459,
      "learning_rate": 6.7063125000000005e-06,
      "loss": 3.6533,
      "step": 692700
    },
    {
      "epoch": 148.5420240137221,
      "grad_norm": 6.522363662719727,
      "learning_rate": 6.7000624999999995e-06,
      "loss": 3.6705,
      "step": 692800
    },
    {
      "epoch": 148.56346483704974,
      "grad_norm": 8.937786102294922,
      "learning_rate": 6.6938125e-06,
      "loss": 3.7007,
      "step": 692900
    },
    {
      "epoch": 148.58490566037736,
      "grad_norm": 6.108944416046143,
      "learning_rate": 6.6875625e-06,
      "loss": 3.7101,
      "step": 693000
    },
    {
      "epoch": 148.60634648370498,
      "grad_norm": 6.370140552520752,
      "learning_rate": 6.681312500000001e-06,
      "loss": 3.6618,
      "step": 693100
    },
    {
      "epoch": 148.62778730703258,
      "grad_norm": 5.428968906402588,
      "learning_rate": 6.6750625e-06,
      "loss": 3.6428,
      "step": 693200
    },
    {
      "epoch": 148.6492281303602,
      "grad_norm": 7.069627285003662,
      "learning_rate": 6.6688125000000005e-06,
      "loss": 3.708,
      "step": 693300
    },
    {
      "epoch": 148.67066895368782,
      "grad_norm": 6.4256391525268555,
      "learning_rate": 6.6625624999999995e-06,
      "loss": 3.6868,
      "step": 693400
    },
    {
      "epoch": 148.69210977701545,
      "grad_norm": 6.688368320465088,
      "learning_rate": 6.6563125e-06,
      "loss": 3.7108,
      "step": 693500
    },
    {
      "epoch": 148.71355060034304,
      "grad_norm": 6.882940769195557,
      "learning_rate": 6.6500625e-06,
      "loss": 3.6808,
      "step": 693600
    },
    {
      "epoch": 148.73499142367066,
      "grad_norm": 6.325181484222412,
      "learning_rate": 6.643812500000001e-06,
      "loss": 3.624,
      "step": 693700
    },
    {
      "epoch": 148.7564322469983,
      "grad_norm": 7.273296356201172,
      "learning_rate": 6.6375625e-06,
      "loss": 3.6791,
      "step": 693800
    },
    {
      "epoch": 148.7778730703259,
      "grad_norm": 6.438963890075684,
      "learning_rate": 6.6313125e-06,
      "loss": 3.6733,
      "step": 693900
    },
    {
      "epoch": 148.7993138936535,
      "grad_norm": 6.407835960388184,
      "learning_rate": 6.625062499999999e-06,
      "loss": 3.6788,
      "step": 694000
    },
    {
      "epoch": 148.82075471698113,
      "grad_norm": 6.51494836807251,
      "learning_rate": 6.6188125e-06,
      "loss": 3.6745,
      "step": 694100
    },
    {
      "epoch": 148.84219554030875,
      "grad_norm": 6.5000128746032715,
      "learning_rate": 6.6125625e-06,
      "loss": 3.6828,
      "step": 694200
    },
    {
      "epoch": 148.86363636363637,
      "grad_norm": 6.346214294433594,
      "learning_rate": 6.606312500000001e-06,
      "loss": 3.6821,
      "step": 694300
    },
    {
      "epoch": 148.88507718696397,
      "grad_norm": 6.849856853485107,
      "learning_rate": 6.6000625e-06,
      "loss": 3.6501,
      "step": 694400
    },
    {
      "epoch": 148.9065180102916,
      "grad_norm": 6.695188999176025,
      "learning_rate": 6.5938125e-06,
      "loss": 3.6547,
      "step": 694500
    },
    {
      "epoch": 148.92795883361921,
      "grad_norm": 7.269602298736572,
      "learning_rate": 6.587562499999999e-06,
      "loss": 3.6826,
      "step": 694600
    },
    {
      "epoch": 148.94939965694684,
      "grad_norm": 6.54005241394043,
      "learning_rate": 6.5813125e-06,
      "loss": 3.6608,
      "step": 694700
    },
    {
      "epoch": 148.97084048027443,
      "grad_norm": 6.456014633178711,
      "learning_rate": 6.5750625e-06,
      "loss": 3.7099,
      "step": 694800
    },
    {
      "epoch": 148.99228130360206,
      "grad_norm": 6.451436996459961,
      "learning_rate": 6.5688125000000006e-06,
      "loss": 3.6908,
      "step": 694900
    },
    {
      "epoch": 149.01372212692968,
      "grad_norm": 6.621446132659912,
      "learning_rate": 6.562562500000001e-06,
      "loss": 3.6266,
      "step": 695000
    },
    {
      "epoch": 149.0351629502573,
      "grad_norm": 6.921154499053955,
      "learning_rate": 6.5563125e-06,
      "loss": 3.6485,
      "step": 695100
    },
    {
      "epoch": 149.0566037735849,
      "grad_norm": 6.490777015686035,
      "learning_rate": 6.550062500000001e-06,
      "loss": 3.6875,
      "step": 695200
    },
    {
      "epoch": 149.07804459691252,
      "grad_norm": 6.386021614074707,
      "learning_rate": 6.5438125e-06,
      "loss": 3.6069,
      "step": 695300
    },
    {
      "epoch": 149.09948542024014,
      "grad_norm": 6.480653762817383,
      "learning_rate": 6.537562500000001e-06,
      "loss": 3.6459,
      "step": 695400
    },
    {
      "epoch": 149.12092624356777,
      "grad_norm": 6.839086055755615,
      "learning_rate": 6.5313125000000005e-06,
      "loss": 3.6282,
      "step": 695500
    },
    {
      "epoch": 149.14236706689536,
      "grad_norm": 6.694073677062988,
      "learning_rate": 6.525062500000001e-06,
      "loss": 3.6862,
      "step": 695600
    },
    {
      "epoch": 149.16380789022298,
      "grad_norm": 6.848136901855469,
      "learning_rate": 6.5188125e-06,
      "loss": 3.6572,
      "step": 695700
    },
    {
      "epoch": 149.1852487135506,
      "grad_norm": 7.452260971069336,
      "learning_rate": 6.512562500000001e-06,
      "loss": 3.6402,
      "step": 695800
    },
    {
      "epoch": 149.20668953687823,
      "grad_norm": 6.827702522277832,
      "learning_rate": 6.5063125e-06,
      "loss": 3.6595,
      "step": 695900
    },
    {
      "epoch": 149.22813036020582,
      "grad_norm": 7.974152088165283,
      "learning_rate": 6.500062500000001e-06,
      "loss": 3.6645,
      "step": 696000
    },
    {
      "epoch": 149.24957118353345,
      "grad_norm": 6.973063945770264,
      "learning_rate": 6.4938125000000004e-06,
      "loss": 3.635,
      "step": 696100
    },
    {
      "epoch": 149.27101200686107,
      "grad_norm": 6.530458927154541,
      "learning_rate": 6.487562500000001e-06,
      "loss": 3.752,
      "step": 696200
    },
    {
      "epoch": 149.29245283018867,
      "grad_norm": 6.446958065032959,
      "learning_rate": 6.4813125e-06,
      "loss": 3.6218,
      "step": 696300
    },
    {
      "epoch": 149.3138936535163,
      "grad_norm": 8.021924018859863,
      "learning_rate": 6.475062500000001e-06,
      "loss": 3.6887,
      "step": 696400
    },
    {
      "epoch": 149.3353344768439,
      "grad_norm": 7.676164150238037,
      "learning_rate": 6.4688125e-06,
      "loss": 3.6927,
      "step": 696500
    },
    {
      "epoch": 149.35677530017153,
      "grad_norm": 6.60526180267334,
      "learning_rate": 6.4625625000000005e-06,
      "loss": 3.6798,
      "step": 696600
    },
    {
      "epoch": 149.37821612349913,
      "grad_norm": 7.201622486114502,
      "learning_rate": 6.4563125e-06,
      "loss": 3.6995,
      "step": 696700
    },
    {
      "epoch": 149.39965694682675,
      "grad_norm": 7.133949279785156,
      "learning_rate": 6.450062500000001e-06,
      "loss": 3.6424,
      "step": 696800
    },
    {
      "epoch": 149.42109777015438,
      "grad_norm": 5.99581241607666,
      "learning_rate": 6.4438125e-06,
      "loss": 3.6254,
      "step": 696900
    },
    {
      "epoch": 149.442538593482,
      "grad_norm": 7.286593914031982,
      "learning_rate": 6.437562500000001e-06,
      "loss": 3.6505,
      "step": 697000
    },
    {
      "epoch": 149.4639794168096,
      "grad_norm": 5.854316711425781,
      "learning_rate": 6.431312500000001e-06,
      "loss": 3.6256,
      "step": 697100
    },
    {
      "epoch": 149.48542024013722,
      "grad_norm": 7.09418249130249,
      "learning_rate": 6.4250625000000005e-06,
      "loss": 3.6559,
      "step": 697200
    },
    {
      "epoch": 149.50686106346484,
      "grad_norm": 6.849321365356445,
      "learning_rate": 6.4188125e-06,
      "loss": 3.7076,
      "step": 697300
    },
    {
      "epoch": 149.52830188679246,
      "grad_norm": 7.253747463226318,
      "learning_rate": 6.412562500000001e-06,
      "loss": 3.6848,
      "step": 697400
    },
    {
      "epoch": 149.54974271012006,
      "grad_norm": 6.700285911560059,
      "learning_rate": 6.4063125e-06,
      "loss": 3.6895,
      "step": 697500
    },
    {
      "epoch": 149.57118353344768,
      "grad_norm": 7.12155294418335,
      "learning_rate": 6.400062500000001e-06,
      "loss": 3.6774,
      "step": 697600
    },
    {
      "epoch": 149.5926243567753,
      "grad_norm": 7.1329026222229,
      "learning_rate": 6.3938125000000005e-06,
      "loss": 3.6637,
      "step": 697700
    },
    {
      "epoch": 149.61406518010293,
      "grad_norm": 6.612460613250732,
      "learning_rate": 6.387562500000001e-06,
      "loss": 3.6698,
      "step": 697800
    },
    {
      "epoch": 149.63550600343052,
      "grad_norm": 6.745285511016846,
      "learning_rate": 6.3813125e-06,
      "loss": 3.6457,
      "step": 697900
    },
    {
      "epoch": 149.65694682675814,
      "grad_norm": 7.051354885101318,
      "learning_rate": 6.375062500000001e-06,
      "loss": 3.6318,
      "step": 698000
    },
    {
      "epoch": 149.67838765008577,
      "grad_norm": 6.847908973693848,
      "learning_rate": 6.3688125e-06,
      "loss": 3.6249,
      "step": 698100
    },
    {
      "epoch": 149.6998284734134,
      "grad_norm": 6.399984836578369,
      "learning_rate": 6.362562500000001e-06,
      "loss": 3.6566,
      "step": 698200
    },
    {
      "epoch": 149.72126929674099,
      "grad_norm": 6.408661365509033,
      "learning_rate": 6.3563125000000005e-06,
      "loss": 3.6626,
      "step": 698300
    },
    {
      "epoch": 149.7427101200686,
      "grad_norm": 6.528131008148193,
      "learning_rate": 6.350062500000001e-06,
      "loss": 3.6713,
      "step": 698400
    },
    {
      "epoch": 149.76415094339623,
      "grad_norm": 6.283811569213867,
      "learning_rate": 6.3438125e-06,
      "loss": 3.6581,
      "step": 698500
    },
    {
      "epoch": 149.78559176672385,
      "grad_norm": 6.105199813842773,
      "learning_rate": 6.337562500000001e-06,
      "loss": 3.6372,
      "step": 698600
    },
    {
      "epoch": 149.80703259005145,
      "grad_norm": 6.799201011657715,
      "learning_rate": 6.3313125e-06,
      "loss": 3.6426,
      "step": 698700
    },
    {
      "epoch": 149.82847341337907,
      "grad_norm": 7.398725509643555,
      "learning_rate": 6.3250625000000006e-06,
      "loss": 3.6304,
      "step": 698800
    },
    {
      "epoch": 149.8499142367067,
      "grad_norm": 5.926510334014893,
      "learning_rate": 6.3188125e-06,
      "loss": 3.6652,
      "step": 698900
    },
    {
      "epoch": 149.87135506003432,
      "grad_norm": 6.654865741729736,
      "learning_rate": 6.312562500000001e-06,
      "loss": 3.7356,
      "step": 699000
    },
    {
      "epoch": 149.8927958833619,
      "grad_norm": 7.728723049163818,
      "learning_rate": 6.3063125e-06,
      "loss": 3.6438,
      "step": 699100
    },
    {
      "epoch": 149.91423670668954,
      "grad_norm": 6.579214096069336,
      "learning_rate": 6.300062500000001e-06,
      "loss": 3.7161,
      "step": 699200
    },
    {
      "epoch": 149.93567753001716,
      "grad_norm": 7.210402488708496,
      "learning_rate": 6.2938125e-06,
      "loss": 3.6394,
      "step": 699300
    },
    {
      "epoch": 149.95711835334478,
      "grad_norm": 6.809680461883545,
      "learning_rate": 6.2875625000000005e-06,
      "loss": 3.6933,
      "step": 699400
    },
    {
      "epoch": 149.97855917667238,
      "grad_norm": 6.131692409515381,
      "learning_rate": 6.2813125e-06,
      "loss": 3.6361,
      "step": 699500
    },
    {
      "epoch": 150.0,
      "grad_norm": 9.361842155456543,
      "learning_rate": 6.275062500000001e-06,
      "loss": 3.6072,
      "step": 699600
    },
    {
      "epoch": 150.02144082332762,
      "grad_norm": 6.908791542053223,
      "learning_rate": 6.2688125e-06,
      "loss": 3.6622,
      "step": 699700
    },
    {
      "epoch": 150.04288164665522,
      "grad_norm": 6.721381187438965,
      "learning_rate": 6.262562500000001e-06,
      "loss": 3.6446,
      "step": 699800
    },
    {
      "epoch": 150.06432246998284,
      "grad_norm": 6.8635478019714355,
      "learning_rate": 6.2563125e-06,
      "loss": 3.6324,
      "step": 699900
    },
    {
      "epoch": 150.08576329331046,
      "grad_norm": 7.606487274169922,
      "learning_rate": 6.2500625000000004e-06,
      "loss": 3.6255,
      "step": 700000
    },
    {
      "epoch": 150.1072041166381,
      "grad_norm": 6.590043067932129,
      "learning_rate": 6.2438125e-06,
      "loss": 3.6217,
      "step": 700100
    },
    {
      "epoch": 150.12864493996568,
      "grad_norm": 6.759413242340088,
      "learning_rate": 6.2375625e-06,
      "loss": 3.6296,
      "step": 700200
    },
    {
      "epoch": 150.1500857632933,
      "grad_norm": 6.998116970062256,
      "learning_rate": 6.2313125e-06,
      "loss": 3.6314,
      "step": 700300
    },
    {
      "epoch": 150.17152658662093,
      "grad_norm": 6.9190497398376465,
      "learning_rate": 6.2250625e-06,
      "loss": 3.6351,
      "step": 700400
    },
    {
      "epoch": 150.19296740994855,
      "grad_norm": 6.694782733917236,
      "learning_rate": 6.2188125e-06,
      "loss": 3.6255,
      "step": 700500
    },
    {
      "epoch": 150.21440823327615,
      "grad_norm": 6.294936656951904,
      "learning_rate": 6.2125625e-06,
      "loss": 3.5979,
      "step": 700600
    },
    {
      "epoch": 150.23584905660377,
      "grad_norm": 6.791430473327637,
      "learning_rate": 6.2063125e-06,
      "loss": 3.6412,
      "step": 700700
    },
    {
      "epoch": 150.2572898799314,
      "grad_norm": 6.855217933654785,
      "learning_rate": 6.2000625e-06,
      "loss": 3.6272,
      "step": 700800
    },
    {
      "epoch": 150.27873070325901,
      "grad_norm": 7.022468090057373,
      "learning_rate": 6.1938125e-06,
      "loss": 3.6553,
      "step": 700900
    },
    {
      "epoch": 150.3001715265866,
      "grad_norm": 6.7577619552612305,
      "learning_rate": 6.1875625e-06,
      "loss": 3.6471,
      "step": 701000
    },
    {
      "epoch": 150.32161234991423,
      "grad_norm": 6.560097694396973,
      "learning_rate": 6.1813125e-06,
      "loss": 3.6008,
      "step": 701100
    },
    {
      "epoch": 150.34305317324186,
      "grad_norm": 6.479852676391602,
      "learning_rate": 6.1750625e-06,
      "loss": 3.5824,
      "step": 701200
    },
    {
      "epoch": 150.36449399656948,
      "grad_norm": 6.624212265014648,
      "learning_rate": 6.168812500000001e-06,
      "loss": 3.6505,
      "step": 701300
    },
    {
      "epoch": 150.38593481989707,
      "grad_norm": 6.826616287231445,
      "learning_rate": 6.162562500000001e-06,
      "loss": 3.6766,
      "step": 701400
    },
    {
      "epoch": 150.4073756432247,
      "grad_norm": 6.690390110015869,
      "learning_rate": 6.156312500000001e-06,
      "loss": 3.6979,
      "step": 701500
    },
    {
      "epoch": 150.42881646655232,
      "grad_norm": 6.946303367614746,
      "learning_rate": 6.1500625000000005e-06,
      "loss": 3.7438,
      "step": 701600
    },
    {
      "epoch": 150.45025728987994,
      "grad_norm": 6.487625598907471,
      "learning_rate": 6.1438125e-06,
      "loss": 3.643,
      "step": 701700
    },
    {
      "epoch": 150.47169811320754,
      "grad_norm": 6.805403232574463,
      "learning_rate": 6.1375625e-06,
      "loss": 3.6449,
      "step": 701800
    },
    {
      "epoch": 150.49313893653516,
      "grad_norm": 6.98800802230835,
      "learning_rate": 6.131312500000001e-06,
      "loss": 3.6828,
      "step": 701900
    },
    {
      "epoch": 150.51457975986278,
      "grad_norm": 6.894982814788818,
      "learning_rate": 6.125062500000001e-06,
      "loss": 3.6381,
      "step": 702000
    },
    {
      "epoch": 150.5360205831904,
      "grad_norm": 6.941125869750977,
      "learning_rate": 6.118812500000001e-06,
      "loss": 3.6777,
      "step": 702100
    },
    {
      "epoch": 150.557461406518,
      "grad_norm": 6.481497764587402,
      "learning_rate": 6.1125625000000005e-06,
      "loss": 3.6003,
      "step": 702200
    },
    {
      "epoch": 150.57890222984562,
      "grad_norm": 6.544829368591309,
      "learning_rate": 6.1063125e-06,
      "loss": 3.6743,
      "step": 702300
    },
    {
      "epoch": 150.60034305317325,
      "grad_norm": 6.144868850708008,
      "learning_rate": 6.1000625e-06,
      "loss": 3.7128,
      "step": 702400
    },
    {
      "epoch": 150.62178387650087,
      "grad_norm": 6.608357906341553,
      "learning_rate": 6.093812500000001e-06,
      "loss": 3.6059,
      "step": 702500
    },
    {
      "epoch": 150.64322469982847,
      "grad_norm": 6.643477439880371,
      "learning_rate": 6.087562500000001e-06,
      "loss": 3.6661,
      "step": 702600
    },
    {
      "epoch": 150.6646655231561,
      "grad_norm": 6.702949047088623,
      "learning_rate": 6.0813125000000006e-06,
      "loss": 3.6628,
      "step": 702700
    },
    {
      "epoch": 150.6861063464837,
      "grad_norm": 7.447056770324707,
      "learning_rate": 6.0750625e-06,
      "loss": 3.6542,
      "step": 702800
    },
    {
      "epoch": 150.70754716981133,
      "grad_norm": 6.479084014892578,
      "learning_rate": 6.0688125e-06,
      "loss": 3.6498,
      "step": 702900
    },
    {
      "epoch": 150.72898799313893,
      "grad_norm": 6.4685959815979,
      "learning_rate": 6.0625625e-06,
      "loss": 3.6207,
      "step": 703000
    },
    {
      "epoch": 150.75042881646655,
      "grad_norm": 7.111886024475098,
      "learning_rate": 6.056312500000001e-06,
      "loss": 3.6422,
      "step": 703100
    },
    {
      "epoch": 150.77186963979418,
      "grad_norm": 6.437946319580078,
      "learning_rate": 6.050062500000001e-06,
      "loss": 3.6002,
      "step": 703200
    },
    {
      "epoch": 150.79331046312177,
      "grad_norm": 6.887701511383057,
      "learning_rate": 6.0438125000000005e-06,
      "loss": 3.6387,
      "step": 703300
    },
    {
      "epoch": 150.8147512864494,
      "grad_norm": 6.494271755218506,
      "learning_rate": 6.0375625e-06,
      "loss": 3.6634,
      "step": 703400
    },
    {
      "epoch": 150.83619210977702,
      "grad_norm": 7.1462626457214355,
      "learning_rate": 6.0313125e-06,
      "loss": 3.6716,
      "step": 703500
    },
    {
      "epoch": 150.85763293310464,
      "grad_norm": 6.362194061279297,
      "learning_rate": 6.025062500000001e-06,
      "loss": 3.6929,
      "step": 703600
    },
    {
      "epoch": 150.87907375643223,
      "grad_norm": 6.9307451248168945,
      "learning_rate": 6.018812500000001e-06,
      "loss": 3.6908,
      "step": 703700
    },
    {
      "epoch": 150.90051457975986,
      "grad_norm": 6.587596416473389,
      "learning_rate": 6.012562500000001e-06,
      "loss": 3.6544,
      "step": 703800
    },
    {
      "epoch": 150.92195540308748,
      "grad_norm": 6.882296562194824,
      "learning_rate": 6.0063125000000004e-06,
      "loss": 3.6968,
      "step": 703900
    },
    {
      "epoch": 150.9433962264151,
      "grad_norm": 6.499125957489014,
      "learning_rate": 6.0000625e-06,
      "loss": 3.5998,
      "step": 704000
    },
    {
      "epoch": 150.9648370497427,
      "grad_norm": 6.691563606262207,
      "learning_rate": 5.9938125e-06,
      "loss": 3.6378,
      "step": 704100
    },
    {
      "epoch": 150.98627787307032,
      "grad_norm": 6.095218658447266,
      "learning_rate": 5.987562500000001e-06,
      "loss": 3.6933,
      "step": 704200
    },
    {
      "epoch": 151.00771869639794,
      "grad_norm": 7.145400524139404,
      "learning_rate": 5.981312500000001e-06,
      "loss": 3.6096,
      "step": 704300
    },
    {
      "epoch": 151.02915951972557,
      "grad_norm": 6.99401330947876,
      "learning_rate": 5.9750625000000005e-06,
      "loss": 3.6352,
      "step": 704400
    },
    {
      "epoch": 151.05060034305316,
      "grad_norm": 7.332025051116943,
      "learning_rate": 5.9688125e-06,
      "loss": 3.6444,
      "step": 704500
    },
    {
      "epoch": 151.07204116638079,
      "grad_norm": 7.22443151473999,
      "learning_rate": 5.9625625e-06,
      "loss": 3.6145,
      "step": 704600
    },
    {
      "epoch": 151.0934819897084,
      "grad_norm": 6.775599956512451,
      "learning_rate": 5.9563125e-06,
      "loss": 3.6125,
      "step": 704700
    },
    {
      "epoch": 151.11492281303603,
      "grad_norm": 6.864396572113037,
      "learning_rate": 5.950062500000001e-06,
      "loss": 3.6586,
      "step": 704800
    },
    {
      "epoch": 151.13636363636363,
      "grad_norm": 6.584860801696777,
      "learning_rate": 5.943812500000001e-06,
      "loss": 3.6533,
      "step": 704900
    },
    {
      "epoch": 151.15780445969125,
      "grad_norm": 7.810472011566162,
      "learning_rate": 5.9375625000000005e-06,
      "loss": 3.6354,
      "step": 705000
    },
    {
      "epoch": 151.17924528301887,
      "grad_norm": 6.819258689880371,
      "learning_rate": 5.9313125e-06,
      "loss": 3.5954,
      "step": 705100
    },
    {
      "epoch": 151.2006861063465,
      "grad_norm": 6.632688522338867,
      "learning_rate": 5.9250625e-06,
      "loss": 3.6368,
      "step": 705200
    },
    {
      "epoch": 151.2221269296741,
      "grad_norm": 6.834741592407227,
      "learning_rate": 5.9188125e-06,
      "loss": 3.6314,
      "step": 705300
    },
    {
      "epoch": 151.2435677530017,
      "grad_norm": 6.535689353942871,
      "learning_rate": 5.912562500000001e-06,
      "loss": 3.7028,
      "step": 705400
    },
    {
      "epoch": 151.26500857632934,
      "grad_norm": 7.558220386505127,
      "learning_rate": 5.9063125000000006e-06,
      "loss": 3.6698,
      "step": 705500
    },
    {
      "epoch": 151.28644939965696,
      "grad_norm": 7.773881912231445,
      "learning_rate": 5.9000625e-06,
      "loss": 3.6629,
      "step": 705600
    },
    {
      "epoch": 151.30789022298455,
      "grad_norm": 6.250365734100342,
      "learning_rate": 5.8938125e-06,
      "loss": 3.6174,
      "step": 705700
    },
    {
      "epoch": 151.32933104631218,
      "grad_norm": 7.127073764801025,
      "learning_rate": 5.8875625e-06,
      "loss": 3.6218,
      "step": 705800
    },
    {
      "epoch": 151.3507718696398,
      "grad_norm": 6.903252601623535,
      "learning_rate": 5.8813125e-06,
      "loss": 3.6555,
      "step": 705900
    },
    {
      "epoch": 151.37221269296742,
      "grad_norm": 7.526974201202393,
      "learning_rate": 5.875062500000001e-06,
      "loss": 3.6484,
      "step": 706000
    },
    {
      "epoch": 151.39365351629502,
      "grad_norm": 6.883859634399414,
      "learning_rate": 5.8688125000000005e-06,
      "loss": 3.633,
      "step": 706100
    },
    {
      "epoch": 151.41509433962264,
      "grad_norm": 7.050106048583984,
      "learning_rate": 5.8625625e-06,
      "loss": 3.6221,
      "step": 706200
    },
    {
      "epoch": 151.43653516295026,
      "grad_norm": 7.064256191253662,
      "learning_rate": 5.8563125e-06,
      "loss": 3.6878,
      "step": 706300
    },
    {
      "epoch": 151.4579759862779,
      "grad_norm": 6.810272216796875,
      "learning_rate": 5.8500625e-06,
      "loss": 3.6887,
      "step": 706400
    },
    {
      "epoch": 151.47941680960548,
      "grad_norm": 6.813118934631348,
      "learning_rate": 5.843812500000001e-06,
      "loss": 3.648,
      "step": 706500
    },
    {
      "epoch": 151.5008576329331,
      "grad_norm": 6.8879241943359375,
      "learning_rate": 5.8375625000000006e-06,
      "loss": 3.6846,
      "step": 706600
    },
    {
      "epoch": 151.52229845626073,
      "grad_norm": 6.692802429199219,
      "learning_rate": 5.8313125e-06,
      "loss": 3.5964,
      "step": 706700
    },
    {
      "epoch": 151.54373927958832,
      "grad_norm": 6.4158759117126465,
      "learning_rate": 5.8250625e-06,
      "loss": 3.6182,
      "step": 706800
    },
    {
      "epoch": 151.56518010291595,
      "grad_norm": 6.429737091064453,
      "learning_rate": 5.8188125e-06,
      "loss": 3.6242,
      "step": 706900
    },
    {
      "epoch": 151.58662092624357,
      "grad_norm": 6.360893726348877,
      "learning_rate": 5.8125625e-06,
      "loss": 3.6489,
      "step": 707000
    },
    {
      "epoch": 151.6080617495712,
      "grad_norm": 6.360195636749268,
      "learning_rate": 5.806312500000001e-06,
      "loss": 3.6722,
      "step": 707100
    },
    {
      "epoch": 151.6295025728988,
      "grad_norm": 7.1040873527526855,
      "learning_rate": 5.8000625000000005e-06,
      "loss": 3.5815,
      "step": 707200
    },
    {
      "epoch": 151.6509433962264,
      "grad_norm": 6.658957481384277,
      "learning_rate": 5.7938125e-06,
      "loss": 3.6365,
      "step": 707300
    },
    {
      "epoch": 151.67238421955403,
      "grad_norm": 6.99950647354126,
      "learning_rate": 5.7875625e-06,
      "loss": 3.6593,
      "step": 707400
    },
    {
      "epoch": 151.69382504288166,
      "grad_norm": 7.2477192878723145,
      "learning_rate": 5.7813125e-06,
      "loss": 3.6369,
      "step": 707500
    },
    {
      "epoch": 151.71526586620925,
      "grad_norm": 6.153797149658203,
      "learning_rate": 5.7750625e-06,
      "loss": 3.6515,
      "step": 707600
    },
    {
      "epoch": 151.73670668953687,
      "grad_norm": 7.053964614868164,
      "learning_rate": 5.768812500000001e-06,
      "loss": 3.5818,
      "step": 707700
    },
    {
      "epoch": 151.7581475128645,
      "grad_norm": 6.673648357391357,
      "learning_rate": 5.7625625000000004e-06,
      "loss": 3.6833,
      "step": 707800
    },
    {
      "epoch": 151.77958833619212,
      "grad_norm": 6.641956329345703,
      "learning_rate": 5.7563125e-06,
      "loss": 3.5682,
      "step": 707900
    },
    {
      "epoch": 151.80102915951971,
      "grad_norm": 6.960512161254883,
      "learning_rate": 5.7500625e-06,
      "loss": 3.6404,
      "step": 708000
    },
    {
      "epoch": 151.82246998284734,
      "grad_norm": 7.414548873901367,
      "learning_rate": 5.7438125e-06,
      "loss": 3.6813,
      "step": 708100
    },
    {
      "epoch": 151.84391080617496,
      "grad_norm": 7.612649917602539,
      "learning_rate": 5.7375625e-06,
      "loss": 3.5656,
      "step": 708200
    },
    {
      "epoch": 151.86535162950258,
      "grad_norm": 6.12654972076416,
      "learning_rate": 5.7313125000000005e-06,
      "loss": 3.6357,
      "step": 708300
    },
    {
      "epoch": 151.88679245283018,
      "grad_norm": 6.952517509460449,
      "learning_rate": 5.7250625e-06,
      "loss": 3.59,
      "step": 708400
    },
    {
      "epoch": 151.9082332761578,
      "grad_norm": 7.107546806335449,
      "learning_rate": 5.7188125e-06,
      "loss": 3.6798,
      "step": 708500
    },
    {
      "epoch": 151.92967409948542,
      "grad_norm": 6.676445960998535,
      "learning_rate": 5.7125625e-06,
      "loss": 3.6184,
      "step": 708600
    },
    {
      "epoch": 151.95111492281305,
      "grad_norm": 6.989302158355713,
      "learning_rate": 5.7063125e-06,
      "loss": 3.6223,
      "step": 708700
    },
    {
      "epoch": 151.97255574614064,
      "grad_norm": 7.340760707855225,
      "learning_rate": 5.7000625e-06,
      "loss": 3.7029,
      "step": 708800
    },
    {
      "epoch": 151.99399656946827,
      "grad_norm": 6.427983283996582,
      "learning_rate": 5.6938125000000005e-06,
      "loss": 3.6405,
      "step": 708900
    },
    {
      "epoch": 152.0154373927959,
      "grad_norm": 6.757624626159668,
      "learning_rate": 5.6875625e-06,
      "loss": 3.6576,
      "step": 709000
    },
    {
      "epoch": 152.0368782161235,
      "grad_norm": 6.79271936416626,
      "learning_rate": 5.6813125e-06,
      "loss": 3.6076,
      "step": 709100
    },
    {
      "epoch": 152.0583190394511,
      "grad_norm": 6.502804756164551,
      "learning_rate": 5.6750625e-06,
      "loss": 3.6387,
      "step": 709200
    },
    {
      "epoch": 152.07975986277873,
      "grad_norm": 6.257351398468018,
      "learning_rate": 5.6688125e-06,
      "loss": 3.6046,
      "step": 709300
    },
    {
      "epoch": 152.10120068610635,
      "grad_norm": 7.086249351501465,
      "learning_rate": 5.6625625000000006e-06,
      "loss": 3.6827,
      "step": 709400
    },
    {
      "epoch": 152.12264150943398,
      "grad_norm": 7.027621746063232,
      "learning_rate": 5.6563125e-06,
      "loss": 3.6207,
      "step": 709500
    },
    {
      "epoch": 152.14408233276157,
      "grad_norm": 7.720064640045166,
      "learning_rate": 5.6500625e-06,
      "loss": 3.6251,
      "step": 709600
    },
    {
      "epoch": 152.1655231560892,
      "grad_norm": 6.7573676109313965,
      "learning_rate": 5.6438125e-06,
      "loss": 3.6013,
      "step": 709700
    },
    {
      "epoch": 152.18696397941682,
      "grad_norm": 6.798660755157471,
      "learning_rate": 5.6375625e-06,
      "loss": 3.616,
      "step": 709800
    },
    {
      "epoch": 152.2084048027444,
      "grad_norm": 7.163356781005859,
      "learning_rate": 5.6313125e-06,
      "loss": 3.6042,
      "step": 709900
    },
    {
      "epoch": 152.22984562607203,
      "grad_norm": 6.6917524337768555,
      "learning_rate": 5.6250625000000005e-06,
      "loss": 3.6853,
      "step": 710000
    },
    {
      "epoch": 152.25128644939966,
      "grad_norm": 7.156410217285156,
      "learning_rate": 5.6188125e-06,
      "loss": 3.6285,
      "step": 710100
    },
    {
      "epoch": 152.27272727272728,
      "grad_norm": 7.177452087402344,
      "learning_rate": 5.6125625e-06,
      "loss": 3.6004,
      "step": 710200
    },
    {
      "epoch": 152.29416809605488,
      "grad_norm": 7.020306587219238,
      "learning_rate": 5.6063125e-06,
      "loss": 3.6855,
      "step": 710300
    },
    {
      "epoch": 152.3156089193825,
      "grad_norm": 6.910113334655762,
      "learning_rate": 5.6000625e-06,
      "loss": 3.6229,
      "step": 710400
    },
    {
      "epoch": 152.33704974271012,
      "grad_norm": 6.807791233062744,
      "learning_rate": 5.5938125e-06,
      "loss": 3.6444,
      "step": 710500
    },
    {
      "epoch": 152.35849056603774,
      "grad_norm": 6.662363052368164,
      "learning_rate": 5.5875625000000004e-06,
      "loss": 3.6473,
      "step": 710600
    },
    {
      "epoch": 152.37993138936534,
      "grad_norm": 6.440183639526367,
      "learning_rate": 5.5813125e-06,
      "loss": 3.6138,
      "step": 710700
    },
    {
      "epoch": 152.40137221269296,
      "grad_norm": 6.928486347198486,
      "learning_rate": 5.5750625e-06,
      "loss": 3.599,
      "step": 710800
    },
    {
      "epoch": 152.42281303602059,
      "grad_norm": 7.068746089935303,
      "learning_rate": 5.5688125e-06,
      "loss": 3.7062,
      "step": 710900
    },
    {
      "epoch": 152.4442538593482,
      "grad_norm": 7.454530715942383,
      "learning_rate": 5.5625625e-06,
      "loss": 3.6072,
      "step": 711000
    },
    {
      "epoch": 152.4656946826758,
      "grad_norm": 6.030477046966553,
      "learning_rate": 5.5563125e-06,
      "loss": 3.5819,
      "step": 711100
    },
    {
      "epoch": 152.48713550600343,
      "grad_norm": 6.119403839111328,
      "learning_rate": 5.5500625e-06,
      "loss": 3.5974,
      "step": 711200
    },
    {
      "epoch": 152.50857632933105,
      "grad_norm": 7.132974624633789,
      "learning_rate": 5.5438125e-06,
      "loss": 3.6879,
      "step": 711300
    },
    {
      "epoch": 152.53001715265867,
      "grad_norm": 6.986180305480957,
      "learning_rate": 5.5375625e-06,
      "loss": 3.6543,
      "step": 711400
    },
    {
      "epoch": 152.55145797598627,
      "grad_norm": 6.827364444732666,
      "learning_rate": 5.5313125e-06,
      "loss": 3.6034,
      "step": 711500
    },
    {
      "epoch": 152.5728987993139,
      "grad_norm": 6.427201271057129,
      "learning_rate": 5.5250625e-06,
      "loss": 3.6209,
      "step": 711600
    },
    {
      "epoch": 152.5943396226415,
      "grad_norm": 7.392493724822998,
      "learning_rate": 5.5188125e-06,
      "loss": 3.6406,
      "step": 711700
    },
    {
      "epoch": 152.61578044596914,
      "grad_norm": 6.716546535491943,
      "learning_rate": 5.5125625e-06,
      "loss": 3.6706,
      "step": 711800
    },
    {
      "epoch": 152.63722126929673,
      "grad_norm": 7.05855655670166,
      "learning_rate": 5.5063125e-06,
      "loss": 3.6285,
      "step": 711900
    },
    {
      "epoch": 152.65866209262435,
      "grad_norm": 6.829906940460205,
      "learning_rate": 5.5000625e-06,
      "loss": 3.6147,
      "step": 712000
    },
    {
      "epoch": 152.68010291595198,
      "grad_norm": 7.009786605834961,
      "learning_rate": 5.4938125e-06,
      "loss": 3.586,
      "step": 712100
    },
    {
      "epoch": 152.7015437392796,
      "grad_norm": 6.4450812339782715,
      "learning_rate": 5.4875625e-06,
      "loss": 3.5943,
      "step": 712200
    },
    {
      "epoch": 152.7229845626072,
      "grad_norm": 7.259013652801514,
      "learning_rate": 5.4813125e-06,
      "loss": 3.6385,
      "step": 712300
    },
    {
      "epoch": 152.74442538593482,
      "grad_norm": 6.554782867431641,
      "learning_rate": 5.4750625e-06,
      "loss": 3.6779,
      "step": 712400
    },
    {
      "epoch": 152.76586620926244,
      "grad_norm": 7.9132161140441895,
      "learning_rate": 5.4688125e-06,
      "loss": 3.6365,
      "step": 712500
    },
    {
      "epoch": 152.78730703259006,
      "grad_norm": 6.092172622680664,
      "learning_rate": 5.4625625e-06,
      "loss": 3.5825,
      "step": 712600
    },
    {
      "epoch": 152.80874785591766,
      "grad_norm": 6.54718017578125,
      "learning_rate": 5.4563125e-06,
      "loss": 3.5699,
      "step": 712700
    },
    {
      "epoch": 152.83018867924528,
      "grad_norm": 6.621304988861084,
      "learning_rate": 5.4500625e-06,
      "loss": 3.6522,
      "step": 712800
    },
    {
      "epoch": 152.8516295025729,
      "grad_norm": 6.923540115356445,
      "learning_rate": 5.4438125e-06,
      "loss": 3.638,
      "step": 712900
    },
    {
      "epoch": 152.87307032590053,
      "grad_norm": 6.709779262542725,
      "learning_rate": 5.4375625e-06,
      "loss": 3.6243,
      "step": 713000
    },
    {
      "epoch": 152.89451114922812,
      "grad_norm": 6.472630023956299,
      "learning_rate": 5.4313125e-06,
      "loss": 3.6493,
      "step": 713100
    },
    {
      "epoch": 152.91595197255575,
      "grad_norm": 6.856067657470703,
      "learning_rate": 5.4250625e-06,
      "loss": 3.6646,
      "step": 713200
    },
    {
      "epoch": 152.93739279588337,
      "grad_norm": 7.054863929748535,
      "learning_rate": 5.4188125e-06,
      "loss": 3.6244,
      "step": 713300
    },
    {
      "epoch": 152.95883361921096,
      "grad_norm": 7.204014301300049,
      "learning_rate": 5.4125624999999996e-06,
      "loss": 3.6061,
      "step": 713400
    },
    {
      "epoch": 152.9802744425386,
      "grad_norm": 6.655376434326172,
      "learning_rate": 5.4063125e-06,
      "loss": 3.5869,
      "step": 713500
    },
    {
      "epoch": 153.0017152658662,
      "grad_norm": 7.110823631286621,
      "learning_rate": 5.4000625e-06,
      "loss": 3.627,
      "step": 713600
    },
    {
      "epoch": 153.02315608919383,
      "grad_norm": 6.839664936065674,
      "learning_rate": 5.393812500000001e-06,
      "loss": 3.6476,
      "step": 713700
    },
    {
      "epoch": 153.04459691252143,
      "grad_norm": 7.141910076141357,
      "learning_rate": 5.387562500000001e-06,
      "loss": 3.6017,
      "step": 713800
    },
    {
      "epoch": 153.06603773584905,
      "grad_norm": 7.578152179718018,
      "learning_rate": 5.3813125000000005e-06,
      "loss": 3.6792,
      "step": 713900
    },
    {
      "epoch": 153.08747855917667,
      "grad_norm": 7.315171718597412,
      "learning_rate": 5.3750625e-06,
      "loss": 3.649,
      "step": 714000
    },
    {
      "epoch": 153.1089193825043,
      "grad_norm": 7.390954494476318,
      "learning_rate": 5.3688125e-06,
      "loss": 3.6065,
      "step": 714100
    },
    {
      "epoch": 153.1303602058319,
      "grad_norm": 6.862209796905518,
      "learning_rate": 5.362562500000001e-06,
      "loss": 3.5566,
      "step": 714200
    },
    {
      "epoch": 153.15180102915951,
      "grad_norm": 6.870270252227783,
      "learning_rate": 5.356312500000001e-06,
      "loss": 3.6143,
      "step": 714300
    },
    {
      "epoch": 153.17324185248714,
      "grad_norm": 6.95469331741333,
      "learning_rate": 5.350062500000001e-06,
      "loss": 3.6554,
      "step": 714400
    },
    {
      "epoch": 153.19468267581476,
      "grad_norm": 6.057016849517822,
      "learning_rate": 5.3438125000000004e-06,
      "loss": 3.6929,
      "step": 714500
    },
    {
      "epoch": 153.21612349914236,
      "grad_norm": 5.812347412109375,
      "learning_rate": 5.3375625e-06,
      "loss": 3.6043,
      "step": 714600
    },
    {
      "epoch": 153.23756432246998,
      "grad_norm": 6.783260345458984,
      "learning_rate": 5.3313125e-06,
      "loss": 3.6131,
      "step": 714700
    },
    {
      "epoch": 153.2590051457976,
      "grad_norm": 7.185405254364014,
      "learning_rate": 5.325062500000001e-06,
      "loss": 3.6423,
      "step": 714800
    },
    {
      "epoch": 153.28044596912522,
      "grad_norm": 7.5803446769714355,
      "learning_rate": 5.318812500000001e-06,
      "loss": 3.5854,
      "step": 714900
    },
    {
      "epoch": 153.30188679245282,
      "grad_norm": 7.020986080169678,
      "learning_rate": 5.3125625000000005e-06,
      "loss": 3.5846,
      "step": 715000
    },
    {
      "epoch": 153.32332761578044,
      "grad_norm": 7.0351948738098145,
      "learning_rate": 5.3063125e-06,
      "loss": 3.6371,
      "step": 715100
    },
    {
      "epoch": 153.34476843910807,
      "grad_norm": 7.362411022186279,
      "learning_rate": 5.3000625e-06,
      "loss": 3.6276,
      "step": 715200
    },
    {
      "epoch": 153.3662092624357,
      "grad_norm": 6.694530963897705,
      "learning_rate": 5.293812500000001e-06,
      "loss": 3.6217,
      "step": 715300
    },
    {
      "epoch": 153.38765008576328,
      "grad_norm": 7.000057697296143,
      "learning_rate": 5.287562500000001e-06,
      "loss": 3.5622,
      "step": 715400
    },
    {
      "epoch": 153.4090909090909,
      "grad_norm": 6.667577266693115,
      "learning_rate": 5.281312500000001e-06,
      "loss": 3.5643,
      "step": 715500
    },
    {
      "epoch": 153.43053173241853,
      "grad_norm": 6.739740371704102,
      "learning_rate": 5.2750625000000005e-06,
      "loss": 3.6416,
      "step": 715600
    },
    {
      "epoch": 153.45197255574615,
      "grad_norm": 7.502660274505615,
      "learning_rate": 5.2688125e-06,
      "loss": 3.6372,
      "step": 715700
    },
    {
      "epoch": 153.47341337907375,
      "grad_norm": 6.606334686279297,
      "learning_rate": 5.2625625e-06,
      "loss": 3.6121,
      "step": 715800
    },
    {
      "epoch": 153.49485420240137,
      "grad_norm": 7.039868354797363,
      "learning_rate": 5.256312500000001e-06,
      "loss": 3.6105,
      "step": 715900
    },
    {
      "epoch": 153.516295025729,
      "grad_norm": 6.622159957885742,
      "learning_rate": 5.250062500000001e-06,
      "loss": 3.625,
      "step": 716000
    },
    {
      "epoch": 153.53773584905662,
      "grad_norm": 6.953969478607178,
      "learning_rate": 5.2438125000000005e-06,
      "loss": 3.5954,
      "step": 716100
    },
    {
      "epoch": 153.5591766723842,
      "grad_norm": 6.549685955047607,
      "learning_rate": 5.2375625e-06,
      "loss": 3.6638,
      "step": 716200
    },
    {
      "epoch": 153.58061749571183,
      "grad_norm": 7.589366436004639,
      "learning_rate": 5.2313125e-06,
      "loss": 3.6335,
      "step": 716300
    },
    {
      "epoch": 153.60205831903946,
      "grad_norm": 7.030323028564453,
      "learning_rate": 5.2250625e-06,
      "loss": 3.6027,
      "step": 716400
    },
    {
      "epoch": 153.62349914236708,
      "grad_norm": 7.027409553527832,
      "learning_rate": 5.218812500000001e-06,
      "loss": 3.6286,
      "step": 716500
    },
    {
      "epoch": 153.64493996569468,
      "grad_norm": 6.326718330383301,
      "learning_rate": 5.212562500000001e-06,
      "loss": 3.6406,
      "step": 716600
    },
    {
      "epoch": 153.6663807890223,
      "grad_norm": 6.731290340423584,
      "learning_rate": 5.2063125000000005e-06,
      "loss": 3.6912,
      "step": 716700
    },
    {
      "epoch": 153.68782161234992,
      "grad_norm": 6.440482139587402,
      "learning_rate": 5.2000625e-06,
      "loss": 3.6286,
      "step": 716800
    },
    {
      "epoch": 153.70926243567752,
      "grad_norm": 6.6237616539001465,
      "learning_rate": 5.1938125e-06,
      "loss": 3.6002,
      "step": 716900
    },
    {
      "epoch": 153.73070325900514,
      "grad_norm": 6.964976787567139,
      "learning_rate": 5.1875625e-06,
      "loss": 3.6126,
      "step": 717000
    },
    {
      "epoch": 153.75214408233276,
      "grad_norm": 6.7635908126831055,
      "learning_rate": 5.181312500000001e-06,
      "loss": 3.6524,
      "step": 717100
    },
    {
      "epoch": 153.77358490566039,
      "grad_norm": 6.499034881591797,
      "learning_rate": 5.1750625000000006e-06,
      "loss": 3.6081,
      "step": 717200
    },
    {
      "epoch": 153.79502572898798,
      "grad_norm": 6.772740364074707,
      "learning_rate": 5.1688125e-06,
      "loss": 3.6333,
      "step": 717300
    },
    {
      "epoch": 153.8164665523156,
      "grad_norm": 7.701849937438965,
      "learning_rate": 5.1625625e-06,
      "loss": 3.6406,
      "step": 717400
    },
    {
      "epoch": 153.83790737564323,
      "grad_norm": 6.104419231414795,
      "learning_rate": 5.1563125e-06,
      "loss": 3.5873,
      "step": 717500
    },
    {
      "epoch": 153.85934819897085,
      "grad_norm": 7.3081207275390625,
      "learning_rate": 5.1500625e-06,
      "loss": 3.5886,
      "step": 717600
    },
    {
      "epoch": 153.88078902229844,
      "grad_norm": 6.60187292098999,
      "learning_rate": 5.143812500000001e-06,
      "loss": 3.6584,
      "step": 717700
    },
    {
      "epoch": 153.90222984562607,
      "grad_norm": 7.017565727233887,
      "learning_rate": 5.1375625000000005e-06,
      "loss": 3.6285,
      "step": 717800
    },
    {
      "epoch": 153.9236706689537,
      "grad_norm": 6.975649833679199,
      "learning_rate": 5.1313125e-06,
      "loss": 3.5766,
      "step": 717900
    },
    {
      "epoch": 153.9451114922813,
      "grad_norm": 6.730263710021973,
      "learning_rate": 5.1250625e-06,
      "loss": 3.5873,
      "step": 718000
    },
    {
      "epoch": 153.9665523156089,
      "grad_norm": 6.840561866760254,
      "learning_rate": 5.1188125e-06,
      "loss": 3.6194,
      "step": 718100
    },
    {
      "epoch": 153.98799313893653,
      "grad_norm": 7.033782482147217,
      "learning_rate": 5.112562500000001e-06,
      "loss": 3.6069,
      "step": 718200
    },
    {
      "epoch": 154.00943396226415,
      "grad_norm": 7.140526294708252,
      "learning_rate": 5.106312500000001e-06,
      "loss": 3.5309,
      "step": 718300
    },
    {
      "epoch": 154.03087478559178,
      "grad_norm": 6.350671768188477,
      "learning_rate": 5.1000625000000004e-06,
      "loss": 3.602,
      "step": 718400
    },
    {
      "epoch": 154.05231560891937,
      "grad_norm": 6.936803817749023,
      "learning_rate": 5.0938125e-06,
      "loss": 3.609,
      "step": 718500
    },
    {
      "epoch": 154.073756432247,
      "grad_norm": 7.141836643218994,
      "learning_rate": 5.0875625e-06,
      "loss": 3.5911,
      "step": 718600
    },
    {
      "epoch": 154.09519725557462,
      "grad_norm": 6.986064910888672,
      "learning_rate": 5.0813125e-06,
      "loss": 3.56,
      "step": 718700
    },
    {
      "epoch": 154.11663807890224,
      "grad_norm": 7.1814398765563965,
      "learning_rate": 5.075062500000001e-06,
      "loss": 3.615,
      "step": 718800
    },
    {
      "epoch": 154.13807890222984,
      "grad_norm": 6.3916239738464355,
      "learning_rate": 5.0688125000000005e-06,
      "loss": 3.6209,
      "step": 718900
    },
    {
      "epoch": 154.15951972555746,
      "grad_norm": 7.224372863769531,
      "learning_rate": 5.0625625e-06,
      "loss": 3.6064,
      "step": 719000
    },
    {
      "epoch": 154.18096054888508,
      "grad_norm": 7.665399551391602,
      "learning_rate": 5.0563125e-06,
      "loss": 3.6104,
      "step": 719100
    },
    {
      "epoch": 154.2024013722127,
      "grad_norm": 7.798466205596924,
      "learning_rate": 5.0500625e-06,
      "loss": 3.6317,
      "step": 719200
    },
    {
      "epoch": 154.2238421955403,
      "grad_norm": 7.105984687805176,
      "learning_rate": 5.0438125e-06,
      "loss": 3.652,
      "step": 719300
    },
    {
      "epoch": 154.24528301886792,
      "grad_norm": 6.778387069702148,
      "learning_rate": 5.037562500000001e-06,
      "loss": 3.6143,
      "step": 719400
    },
    {
      "epoch": 154.26672384219555,
      "grad_norm": 6.8467936515808105,
      "learning_rate": 5.0313125000000005e-06,
      "loss": 3.5929,
      "step": 719500
    },
    {
      "epoch": 154.28816466552317,
      "grad_norm": 7.068326473236084,
      "learning_rate": 5.0250625e-06,
      "loss": 3.5735,
      "step": 719600
    },
    {
      "epoch": 154.30960548885076,
      "grad_norm": 6.933059215545654,
      "learning_rate": 5.0188125e-06,
      "loss": 3.5926,
      "step": 719700
    },
    {
      "epoch": 154.3310463121784,
      "grad_norm": 6.48450231552124,
      "learning_rate": 5.0125625e-06,
      "loss": 3.5902,
      "step": 719800
    },
    {
      "epoch": 154.352487135506,
      "grad_norm": 6.240548610687256,
      "learning_rate": 5.0063125e-06,
      "loss": 3.5757,
      "step": 719900
    },
    {
      "epoch": 154.37392795883363,
      "grad_norm": 6.886697769165039,
      "learning_rate": 5.0000625000000005e-06,
      "loss": 3.6384,
      "step": 720000
    },
    {
      "epoch": 154.39536878216123,
      "grad_norm": 6.806443214416504,
      "learning_rate": 4.9938125e-06,
      "loss": 3.516,
      "step": 720100
    },
    {
      "epoch": 154.41680960548885,
      "grad_norm": 7.328761577606201,
      "learning_rate": 4.9875625e-06,
      "loss": 3.6631,
      "step": 720200
    },
    {
      "epoch": 154.43825042881647,
      "grad_norm": 6.455292224884033,
      "learning_rate": 4.9813125e-06,
      "loss": 3.6109,
      "step": 720300
    },
    {
      "epoch": 154.45969125214407,
      "grad_norm": 7.10159158706665,
      "learning_rate": 4.9750625e-06,
      "loss": 3.6153,
      "step": 720400
    },
    {
      "epoch": 154.4811320754717,
      "grad_norm": 7.13785982131958,
      "learning_rate": 4.9688125e-06,
      "loss": 3.654,
      "step": 720500
    },
    {
      "epoch": 154.50257289879931,
      "grad_norm": 6.854796886444092,
      "learning_rate": 4.9625625000000005e-06,
      "loss": 3.5854,
      "step": 720600
    },
    {
      "epoch": 154.52401372212694,
      "grad_norm": 6.877481460571289,
      "learning_rate": 4.9563125e-06,
      "loss": 3.6488,
      "step": 720700
    },
    {
      "epoch": 154.54545454545453,
      "grad_norm": 7.851528644561768,
      "learning_rate": 4.9500625e-06,
      "loss": 3.6151,
      "step": 720800
    },
    {
      "epoch": 154.56689536878216,
      "grad_norm": 6.412365913391113,
      "learning_rate": 4.9438125e-06,
      "loss": 3.6374,
      "step": 720900
    },
    {
      "epoch": 154.58833619210978,
      "grad_norm": 6.732183456420898,
      "learning_rate": 4.9375625e-06,
      "loss": 3.6348,
      "step": 721000
    },
    {
      "epoch": 154.6097770154374,
      "grad_norm": 6.601748466491699,
      "learning_rate": 4.9313125000000006e-06,
      "loss": 3.6276,
      "step": 721100
    },
    {
      "epoch": 154.631217838765,
      "grad_norm": 6.793120861053467,
      "learning_rate": 4.9250625e-06,
      "loss": 3.5929,
      "step": 721200
    },
    {
      "epoch": 154.65265866209262,
      "grad_norm": 7.317414283752441,
      "learning_rate": 4.9188125e-06,
      "loss": 3.6499,
      "step": 721300
    },
    {
      "epoch": 154.67409948542024,
      "grad_norm": 6.546323299407959,
      "learning_rate": 4.9125625e-06,
      "loss": 3.6401,
      "step": 721400
    },
    {
      "epoch": 154.69554030874787,
      "grad_norm": 7.240065097808838,
      "learning_rate": 4.9063125e-06,
      "loss": 3.5667,
      "step": 721500
    },
    {
      "epoch": 154.71698113207546,
      "grad_norm": 6.592573165893555,
      "learning_rate": 4.9000625e-06,
      "loss": 3.5912,
      "step": 721600
    },
    {
      "epoch": 154.73842195540308,
      "grad_norm": 6.711824893951416,
      "learning_rate": 4.8938125000000005e-06,
      "loss": 3.6111,
      "step": 721700
    },
    {
      "epoch": 154.7598627787307,
      "grad_norm": 9.31395149230957,
      "learning_rate": 4.8875625e-06,
      "loss": 3.5459,
      "step": 721800
    },
    {
      "epoch": 154.78130360205833,
      "grad_norm": 6.490617752075195,
      "learning_rate": 4.8813125e-06,
      "loss": 3.5932,
      "step": 721900
    },
    {
      "epoch": 154.80274442538592,
      "grad_norm": 7.761518478393555,
      "learning_rate": 4.8750625e-06,
      "loss": 3.6197,
      "step": 722000
    },
    {
      "epoch": 154.82418524871355,
      "grad_norm": 7.0817952156066895,
      "learning_rate": 4.8688125e-06,
      "loss": 3.6441,
      "step": 722100
    },
    {
      "epoch": 154.84562607204117,
      "grad_norm": 7.21391487121582,
      "learning_rate": 4.8625625e-06,
      "loss": 3.6475,
      "step": 722200
    },
    {
      "epoch": 154.8670668953688,
      "grad_norm": 7.524967193603516,
      "learning_rate": 4.8563125000000004e-06,
      "loss": 3.6135,
      "step": 722300
    },
    {
      "epoch": 154.8885077186964,
      "grad_norm": 7.276371002197266,
      "learning_rate": 4.8500625e-06,
      "loss": 3.6355,
      "step": 722400
    },
    {
      "epoch": 154.909948542024,
      "grad_norm": 6.494939804077148,
      "learning_rate": 4.8438125e-06,
      "loss": 3.6607,
      "step": 722500
    },
    {
      "epoch": 154.93138936535163,
      "grad_norm": 6.998982906341553,
      "learning_rate": 4.8375625e-06,
      "loss": 3.628,
      "step": 722600
    },
    {
      "epoch": 154.95283018867926,
      "grad_norm": 6.658237934112549,
      "learning_rate": 4.8313125e-06,
      "loss": 3.6115,
      "step": 722700
    },
    {
      "epoch": 154.97427101200685,
      "grad_norm": 6.989109992980957,
      "learning_rate": 4.8250625e-06,
      "loss": 3.5925,
      "step": 722800
    },
    {
      "epoch": 154.99571183533448,
      "grad_norm": 6.641017436981201,
      "learning_rate": 4.8188125e-06,
      "loss": 3.5995,
      "step": 722900
    },
    {
      "epoch": 155.0171526586621,
      "grad_norm": 6.928835391998291,
      "learning_rate": 4.8125625e-06,
      "loss": 3.5563,
      "step": 723000
    },
    {
      "epoch": 155.03859348198972,
      "grad_norm": 6.850916385650635,
      "learning_rate": 4.8063125e-06,
      "loss": 3.518,
      "step": 723100
    },
    {
      "epoch": 155.06003430531732,
      "grad_norm": 6.6347527503967285,
      "learning_rate": 4.8000625e-06,
      "loss": 3.5956,
      "step": 723200
    },
    {
      "epoch": 155.08147512864494,
      "grad_norm": 7.232348442077637,
      "learning_rate": 4.7938125e-06,
      "loss": 3.6518,
      "step": 723300
    },
    {
      "epoch": 155.10291595197256,
      "grad_norm": 7.064138412475586,
      "learning_rate": 4.7875625e-06,
      "loss": 3.6239,
      "step": 723400
    },
    {
      "epoch": 155.12435677530019,
      "grad_norm": 6.892950534820557,
      "learning_rate": 4.7813125e-06,
      "loss": 3.5903,
      "step": 723500
    },
    {
      "epoch": 155.14579759862778,
      "grad_norm": 7.351735591888428,
      "learning_rate": 4.7750625e-06,
      "loss": 3.5891,
      "step": 723600
    },
    {
      "epoch": 155.1672384219554,
      "grad_norm": 6.77475643157959,
      "learning_rate": 4.7688125e-06,
      "loss": 3.5816,
      "step": 723700
    },
    {
      "epoch": 155.18867924528303,
      "grad_norm": 6.527944087982178,
      "learning_rate": 4.7625625e-06,
      "loss": 3.597,
      "step": 723800
    },
    {
      "epoch": 155.21012006861062,
      "grad_norm": 6.571155548095703,
      "learning_rate": 4.7563125e-06,
      "loss": 3.5176,
      "step": 723900
    },
    {
      "epoch": 155.23156089193824,
      "grad_norm": 6.497104644775391,
      "learning_rate": 4.7500625e-06,
      "loss": 3.6074,
      "step": 724000
    },
    {
      "epoch": 155.25300171526587,
      "grad_norm": 6.268789768218994,
      "learning_rate": 4.7438125e-06,
      "loss": 3.5737,
      "step": 724100
    },
    {
      "epoch": 155.2744425385935,
      "grad_norm": 6.9509687423706055,
      "learning_rate": 4.7375625e-06,
      "loss": 3.6385,
      "step": 724200
    },
    {
      "epoch": 155.29588336192108,
      "grad_norm": 6.709533214569092,
      "learning_rate": 4.7313125e-06,
      "loss": 3.6044,
      "step": 724300
    },
    {
      "epoch": 155.3173241852487,
      "grad_norm": 7.112617492675781,
      "learning_rate": 4.7250625e-06,
      "loss": 3.6344,
      "step": 724400
    },
    {
      "epoch": 155.33876500857633,
      "grad_norm": 6.853000640869141,
      "learning_rate": 4.7188125e-06,
      "loss": 3.5749,
      "step": 724500
    },
    {
      "epoch": 155.36020583190395,
      "grad_norm": 7.585710048675537,
      "learning_rate": 4.7125625e-06,
      "loss": 3.5555,
      "step": 724600
    },
    {
      "epoch": 155.38164665523155,
      "grad_norm": 6.747515678405762,
      "learning_rate": 4.7063125e-06,
      "loss": 3.5862,
      "step": 724700
    },
    {
      "epoch": 155.40308747855917,
      "grad_norm": 7.227668762207031,
      "learning_rate": 4.7000625e-06,
      "loss": 3.5662,
      "step": 724800
    },
    {
      "epoch": 155.4245283018868,
      "grad_norm": 6.332818031311035,
      "learning_rate": 4.6938125e-06,
      "loss": 3.6258,
      "step": 724900
    },
    {
      "epoch": 155.44596912521442,
      "grad_norm": 6.941192150115967,
      "learning_rate": 4.6875625e-06,
      "loss": 3.6757,
      "step": 725000
    },
    {
      "epoch": 155.467409948542,
      "grad_norm": 6.622646331787109,
      "learning_rate": 4.6813124999999996e-06,
      "loss": 3.5811,
      "step": 725100
    },
    {
      "epoch": 155.48885077186964,
      "grad_norm": 6.55561637878418,
      "learning_rate": 4.6750625e-06,
      "loss": 3.5596,
      "step": 725200
    },
    {
      "epoch": 155.51029159519726,
      "grad_norm": 7.055426120758057,
      "learning_rate": 4.6688125e-06,
      "loss": 3.5841,
      "step": 725300
    },
    {
      "epoch": 155.53173241852488,
      "grad_norm": 6.601551055908203,
      "learning_rate": 4.6625625e-06,
      "loss": 3.6242,
      "step": 725400
    },
    {
      "epoch": 155.55317324185248,
      "grad_norm": 7.36794900894165,
      "learning_rate": 4.6563125e-06,
      "loss": 3.612,
      "step": 725500
    },
    {
      "epoch": 155.5746140651801,
      "grad_norm": 6.141005039215088,
      "learning_rate": 4.6500625e-06,
      "loss": 3.6037,
      "step": 725600
    },
    {
      "epoch": 155.59605488850772,
      "grad_norm": 6.768596172332764,
      "learning_rate": 4.6438124999999995e-06,
      "loss": 3.6041,
      "step": 725700
    },
    {
      "epoch": 155.61749571183535,
      "grad_norm": 7.194897174835205,
      "learning_rate": 4.6375625e-06,
      "loss": 3.6108,
      "step": 725800
    },
    {
      "epoch": 155.63893653516294,
      "grad_norm": 7.224976539611816,
      "learning_rate": 4.6313125e-06,
      "loss": 3.5969,
      "step": 725900
    },
    {
      "epoch": 155.66037735849056,
      "grad_norm": 7.035651206970215,
      "learning_rate": 4.6250625e-06,
      "loss": 3.6228,
      "step": 726000
    },
    {
      "epoch": 155.6818181818182,
      "grad_norm": 6.80369758605957,
      "learning_rate": 4.6188125e-06,
      "loss": 3.5782,
      "step": 726100
    },
    {
      "epoch": 155.7032590051458,
      "grad_norm": 7.6165385246276855,
      "learning_rate": 4.6125625000000004e-06,
      "loss": 3.6436,
      "step": 726200
    },
    {
      "epoch": 155.7246998284734,
      "grad_norm": 7.139092445373535,
      "learning_rate": 4.6063125e-06,
      "loss": 3.6059,
      "step": 726300
    },
    {
      "epoch": 155.74614065180103,
      "grad_norm": 7.2555251121521,
      "learning_rate": 4.6000625e-06,
      "loss": 3.6027,
      "step": 726400
    },
    {
      "epoch": 155.76758147512865,
      "grad_norm": 6.852422714233398,
      "learning_rate": 4.593812500000001e-06,
      "loss": 3.627,
      "step": 726500
    },
    {
      "epoch": 155.78902229845627,
      "grad_norm": 6.859622955322266,
      "learning_rate": 4.587562500000001e-06,
      "loss": 3.607,
      "step": 726600
    },
    {
      "epoch": 155.81046312178387,
      "grad_norm": 6.752528190612793,
      "learning_rate": 4.5813125000000005e-06,
      "loss": 3.6166,
      "step": 726700
    },
    {
      "epoch": 155.8319039451115,
      "grad_norm": 7.356106281280518,
      "learning_rate": 4.5750625e-06,
      "loss": 3.5909,
      "step": 726800
    },
    {
      "epoch": 155.85334476843911,
      "grad_norm": 7.020792484283447,
      "learning_rate": 4.5688125e-06,
      "loss": 3.6426,
      "step": 726900
    },
    {
      "epoch": 155.87478559176674,
      "grad_norm": 6.222052574157715,
      "learning_rate": 4.562562500000001e-06,
      "loss": 3.6381,
      "step": 727000
    },
    {
      "epoch": 155.89622641509433,
      "grad_norm": 7.482675075531006,
      "learning_rate": 4.556312500000001e-06,
      "loss": 3.6211,
      "step": 727100
    },
    {
      "epoch": 155.91766723842196,
      "grad_norm": 7.845301628112793,
      "learning_rate": 4.550062500000001e-06,
      "loss": 3.646,
      "step": 727200
    },
    {
      "epoch": 155.93910806174958,
      "grad_norm": 6.547406196594238,
      "learning_rate": 4.5438125000000005e-06,
      "loss": 3.5905,
      "step": 727300
    },
    {
      "epoch": 155.96054888507717,
      "grad_norm": 6.623323917388916,
      "learning_rate": 4.5375625e-06,
      "loss": 3.6671,
      "step": 727400
    },
    {
      "epoch": 155.9819897084048,
      "grad_norm": 7.2385663986206055,
      "learning_rate": 4.5313125e-06,
      "loss": 3.5737,
      "step": 727500
    },
    {
      "epoch": 156.00343053173242,
      "grad_norm": 6.719806671142578,
      "learning_rate": 4.525062500000001e-06,
      "loss": 3.5953,
      "step": 727600
    },
    {
      "epoch": 156.02487135506004,
      "grad_norm": 6.593441486358643,
      "learning_rate": 4.518812500000001e-06,
      "loss": 3.597,
      "step": 727700
    },
    {
      "epoch": 156.04631217838764,
      "grad_norm": 7.189013481140137,
      "learning_rate": 4.5125625000000006e-06,
      "loss": 3.5464,
      "step": 727800
    },
    {
      "epoch": 156.06775300171526,
      "grad_norm": 6.645116329193115,
      "learning_rate": 4.5063125e-06,
      "loss": 3.5766,
      "step": 727900
    },
    {
      "epoch": 156.08919382504288,
      "grad_norm": 7.04658317565918,
      "learning_rate": 4.5000625e-06,
      "loss": 3.6184,
      "step": 728000
    },
    {
      "epoch": 156.1106346483705,
      "grad_norm": 7.398184776306152,
      "learning_rate": 4.4938125e-06,
      "loss": 3.5756,
      "step": 728100
    },
    {
      "epoch": 156.1320754716981,
      "grad_norm": 7.212347507476807,
      "learning_rate": 4.487562500000001e-06,
      "loss": 3.5478,
      "step": 728200
    },
    {
      "epoch": 156.15351629502572,
      "grad_norm": 7.462826728820801,
      "learning_rate": 4.481312500000001e-06,
      "loss": 3.5906,
      "step": 728300
    },
    {
      "epoch": 156.17495711835335,
      "grad_norm": 7.473923206329346,
      "learning_rate": 4.4750625000000005e-06,
      "loss": 3.5769,
      "step": 728400
    },
    {
      "epoch": 156.19639794168097,
      "grad_norm": 6.374352931976318,
      "learning_rate": 4.4688125e-06,
      "loss": 3.5253,
      "step": 728500
    },
    {
      "epoch": 156.21783876500857,
      "grad_norm": 6.764925479888916,
      "learning_rate": 4.4625625e-06,
      "loss": 3.6419,
      "step": 728600
    },
    {
      "epoch": 156.2392795883362,
      "grad_norm": 7.475151062011719,
      "learning_rate": 4.4563125e-06,
      "loss": 3.6028,
      "step": 728700
    },
    {
      "epoch": 156.2607204116638,
      "grad_norm": 7.025265693664551,
      "learning_rate": 4.450062500000001e-06,
      "loss": 3.6453,
      "step": 728800
    },
    {
      "epoch": 156.28216123499143,
      "grad_norm": 7.336990833282471,
      "learning_rate": 4.443812500000001e-06,
      "loss": 3.6129,
      "step": 728900
    },
    {
      "epoch": 156.30360205831903,
      "grad_norm": 6.427384853363037,
      "learning_rate": 4.4375625000000004e-06,
      "loss": 3.5661,
      "step": 729000
    },
    {
      "epoch": 156.32504288164665,
      "grad_norm": 6.645752429962158,
      "learning_rate": 4.4313125e-06,
      "loss": 3.6029,
      "step": 729100
    },
    {
      "epoch": 156.34648370497428,
      "grad_norm": 6.736866474151611,
      "learning_rate": 4.4250625e-06,
      "loss": 3.5879,
      "step": 729200
    },
    {
      "epoch": 156.3679245283019,
      "grad_norm": 7.186858654022217,
      "learning_rate": 4.4188125e-06,
      "loss": 3.6413,
      "step": 729300
    },
    {
      "epoch": 156.3893653516295,
      "grad_norm": 6.886019229888916,
      "learning_rate": 4.412562500000001e-06,
      "loss": 3.6536,
      "step": 729400
    },
    {
      "epoch": 156.41080617495712,
      "grad_norm": 7.5066986083984375,
      "learning_rate": 4.4063125000000005e-06,
      "loss": 3.5814,
      "step": 729500
    },
    {
      "epoch": 156.43224699828474,
      "grad_norm": 6.859293460845947,
      "learning_rate": 4.4000625e-06,
      "loss": 3.5864,
      "step": 729600
    },
    {
      "epoch": 156.45368782161236,
      "grad_norm": 7.122676372528076,
      "learning_rate": 4.3938125e-06,
      "loss": 3.6433,
      "step": 729700
    },
    {
      "epoch": 156.47512864493996,
      "grad_norm": 7.18043851852417,
      "learning_rate": 4.3875625e-06,
      "loss": 3.564,
      "step": 729800
    },
    {
      "epoch": 156.49656946826758,
      "grad_norm": 7.130781650543213,
      "learning_rate": 4.381312500000001e-06,
      "loss": 3.5962,
      "step": 729900
    },
    {
      "epoch": 156.5180102915952,
      "grad_norm": 6.831628322601318,
      "learning_rate": 4.375062500000001e-06,
      "loss": 3.6623,
      "step": 730000
    },
    {
      "epoch": 156.53945111492283,
      "grad_norm": 7.475581645965576,
      "learning_rate": 4.3688125000000004e-06,
      "loss": 3.6347,
      "step": 730100
    },
    {
      "epoch": 156.56089193825042,
      "grad_norm": 7.339512348175049,
      "learning_rate": 4.3625625e-06,
      "loss": 3.5927,
      "step": 730200
    },
    {
      "epoch": 156.58233276157804,
      "grad_norm": 7.443888187408447,
      "learning_rate": 4.3563125e-06,
      "loss": 3.5892,
      "step": 730300
    },
    {
      "epoch": 156.60377358490567,
      "grad_norm": 5.999162197113037,
      "learning_rate": 4.3500625e-06,
      "loss": 3.5228,
      "step": 730400
    },
    {
      "epoch": 156.62521440823326,
      "grad_norm": 6.932048797607422,
      "learning_rate": 4.343812500000001e-06,
      "loss": 3.571,
      "step": 730500
    },
    {
      "epoch": 156.64665523156089,
      "grad_norm": 7.1405181884765625,
      "learning_rate": 4.3375625000000005e-06,
      "loss": 3.5653,
      "step": 730600
    },
    {
      "epoch": 156.6680960548885,
      "grad_norm": 8.038538932800293,
      "learning_rate": 4.3313125e-06,
      "loss": 3.6396,
      "step": 730700
    },
    {
      "epoch": 156.68953687821613,
      "grad_norm": 7.193622589111328,
      "learning_rate": 4.3250625e-06,
      "loss": 3.573,
      "step": 730800
    },
    {
      "epoch": 156.71097770154373,
      "grad_norm": 6.926695823669434,
      "learning_rate": 4.3188125e-06,
      "loss": 3.5659,
      "step": 730900
    },
    {
      "epoch": 156.73241852487135,
      "grad_norm": 6.5015387535095215,
      "learning_rate": 4.3125625e-06,
      "loss": 3.6128,
      "step": 731000
    },
    {
      "epoch": 156.75385934819897,
      "grad_norm": 6.674424648284912,
      "learning_rate": 4.306312500000001e-06,
      "loss": 3.5717,
      "step": 731100
    },
    {
      "epoch": 156.7753001715266,
      "grad_norm": 7.316368579864502,
      "learning_rate": 4.3000625000000005e-06,
      "loss": 3.5763,
      "step": 731200
    },
    {
      "epoch": 156.7967409948542,
      "grad_norm": 6.721755504608154,
      "learning_rate": 4.2938125e-06,
      "loss": 3.5529,
      "step": 731300
    },
    {
      "epoch": 156.8181818181818,
      "grad_norm": 7.0067524909973145,
      "learning_rate": 4.2875625e-06,
      "loss": 3.5882,
      "step": 731400
    },
    {
      "epoch": 156.83962264150944,
      "grad_norm": 6.407566547393799,
      "learning_rate": 4.2813125e-06,
      "loss": 3.5806,
      "step": 731500
    },
    {
      "epoch": 156.86106346483706,
      "grad_norm": 7.538599014282227,
      "learning_rate": 4.2750625e-06,
      "loss": 3.5964,
      "step": 731600
    },
    {
      "epoch": 156.88250428816465,
      "grad_norm": 7.003509044647217,
      "learning_rate": 4.2688125000000006e-06,
      "loss": 3.6545,
      "step": 731700
    },
    {
      "epoch": 156.90394511149228,
      "grad_norm": 7.141369819641113,
      "learning_rate": 4.2625625e-06,
      "loss": 3.6455,
      "step": 731800
    },
    {
      "epoch": 156.9253859348199,
      "grad_norm": 7.506216049194336,
      "learning_rate": 4.2563125e-06,
      "loss": 3.6342,
      "step": 731900
    },
    {
      "epoch": 156.94682675814752,
      "grad_norm": 6.554092884063721,
      "learning_rate": 4.2500625e-06,
      "loss": 3.5707,
      "step": 732000
    },
    {
      "epoch": 156.96826758147512,
      "grad_norm": 7.745528221130371,
      "learning_rate": 4.2438125e-06,
      "loss": 3.6109,
      "step": 732100
    },
    {
      "epoch": 156.98970840480274,
      "grad_norm": 6.07474422454834,
      "learning_rate": 4.2375625e-06,
      "loss": 3.588,
      "step": 732200
    },
    {
      "epoch": 157.01114922813036,
      "grad_norm": 6.680838584899902,
      "learning_rate": 4.2313125000000005e-06,
      "loss": 3.5718,
      "step": 732300
    },
    {
      "epoch": 157.032590051458,
      "grad_norm": 6.77605676651001,
      "learning_rate": 4.2250625e-06,
      "loss": 3.6762,
      "step": 732400
    },
    {
      "epoch": 157.05403087478558,
      "grad_norm": 7.474244117736816,
      "learning_rate": 4.2188125e-06,
      "loss": 3.6035,
      "step": 732500
    },
    {
      "epoch": 157.0754716981132,
      "grad_norm": 6.735761642456055,
      "learning_rate": 4.2125625e-06,
      "loss": 3.5892,
      "step": 732600
    },
    {
      "epoch": 157.09691252144083,
      "grad_norm": 7.116811275482178,
      "learning_rate": 4.2063125e-06,
      "loss": 3.578,
      "step": 732700
    },
    {
      "epoch": 157.11835334476845,
      "grad_norm": 7.2005438804626465,
      "learning_rate": 4.200062500000001e-06,
      "loss": 3.5914,
      "step": 732800
    },
    {
      "epoch": 157.13979416809605,
      "grad_norm": 6.901341915130615,
      "learning_rate": 4.1938125000000004e-06,
      "loss": 3.6133,
      "step": 732900
    },
    {
      "epoch": 157.16123499142367,
      "grad_norm": 7.228114604949951,
      "learning_rate": 4.1875625e-06,
      "loss": 3.583,
      "step": 733000
    },
    {
      "epoch": 157.1826758147513,
      "grad_norm": 6.997003555297852,
      "learning_rate": 4.1813125e-06,
      "loss": 3.5816,
      "step": 733100
    },
    {
      "epoch": 157.20411663807892,
      "grad_norm": 7.259235858917236,
      "learning_rate": 4.1750625e-06,
      "loss": 3.558,
      "step": 733200
    },
    {
      "epoch": 157.2255574614065,
      "grad_norm": 7.301018238067627,
      "learning_rate": 4.1688125e-06,
      "loss": 3.5575,
      "step": 733300
    },
    {
      "epoch": 157.24699828473413,
      "grad_norm": 6.480300426483154,
      "learning_rate": 4.1625625000000005e-06,
      "loss": 3.5853,
      "step": 733400
    },
    {
      "epoch": 157.26843910806176,
      "grad_norm": 8.011465072631836,
      "learning_rate": 4.1563125e-06,
      "loss": 3.5359,
      "step": 733500
    },
    {
      "epoch": 157.28987993138938,
      "grad_norm": 7.333527088165283,
      "learning_rate": 4.1500625e-06,
      "loss": 3.5785,
      "step": 733600
    },
    {
      "epoch": 157.31132075471697,
      "grad_norm": 6.829409599304199,
      "learning_rate": 4.1438125e-06,
      "loss": 3.5491,
      "step": 733700
    },
    {
      "epoch": 157.3327615780446,
      "grad_norm": 7.387899398803711,
      "learning_rate": 4.1375625e-06,
      "loss": 3.585,
      "step": 733800
    },
    {
      "epoch": 157.35420240137222,
      "grad_norm": 6.555727005004883,
      "learning_rate": 4.1313125e-06,
      "loss": 3.5722,
      "step": 733900
    },
    {
      "epoch": 157.37564322469981,
      "grad_norm": 7.0757293701171875,
      "learning_rate": 4.1250625000000005e-06,
      "loss": 3.6082,
      "step": 734000
    },
    {
      "epoch": 157.39708404802744,
      "grad_norm": 7.2555251121521,
      "learning_rate": 4.1188125e-06,
      "loss": 3.5856,
      "step": 734100
    },
    {
      "epoch": 157.41852487135506,
      "grad_norm": 6.853917121887207,
      "learning_rate": 4.1125625e-06,
      "loss": 3.5997,
      "step": 734200
    },
    {
      "epoch": 157.43996569468268,
      "grad_norm": 7.505124092102051,
      "learning_rate": 4.1063125e-06,
      "loss": 3.5883,
      "step": 734300
    },
    {
      "epoch": 157.46140651801028,
      "grad_norm": 7.092290878295898,
      "learning_rate": 4.1000625e-06,
      "loss": 3.6004,
      "step": 734400
    },
    {
      "epoch": 157.4828473413379,
      "grad_norm": 8.489636421203613,
      "learning_rate": 4.0938125e-06,
      "loss": 3.5433,
      "step": 734500
    },
    {
      "epoch": 157.50428816466552,
      "grad_norm": 7.250746726989746,
      "learning_rate": 4.0875625e-06,
      "loss": 3.5586,
      "step": 734600
    },
    {
      "epoch": 157.52572898799315,
      "grad_norm": 6.402650356292725,
      "learning_rate": 4.0813125e-06,
      "loss": 3.6721,
      "step": 734700
    },
    {
      "epoch": 157.54716981132074,
      "grad_norm": 6.8805694580078125,
      "learning_rate": 4.0750625e-06,
      "loss": 3.56,
      "step": 734800
    },
    {
      "epoch": 157.56861063464837,
      "grad_norm": 7.2173357009887695,
      "learning_rate": 4.0688125e-06,
      "loss": 3.5847,
      "step": 734900
    },
    {
      "epoch": 157.590051457976,
      "grad_norm": 6.7949018478393555,
      "learning_rate": 4.0625625e-06,
      "loss": 3.6352,
      "step": 735000
    },
    {
      "epoch": 157.6114922813036,
      "grad_norm": 7.072589874267578,
      "learning_rate": 4.0563125e-06,
      "loss": 3.6032,
      "step": 735100
    },
    {
      "epoch": 157.6329331046312,
      "grad_norm": 7.47831392288208,
      "learning_rate": 4.0500625e-06,
      "loss": 3.6202,
      "step": 735200
    },
    {
      "epoch": 157.65437392795883,
      "grad_norm": 7.467019081115723,
      "learning_rate": 4.0438125e-06,
      "loss": 3.5786,
      "step": 735300
    },
    {
      "epoch": 157.67581475128645,
      "grad_norm": 6.615072727203369,
      "learning_rate": 4.0375625e-06,
      "loss": 3.557,
      "step": 735400
    },
    {
      "epoch": 157.69725557461408,
      "grad_norm": 7.181562423706055,
      "learning_rate": 4.0313125e-06,
      "loss": 3.545,
      "step": 735500
    },
    {
      "epoch": 157.71869639794167,
      "grad_norm": 6.021951675415039,
      "learning_rate": 4.0250625e-06,
      "loss": 3.5853,
      "step": 735600
    },
    {
      "epoch": 157.7401372212693,
      "grad_norm": 7.082596302032471,
      "learning_rate": 4.0188125e-06,
      "loss": 3.5987,
      "step": 735700
    },
    {
      "epoch": 157.76157804459692,
      "grad_norm": 7.19313383102417,
      "learning_rate": 4.0125625e-06,
      "loss": 3.5363,
      "step": 735800
    },
    {
      "epoch": 157.78301886792454,
      "grad_norm": 7.29428768157959,
      "learning_rate": 4.0063125e-06,
      "loss": 3.5932,
      "step": 735900
    },
    {
      "epoch": 157.80445969125213,
      "grad_norm": 7.743588924407959,
      "learning_rate": 4.0000625e-06,
      "loss": 3.6096,
      "step": 736000
    },
    {
      "epoch": 157.82590051457976,
      "grad_norm": 6.583525657653809,
      "learning_rate": 3.9938125e-06,
      "loss": 3.5683,
      "step": 736100
    },
    {
      "epoch": 157.84734133790738,
      "grad_norm": 6.939101219177246,
      "learning_rate": 3.9875625e-06,
      "loss": 3.5638,
      "step": 736200
    },
    {
      "epoch": 157.868782161235,
      "grad_norm": 7.953454494476318,
      "learning_rate": 3.9813125e-06,
      "loss": 3.6056,
      "step": 736300
    },
    {
      "epoch": 157.8902229845626,
      "grad_norm": 7.176607608795166,
      "learning_rate": 3.9750625e-06,
      "loss": 3.6015,
      "step": 736400
    },
    {
      "epoch": 157.91166380789022,
      "grad_norm": 6.620175838470459,
      "learning_rate": 3.9688125e-06,
      "loss": 3.5911,
      "step": 736500
    },
    {
      "epoch": 157.93310463121784,
      "grad_norm": 6.6696672439575195,
      "learning_rate": 3.9625625e-06,
      "loss": 3.6211,
      "step": 736600
    },
    {
      "epoch": 157.95454545454547,
      "grad_norm": 8.443490982055664,
      "learning_rate": 3.9563125e-06,
      "loss": 3.639,
      "step": 736700
    },
    {
      "epoch": 157.97598627787306,
      "grad_norm": 6.943744659423828,
      "learning_rate": 3.9500625e-06,
      "loss": 3.5609,
      "step": 736800
    },
    {
      "epoch": 157.99742710120069,
      "grad_norm": 6.945112705230713,
      "learning_rate": 3.9438125e-06,
      "loss": 3.5973,
      "step": 736900
    },
    {
      "epoch": 158.0188679245283,
      "grad_norm": 7.674106121063232,
      "learning_rate": 3.9375625e-06,
      "loss": 3.5461,
      "step": 737000
    },
    {
      "epoch": 158.04030874785593,
      "grad_norm": 6.939491271972656,
      "learning_rate": 3.9313125e-06,
      "loss": 3.5372,
      "step": 737100
    },
    {
      "epoch": 158.06174957118353,
      "grad_norm": 7.803963661193848,
      "learning_rate": 3.9250625e-06,
      "loss": 3.5331,
      "step": 737200
    },
    {
      "epoch": 158.08319039451115,
      "grad_norm": 6.8264923095703125,
      "learning_rate": 3.9188125e-06,
      "loss": 3.6301,
      "step": 737300
    },
    {
      "epoch": 158.10463121783877,
      "grad_norm": 7.208566665649414,
      "learning_rate": 3.9125624999999995e-06,
      "loss": 3.6141,
      "step": 737400
    },
    {
      "epoch": 158.12607204116637,
      "grad_norm": 7.195069313049316,
      "learning_rate": 3.9063125e-06,
      "loss": 3.6098,
      "step": 737500
    },
    {
      "epoch": 158.147512864494,
      "grad_norm": 7.588021278381348,
      "learning_rate": 3.9000625e-06,
      "loss": 3.6747,
      "step": 737600
    },
    {
      "epoch": 158.1689536878216,
      "grad_norm": 7.547250270843506,
      "learning_rate": 3.8938125e-06,
      "loss": 3.5348,
      "step": 737700
    },
    {
      "epoch": 158.19039451114924,
      "grad_norm": 6.557928562164307,
      "learning_rate": 3.8875625e-06,
      "loss": 3.6023,
      "step": 737800
    },
    {
      "epoch": 158.21183533447683,
      "grad_norm": 6.894586086273193,
      "learning_rate": 3.8813125e-06,
      "loss": 3.5706,
      "step": 737900
    },
    {
      "epoch": 158.23327615780445,
      "grad_norm": 7.750863075256348,
      "learning_rate": 3.8750624999999995e-06,
      "loss": 3.5766,
      "step": 738000
    },
    {
      "epoch": 158.25471698113208,
      "grad_norm": 7.2486796379089355,
      "learning_rate": 3.8688125e-06,
      "loss": 3.577,
      "step": 738100
    },
    {
      "epoch": 158.2761578044597,
      "grad_norm": 7.311401844024658,
      "learning_rate": 3.8625625e-06,
      "loss": 3.5611,
      "step": 738200
    },
    {
      "epoch": 158.2975986277873,
      "grad_norm": 6.898153305053711,
      "learning_rate": 3.8563125e-06,
      "loss": 3.5819,
      "step": 738300
    },
    {
      "epoch": 158.31903945111492,
      "grad_norm": 7.463123321533203,
      "learning_rate": 3.8500625e-06,
      "loss": 3.5748,
      "step": 738400
    },
    {
      "epoch": 158.34048027444254,
      "grad_norm": 7.423544406890869,
      "learning_rate": 3.8438124999999995e-06,
      "loss": 3.6224,
      "step": 738500
    },
    {
      "epoch": 158.36192109777016,
      "grad_norm": 7.154081344604492,
      "learning_rate": 3.8375625e-06,
      "loss": 3.5842,
      "step": 738600
    },
    {
      "epoch": 158.38336192109776,
      "grad_norm": 6.79712438583374,
      "learning_rate": 3.831312500000001e-06,
      "loss": 3.602,
      "step": 738700
    },
    {
      "epoch": 158.40480274442538,
      "grad_norm": 8.009566307067871,
      "learning_rate": 3.825062500000001e-06,
      "loss": 3.5876,
      "step": 738800
    },
    {
      "epoch": 158.426243567753,
      "grad_norm": 6.959238529205322,
      "learning_rate": 3.818812500000001e-06,
      "loss": 3.6191,
      "step": 738900
    },
    {
      "epoch": 158.44768439108063,
      "grad_norm": 6.963608264923096,
      "learning_rate": 3.8125625000000005e-06,
      "loss": 3.587,
      "step": 739000
    },
    {
      "epoch": 158.46912521440822,
      "grad_norm": 7.921525955200195,
      "learning_rate": 3.8063125000000003e-06,
      "loss": 3.5879,
      "step": 739100
    },
    {
      "epoch": 158.49056603773585,
      "grad_norm": 6.586167812347412,
      "learning_rate": 3.8000625000000006e-06,
      "loss": 3.5334,
      "step": 739200
    },
    {
      "epoch": 158.51200686106347,
      "grad_norm": 6.975504398345947,
      "learning_rate": 3.7938125000000004e-06,
      "loss": 3.6179,
      "step": 739300
    },
    {
      "epoch": 158.5334476843911,
      "grad_norm": 7.16622257232666,
      "learning_rate": 3.7875625000000003e-06,
      "loss": 3.5834,
      "step": 739400
    },
    {
      "epoch": 158.5548885077187,
      "grad_norm": 6.643720626831055,
      "learning_rate": 3.7813125000000006e-06,
      "loss": 3.5517,
      "step": 739500
    },
    {
      "epoch": 158.5763293310463,
      "grad_norm": 6.919569969177246,
      "learning_rate": 3.7750625000000004e-06,
      "loss": 3.5761,
      "step": 739600
    },
    {
      "epoch": 158.59777015437393,
      "grad_norm": 6.671061038970947,
      "learning_rate": 3.7688125000000003e-06,
      "loss": 3.5856,
      "step": 739700
    },
    {
      "epoch": 158.61921097770156,
      "grad_norm": 6.697598934173584,
      "learning_rate": 3.7625625000000005e-06,
      "loss": 3.6114,
      "step": 739800
    },
    {
      "epoch": 158.64065180102915,
      "grad_norm": 7.072613716125488,
      "learning_rate": 3.7563125000000004e-06,
      "loss": 3.5634,
      "step": 739900
    },
    {
      "epoch": 158.66209262435677,
      "grad_norm": 7.023778438568115,
      "learning_rate": 3.7500625000000007e-06,
      "loss": 3.5133,
      "step": 740000
    },
    {
      "epoch": 158.6835334476844,
      "grad_norm": 6.420537948608398,
      "learning_rate": 3.7438125000000005e-06,
      "loss": 3.6469,
      "step": 740100
    },
    {
      "epoch": 158.70497427101202,
      "grad_norm": 7.2296929359436035,
      "learning_rate": 3.7375625000000004e-06,
      "loss": 3.5981,
      "step": 740200
    },
    {
      "epoch": 158.72641509433961,
      "grad_norm": 7.037564277648926,
      "learning_rate": 3.7313125000000006e-06,
      "loss": 3.5396,
      "step": 740300
    },
    {
      "epoch": 158.74785591766724,
      "grad_norm": 7.1106486320495605,
      "learning_rate": 3.7250625000000005e-06,
      "loss": 3.6477,
      "step": 740400
    },
    {
      "epoch": 158.76929674099486,
      "grad_norm": 6.917901039123535,
      "learning_rate": 3.7188125000000003e-06,
      "loss": 3.5656,
      "step": 740500
    },
    {
      "epoch": 158.79073756432248,
      "grad_norm": 6.720849514007568,
      "learning_rate": 3.7125625000000006e-06,
      "loss": 3.6023,
      "step": 740600
    },
    {
      "epoch": 158.81217838765008,
      "grad_norm": 7.509889602661133,
      "learning_rate": 3.7063125000000004e-06,
      "loss": 3.515,
      "step": 740700
    },
    {
      "epoch": 158.8336192109777,
      "grad_norm": 6.788159370422363,
      "learning_rate": 3.7000625000000003e-06,
      "loss": 3.6296,
      "step": 740800
    },
    {
      "epoch": 158.85506003430532,
      "grad_norm": 6.716022491455078,
      "learning_rate": 3.6938125000000006e-06,
      "loss": 3.6084,
      "step": 740900
    },
    {
      "epoch": 158.87650085763292,
      "grad_norm": 7.097171306610107,
      "learning_rate": 3.6875625000000004e-06,
      "loss": 3.5734,
      "step": 741000
    },
    {
      "epoch": 158.89794168096054,
      "grad_norm": 7.1781463623046875,
      "learning_rate": 3.6813125000000003e-06,
      "loss": 3.5301,
      "step": 741100
    },
    {
      "epoch": 158.91938250428817,
      "grad_norm": 6.85549783706665,
      "learning_rate": 3.6750625000000005e-06,
      "loss": 3.5473,
      "step": 741200
    },
    {
      "epoch": 158.9408233276158,
      "grad_norm": 6.685283660888672,
      "learning_rate": 3.6688125000000004e-06,
      "loss": 3.5682,
      "step": 741300
    },
    {
      "epoch": 158.96226415094338,
      "grad_norm": 6.79307222366333,
      "learning_rate": 3.6625625000000002e-06,
      "loss": 3.497,
      "step": 741400
    },
    {
      "epoch": 158.983704974271,
      "grad_norm": 6.56520938873291,
      "learning_rate": 3.6563125000000005e-06,
      "loss": 3.6255,
      "step": 741500
    },
    {
      "epoch": 159.00514579759863,
      "grad_norm": 7.7330121994018555,
      "learning_rate": 3.6500625000000003e-06,
      "loss": 3.5845,
      "step": 741600
    },
    {
      "epoch": 159.02658662092625,
      "grad_norm": 6.430191993713379,
      "learning_rate": 3.6438125e-06,
      "loss": 3.5716,
      "step": 741700
    },
    {
      "epoch": 159.04802744425385,
      "grad_norm": 6.9831743240356445,
      "learning_rate": 3.6375625000000005e-06,
      "loss": 3.6177,
      "step": 741800
    },
    {
      "epoch": 159.06946826758147,
      "grad_norm": 7.0018839836120605,
      "learning_rate": 3.6313125000000003e-06,
      "loss": 3.6192,
      "step": 741900
    },
    {
      "epoch": 159.0909090909091,
      "grad_norm": 6.7336931228637695,
      "learning_rate": 3.6250625e-06,
      "loss": 3.6091,
      "step": 742000
    },
    {
      "epoch": 2457.2847682119204,
      "grad_norm": 1.9803874492645264,
      "learning_rate": 2.145773076923077e-05,
      "loss": 1.5445,
      "step": 742100
    },
    {
      "epoch": 2457.615894039735,
      "grad_norm": 2.1381707191467285,
      "learning_rate": 2.1453884615384616e-05,
      "loss": 0.9182,
      "step": 742200
    },
    {
      "epoch": 2457.9470198675494,
      "grad_norm": 1.532760500907898,
      "learning_rate": 2.145003846153846e-05,
      "loss": 0.8977,
      "step": 742300
    },
    {
      "epoch": 2458.2781456953644,
      "grad_norm": 1.1465873718261719,
      "learning_rate": 2.144619230769231e-05,
      "loss": 0.8907,
      "step": 742400
    },
    {
      "epoch": 2458.609271523179,
      "grad_norm": 1.1507515907287598,
      "learning_rate": 2.1442346153846156e-05,
      "loss": 0.8857,
      "step": 742500
    },
    {
      "epoch": 159.21955403087478,
      "grad_norm": 6.83584451675415,
      "learning_rate": 2.14385e-05,
      "loss": 4.1089,
      "step": 742600
    },
    {
      "epoch": 159.2409948542024,
      "grad_norm": 6.740458011627197,
      "learning_rate": 2.1434653846153847e-05,
      "loss": 3.6831,
      "step": 742700
    },
    {
      "epoch": 159.26243567753002,
      "grad_norm": 7.266383171081543,
      "learning_rate": 2.1430807692307696e-05,
      "loss": 3.6169,
      "step": 742800
    },
    {
      "epoch": 159.28387650085764,
      "grad_norm": 7.820989608764648,
      "learning_rate": 2.142696153846154e-05,
      "loss": 3.6058,
      "step": 742900
    },
    {
      "epoch": 159.30531732418524,
      "grad_norm": 6.523751735687256,
      "learning_rate": 2.1423115384615387e-05,
      "loss": 3.6126,
      "step": 743000
    },
    {
      "epoch": 159.32675814751286,
      "grad_norm": 7.595593452453613,
      "learning_rate": 2.1419269230769233e-05,
      "loss": 3.6646,
      "step": 743100
    },
    {
      "epoch": 159.34819897084049,
      "grad_norm": 7.321177959442139,
      "learning_rate": 2.1415423076923078e-05,
      "loss": 3.6048,
      "step": 743200
    },
    {
      "epoch": 159.3696397941681,
      "grad_norm": 7.051764011383057,
      "learning_rate": 2.1411576923076924e-05,
      "loss": 3.6401,
      "step": 743300
    },
    {
      "epoch": 159.3910806174957,
      "grad_norm": 7.488952159881592,
      "learning_rate": 2.1407730769230773e-05,
      "loss": 3.6571,
      "step": 743400
    },
    {
      "epoch": 159.41252144082333,
      "grad_norm": 6.565040111541748,
      "learning_rate": 2.1403884615384618e-05,
      "loss": 3.5979,
      "step": 743500
    },
    {
      "epoch": 159.43396226415095,
      "grad_norm": 7.438258171081543,
      "learning_rate": 2.1400038461538464e-05,
      "loss": 3.6483,
      "step": 743600
    },
    {
      "epoch": 159.45540308747857,
      "grad_norm": 7.0626373291015625,
      "learning_rate": 2.139619230769231e-05,
      "loss": 3.6219,
      "step": 743700
    },
    {
      "epoch": 159.47684391080617,
      "grad_norm": 6.403926372528076,
      "learning_rate": 2.1392346153846155e-05,
      "loss": 3.6514,
      "step": 743800
    },
    {
      "epoch": 159.4982847341338,
      "grad_norm": 6.775505542755127,
      "learning_rate": 2.13885e-05,
      "loss": 3.6261,
      "step": 743900
    },
    {
      "epoch": 159.5197255574614,
      "grad_norm": 6.812418460845947,
      "learning_rate": 2.1384653846153846e-05,
      "loss": 3.6325,
      "step": 744000
    },
    {
      "epoch": 159.54116638078904,
      "grad_norm": 7.017246246337891,
      "learning_rate": 2.1380807692307695e-05,
      "loss": 3.6112,
      "step": 744100
    },
    {
      "epoch": 159.56260720411663,
      "grad_norm": 7.370894908905029,
      "learning_rate": 2.137696153846154e-05,
      "loss": 3.6536,
      "step": 744200
    },
    {
      "epoch": 159.58404802744425,
      "grad_norm": 6.131627082824707,
      "learning_rate": 2.1373115384615386e-05,
      "loss": 3.6401,
      "step": 744300
    },
    {
      "epoch": 159.60548885077188,
      "grad_norm": 6.947584629058838,
      "learning_rate": 2.136926923076923e-05,
      "loss": 3.653,
      "step": 744400
    },
    {
      "epoch": 159.62692967409947,
      "grad_norm": 6.510538578033447,
      "learning_rate": 2.1365423076923077e-05,
      "loss": 3.6059,
      "step": 744500
    },
    {
      "epoch": 159.6483704974271,
      "grad_norm": 6.712132930755615,
      "learning_rate": 2.1361576923076922e-05,
      "loss": 3.6886,
      "step": 744600
    },
    {
      "epoch": 159.66981132075472,
      "grad_norm": 6.38961124420166,
      "learning_rate": 2.135773076923077e-05,
      "loss": 3.5729,
      "step": 744700
    },
    {
      "epoch": 159.69125214408234,
      "grad_norm": 6.870024681091309,
      "learning_rate": 2.1353884615384617e-05,
      "loss": 3.6368,
      "step": 744800
    },
    {
      "epoch": 159.71269296740994,
      "grad_norm": 7.370943546295166,
      "learning_rate": 2.1350038461538462e-05,
      "loss": 3.6027,
      "step": 744900
    },
    {
      "epoch": 159.73413379073756,
      "grad_norm": 8.797344207763672,
      "learning_rate": 2.1346192307692308e-05,
      "loss": 3.6535,
      "step": 745000
    },
    {
      "epoch": 159.75557461406518,
      "grad_norm": 7.222891807556152,
      "learning_rate": 2.1342346153846154e-05,
      "loss": 3.6877,
      "step": 745100
    },
    {
      "epoch": 159.7770154373928,
      "grad_norm": 6.7175726890563965,
      "learning_rate": 2.13385e-05,
      "loss": 3.6076,
      "step": 745200
    },
    {
      "epoch": 159.7984562607204,
      "grad_norm": 6.31072473526001,
      "learning_rate": 2.1334653846153845e-05,
      "loss": 3.5672,
      "step": 745300
    },
    {
      "epoch": 159.81989708404802,
      "grad_norm": 6.750406742095947,
      "learning_rate": 2.1330807692307694e-05,
      "loss": 3.6514,
      "step": 745400
    },
    {
      "epoch": 159.84133790737565,
      "grad_norm": 6.962303638458252,
      "learning_rate": 2.132696153846154e-05,
      "loss": 3.5898,
      "step": 745500
    },
    {
      "epoch": 159.86277873070327,
      "grad_norm": 6.619650840759277,
      "learning_rate": 2.1323115384615385e-05,
      "loss": 3.6499,
      "step": 745600
    },
    {
      "epoch": 159.88421955403086,
      "grad_norm": 6.432894229888916,
      "learning_rate": 2.131926923076923e-05,
      "loss": 3.6216,
      "step": 745700
    },
    {
      "epoch": 159.9056603773585,
      "grad_norm": 6.692846775054932,
      "learning_rate": 2.1315423076923076e-05,
      "loss": 3.6309,
      "step": 745800
    },
    {
      "epoch": 159.9271012006861,
      "grad_norm": 6.160727500915527,
      "learning_rate": 2.1311576923076925e-05,
      "loss": 3.5954,
      "step": 745900
    },
    {
      "epoch": 159.94854202401373,
      "grad_norm": 6.898289203643799,
      "learning_rate": 2.130773076923077e-05,
      "loss": 3.5747,
      "step": 746000
    },
    {
      "epoch": 159.96998284734133,
      "grad_norm": 6.901418209075928,
      "learning_rate": 2.1303884615384616e-05,
      "loss": 3.6606,
      "step": 746100
    },
    {
      "epoch": 159.99142367066895,
      "grad_norm": 6.686406135559082,
      "learning_rate": 2.1300038461538465e-05,
      "loss": 3.6474,
      "step": 746200
    },
    {
      "epoch": 160.01286449399657,
      "grad_norm": 6.811256408691406,
      "learning_rate": 2.129619230769231e-05,
      "loss": 3.6349,
      "step": 746300
    },
    {
      "epoch": 160.0343053173242,
      "grad_norm": 6.759237289428711,
      "learning_rate": 2.1292346153846156e-05,
      "loss": 3.5738,
      "step": 746400
    },
    {
      "epoch": 160.0557461406518,
      "grad_norm": 6.412306308746338,
      "learning_rate": 2.12885e-05,
      "loss": 3.6478,
      "step": 746500
    },
    {
      "epoch": 160.07718696397941,
      "grad_norm": 6.3749003410339355,
      "learning_rate": 2.1284653846153847e-05,
      "loss": 3.6012,
      "step": 746600
    },
    {
      "epoch": 160.09862778730704,
      "grad_norm": 7.4888081550598145,
      "learning_rate": 2.1280807692307696e-05,
      "loss": 3.5415,
      "step": 746700
    },
    {
      "epoch": 160.12006861063466,
      "grad_norm": 6.2659525871276855,
      "learning_rate": 2.127696153846154e-05,
      "loss": 3.5686,
      "step": 746800
    },
    {
      "epoch": 160.14150943396226,
      "grad_norm": 7.106771945953369,
      "learning_rate": 2.1273115384615387e-05,
      "loss": 3.5919,
      "step": 746900
    },
    {
      "epoch": 160.16295025728988,
      "grad_norm": 6.27841854095459,
      "learning_rate": 2.1269269230769232e-05,
      "loss": 3.6241,
      "step": 747000
    },
    {
      "epoch": 160.1843910806175,
      "grad_norm": 6.586430549621582,
      "learning_rate": 2.1265423076923078e-05,
      "loss": 3.598,
      "step": 747100
    },
    {
      "epoch": 160.20583190394512,
      "grad_norm": 7.233635902404785,
      "learning_rate": 2.1261576923076923e-05,
      "loss": 3.5943,
      "step": 747200
    },
    {
      "epoch": 160.22727272727272,
      "grad_norm": 6.8152055740356445,
      "learning_rate": 2.1257730769230772e-05,
      "loss": 3.6207,
      "step": 747300
    },
    {
      "epoch": 160.24871355060034,
      "grad_norm": 6.380485534667969,
      "learning_rate": 2.1253884615384618e-05,
      "loss": 3.6466,
      "step": 747400
    },
    {
      "epoch": 160.27015437392797,
      "grad_norm": 6.424431324005127,
      "learning_rate": 2.1250038461538463e-05,
      "loss": 3.6069,
      "step": 747500
    },
    {
      "epoch": 160.29159519725556,
      "grad_norm": 7.87168550491333,
      "learning_rate": 2.124619230769231e-05,
      "loss": 3.6168,
      "step": 747600
    },
    {
      "epoch": 160.31303602058318,
      "grad_norm": 6.437410354614258,
      "learning_rate": 2.1242346153846154e-05,
      "loss": 3.6197,
      "step": 747700
    },
    {
      "epoch": 160.3344768439108,
      "grad_norm": 6.943665981292725,
      "learning_rate": 2.12385e-05,
      "loss": 3.5771,
      "step": 747800
    },
    {
      "epoch": 160.35591766723843,
      "grad_norm": 6.917453289031982,
      "learning_rate": 2.1234653846153846e-05,
      "loss": 3.6503,
      "step": 747900
    },
    {
      "epoch": 160.37735849056602,
      "grad_norm": 6.884959697723389,
      "learning_rate": 2.1230807692307694e-05,
      "loss": 3.6508,
      "step": 748000
    },
    {
      "epoch": 160.39879931389365,
      "grad_norm": 7.091514587402344,
      "learning_rate": 2.122696153846154e-05,
      "loss": 3.6227,
      "step": 748100
    },
    {
      "epoch": 160.42024013722127,
      "grad_norm": 6.8392534255981445,
      "learning_rate": 2.1223115384615386e-05,
      "loss": 3.6019,
      "step": 748200
    },
    {
      "epoch": 160.4416809605489,
      "grad_norm": 7.111740589141846,
      "learning_rate": 2.121926923076923e-05,
      "loss": 3.611,
      "step": 748300
    },
    {
      "epoch": 160.4631217838765,
      "grad_norm": 6.646969795227051,
      "learning_rate": 2.1215423076923077e-05,
      "loss": 3.618,
      "step": 748400
    },
    {
      "epoch": 160.4845626072041,
      "grad_norm": 7.316531181335449,
      "learning_rate": 2.1211576923076922e-05,
      "loss": 3.6244,
      "step": 748500
    },
    {
      "epoch": 160.50600343053173,
      "grad_norm": 6.611591339111328,
      "learning_rate": 2.120773076923077e-05,
      "loss": 3.6218,
      "step": 748600
    },
    {
      "epoch": 160.52744425385936,
      "grad_norm": 6.831427574157715,
      "learning_rate": 2.1203884615384617e-05,
      "loss": 3.6016,
      "step": 748700
    },
    {
      "epoch": 160.54888507718695,
      "grad_norm": 6.281675815582275,
      "learning_rate": 2.1200038461538462e-05,
      "loss": 3.5641,
      "step": 748800
    },
    {
      "epoch": 160.57032590051458,
      "grad_norm": 6.821040153503418,
      "learning_rate": 2.1196192307692308e-05,
      "loss": 3.5948,
      "step": 748900
    },
    {
      "epoch": 160.5917667238422,
      "grad_norm": 6.426476001739502,
      "learning_rate": 2.1192346153846153e-05,
      "loss": 3.643,
      "step": 749000
    },
    {
      "epoch": 160.61320754716982,
      "grad_norm": 6.651496410369873,
      "learning_rate": 2.11885e-05,
      "loss": 3.6327,
      "step": 749100
    },
    {
      "epoch": 160.63464837049742,
      "grad_norm": 7.00852108001709,
      "learning_rate": 2.1184653846153848e-05,
      "loss": 3.5785,
      "step": 749200
    },
    {
      "epoch": 160.65608919382504,
      "grad_norm": 6.882009029388428,
      "learning_rate": 2.1180807692307693e-05,
      "loss": 3.5372,
      "step": 749300
    },
    {
      "epoch": 160.67753001715266,
      "grad_norm": 6.5781049728393555,
      "learning_rate": 2.117696153846154e-05,
      "loss": 3.5863,
      "step": 749400
    },
    {
      "epoch": 160.69897084048029,
      "grad_norm": 6.808924198150635,
      "learning_rate": 2.1173115384615384e-05,
      "loss": 3.6662,
      "step": 749500
    },
    {
      "epoch": 160.72041166380788,
      "grad_norm": 6.655938625335693,
      "learning_rate": 2.116926923076923e-05,
      "loss": 3.5816,
      "step": 749600
    },
    {
      "epoch": 160.7418524871355,
      "grad_norm": 6.532504558563232,
      "learning_rate": 2.116542307692308e-05,
      "loss": 3.5724,
      "step": 749700
    },
    {
      "epoch": 160.76329331046313,
      "grad_norm": 6.538039684295654,
      "learning_rate": 2.1161576923076924e-05,
      "loss": 3.5814,
      "step": 749800
    },
    {
      "epoch": 160.78473413379075,
      "grad_norm": 7.0979485511779785,
      "learning_rate": 2.115773076923077e-05,
      "loss": 3.6393,
      "step": 749900
    },
    {
      "epoch": 160.80617495711834,
      "grad_norm": 6.616674900054932,
      "learning_rate": 2.1153884615384615e-05,
      "loss": 3.5977,
      "step": 750000
    },
    {
      "epoch": 160.82761578044597,
      "grad_norm": 6.332401752471924,
      "learning_rate": 2.1150038461538464e-05,
      "loss": 3.6279,
      "step": 750100
    },
    {
      "epoch": 160.8490566037736,
      "grad_norm": 6.563576698303223,
      "learning_rate": 2.114619230769231e-05,
      "loss": 3.557,
      "step": 750200
    },
    {
      "epoch": 160.8704974271012,
      "grad_norm": 6.852961540222168,
      "learning_rate": 2.1142346153846155e-05,
      "loss": 3.624,
      "step": 750300
    },
    {
      "epoch": 160.8919382504288,
      "grad_norm": 6.253123760223389,
      "learning_rate": 2.11385e-05,
      "loss": 3.5996,
      "step": 750400
    },
    {
      "epoch": 160.91337907375643,
      "grad_norm": 6.316579818725586,
      "learning_rate": 2.113465384615385e-05,
      "loss": 3.5624,
      "step": 750500
    },
    {
      "epoch": 160.93481989708405,
      "grad_norm": 6.276593208312988,
      "learning_rate": 2.1130807692307695e-05,
      "loss": 3.6263,
      "step": 750600
    },
    {
      "epoch": 160.95626072041168,
      "grad_norm": 6.820072174072266,
      "learning_rate": 2.112696153846154e-05,
      "loss": 3.6302,
      "step": 750700
    },
    {
      "epoch": 160.97770154373927,
      "grad_norm": 6.5185866355896,
      "learning_rate": 2.1123115384615387e-05,
      "loss": 3.6268,
      "step": 750800
    },
    {
      "epoch": 160.9991423670669,
      "grad_norm": 6.5571465492248535,
      "learning_rate": 2.1119269230769232e-05,
      "loss": 3.6205,
      "step": 750900
    },
    {
      "epoch": 161.02058319039452,
      "grad_norm": 6.574254035949707,
      "learning_rate": 2.1115423076923078e-05,
      "loss": 3.5921,
      "step": 751000
    },
    {
      "epoch": 161.0420240137221,
      "grad_norm": 6.479860782623291,
      "learning_rate": 2.1111576923076923e-05,
      "loss": 3.5833,
      "step": 751100
    },
    {
      "epoch": 161.06346483704974,
      "grad_norm": 6.657464027404785,
      "learning_rate": 2.1107730769230772e-05,
      "loss": 3.5417,
      "step": 751200
    },
    {
      "epoch": 161.08490566037736,
      "grad_norm": 7.757206916809082,
      "learning_rate": 2.1103884615384618e-05,
      "loss": 3.5625,
      "step": 751300
    },
    {
      "epoch": 161.10634648370498,
      "grad_norm": 6.427140712738037,
      "learning_rate": 2.1100038461538463e-05,
      "loss": 3.5617,
      "step": 751400
    },
    {
      "epoch": 161.12778730703258,
      "grad_norm": 6.16396427154541,
      "learning_rate": 2.109619230769231e-05,
      "loss": 3.55,
      "step": 751500
    },
    {
      "epoch": 161.1492281303602,
      "grad_norm": 6.244646072387695,
      "learning_rate": 2.1092346153846154e-05,
      "loss": 3.5552,
      "step": 751600
    },
    {
      "epoch": 161.17066895368782,
      "grad_norm": 6.191238880157471,
      "learning_rate": 2.10885e-05,
      "loss": 3.6105,
      "step": 751700
    },
    {
      "epoch": 161.19210977701545,
      "grad_norm": 6.381317138671875,
      "learning_rate": 2.108465384615385e-05,
      "loss": 3.5999,
      "step": 751800
    },
    {
      "epoch": 161.21355060034304,
      "grad_norm": 6.359114646911621,
      "learning_rate": 2.1080807692307694e-05,
      "loss": 3.5849,
      "step": 751900
    },
    {
      "epoch": 161.23499142367066,
      "grad_norm": 6.033938884735107,
      "learning_rate": 2.107696153846154e-05,
      "loss": 3.5471,
      "step": 752000
    },
    {
      "epoch": 161.2564322469983,
      "grad_norm": 6.531369686126709,
      "learning_rate": 2.1073115384615385e-05,
      "loss": 3.5321,
      "step": 752100
    },
    {
      "epoch": 161.2778730703259,
      "grad_norm": 6.314293384552002,
      "learning_rate": 2.106926923076923e-05,
      "loss": 3.6034,
      "step": 752200
    },
    {
      "epoch": 161.2993138936535,
      "grad_norm": 6.149968147277832,
      "learning_rate": 2.1065423076923076e-05,
      "loss": 3.6165,
      "step": 752300
    },
    {
      "epoch": 161.32075471698113,
      "grad_norm": 7.365039348602295,
      "learning_rate": 2.1061576923076922e-05,
      "loss": 3.5809,
      "step": 752400
    },
    {
      "epoch": 161.34219554030875,
      "grad_norm": 7.365046977996826,
      "learning_rate": 2.105773076923077e-05,
      "loss": 3.5875,
      "step": 752500
    },
    {
      "epoch": 161.36363636363637,
      "grad_norm": 6.832093238830566,
      "learning_rate": 2.1053884615384616e-05,
      "loss": 3.582,
      "step": 752600
    },
    {
      "epoch": 161.38507718696397,
      "grad_norm": 6.9497294425964355,
      "learning_rate": 2.1050038461538462e-05,
      "loss": 3.5807,
      "step": 752700
    },
    {
      "epoch": 161.4065180102916,
      "grad_norm": 6.699814796447754,
      "learning_rate": 2.1046192307692307e-05,
      "loss": 3.5739,
      "step": 752800
    },
    {
      "epoch": 161.42795883361921,
      "grad_norm": 6.783604145050049,
      "learning_rate": 2.1042346153846153e-05,
      "loss": 3.5304,
      "step": 752900
    },
    {
      "epoch": 161.44939965694684,
      "grad_norm": 6.02886438369751,
      "learning_rate": 2.10385e-05,
      "loss": 3.585,
      "step": 753000
    },
    {
      "epoch": 161.47084048027443,
      "grad_norm": 6.884198188781738,
      "learning_rate": 2.1034653846153847e-05,
      "loss": 3.602,
      "step": 753100
    },
    {
      "epoch": 161.49228130360206,
      "grad_norm": 5.9520087242126465,
      "learning_rate": 2.1030807692307693e-05,
      "loss": 3.62,
      "step": 753200
    },
    {
      "epoch": 161.51372212692968,
      "grad_norm": 6.23983907699585,
      "learning_rate": 2.102696153846154e-05,
      "loss": 3.5386,
      "step": 753300
    },
    {
      "epoch": 161.5351629502573,
      "grad_norm": 6.497923851013184,
      "learning_rate": 2.1023115384615384e-05,
      "loss": 3.5772,
      "step": 753400
    },
    {
      "epoch": 161.5566037735849,
      "grad_norm": 6.331949234008789,
      "learning_rate": 2.101926923076923e-05,
      "loss": 3.587,
      "step": 753500
    },
    {
      "epoch": 161.57804459691252,
      "grad_norm": 6.722246170043945,
      "learning_rate": 2.101542307692308e-05,
      "loss": 3.6051,
      "step": 753600
    },
    {
      "epoch": 161.59948542024014,
      "grad_norm": 7.596451759338379,
      "learning_rate": 2.1011576923076924e-05,
      "loss": 3.5211,
      "step": 753700
    },
    {
      "epoch": 161.62092624356777,
      "grad_norm": 6.7215375900268555,
      "learning_rate": 2.100773076923077e-05,
      "loss": 3.6086,
      "step": 753800
    },
    {
      "epoch": 161.64236706689536,
      "grad_norm": 6.422523021697998,
      "learning_rate": 2.100388461538462e-05,
      "loss": 3.5594,
      "step": 753900
    },
    {
      "epoch": 161.66380789022298,
      "grad_norm": 7.477736473083496,
      "learning_rate": 2.1000038461538464e-05,
      "loss": 3.597,
      "step": 754000
    },
    {
      "epoch": 161.6852487135506,
      "grad_norm": 6.445387840270996,
      "learning_rate": 2.099619230769231e-05,
      "loss": 3.5398,
      "step": 754100
    },
    {
      "epoch": 161.70668953687823,
      "grad_norm": 6.143338203430176,
      "learning_rate": 2.0992346153846155e-05,
      "loss": 3.5282,
      "step": 754200
    },
    {
      "epoch": 161.72813036020582,
      "grad_norm": 6.284318923950195,
      "learning_rate": 2.09885e-05,
      "loss": 3.5755,
      "step": 754300
    },
    {
      "epoch": 161.74957118353345,
      "grad_norm": 6.333001613616943,
      "learning_rate": 2.098465384615385e-05,
      "loss": 3.5894,
      "step": 754400
    },
    {
      "epoch": 161.77101200686107,
      "grad_norm": 6.455990314483643,
      "learning_rate": 2.0980807692307695e-05,
      "loss": 3.6028,
      "step": 754500
    },
    {
      "epoch": 161.79245283018867,
      "grad_norm": 6.959081649780273,
      "learning_rate": 2.097696153846154e-05,
      "loss": 3.5821,
      "step": 754600
    },
    {
      "epoch": 161.8138936535163,
      "grad_norm": 6.44229793548584,
      "learning_rate": 2.0973115384615386e-05,
      "loss": 3.5884,
      "step": 754700
    },
    {
      "epoch": 161.8353344768439,
      "grad_norm": 6.581925868988037,
      "learning_rate": 2.0969269230769232e-05,
      "loss": 3.5798,
      "step": 754800
    },
    {
      "epoch": 161.85677530017153,
      "grad_norm": 6.483334064483643,
      "learning_rate": 2.0965423076923077e-05,
      "loss": 3.6151,
      "step": 754900
    },
    {
      "epoch": 161.87821612349913,
      "grad_norm": 6.568103313446045,
      "learning_rate": 2.0961576923076926e-05,
      "loss": 3.5827,
      "step": 755000
    },
    {
      "epoch": 161.89965694682675,
      "grad_norm": 6.640198230743408,
      "learning_rate": 2.0957730769230772e-05,
      "loss": 3.6096,
      "step": 755100
    },
    {
      "epoch": 161.92109777015438,
      "grad_norm": 6.835062503814697,
      "learning_rate": 2.0953884615384617e-05,
      "loss": 3.5581,
      "step": 755200
    },
    {
      "epoch": 161.942538593482,
      "grad_norm": 6.186089992523193,
      "learning_rate": 2.0950038461538463e-05,
      "loss": 3.5596,
      "step": 755300
    },
    {
      "epoch": 161.9639794168096,
      "grad_norm": 6.518224716186523,
      "learning_rate": 2.094619230769231e-05,
      "loss": 3.6476,
      "step": 755400
    },
    {
      "epoch": 161.98542024013722,
      "grad_norm": 6.529860973358154,
      "learning_rate": 2.0942346153846154e-05,
      "loss": 3.6115,
      "step": 755500
    },
    {
      "epoch": 162.00686106346484,
      "grad_norm": 6.6395182609558105,
      "learning_rate": 2.09385e-05,
      "loss": 3.5758,
      "step": 755600
    },
    {
      "epoch": 162.02830188679246,
      "grad_norm": 6.829182147979736,
      "learning_rate": 2.093465384615385e-05,
      "loss": 3.4953,
      "step": 755700
    },
    {
      "epoch": 162.04974271012006,
      "grad_norm": 6.640682220458984,
      "learning_rate": 2.0930807692307694e-05,
      "loss": 3.5213,
      "step": 755800
    },
    {
      "epoch": 162.07118353344768,
      "grad_norm": 7.385309219360352,
      "learning_rate": 2.092696153846154e-05,
      "loss": 3.5171,
      "step": 755900
    },
    {
      "epoch": 162.0926243567753,
      "grad_norm": 6.048827171325684,
      "learning_rate": 2.0923115384615385e-05,
      "loss": 3.5476,
      "step": 756000
    },
    {
      "epoch": 162.11406518010293,
      "grad_norm": 6.514060974121094,
      "learning_rate": 2.091926923076923e-05,
      "loss": 3.5903,
      "step": 756100
    },
    {
      "epoch": 162.13550600343052,
      "grad_norm": 7.283120632171631,
      "learning_rate": 2.0915423076923076e-05,
      "loss": 3.5385,
      "step": 756200
    },
    {
      "epoch": 162.15694682675814,
      "grad_norm": 7.209707736968994,
      "learning_rate": 2.0911576923076925e-05,
      "loss": 3.559,
      "step": 756300
    },
    {
      "epoch": 162.17838765008577,
      "grad_norm": 5.937460899353027,
      "learning_rate": 2.090773076923077e-05,
      "loss": 3.5752,
      "step": 756400
    },
    {
      "epoch": 162.1998284734134,
      "grad_norm": 6.230171203613281,
      "learning_rate": 2.0903884615384616e-05,
      "loss": 3.5564,
      "step": 756500
    },
    {
      "epoch": 162.22126929674099,
      "grad_norm": 6.322385311126709,
      "learning_rate": 2.090003846153846e-05,
      "loss": 3.5132,
      "step": 756600
    },
    {
      "epoch": 162.2427101200686,
      "grad_norm": 6.4175591468811035,
      "learning_rate": 2.0896192307692307e-05,
      "loss": 3.4935,
      "step": 756700
    },
    {
      "epoch": 162.26415094339623,
      "grad_norm": 6.3742194175720215,
      "learning_rate": 2.0892346153846153e-05,
      "loss": 3.5821,
      "step": 756800
    },
    {
      "epoch": 162.28559176672385,
      "grad_norm": 5.419445514678955,
      "learning_rate": 2.0888499999999998e-05,
      "loss": 3.6302,
      "step": 756900
    },
    {
      "epoch": 162.30703259005145,
      "grad_norm": 6.539738655090332,
      "learning_rate": 2.0884653846153847e-05,
      "loss": 3.5452,
      "step": 757000
    },
    {
      "epoch": 162.32847341337907,
      "grad_norm": 6.010847091674805,
      "learning_rate": 2.0880807692307693e-05,
      "loss": 3.5829,
      "step": 757100
    },
    {
      "epoch": 162.3499142367067,
      "grad_norm": 6.9501729011535645,
      "learning_rate": 2.0876961538461538e-05,
      "loss": 3.5725,
      "step": 757200
    },
    {
      "epoch": 162.37135506003432,
      "grad_norm": 6.411221027374268,
      "learning_rate": 2.0873115384615384e-05,
      "loss": 3.5332,
      "step": 757300
    },
    {
      "epoch": 162.3927958833619,
      "grad_norm": 6.480717182159424,
      "learning_rate": 2.0869269230769233e-05,
      "loss": 3.6081,
      "step": 757400
    },
    {
      "epoch": 162.41423670668954,
      "grad_norm": 7.363650321960449,
      "learning_rate": 2.086542307692308e-05,
      "loss": 3.5514,
      "step": 757500
    },
    {
      "epoch": 162.43567753001716,
      "grad_norm": 6.393014430999756,
      "learning_rate": 2.0861576923076924e-05,
      "loss": 3.5688,
      "step": 757600
    },
    {
      "epoch": 162.45711835334478,
      "grad_norm": 6.855967998504639,
      "learning_rate": 2.085773076923077e-05,
      "loss": 3.5928,
      "step": 757700
    },
    {
      "epoch": 162.47855917667238,
      "grad_norm": 6.086268901824951,
      "learning_rate": 2.085388461538462e-05,
      "loss": 3.5658,
      "step": 757800
    },
    {
      "epoch": 162.5,
      "grad_norm": 6.569910526275635,
      "learning_rate": 2.0850038461538464e-05,
      "loss": 3.5585,
      "step": 757900
    },
    {
      "epoch": 162.52144082332762,
      "grad_norm": 6.582766532897949,
      "learning_rate": 2.084619230769231e-05,
      "loss": 3.6,
      "step": 758000
    },
    {
      "epoch": 162.54288164665522,
      "grad_norm": 6.783684730529785,
      "learning_rate": 2.0842346153846155e-05,
      "loss": 3.5991,
      "step": 758100
    },
    {
      "epoch": 162.56432246998284,
      "grad_norm": 6.894627571105957,
      "learning_rate": 2.08385e-05,
      "loss": 3.4957,
      "step": 758200
    },
    {
      "epoch": 162.58576329331046,
      "grad_norm": 6.619060039520264,
      "learning_rate": 2.083465384615385e-05,
      "loss": 3.5875,
      "step": 758300
    },
    {
      "epoch": 162.6072041166381,
      "grad_norm": 5.527307987213135,
      "learning_rate": 2.0830807692307695e-05,
      "loss": 3.5928,
      "step": 758400
    },
    {
      "epoch": 162.62864493996568,
      "grad_norm": 6.733542442321777,
      "learning_rate": 2.082696153846154e-05,
      "loss": 3.59,
      "step": 758500
    },
    {
      "epoch": 162.6500857632933,
      "grad_norm": 6.845154762268066,
      "learning_rate": 2.0823115384615386e-05,
      "loss": 3.5341,
      "step": 758600
    },
    {
      "epoch": 162.67152658662093,
      "grad_norm": 6.40281867980957,
      "learning_rate": 2.081926923076923e-05,
      "loss": 3.5826,
      "step": 758700
    },
    {
      "epoch": 162.69296740994855,
      "grad_norm": 6.545455455780029,
      "learning_rate": 2.0815423076923077e-05,
      "loss": 3.5387,
      "step": 758800
    },
    {
      "epoch": 162.71440823327615,
      "grad_norm": 6.207784652709961,
      "learning_rate": 2.0811576923076926e-05,
      "loss": 3.5298,
      "step": 758900
    },
    {
      "epoch": 162.73584905660377,
      "grad_norm": 6.204585552215576,
      "learning_rate": 2.080773076923077e-05,
      "loss": 3.5659,
      "step": 759000
    },
    {
      "epoch": 162.7572898799314,
      "grad_norm": 6.475404739379883,
      "learning_rate": 2.0803884615384617e-05,
      "loss": 3.5713,
      "step": 759100
    },
    {
      "epoch": 162.77873070325901,
      "grad_norm": 6.680685997009277,
      "learning_rate": 2.0800038461538463e-05,
      "loss": 3.5638,
      "step": 759200
    },
    {
      "epoch": 162.8001715265866,
      "grad_norm": 6.380827903747559,
      "learning_rate": 2.0796192307692308e-05,
      "loss": 3.5461,
      "step": 759300
    },
    {
      "epoch": 162.82161234991423,
      "grad_norm": 6.635835647583008,
      "learning_rate": 2.0792346153846154e-05,
      "loss": 3.5236,
      "step": 759400
    },
    {
      "epoch": 162.84305317324186,
      "grad_norm": 6.7489447593688965,
      "learning_rate": 2.07885e-05,
      "loss": 3.5333,
      "step": 759500
    },
    {
      "epoch": 162.86449399656948,
      "grad_norm": 6.086916446685791,
      "learning_rate": 2.0784653846153848e-05,
      "loss": 3.5457,
      "step": 759600
    },
    {
      "epoch": 162.88593481989707,
      "grad_norm": 6.556855201721191,
      "learning_rate": 2.0780807692307694e-05,
      "loss": 3.513,
      "step": 759700
    },
    {
      "epoch": 162.9073756432247,
      "grad_norm": 6.474742889404297,
      "learning_rate": 2.077696153846154e-05,
      "loss": 3.5519,
      "step": 759800
    },
    {
      "epoch": 162.92881646655232,
      "grad_norm": 5.856494903564453,
      "learning_rate": 2.0773115384615385e-05,
      "loss": 3.532,
      "step": 759900
    },
    {
      "epoch": 162.95025728987994,
      "grad_norm": 5.90710973739624,
      "learning_rate": 2.076926923076923e-05,
      "loss": 3.5162,
      "step": 760000
    },
    {
      "epoch": 162.97169811320754,
      "grad_norm": 6.866057872772217,
      "learning_rate": 2.0765423076923076e-05,
      "loss": 3.6267,
      "step": 760100
    },
    {
      "epoch": 162.99313893653516,
      "grad_norm": 5.960622787475586,
      "learning_rate": 2.0761576923076925e-05,
      "loss": 3.5147,
      "step": 760200
    },
    {
      "epoch": 163.01457975986278,
      "grad_norm": 6.051596164703369,
      "learning_rate": 2.075773076923077e-05,
      "loss": 3.496,
      "step": 760300
    },
    {
      "epoch": 163.0360205831904,
      "grad_norm": 6.196323871612549,
      "learning_rate": 2.0753884615384616e-05,
      "loss": 3.5342,
      "step": 760400
    },
    {
      "epoch": 163.057461406518,
      "grad_norm": 6.7764973640441895,
      "learning_rate": 2.075003846153846e-05,
      "loss": 3.4957,
      "step": 760500
    },
    {
      "epoch": 163.07890222984562,
      "grad_norm": 6.753708362579346,
      "learning_rate": 2.0746192307692307e-05,
      "loss": 3.4916,
      "step": 760600
    },
    {
      "epoch": 163.10034305317325,
      "grad_norm": 6.689820766448975,
      "learning_rate": 2.0742346153846152e-05,
      "loss": 3.5263,
      "step": 760700
    },
    {
      "epoch": 163.12178387650087,
      "grad_norm": 6.318154811859131,
      "learning_rate": 2.07385e-05,
      "loss": 3.5268,
      "step": 760800
    },
    {
      "epoch": 163.14322469982847,
      "grad_norm": 5.97429084777832,
      "learning_rate": 2.0734653846153847e-05,
      "loss": 3.5259,
      "step": 760900
    },
    {
      "epoch": 163.1646655231561,
      "grad_norm": 6.852457523345947,
      "learning_rate": 2.0730807692307693e-05,
      "loss": 3.5161,
      "step": 761000
    },
    {
      "epoch": 163.1861063464837,
      "grad_norm": 6.745627403259277,
      "learning_rate": 2.0726961538461538e-05,
      "loss": 3.4933,
      "step": 761100
    },
    {
      "epoch": 163.20754716981133,
      "grad_norm": 6.326502799987793,
      "learning_rate": 2.0723115384615384e-05,
      "loss": 3.5396,
      "step": 761200
    },
    {
      "epoch": 163.22898799313893,
      "grad_norm": 6.289747714996338,
      "learning_rate": 2.0719269230769233e-05,
      "loss": 3.5384,
      "step": 761300
    },
    {
      "epoch": 163.25042881646655,
      "grad_norm": 6.416881084442139,
      "learning_rate": 2.0715423076923078e-05,
      "loss": 3.5005,
      "step": 761400
    },
    {
      "epoch": 163.27186963979418,
      "grad_norm": 6.437971115112305,
      "learning_rate": 2.0711576923076924e-05,
      "loss": 3.5458,
      "step": 761500
    },
    {
      "epoch": 163.29331046312177,
      "grad_norm": 6.0465288162231445,
      "learning_rate": 2.0707730769230773e-05,
      "loss": 3.517,
      "step": 761600
    },
    {
      "epoch": 163.3147512864494,
      "grad_norm": 7.047622203826904,
      "learning_rate": 2.0703884615384618e-05,
      "loss": 3.5369,
      "step": 761700
    },
    {
      "epoch": 163.33619210977702,
      "grad_norm": 6.360851764678955,
      "learning_rate": 2.0700038461538464e-05,
      "loss": 3.5542,
      "step": 761800
    },
    {
      "epoch": 163.35763293310464,
      "grad_norm": 5.491774082183838,
      "learning_rate": 2.069619230769231e-05,
      "loss": 3.5191,
      "step": 761900
    },
    {
      "epoch": 163.37907375643223,
      "grad_norm": 6.705452919006348,
      "learning_rate": 2.0692346153846155e-05,
      "loss": 3.5695,
      "step": 762000
    },
    {
      "epoch": 163.40051457975986,
      "grad_norm": 5.807234287261963,
      "learning_rate": 2.0688500000000004e-05,
      "loss": 3.5681,
      "step": 762100
    },
    {
      "epoch": 163.42195540308748,
      "grad_norm": 7.029394626617432,
      "learning_rate": 2.068465384615385e-05,
      "loss": 3.5174,
      "step": 762200
    },
    {
      "epoch": 163.4433962264151,
      "grad_norm": 6.6295247077941895,
      "learning_rate": 2.0680807692307695e-05,
      "loss": 3.5019,
      "step": 762300
    },
    {
      "epoch": 163.4648370497427,
      "grad_norm": 6.476529598236084,
      "learning_rate": 2.067696153846154e-05,
      "loss": 3.5304,
      "step": 762400
    },
    {
      "epoch": 163.48627787307032,
      "grad_norm": 6.954097270965576,
      "learning_rate": 2.0673115384615386e-05,
      "loss": 3.5472,
      "step": 762500
    },
    {
      "epoch": 163.50771869639794,
      "grad_norm": 7.022149562835693,
      "learning_rate": 2.066926923076923e-05,
      "loss": 3.5082,
      "step": 762600
    },
    {
      "epoch": 163.52915951972557,
      "grad_norm": 6.1006340980529785,
      "learning_rate": 2.0665423076923077e-05,
      "loss": 3.5501,
      "step": 762700
    },
    {
      "epoch": 163.55060034305316,
      "grad_norm": 6.192938327789307,
      "learning_rate": 2.0661576923076926e-05,
      "loss": 3.5454,
      "step": 762800
    },
    {
      "epoch": 163.57204116638079,
      "grad_norm": 6.14459753036499,
      "learning_rate": 2.065773076923077e-05,
      "loss": 3.5022,
      "step": 762900
    },
    {
      "epoch": 163.5934819897084,
      "grad_norm": 5.813214302062988,
      "learning_rate": 2.0653884615384617e-05,
      "loss": 3.5309,
      "step": 763000
    },
    {
      "epoch": 163.61492281303603,
      "grad_norm": 6.430505275726318,
      "learning_rate": 2.0650038461538462e-05,
      "loss": 3.5322,
      "step": 763100
    },
    {
      "epoch": 163.63636363636363,
      "grad_norm": 6.317347526550293,
      "learning_rate": 2.0646192307692308e-05,
      "loss": 3.5688,
      "step": 763200
    },
    {
      "epoch": 163.65780445969125,
      "grad_norm": 6.338972091674805,
      "learning_rate": 2.0642346153846153e-05,
      "loss": 3.5367,
      "step": 763300
    },
    {
      "epoch": 163.67924528301887,
      "grad_norm": 6.6635942459106445,
      "learning_rate": 2.0638500000000002e-05,
      "loss": 3.5574,
      "step": 763400
    },
    {
      "epoch": 163.7006861063465,
      "grad_norm": 6.157321453094482,
      "learning_rate": 2.0634653846153848e-05,
      "loss": 3.5389,
      "step": 763500
    },
    {
      "epoch": 163.7221269296741,
      "grad_norm": 6.535048484802246,
      "learning_rate": 2.0630807692307693e-05,
      "loss": 3.4874,
      "step": 763600
    },
    {
      "epoch": 163.7435677530017,
      "grad_norm": 5.7369704246521,
      "learning_rate": 2.062696153846154e-05,
      "loss": 3.5171,
      "step": 763700
    },
    {
      "epoch": 163.76500857632934,
      "grad_norm": 6.280175685882568,
      "learning_rate": 2.0623115384615385e-05,
      "loss": 3.5645,
      "step": 763800
    },
    {
      "epoch": 163.78644939965696,
      "grad_norm": 6.226155757904053,
      "learning_rate": 2.061926923076923e-05,
      "loss": 3.57,
      "step": 763900
    },
    {
      "epoch": 163.80789022298455,
      "grad_norm": 5.529291152954102,
      "learning_rate": 2.0615423076923076e-05,
      "loss": 3.5614,
      "step": 764000
    },
    {
      "epoch": 163.82933104631218,
      "grad_norm": 6.684733867645264,
      "learning_rate": 2.0611576923076925e-05,
      "loss": 3.506,
      "step": 764100
    },
    {
      "epoch": 163.8507718696398,
      "grad_norm": 6.1151814460754395,
      "learning_rate": 2.060773076923077e-05,
      "loss": 3.5476,
      "step": 764200
    },
    {
      "epoch": 163.87221269296742,
      "grad_norm": 5.7150373458862305,
      "learning_rate": 2.0603884615384616e-05,
      "loss": 3.5266,
      "step": 764300
    },
    {
      "epoch": 163.89365351629502,
      "grad_norm": 6.016876220703125,
      "learning_rate": 2.060003846153846e-05,
      "loss": 3.6139,
      "step": 764400
    },
    {
      "epoch": 163.91509433962264,
      "grad_norm": 5.938582420349121,
      "learning_rate": 2.0596192307692307e-05,
      "loss": 3.5412,
      "step": 764500
    },
    {
      "epoch": 163.93653516295026,
      "grad_norm": 6.389708042144775,
      "learning_rate": 2.0592346153846152e-05,
      "loss": 3.5266,
      "step": 764600
    },
    {
      "epoch": 163.9579759862779,
      "grad_norm": 6.8208441734313965,
      "learning_rate": 2.05885e-05,
      "loss": 3.4911,
      "step": 764700
    },
    {
      "epoch": 163.97941680960548,
      "grad_norm": 6.683491230010986,
      "learning_rate": 2.0584653846153847e-05,
      "loss": 3.5405,
      "step": 764800
    },
    {
      "epoch": 164.0008576329331,
      "grad_norm": 6.523882865905762,
      "learning_rate": 2.0580807692307692e-05,
      "loss": 3.5975,
      "step": 764900
    },
    {
      "epoch": 164.02229845626073,
      "grad_norm": 5.925934791564941,
      "learning_rate": 2.0576961538461538e-05,
      "loss": 3.4747,
      "step": 765000
    },
    {
      "epoch": 164.04373927958832,
      "grad_norm": 6.549473285675049,
      "learning_rate": 2.0573115384615387e-05,
      "loss": 3.4504,
      "step": 765100
    },
    {
      "epoch": 164.06518010291595,
      "grad_norm": 6.088228225708008,
      "learning_rate": 2.0569269230769232e-05,
      "loss": 3.5105,
      "step": 765200
    },
    {
      "epoch": 164.08662092624357,
      "grad_norm": 6.221730709075928,
      "learning_rate": 2.0565423076923078e-05,
      "loss": 3.5364,
      "step": 765300
    },
    {
      "epoch": 164.1080617495712,
      "grad_norm": 6.645534038543701,
      "learning_rate": 2.0561576923076927e-05,
      "loss": 3.5049,
      "step": 765400
    },
    {
      "epoch": 164.1295025728988,
      "grad_norm": 6.655810356140137,
      "learning_rate": 2.0557730769230772e-05,
      "loss": 3.5162,
      "step": 765500
    },
    {
      "epoch": 164.1509433962264,
      "grad_norm": 5.92470121383667,
      "learning_rate": 2.0553884615384618e-05,
      "loss": 3.4768,
      "step": 765600
    },
    {
      "epoch": 164.17238421955403,
      "grad_norm": 6.176349639892578,
      "learning_rate": 2.0550038461538463e-05,
      "loss": 3.4757,
      "step": 765700
    },
    {
      "epoch": 164.19382504288166,
      "grad_norm": 7.095032691955566,
      "learning_rate": 2.054619230769231e-05,
      "loss": 3.5385,
      "step": 765800
    },
    {
      "epoch": 164.21526586620925,
      "grad_norm": 6.704504013061523,
      "learning_rate": 2.0542346153846154e-05,
      "loss": 3.4845,
      "step": 765900
    },
    {
      "epoch": 164.23670668953687,
      "grad_norm": 6.027890205383301,
      "learning_rate": 2.0538500000000003e-05,
      "loss": 3.4326,
      "step": 766000
    },
    {
      "epoch": 164.2581475128645,
      "grad_norm": 6.089381217956543,
      "learning_rate": 2.053465384615385e-05,
      "loss": 3.4961,
      "step": 766100
    },
    {
      "epoch": 164.27958833619212,
      "grad_norm": 6.8173909187316895,
      "learning_rate": 2.0530807692307694e-05,
      "loss": 3.5549,
      "step": 766200
    },
    {
      "epoch": 164.30102915951971,
      "grad_norm": 6.415173530578613,
      "learning_rate": 2.052696153846154e-05,
      "loss": 3.4977,
      "step": 766300
    },
    {
      "epoch": 164.32246998284734,
      "grad_norm": 6.15323543548584,
      "learning_rate": 2.0523115384615385e-05,
      "loss": 3.4961,
      "step": 766400
    },
    {
      "epoch": 164.34391080617496,
      "grad_norm": 6.1148905754089355,
      "learning_rate": 2.051926923076923e-05,
      "loss": 3.5149,
      "step": 766500
    },
    {
      "epoch": 164.36535162950258,
      "grad_norm": 6.700808048248291,
      "learning_rate": 2.0515423076923077e-05,
      "loss": 3.5181,
      "step": 766600
    },
    {
      "epoch": 164.38679245283018,
      "grad_norm": 6.401302337646484,
      "learning_rate": 2.0511576923076926e-05,
      "loss": 3.4747,
      "step": 766700
    },
    {
      "epoch": 164.4082332761578,
      "grad_norm": 6.138028144836426,
      "learning_rate": 2.050773076923077e-05,
      "loss": 3.5481,
      "step": 766800
    },
    {
      "epoch": 164.42967409948542,
      "grad_norm": 6.131472110748291,
      "learning_rate": 2.0503884615384617e-05,
      "loss": 3.5203,
      "step": 766900
    },
    {
      "epoch": 164.45111492281305,
      "grad_norm": 6.569371700286865,
      "learning_rate": 2.0500038461538462e-05,
      "loss": 3.4831,
      "step": 767000
    },
    {
      "epoch": 164.47255574614064,
      "grad_norm": 6.616828441619873,
      "learning_rate": 2.0496192307692308e-05,
      "loss": 3.5005,
      "step": 767100
    },
    {
      "epoch": 164.49399656946827,
      "grad_norm": 6.253148078918457,
      "learning_rate": 2.0492346153846153e-05,
      "loss": 3.4929,
      "step": 767200
    },
    {
      "epoch": 164.5154373927959,
      "grad_norm": 6.471380233764648,
      "learning_rate": 2.0488500000000002e-05,
      "loss": 3.5072,
      "step": 767300
    },
    {
      "epoch": 164.5368782161235,
      "grad_norm": 6.287395000457764,
      "learning_rate": 2.0484653846153848e-05,
      "loss": 3.521,
      "step": 767400
    },
    {
      "epoch": 164.5583190394511,
      "grad_norm": 6.081545829772949,
      "learning_rate": 2.0480807692307693e-05,
      "loss": 3.5046,
      "step": 767500
    },
    {
      "epoch": 164.57975986277873,
      "grad_norm": 6.700383186340332,
      "learning_rate": 2.047696153846154e-05,
      "loss": 3.4999,
      "step": 767600
    },
    {
      "epoch": 164.60120068610635,
      "grad_norm": 5.779429912567139,
      "learning_rate": 2.0473115384615384e-05,
      "loss": 3.5271,
      "step": 767700
    },
    {
      "epoch": 164.62264150943398,
      "grad_norm": 5.988330841064453,
      "learning_rate": 2.046926923076923e-05,
      "loss": 3.4867,
      "step": 767800
    },
    {
      "epoch": 164.64408233276157,
      "grad_norm": 6.114367961883545,
      "learning_rate": 2.046542307692308e-05,
      "loss": 3.4858,
      "step": 767900
    },
    {
      "epoch": 164.6655231560892,
      "grad_norm": 6.333316802978516,
      "learning_rate": 2.0461576923076924e-05,
      "loss": 3.6159,
      "step": 768000
    },
    {
      "epoch": 164.68696397941682,
      "grad_norm": 6.187688827514648,
      "learning_rate": 2.045773076923077e-05,
      "loss": 3.5256,
      "step": 768100
    },
    {
      "epoch": 164.70840480274444,
      "grad_norm": 5.989141941070557,
      "learning_rate": 2.0453884615384615e-05,
      "loss": 3.527,
      "step": 768200
    },
    {
      "epoch": 164.72984562607203,
      "grad_norm": 6.0139265060424805,
      "learning_rate": 2.045003846153846e-05,
      "loss": 3.4799,
      "step": 768300
    },
    {
      "epoch": 164.75128644939966,
      "grad_norm": 5.776011943817139,
      "learning_rate": 2.0446192307692306e-05,
      "loss": 3.4815,
      "step": 768400
    },
    {
      "epoch": 164.77272727272728,
      "grad_norm": 6.855440139770508,
      "learning_rate": 2.0442346153846152e-05,
      "loss": 3.522,
      "step": 768500
    },
    {
      "epoch": 164.79416809605488,
      "grad_norm": 6.455166339874268,
      "learning_rate": 2.04385e-05,
      "loss": 3.5382,
      "step": 768600
    },
    {
      "epoch": 164.8156089193825,
      "grad_norm": 7.0895280838012695,
      "learning_rate": 2.0434653846153846e-05,
      "loss": 3.5419,
      "step": 768700
    },
    {
      "epoch": 164.83704974271012,
      "grad_norm": 5.912733554840088,
      "learning_rate": 2.0430807692307692e-05,
      "loss": 3.5121,
      "step": 768800
    },
    {
      "epoch": 164.85849056603774,
      "grad_norm": 5.793008804321289,
      "learning_rate": 2.042696153846154e-05,
      "loss": 3.4744,
      "step": 768900
    },
    {
      "epoch": 164.87993138936534,
      "grad_norm": 5.666532039642334,
      "learning_rate": 2.0423115384615386e-05,
      "loss": 3.5488,
      "step": 769000
    },
    {
      "epoch": 164.90137221269296,
      "grad_norm": 6.190262794494629,
      "learning_rate": 2.0419269230769232e-05,
      "loss": 3.5401,
      "step": 769100
    },
    {
      "epoch": 164.92281303602059,
      "grad_norm": 6.3385233879089355,
      "learning_rate": 2.0415423076923078e-05,
      "loss": 3.4972,
      "step": 769200
    },
    {
      "epoch": 164.9442538593482,
      "grad_norm": 6.4510393142700195,
      "learning_rate": 2.0411576923076926e-05,
      "loss": 3.5403,
      "step": 769300
    },
    {
      "epoch": 164.9656946826758,
      "grad_norm": 5.714751243591309,
      "learning_rate": 2.0407730769230772e-05,
      "loss": 3.5356,
      "step": 769400
    },
    {
      "epoch": 164.98713550600343,
      "grad_norm": 7.0413312911987305,
      "learning_rate": 2.0403884615384618e-05,
      "loss": 3.5527,
      "step": 769500
    },
    {
      "epoch": 165.00857632933105,
      "grad_norm": 5.648406982421875,
      "learning_rate": 2.0400038461538463e-05,
      "loss": 3.5192,
      "step": 769600
    },
    {
      "epoch": 165.03001715265867,
      "grad_norm": 6.229888439178467,
      "learning_rate": 2.039619230769231e-05,
      "loss": 3.4655,
      "step": 769700
    },
    {
      "epoch": 165.05145797598627,
      "grad_norm": 7.0404205322265625,
      "learning_rate": 2.0392346153846154e-05,
      "loss": 3.5104,
      "step": 769800
    },
    {
      "epoch": 165.0728987993139,
      "grad_norm": 5.790242671966553,
      "learning_rate": 2.0388500000000003e-05,
      "loss": 3.4647,
      "step": 769900
    },
    {
      "epoch": 165.0943396226415,
      "grad_norm": 6.046648025512695,
      "learning_rate": 2.038465384615385e-05,
      "loss": 3.4518,
      "step": 770000
    },
    {
      "epoch": 165.11578044596914,
      "grad_norm": 6.7061662673950195,
      "learning_rate": 2.0380807692307694e-05,
      "loss": 3.511,
      "step": 770100
    },
    {
      "epoch": 165.13722126929673,
      "grad_norm": 6.44821834564209,
      "learning_rate": 2.037696153846154e-05,
      "loss": 3.4278,
      "step": 770200
    },
    {
      "epoch": 165.15866209262435,
      "grad_norm": 5.9614152908325195,
      "learning_rate": 2.0373115384615385e-05,
      "loss": 3.4557,
      "step": 770300
    },
    {
      "epoch": 165.18010291595198,
      "grad_norm": 5.638471603393555,
      "learning_rate": 2.036926923076923e-05,
      "loss": 3.4547,
      "step": 770400
    },
    {
      "epoch": 165.2015437392796,
      "grad_norm": 6.518519878387451,
      "learning_rate": 2.036542307692308e-05,
      "loss": 3.4625,
      "step": 770500
    },
    {
      "epoch": 165.2229845626072,
      "grad_norm": 5.960999965667725,
      "learning_rate": 2.0361576923076925e-05,
      "loss": 3.4831,
      "step": 770600
    },
    {
      "epoch": 165.24442538593482,
      "grad_norm": 6.302484512329102,
      "learning_rate": 2.035773076923077e-05,
      "loss": 3.4645,
      "step": 770700
    },
    {
      "epoch": 165.26586620926244,
      "grad_norm": 6.111713886260986,
      "learning_rate": 2.0353884615384616e-05,
      "loss": 3.5246,
      "step": 770800
    },
    {
      "epoch": 165.28730703259006,
      "grad_norm": 6.501290321350098,
      "learning_rate": 2.0350038461538462e-05,
      "loss": 3.474,
      "step": 770900
    },
    {
      "epoch": 165.30874785591766,
      "grad_norm": 6.665517807006836,
      "learning_rate": 2.0346192307692307e-05,
      "loss": 3.4855,
      "step": 771000
    },
    {
      "epoch": 165.33018867924528,
      "grad_norm": 6.160537242889404,
      "learning_rate": 2.0342346153846153e-05,
      "loss": 3.481,
      "step": 771100
    },
    {
      "epoch": 165.3516295025729,
      "grad_norm": 6.342505931854248,
      "learning_rate": 2.0338500000000002e-05,
      "loss": 3.5345,
      "step": 771200
    },
    {
      "epoch": 165.37307032590053,
      "grad_norm": 5.95738410949707,
      "learning_rate": 2.0334653846153847e-05,
      "loss": 3.483,
      "step": 771300
    },
    {
      "epoch": 165.39451114922812,
      "grad_norm": 6.052534103393555,
      "learning_rate": 2.0330807692307693e-05,
      "loss": 3.5208,
      "step": 771400
    },
    {
      "epoch": 165.41595197255575,
      "grad_norm": 6.6959099769592285,
      "learning_rate": 2.032696153846154e-05,
      "loss": 3.4975,
      "step": 771500
    },
    {
      "epoch": 165.43739279588337,
      "grad_norm": 5.976681709289551,
      "learning_rate": 2.0323115384615384e-05,
      "loss": 3.4623,
      "step": 771600
    },
    {
      "epoch": 165.45883361921096,
      "grad_norm": 5.8423991203308105,
      "learning_rate": 2.031926923076923e-05,
      "loss": 3.52,
      "step": 771700
    },
    {
      "epoch": 165.4802744425386,
      "grad_norm": 6.259494304656982,
      "learning_rate": 2.031542307692308e-05,
      "loss": 3.5195,
      "step": 771800
    },
    {
      "epoch": 165.5017152658662,
      "grad_norm": 6.707588195800781,
      "learning_rate": 2.0311576923076924e-05,
      "loss": 3.5193,
      "step": 771900
    },
    {
      "epoch": 165.52315608919383,
      "grad_norm": 6.211493968963623,
      "learning_rate": 2.030773076923077e-05,
      "loss": 3.4581,
      "step": 772000
    },
    {
      "epoch": 165.54459691252143,
      "grad_norm": 6.273778915405273,
      "learning_rate": 2.0303884615384615e-05,
      "loss": 3.5054,
      "step": 772100
    },
    {
      "epoch": 165.56603773584905,
      "grad_norm": 6.560755252838135,
      "learning_rate": 2.030003846153846e-05,
      "loss": 3.4767,
      "step": 772200
    },
    {
      "epoch": 165.58747855917667,
      "grad_norm": 6.381433010101318,
      "learning_rate": 2.0296192307692306e-05,
      "loss": 3.51,
      "step": 772300
    },
    {
      "epoch": 165.6089193825043,
      "grad_norm": 6.490328311920166,
      "learning_rate": 2.0292346153846155e-05,
      "loss": 3.5318,
      "step": 772400
    },
    {
      "epoch": 165.6303602058319,
      "grad_norm": 5.771154880523682,
      "learning_rate": 2.02885e-05,
      "loss": 3.4838,
      "step": 772500
    },
    {
      "epoch": 165.65180102915951,
      "grad_norm": 6.249156951904297,
      "learning_rate": 2.0284653846153846e-05,
      "loss": 3.5088,
      "step": 772600
    },
    {
      "epoch": 165.67324185248714,
      "grad_norm": 5.176155090332031,
      "learning_rate": 2.0280807692307692e-05,
      "loss": 3.519,
      "step": 772700
    },
    {
      "epoch": 165.69468267581476,
      "grad_norm": 6.1678786277771,
      "learning_rate": 2.027696153846154e-05,
      "loss": 3.4636,
      "step": 772800
    },
    {
      "epoch": 165.71612349914236,
      "grad_norm": 5.806883335113525,
      "learning_rate": 2.0273115384615386e-05,
      "loss": 3.5374,
      "step": 772900
    },
    {
      "epoch": 165.73756432246998,
      "grad_norm": 6.356280326843262,
      "learning_rate": 2.0269269230769232e-05,
      "loss": 3.4956,
      "step": 773000
    },
    {
      "epoch": 165.7590051457976,
      "grad_norm": 6.4283528327941895,
      "learning_rate": 2.026542307692308e-05,
      "loss": 3.485,
      "step": 773100
    },
    {
      "epoch": 165.78044596912522,
      "grad_norm": 6.254706382751465,
      "learning_rate": 2.0261576923076926e-05,
      "loss": 3.52,
      "step": 773200
    },
    {
      "epoch": 165.80188679245282,
      "grad_norm": 6.088869094848633,
      "learning_rate": 2.0257730769230772e-05,
      "loss": 3.483,
      "step": 773300
    },
    {
      "epoch": 165.82332761578044,
      "grad_norm": 6.58748722076416,
      "learning_rate": 2.0253884615384617e-05,
      "loss": 3.4862,
      "step": 773400
    },
    {
      "epoch": 165.84476843910807,
      "grad_norm": 5.892899036407471,
      "learning_rate": 2.0250038461538463e-05,
      "loss": 3.4996,
      "step": 773500
    },
    {
      "epoch": 165.8662092624357,
      "grad_norm": 6.995439529418945,
      "learning_rate": 2.024619230769231e-05,
      "loss": 3.4551,
      "step": 773600
    },
    {
      "epoch": 165.88765008576328,
      "grad_norm": 6.570226669311523,
      "learning_rate": 2.0242346153846157e-05,
      "loss": 3.5107,
      "step": 773700
    },
    {
      "epoch": 165.9090909090909,
      "grad_norm": 6.540708065032959,
      "learning_rate": 2.0238500000000003e-05,
      "loss": 3.4862,
      "step": 773800
    },
    {
      "epoch": 165.93053173241853,
      "grad_norm": 6.169581413269043,
      "learning_rate": 2.023465384615385e-05,
      "loss": 3.4852,
      "step": 773900
    },
    {
      "epoch": 165.95197255574615,
      "grad_norm": 6.358745574951172,
      "learning_rate": 2.0230807692307694e-05,
      "loss": 3.4789,
      "step": 774000
    },
    {
      "epoch": 165.97341337907375,
      "grad_norm": 6.259769439697266,
      "learning_rate": 2.022696153846154e-05,
      "loss": 3.5276,
      "step": 774100
    },
    {
      "epoch": 165.99485420240137,
      "grad_norm": 6.4416046142578125,
      "learning_rate": 2.0223115384615385e-05,
      "loss": 3.4905,
      "step": 774200
    },
    {
      "epoch": 166.016295025729,
      "grad_norm": 5.9719319343566895,
      "learning_rate": 2.021926923076923e-05,
      "loss": 3.5113,
      "step": 774300
    },
    {
      "epoch": 166.03773584905662,
      "grad_norm": 5.734916687011719,
      "learning_rate": 2.021542307692308e-05,
      "loss": 3.4659,
      "step": 774400
    },
    {
      "epoch": 166.0591766723842,
      "grad_norm": 6.546979904174805,
      "learning_rate": 2.0211576923076925e-05,
      "loss": 3.4746,
      "step": 774500
    },
    {
      "epoch": 166.08061749571183,
      "grad_norm": 6.0021281242370605,
      "learning_rate": 2.020773076923077e-05,
      "loss": 3.3955,
      "step": 774600
    },
    {
      "epoch": 166.10205831903946,
      "grad_norm": 5.7904887199401855,
      "learning_rate": 2.0203884615384616e-05,
      "loss": 3.4514,
      "step": 774700
    },
    {
      "epoch": 166.12349914236708,
      "grad_norm": 5.919143199920654,
      "learning_rate": 2.020003846153846e-05,
      "loss": 3.4322,
      "step": 774800
    },
    {
      "epoch": 166.14493996569468,
      "grad_norm": 6.314151763916016,
      "learning_rate": 2.0196192307692307e-05,
      "loss": 3.4605,
      "step": 774900
    },
    {
      "epoch": 166.1663807890223,
      "grad_norm": 6.145589828491211,
      "learning_rate": 2.0192346153846156e-05,
      "loss": 3.5099,
      "step": 775000
    },
    {
      "epoch": 166.18782161234992,
      "grad_norm": 6.349383354187012,
      "learning_rate": 2.01885e-05,
      "loss": 3.5228,
      "step": 775100
    },
    {
      "epoch": 166.20926243567752,
      "grad_norm": 6.1883978843688965,
      "learning_rate": 2.0184653846153847e-05,
      "loss": 3.4638,
      "step": 775200
    },
    {
      "epoch": 166.23070325900514,
      "grad_norm": 6.5580220222473145,
      "learning_rate": 2.0180807692307693e-05,
      "loss": 3.4623,
      "step": 775300
    },
    {
      "epoch": 166.25214408233276,
      "grad_norm": 6.8299078941345215,
      "learning_rate": 2.0176961538461538e-05,
      "loss": 3.4376,
      "step": 775400
    },
    {
      "epoch": 166.27358490566039,
      "grad_norm": 6.308149814605713,
      "learning_rate": 2.0173115384615384e-05,
      "loss": 3.4296,
      "step": 775500
    },
    {
      "epoch": 166.29502572898798,
      "grad_norm": 5.975322246551514,
      "learning_rate": 2.016926923076923e-05,
      "loss": 3.4552,
      "step": 775600
    },
    {
      "epoch": 166.3164665523156,
      "grad_norm": 6.020823001861572,
      "learning_rate": 2.0165423076923078e-05,
      "loss": 3.5092,
      "step": 775700
    },
    {
      "epoch": 166.33790737564323,
      "grad_norm": 6.125965118408203,
      "learning_rate": 2.0161576923076924e-05,
      "loss": 3.4751,
      "step": 775800
    },
    {
      "epoch": 166.35934819897085,
      "grad_norm": 5.944917678833008,
      "learning_rate": 2.015773076923077e-05,
      "loss": 3.4433,
      "step": 775900
    },
    {
      "epoch": 166.38078902229844,
      "grad_norm": 6.397696495056152,
      "learning_rate": 2.0153884615384615e-05,
      "loss": 3.5029,
      "step": 776000
    },
    {
      "epoch": 166.40222984562607,
      "grad_norm": 6.349811553955078,
      "learning_rate": 2.015003846153846e-05,
      "loss": 3.4653,
      "step": 776100
    },
    {
      "epoch": 166.4236706689537,
      "grad_norm": 6.302709102630615,
      "learning_rate": 2.0146192307692306e-05,
      "loss": 3.4594,
      "step": 776200
    },
    {
      "epoch": 166.4451114922813,
      "grad_norm": 6.1301164627075195,
      "learning_rate": 2.0142346153846155e-05,
      "loss": 3.4873,
      "step": 776300
    },
    {
      "epoch": 166.4665523156089,
      "grad_norm": 5.886873722076416,
      "learning_rate": 2.01385e-05,
      "loss": 3.4376,
      "step": 776400
    },
    {
      "epoch": 166.48799313893653,
      "grad_norm": 6.514399528503418,
      "learning_rate": 2.0134653846153846e-05,
      "loss": 3.4612,
      "step": 776500
    },
    {
      "epoch": 166.50943396226415,
      "grad_norm": 6.8002471923828125,
      "learning_rate": 2.0130807692307695e-05,
      "loss": 3.5721,
      "step": 776600
    },
    {
      "epoch": 166.53087478559178,
      "grad_norm": 6.032002925872803,
      "learning_rate": 2.012696153846154e-05,
      "loss": 3.4536,
      "step": 776700
    },
    {
      "epoch": 166.55231560891937,
      "grad_norm": 6.463393211364746,
      "learning_rate": 2.0123115384615386e-05,
      "loss": 3.4762,
      "step": 776800
    },
    {
      "epoch": 166.573756432247,
      "grad_norm": 6.7342095375061035,
      "learning_rate": 2.011926923076923e-05,
      "loss": 3.4662,
      "step": 776900
    },
    {
      "epoch": 166.59519725557462,
      "grad_norm": 6.077670097351074,
      "learning_rate": 2.011542307692308e-05,
      "loss": 3.4559,
      "step": 777000
    },
    {
      "epoch": 166.61663807890224,
      "grad_norm": 6.149328708648682,
      "learning_rate": 2.0111576923076926e-05,
      "loss": 3.456,
      "step": 777100
    },
    {
      "epoch": 166.63807890222984,
      "grad_norm": 6.040730953216553,
      "learning_rate": 2.010773076923077e-05,
      "loss": 3.5003,
      "step": 777200
    },
    {
      "epoch": 166.65951972555746,
      "grad_norm": 6.193063259124756,
      "learning_rate": 2.0103884615384617e-05,
      "loss": 3.5038,
      "step": 777300
    },
    {
      "epoch": 166.68096054888508,
      "grad_norm": 5.81021785736084,
      "learning_rate": 2.0100038461538463e-05,
      "loss": 3.5116,
      "step": 777400
    },
    {
      "epoch": 166.7024013722127,
      "grad_norm": 6.606407642364502,
      "learning_rate": 2.0096192307692308e-05,
      "loss": 3.5055,
      "step": 777500
    },
    {
      "epoch": 166.7238421955403,
      "grad_norm": 5.962319850921631,
      "learning_rate": 2.0092346153846157e-05,
      "loss": 3.5326,
      "step": 777600
    },
    {
      "epoch": 166.74528301886792,
      "grad_norm": 6.259228229522705,
      "learning_rate": 2.0088500000000003e-05,
      "loss": 3.4829,
      "step": 777700
    },
    {
      "epoch": 166.76672384219555,
      "grad_norm": 6.24387264251709,
      "learning_rate": 2.0084653846153848e-05,
      "loss": 3.4255,
      "step": 777800
    },
    {
      "epoch": 166.78816466552317,
      "grad_norm": 6.201596736907959,
      "learning_rate": 2.0080807692307694e-05,
      "loss": 3.452,
      "step": 777900
    },
    {
      "epoch": 166.80960548885076,
      "grad_norm": 6.183355808258057,
      "learning_rate": 2.007696153846154e-05,
      "loss": 3.4915,
      "step": 778000
    },
    {
      "epoch": 166.8310463121784,
      "grad_norm": 5.957428455352783,
      "learning_rate": 2.0073115384615385e-05,
      "loss": 3.4891,
      "step": 778100
    },
    {
      "epoch": 166.852487135506,
      "grad_norm": 5.932802200317383,
      "learning_rate": 2.006926923076923e-05,
      "loss": 3.4201,
      "step": 778200
    },
    {
      "epoch": 166.87392795883363,
      "grad_norm": 5.868741035461426,
      "learning_rate": 2.006542307692308e-05,
      "loss": 3.4495,
      "step": 778300
    },
    {
      "epoch": 166.89536878216123,
      "grad_norm": 6.559124946594238,
      "learning_rate": 2.0061576923076925e-05,
      "loss": 3.4422,
      "step": 778400
    },
    {
      "epoch": 166.91680960548885,
      "grad_norm": 5.880673885345459,
      "learning_rate": 2.005773076923077e-05,
      "loss": 3.4206,
      "step": 778500
    },
    {
      "epoch": 166.93825042881647,
      "grad_norm": 6.164416313171387,
      "learning_rate": 2.0053884615384616e-05,
      "loss": 3.5356,
      "step": 778600
    },
    {
      "epoch": 166.95969125214407,
      "grad_norm": 6.742329120635986,
      "learning_rate": 2.005003846153846e-05,
      "loss": 3.5047,
      "step": 778700
    },
    {
      "epoch": 166.9811320754717,
      "grad_norm": 6.295894145965576,
      "learning_rate": 2.0046192307692307e-05,
      "loss": 3.4359,
      "step": 778800
    },
    {
      "epoch": 167.00257289879931,
      "grad_norm": 6.321195602416992,
      "learning_rate": 2.0042346153846156e-05,
      "loss": 3.4661,
      "step": 778900
    },
    {
      "epoch": 167.02401372212694,
      "grad_norm": 6.501720905303955,
      "learning_rate": 2.00385e-05,
      "loss": 3.4055,
      "step": 779000
    },
    {
      "epoch": 167.04545454545453,
      "grad_norm": 6.066605567932129,
      "learning_rate": 2.0034653846153847e-05,
      "loss": 3.4541,
      "step": 779100
    },
    {
      "epoch": 167.06689536878216,
      "grad_norm": 5.4698805809021,
      "learning_rate": 2.0030807692307692e-05,
      "loss": 3.4329,
      "step": 779200
    },
    {
      "epoch": 167.08833619210978,
      "grad_norm": 6.236138343811035,
      "learning_rate": 2.0026961538461538e-05,
      "loss": 3.4086,
      "step": 779300
    },
    {
      "epoch": 167.1097770154374,
      "grad_norm": 6.6363067626953125,
      "learning_rate": 2.0023115384615384e-05,
      "loss": 3.4362,
      "step": 779400
    },
    {
      "epoch": 167.131217838765,
      "grad_norm": 6.152498722076416,
      "learning_rate": 2.0019269230769232e-05,
      "loss": 3.4699,
      "step": 779500
    },
    {
      "epoch": 167.15265866209262,
      "grad_norm": 6.625347137451172,
      "learning_rate": 2.0015423076923078e-05,
      "loss": 3.4655,
      "step": 779600
    },
    {
      "epoch": 167.17409948542024,
      "grad_norm": 5.794754505157471,
      "learning_rate": 2.0011576923076924e-05,
      "loss": 3.4267,
      "step": 779700
    },
    {
      "epoch": 167.19554030874787,
      "grad_norm": 6.05818510055542,
      "learning_rate": 2.000773076923077e-05,
      "loss": 3.4138,
      "step": 779800
    },
    {
      "epoch": 167.21698113207546,
      "grad_norm": 5.810056209564209,
      "learning_rate": 2.0003884615384615e-05,
      "loss": 3.4173,
      "step": 779900
    },
    {
      "epoch": 167.23842195540308,
      "grad_norm": 5.799673080444336,
      "learning_rate": 2.000003846153846e-05,
      "loss": 3.4728,
      "step": 780000
    },
    {
      "epoch": 167.2598627787307,
      "grad_norm": 6.444826126098633,
      "learning_rate": 1.999619230769231e-05,
      "loss": 3.3801,
      "step": 780100
    },
    {
      "epoch": 167.28130360205833,
      "grad_norm": 6.080068111419678,
      "learning_rate": 1.9992346153846155e-05,
      "loss": 3.4849,
      "step": 780200
    },
    {
      "epoch": 167.30274442538592,
      "grad_norm": 6.655782699584961,
      "learning_rate": 1.99885e-05,
      "loss": 3.5087,
      "step": 780300
    },
    {
      "epoch": 167.32418524871355,
      "grad_norm": 6.069910049438477,
      "learning_rate": 1.998465384615385e-05,
      "loss": 3.4984,
      "step": 780400
    },
    {
      "epoch": 167.34562607204117,
      "grad_norm": 5.807507514953613,
      "learning_rate": 1.9980807692307695e-05,
      "loss": 3.4668,
      "step": 780500
    },
    {
      "epoch": 167.3670668953688,
      "grad_norm": 6.301625728607178,
      "learning_rate": 1.997696153846154e-05,
      "loss": 3.45,
      "step": 780600
    },
    {
      "epoch": 167.3885077186964,
      "grad_norm": 6.141231060028076,
      "learning_rate": 1.9973115384615386e-05,
      "loss": 3.4733,
      "step": 780700
    },
    {
      "epoch": 167.409948542024,
      "grad_norm": 5.939678192138672,
      "learning_rate": 1.9969269230769235e-05,
      "loss": 3.4178,
      "step": 780800
    },
    {
      "epoch": 167.43138936535163,
      "grad_norm": 5.658029079437256,
      "learning_rate": 1.996542307692308e-05,
      "loss": 3.4351,
      "step": 780900
    },
    {
      "epoch": 167.45283018867926,
      "grad_norm": 6.633819103240967,
      "learning_rate": 1.9961576923076926e-05,
      "loss": 3.4686,
      "step": 781000
    },
    {
      "epoch": 167.47427101200685,
      "grad_norm": 6.409137725830078,
      "learning_rate": 1.995773076923077e-05,
      "loss": 3.4572,
      "step": 781100
    },
    {
      "epoch": 167.49571183533448,
      "grad_norm": 5.812441825866699,
      "learning_rate": 1.9953884615384617e-05,
      "loss": 3.4611,
      "step": 781200
    },
    {
      "epoch": 167.5171526586621,
      "grad_norm": 6.435277938842773,
      "learning_rate": 1.9950038461538462e-05,
      "loss": 3.4447,
      "step": 781300
    },
    {
      "epoch": 167.53859348198972,
      "grad_norm": 6.915122985839844,
      "learning_rate": 1.9946192307692308e-05,
      "loss": 3.4477,
      "step": 781400
    },
    {
      "epoch": 167.56003430531732,
      "grad_norm": 5.734190940856934,
      "learning_rate": 1.9942346153846157e-05,
      "loss": 3.5189,
      "step": 781500
    },
    {
      "epoch": 167.58147512864494,
      "grad_norm": 6.5113043785095215,
      "learning_rate": 1.9938500000000002e-05,
      "loss": 3.431,
      "step": 781600
    },
    {
      "epoch": 167.60291595197256,
      "grad_norm": 6.479231357574463,
      "learning_rate": 1.9934653846153848e-05,
      "loss": 3.4331,
      "step": 781700
    },
    {
      "epoch": 167.62435677530019,
      "grad_norm": 6.523941993713379,
      "learning_rate": 1.9930807692307693e-05,
      "loss": 3.4923,
      "step": 781800
    },
    {
      "epoch": 167.64579759862778,
      "grad_norm": 5.733123779296875,
      "learning_rate": 1.992696153846154e-05,
      "loss": 3.4551,
      "step": 781900
    },
    {
      "epoch": 167.6672384219554,
      "grad_norm": 6.167287349700928,
      "learning_rate": 1.9923115384615384e-05,
      "loss": 3.4316,
      "step": 782000
    },
    {
      "epoch": 167.68867924528303,
      "grad_norm": 5.804981708526611,
      "learning_rate": 1.9919269230769233e-05,
      "loss": 3.419,
      "step": 782100
    },
    {
      "epoch": 167.71012006861062,
      "grad_norm": 6.17811393737793,
      "learning_rate": 1.991542307692308e-05,
      "loss": 3.4674,
      "step": 782200
    },
    {
      "epoch": 167.73156089193824,
      "grad_norm": 6.011568069458008,
      "learning_rate": 1.9911576923076924e-05,
      "loss": 3.3855,
      "step": 782300
    },
    {
      "epoch": 167.75300171526587,
      "grad_norm": 6.648115158081055,
      "learning_rate": 1.990773076923077e-05,
      "loss": 3.4568,
      "step": 782400
    },
    {
      "epoch": 167.7744425385935,
      "grad_norm": 5.4525322914123535,
      "learning_rate": 1.9903884615384616e-05,
      "loss": 3.4407,
      "step": 782500
    },
    {
      "epoch": 167.79588336192108,
      "grad_norm": 6.239560127258301,
      "learning_rate": 1.990003846153846e-05,
      "loss": 3.436,
      "step": 782600
    },
    {
      "epoch": 167.8173241852487,
      "grad_norm": 7.0941996574401855,
      "learning_rate": 1.9896192307692307e-05,
      "loss": 3.5333,
      "step": 782700
    },
    {
      "epoch": 167.83876500857633,
      "grad_norm": 6.273876667022705,
      "learning_rate": 1.9892346153846156e-05,
      "loss": 3.5099,
      "step": 782800
    },
    {
      "epoch": 167.86020583190395,
      "grad_norm": 5.7417473793029785,
      "learning_rate": 1.98885e-05,
      "loss": 3.4459,
      "step": 782900
    },
    {
      "epoch": 167.88164665523155,
      "grad_norm": 6.677572250366211,
      "learning_rate": 1.9884653846153847e-05,
      "loss": 3.4833,
      "step": 783000
    },
    {
      "epoch": 167.90308747855917,
      "grad_norm": 5.936931610107422,
      "learning_rate": 1.9880807692307692e-05,
      "loss": 3.4636,
      "step": 783100
    },
    {
      "epoch": 167.9245283018868,
      "grad_norm": 6.222657203674316,
      "learning_rate": 1.9876961538461538e-05,
      "loss": 3.4307,
      "step": 783200
    },
    {
      "epoch": 167.94596912521442,
      "grad_norm": 6.676733493804932,
      "learning_rate": 1.9873115384615383e-05,
      "loss": 3.4779,
      "step": 783300
    },
    {
      "epoch": 167.967409948542,
      "grad_norm": 5.800934314727783,
      "learning_rate": 1.9869269230769232e-05,
      "loss": 3.4521,
      "step": 783400
    },
    {
      "epoch": 167.98885077186964,
      "grad_norm": 6.1285810470581055,
      "learning_rate": 1.9865423076923078e-05,
      "loss": 3.4577,
      "step": 783500
    },
    {
      "epoch": 168.01029159519726,
      "grad_norm": 6.124744892120361,
      "learning_rate": 1.9861576923076923e-05,
      "loss": 3.4733,
      "step": 783600
    },
    {
      "epoch": 168.03173241852488,
      "grad_norm": 5.887450695037842,
      "learning_rate": 1.985773076923077e-05,
      "loss": 3.4278,
      "step": 783700
    },
    {
      "epoch": 168.05317324185248,
      "grad_norm": 5.8198771476745605,
      "learning_rate": 1.9853884615384614e-05,
      "loss": 3.4506,
      "step": 783800
    },
    {
      "epoch": 168.0746140651801,
      "grad_norm": 6.384929656982422,
      "learning_rate": 1.9850038461538463e-05,
      "loss": 3.4467,
      "step": 783900
    },
    {
      "epoch": 168.09605488850772,
      "grad_norm": 6.255048751831055,
      "learning_rate": 1.984619230769231e-05,
      "loss": 3.4511,
      "step": 784000
    },
    {
      "epoch": 168.11749571183535,
      "grad_norm": 6.913331985473633,
      "learning_rate": 1.9842346153846154e-05,
      "loss": 3.4268,
      "step": 784100
    },
    {
      "epoch": 168.13893653516294,
      "grad_norm": 6.269534111022949,
      "learning_rate": 1.98385e-05,
      "loss": 3.4229,
      "step": 784200
    },
    {
      "epoch": 168.16037735849056,
      "grad_norm": 6.592642784118652,
      "learning_rate": 1.983465384615385e-05,
      "loss": 3.493,
      "step": 784300
    },
    {
      "epoch": 168.1818181818182,
      "grad_norm": 5.8199076652526855,
      "learning_rate": 1.9830807692307694e-05,
      "loss": 3.4519,
      "step": 784400
    },
    {
      "epoch": 168.2032590051458,
      "grad_norm": 5.829202175140381,
      "learning_rate": 1.982696153846154e-05,
      "loss": 3.4221,
      "step": 784500
    },
    {
      "epoch": 168.2246998284734,
      "grad_norm": 6.535024166107178,
      "learning_rate": 1.9823115384615385e-05,
      "loss": 3.4261,
      "step": 784600
    },
    {
      "epoch": 168.24614065180103,
      "grad_norm": 6.501908779144287,
      "learning_rate": 1.9819269230769234e-05,
      "loss": 3.396,
      "step": 784700
    },
    {
      "epoch": 168.26758147512865,
      "grad_norm": 6.153190612792969,
      "learning_rate": 1.981542307692308e-05,
      "loss": 3.4054,
      "step": 784800
    },
    {
      "epoch": 168.28902229845627,
      "grad_norm": 6.179239749908447,
      "learning_rate": 1.9811576923076925e-05,
      "loss": 3.4117,
      "step": 784900
    },
    {
      "epoch": 168.31046312178387,
      "grad_norm": 5.97346305847168,
      "learning_rate": 1.980773076923077e-05,
      "loss": 3.3931,
      "step": 785000
    },
    {
      "epoch": 168.3319039451115,
      "grad_norm": 5.679152488708496,
      "learning_rate": 1.9803884615384617e-05,
      "loss": 3.4721,
      "step": 785100
    },
    {
      "epoch": 168.35334476843911,
      "grad_norm": 6.69017219543457,
      "learning_rate": 1.9800038461538462e-05,
      "loss": 3.4676,
      "step": 785200
    },
    {
      "epoch": 168.37478559176674,
      "grad_norm": 6.078863620758057,
      "learning_rate": 1.9796192307692308e-05,
      "loss": 3.468,
      "step": 785300
    },
    {
      "epoch": 168.39622641509433,
      "grad_norm": 6.370094299316406,
      "learning_rate": 1.9792346153846157e-05,
      "loss": 3.4304,
      "step": 785400
    },
    {
      "epoch": 168.41766723842196,
      "grad_norm": 5.698960781097412,
      "learning_rate": 1.9788500000000002e-05,
      "loss": 3.4583,
      "step": 785500
    },
    {
      "epoch": 168.43910806174958,
      "grad_norm": 5.563914775848389,
      "learning_rate": 1.9784653846153848e-05,
      "loss": 3.4333,
      "step": 785600
    },
    {
      "epoch": 168.46054888507717,
      "grad_norm": 6.516674518585205,
      "learning_rate": 1.9780807692307693e-05,
      "loss": 3.3994,
      "step": 785700
    },
    {
      "epoch": 168.4819897084048,
      "grad_norm": 6.23898983001709,
      "learning_rate": 1.977696153846154e-05,
      "loss": 3.4699,
      "step": 785800
    },
    {
      "epoch": 168.50343053173242,
      "grad_norm": 5.839228630065918,
      "learning_rate": 1.9773115384615384e-05,
      "loss": 3.4308,
      "step": 785900
    },
    {
      "epoch": 168.52487135506004,
      "grad_norm": 5.467034339904785,
      "learning_rate": 1.9769269230769233e-05,
      "loss": 3.4185,
      "step": 786000
    },
    {
      "epoch": 168.54631217838764,
      "grad_norm": 6.145251750946045,
      "learning_rate": 1.976542307692308e-05,
      "loss": 3.4577,
      "step": 786100
    },
    {
      "epoch": 168.56775300171526,
      "grad_norm": 6.425094127655029,
      "learning_rate": 1.9761576923076924e-05,
      "loss": 3.443,
      "step": 786200
    },
    {
      "epoch": 168.58919382504288,
      "grad_norm": 5.7832818031311035,
      "learning_rate": 1.975773076923077e-05,
      "loss": 3.39,
      "step": 786300
    },
    {
      "epoch": 168.6106346483705,
      "grad_norm": 5.562633991241455,
      "learning_rate": 1.9753884615384615e-05,
      "loss": 3.4333,
      "step": 786400
    },
    {
      "epoch": 168.6320754716981,
      "grad_norm": 6.630843639373779,
      "learning_rate": 1.975003846153846e-05,
      "loss": 3.4121,
      "step": 786500
    },
    {
      "epoch": 168.65351629502572,
      "grad_norm": 6.3414788246154785,
      "learning_rate": 1.974619230769231e-05,
      "loss": 3.4553,
      "step": 786600
    },
    {
      "epoch": 168.67495711835335,
      "grad_norm": 6.455190658569336,
      "learning_rate": 1.9742346153846155e-05,
      "loss": 3.4777,
      "step": 786700
    },
    {
      "epoch": 168.69639794168097,
      "grad_norm": 6.2945709228515625,
      "learning_rate": 1.97385e-05,
      "loss": 3.4825,
      "step": 786800
    },
    {
      "epoch": 168.71783876500857,
      "grad_norm": 5.8077521324157715,
      "learning_rate": 1.9734653846153846e-05,
      "loss": 3.4446,
      "step": 786900
    },
    {
      "epoch": 168.7392795883362,
      "grad_norm": 5.84931755065918,
      "learning_rate": 1.9730807692307692e-05,
      "loss": 3.4189,
      "step": 787000
    },
    {
      "epoch": 168.7607204116638,
      "grad_norm": 5.9534807205200195,
      "learning_rate": 1.9726961538461537e-05,
      "loss": 3.4485,
      "step": 787100
    },
    {
      "epoch": 168.78216123499143,
      "grad_norm": 6.557276725769043,
      "learning_rate": 1.9723115384615383e-05,
      "loss": 3.3997,
      "step": 787200
    },
    {
      "epoch": 168.80360205831903,
      "grad_norm": 6.212785720825195,
      "learning_rate": 1.9719269230769232e-05,
      "loss": 3.4825,
      "step": 787300
    },
    {
      "epoch": 168.82504288164665,
      "grad_norm": 5.835454940795898,
      "learning_rate": 1.9715423076923077e-05,
      "loss": 3.3758,
      "step": 787400
    },
    {
      "epoch": 168.84648370497428,
      "grad_norm": 6.047852993011475,
      "learning_rate": 1.9711576923076923e-05,
      "loss": 3.4111,
      "step": 787500
    },
    {
      "epoch": 168.8679245283019,
      "grad_norm": 6.893226623535156,
      "learning_rate": 1.970773076923077e-05,
      "loss": 3.3978,
      "step": 787600
    },
    {
      "epoch": 168.8893653516295,
      "grad_norm": 6.050606727600098,
      "learning_rate": 1.9703884615384614e-05,
      "loss": 3.4663,
      "step": 787700
    },
    {
      "epoch": 168.91080617495712,
      "grad_norm": 5.79997444152832,
      "learning_rate": 1.9700038461538463e-05,
      "loss": 3.4096,
      "step": 787800
    },
    {
      "epoch": 168.93224699828474,
      "grad_norm": 5.603264331817627,
      "learning_rate": 1.969619230769231e-05,
      "loss": 3.444,
      "step": 787900
    },
    {
      "epoch": 168.95368782161236,
      "grad_norm": 5.8610968589782715,
      "learning_rate": 1.9692346153846154e-05,
      "loss": 3.4612,
      "step": 788000
    },
    {
      "epoch": 168.97512864493996,
      "grad_norm": 6.447404384613037,
      "learning_rate": 1.9688500000000003e-05,
      "loss": 3.3953,
      "step": 788100
    },
    {
      "epoch": 168.99656946826758,
      "grad_norm": 6.91934871673584,
      "learning_rate": 1.968465384615385e-05,
      "loss": 3.4433,
      "step": 788200
    },
    {
      "epoch": 169.0180102915952,
      "grad_norm": 5.52119255065918,
      "learning_rate": 1.9680807692307694e-05,
      "loss": 3.4321,
      "step": 788300
    },
    {
      "epoch": 169.03945111492283,
      "grad_norm": 5.807777404785156,
      "learning_rate": 1.967696153846154e-05,
      "loss": 3.3952,
      "step": 788400
    },
    {
      "epoch": 169.06089193825042,
      "grad_norm": 5.6853203773498535,
      "learning_rate": 1.9673115384615385e-05,
      "loss": 3.4002,
      "step": 788500
    },
    {
      "epoch": 169.08233276157804,
      "grad_norm": 6.198755264282227,
      "learning_rate": 1.9669269230769234e-05,
      "loss": 3.3911,
      "step": 788600
    },
    {
      "epoch": 169.10377358490567,
      "grad_norm": 6.364199161529541,
      "learning_rate": 1.966542307692308e-05,
      "loss": 3.3715,
      "step": 788700
    },
    {
      "epoch": 169.12521440823326,
      "grad_norm": 5.5915961265563965,
      "learning_rate": 1.9661576923076925e-05,
      "loss": 3.4077,
      "step": 788800
    },
    {
      "epoch": 169.14665523156089,
      "grad_norm": 5.9240217208862305,
      "learning_rate": 1.965773076923077e-05,
      "loss": 3.3664,
      "step": 788900
    },
    {
      "epoch": 169.1680960548885,
      "grad_norm": 5.740779399871826,
      "learning_rate": 1.9653884615384616e-05,
      "loss": 3.4495,
      "step": 789000
    },
    {
      "epoch": 169.18953687821613,
      "grad_norm": 6.279512405395508,
      "learning_rate": 1.9650038461538462e-05,
      "loss": 3.409,
      "step": 789100
    },
    {
      "epoch": 169.21097770154373,
      "grad_norm": 5.922521591186523,
      "learning_rate": 1.964619230769231e-05,
      "loss": 3.4382,
      "step": 789200
    },
    {
      "epoch": 169.23241852487135,
      "grad_norm": 6.199739456176758,
      "learning_rate": 1.9642346153846156e-05,
      "loss": 3.3821,
      "step": 789300
    },
    {
      "epoch": 169.25385934819897,
      "grad_norm": 5.792816162109375,
      "learning_rate": 1.9638500000000002e-05,
      "loss": 3.455,
      "step": 789400
    },
    {
      "epoch": 169.2753001715266,
      "grad_norm": 5.974855899810791,
      "learning_rate": 1.9634653846153847e-05,
      "loss": 3.4532,
      "step": 789500
    },
    {
      "epoch": 169.2967409948542,
      "grad_norm": 6.928879737854004,
      "learning_rate": 1.9630807692307693e-05,
      "loss": 3.4411,
      "step": 789600
    },
    {
      "epoch": 169.3181818181818,
      "grad_norm": 6.183805465698242,
      "learning_rate": 1.962696153846154e-05,
      "loss": 3.4377,
      "step": 789700
    },
    {
      "epoch": 169.33962264150944,
      "grad_norm": 5.791213035583496,
      "learning_rate": 1.9623115384615384e-05,
      "loss": 3.4363,
      "step": 789800
    },
    {
      "epoch": 169.36106346483706,
      "grad_norm": 6.2726149559021,
      "learning_rate": 1.9619269230769233e-05,
      "loss": 3.4224,
      "step": 789900
    },
    {
      "epoch": 169.38250428816465,
      "grad_norm": 6.199202060699463,
      "learning_rate": 1.961542307692308e-05,
      "loss": 3.4021,
      "step": 790000
    },
    {
      "epoch": 169.40394511149228,
      "grad_norm": 6.667815208435059,
      "learning_rate": 1.9611576923076924e-05,
      "loss": 3.4754,
      "step": 790100
    },
    {
      "epoch": 169.4253859348199,
      "grad_norm": 5.751468181610107,
      "learning_rate": 1.960773076923077e-05,
      "loss": 3.413,
      "step": 790200
    },
    {
      "epoch": 169.44682675814752,
      "grad_norm": 6.214085578918457,
      "learning_rate": 1.9603884615384615e-05,
      "loss": 3.4465,
      "step": 790300
    },
    {
      "epoch": 169.46826758147512,
      "grad_norm": 6.161166191101074,
      "learning_rate": 1.960003846153846e-05,
      "loss": 3.3875,
      "step": 790400
    },
    {
      "epoch": 169.48970840480274,
      "grad_norm": 6.23643684387207,
      "learning_rate": 1.959619230769231e-05,
      "loss": 3.3446,
      "step": 790500
    },
    {
      "epoch": 169.51114922813036,
      "grad_norm": 6.057136058807373,
      "learning_rate": 1.9592346153846155e-05,
      "loss": 3.3936,
      "step": 790600
    },
    {
      "epoch": 169.532590051458,
      "grad_norm": 5.410373687744141,
      "learning_rate": 1.95885e-05,
      "loss": 3.3919,
      "step": 790700
    },
    {
      "epoch": 169.55403087478558,
      "grad_norm": 6.263164043426514,
      "learning_rate": 1.9584653846153846e-05,
      "loss": 3.442,
      "step": 790800
    },
    {
      "epoch": 169.5754716981132,
      "grad_norm": 5.9504570960998535,
      "learning_rate": 1.958080769230769e-05,
      "loss": 3.4367,
      "step": 790900
    },
    {
      "epoch": 169.59691252144083,
      "grad_norm": 6.561288356781006,
      "learning_rate": 1.9576961538461537e-05,
      "loss": 3.4315,
      "step": 791000
    },
    {
      "epoch": 169.61835334476845,
      "grad_norm": 6.285196781158447,
      "learning_rate": 1.9573115384615383e-05,
      "loss": 3.4623,
      "step": 791100
    },
    {
      "epoch": 169.63979416809605,
      "grad_norm": 6.156809329986572,
      "learning_rate": 1.956926923076923e-05,
      "loss": 3.4203,
      "step": 791200
    },
    {
      "epoch": 169.66123499142367,
      "grad_norm": 5.905623912811279,
      "learning_rate": 1.9565423076923077e-05,
      "loss": 3.4051,
      "step": 791300
    },
    {
      "epoch": 169.6826758147513,
      "grad_norm": 6.461545944213867,
      "learning_rate": 1.9561576923076923e-05,
      "loss": 3.399,
      "step": 791400
    },
    {
      "epoch": 169.70411663807892,
      "grad_norm": 6.182667255401611,
      "learning_rate": 1.9557730769230768e-05,
      "loss": 3.4027,
      "step": 791500
    },
    {
      "epoch": 169.7255574614065,
      "grad_norm": 5.9461283683776855,
      "learning_rate": 1.9553884615384617e-05,
      "loss": 3.419,
      "step": 791600
    },
    {
      "epoch": 169.74699828473413,
      "grad_norm": 5.909560680389404,
      "learning_rate": 1.9550038461538463e-05,
      "loss": 3.3943,
      "step": 791700
    },
    {
      "epoch": 169.76843910806176,
      "grad_norm": 6.072722434997559,
      "learning_rate": 1.9546192307692308e-05,
      "loss": 3.4449,
      "step": 791800
    },
    {
      "epoch": 169.78987993138938,
      "grad_norm": 5.747638702392578,
      "learning_rate": 1.9542346153846157e-05,
      "loss": 3.4077,
      "step": 791900
    },
    {
      "epoch": 169.81132075471697,
      "grad_norm": 6.026177883148193,
      "learning_rate": 1.9538500000000003e-05,
      "loss": 3.4119,
      "step": 792000
    },
    {
      "epoch": 169.8327615780446,
      "grad_norm": 6.326596260070801,
      "learning_rate": 1.9534653846153848e-05,
      "loss": 3.4426,
      "step": 792100
    },
    {
      "epoch": 169.85420240137222,
      "grad_norm": 6.382516860961914,
      "learning_rate": 1.9530807692307694e-05,
      "loss": 3.5196,
      "step": 792200
    },
    {
      "epoch": 169.87564322469981,
      "grad_norm": 6.92181921005249,
      "learning_rate": 1.952696153846154e-05,
      "loss": 3.4379,
      "step": 792300
    },
    {
      "epoch": 169.89708404802744,
      "grad_norm": 6.631533145904541,
      "learning_rate": 1.9523115384615388e-05,
      "loss": 3.4027,
      "step": 792400
    },
    {
      "epoch": 169.91852487135506,
      "grad_norm": 5.868013381958008,
      "learning_rate": 1.9519269230769234e-05,
      "loss": 3.4616,
      "step": 792500
    },
    {
      "epoch": 169.93996569468268,
      "grad_norm": 6.216586112976074,
      "learning_rate": 1.951542307692308e-05,
      "loss": 3.4035,
      "step": 792600
    },
    {
      "epoch": 169.96140651801028,
      "grad_norm": 6.080828666687012,
      "learning_rate": 1.9511576923076925e-05,
      "loss": 3.4299,
      "step": 792700
    },
    {
      "epoch": 169.9828473413379,
      "grad_norm": 6.59291934967041,
      "learning_rate": 1.950773076923077e-05,
      "loss": 3.3915,
      "step": 792800
    },
    {
      "epoch": 170.00428816466552,
      "grad_norm": 5.87285041809082,
      "learning_rate": 1.9503884615384616e-05,
      "loss": 3.3812,
      "step": 792900
    },
    {
      "epoch": 170.02572898799315,
      "grad_norm": 5.641363620758057,
      "learning_rate": 1.950003846153846e-05,
      "loss": 3.4005,
      "step": 793000
    },
    {
      "epoch": 170.04716981132074,
      "grad_norm": 5.978977680206299,
      "learning_rate": 1.949619230769231e-05,
      "loss": 3.3461,
      "step": 793100
    },
    {
      "epoch": 170.06861063464837,
      "grad_norm": 6.3458027839660645,
      "learning_rate": 1.9492346153846156e-05,
      "loss": 3.3784,
      "step": 793200
    },
    {
      "epoch": 170.090051457976,
      "grad_norm": 6.069819927215576,
      "learning_rate": 1.94885e-05,
      "loss": 3.4036,
      "step": 793300
    },
    {
      "epoch": 170.1114922813036,
      "grad_norm": 6.1058268547058105,
      "learning_rate": 1.9484653846153847e-05,
      "loss": 3.4362,
      "step": 793400
    },
    {
      "epoch": 170.1329331046312,
      "grad_norm": 5.650448799133301,
      "learning_rate": 1.9480807692307693e-05,
      "loss": 3.3803,
      "step": 793500
    },
    {
      "epoch": 170.15437392795883,
      "grad_norm": 5.987185001373291,
      "learning_rate": 1.9476961538461538e-05,
      "loss": 3.3922,
      "step": 793600
    },
    {
      "epoch": 170.17581475128645,
      "grad_norm": 6.428112030029297,
      "learning_rate": 1.9473115384615387e-05,
      "loss": 3.4595,
      "step": 793700
    },
    {
      "epoch": 170.19725557461408,
      "grad_norm": 6.159242630004883,
      "learning_rate": 1.9469269230769233e-05,
      "loss": 3.4196,
      "step": 793800
    },
    {
      "epoch": 170.21869639794167,
      "grad_norm": 5.911207675933838,
      "learning_rate": 1.9465423076923078e-05,
      "loss": 3.4543,
      "step": 793900
    },
    {
      "epoch": 170.2401372212693,
      "grad_norm": 6.276352882385254,
      "learning_rate": 1.9461576923076924e-05,
      "loss": 3.4671,
      "step": 794000
    },
    {
      "epoch": 170.26157804459692,
      "grad_norm": 5.8736419677734375,
      "learning_rate": 1.945773076923077e-05,
      "loss": 3.3805,
      "step": 794100
    },
    {
      "epoch": 170.28301886792454,
      "grad_norm": 7.438488960266113,
      "learning_rate": 1.9453884615384615e-05,
      "loss": 3.376,
      "step": 794200
    },
    {
      "epoch": 170.30445969125213,
      "grad_norm": 5.998830318450928,
      "learning_rate": 1.945003846153846e-05,
      "loss": 3.4227,
      "step": 794300
    },
    {
      "epoch": 170.32590051457976,
      "grad_norm": 6.068178176879883,
      "learning_rate": 1.944619230769231e-05,
      "loss": 3.39,
      "step": 794400
    },
    {
      "epoch": 170.34734133790738,
      "grad_norm": 5.954535484313965,
      "learning_rate": 1.9442346153846155e-05,
      "loss": 3.3961,
      "step": 794500
    },
    {
      "epoch": 170.368782161235,
      "grad_norm": 6.947443962097168,
      "learning_rate": 1.94385e-05,
      "loss": 3.3708,
      "step": 794600
    },
    {
      "epoch": 170.3902229845626,
      "grad_norm": 6.463433265686035,
      "learning_rate": 1.9434653846153846e-05,
      "loss": 3.4243,
      "step": 794700
    },
    {
      "epoch": 170.41166380789022,
      "grad_norm": 5.935719013214111,
      "learning_rate": 1.943080769230769e-05,
      "loss": 3.4489,
      "step": 794800
    },
    {
      "epoch": 170.43310463121784,
      "grad_norm": 6.129790306091309,
      "learning_rate": 1.9426961538461537e-05,
      "loss": 3.4031,
      "step": 794900
    },
    {
      "epoch": 170.45454545454547,
      "grad_norm": 6.052368640899658,
      "learning_rate": 1.9423115384615386e-05,
      "loss": 3.426,
      "step": 795000
    },
    {
      "epoch": 170.47598627787306,
      "grad_norm": 6.133359432220459,
      "learning_rate": 1.941926923076923e-05,
      "loss": 3.4406,
      "step": 795100
    },
    {
      "epoch": 170.49742710120069,
      "grad_norm": 5.893537521362305,
      "learning_rate": 1.9415423076923077e-05,
      "loss": 3.4031,
      "step": 795200
    },
    {
      "epoch": 170.5188679245283,
      "grad_norm": 6.4844536781311035,
      "learning_rate": 1.9411576923076922e-05,
      "loss": 3.4062,
      "step": 795300
    },
    {
      "epoch": 170.54030874785593,
      "grad_norm": 6.106669902801514,
      "learning_rate": 1.940773076923077e-05,
      "loss": 3.4378,
      "step": 795400
    },
    {
      "epoch": 170.56174957118353,
      "grad_norm": 5.849237442016602,
      "learning_rate": 1.9403884615384617e-05,
      "loss": 3.3902,
      "step": 795500
    },
    {
      "epoch": 170.58319039451115,
      "grad_norm": 6.152278423309326,
      "learning_rate": 1.9400038461538462e-05,
      "loss": 3.439,
      "step": 795600
    },
    {
      "epoch": 170.60463121783877,
      "grad_norm": 6.1793036460876465,
      "learning_rate": 1.9396192307692308e-05,
      "loss": 3.4417,
      "step": 795700
    },
    {
      "epoch": 170.62607204116637,
      "grad_norm": 6.298708915710449,
      "learning_rate": 1.9392346153846157e-05,
      "loss": 3.3439,
      "step": 795800
    },
    {
      "epoch": 170.647512864494,
      "grad_norm": 6.361823558807373,
      "learning_rate": 1.9388500000000002e-05,
      "loss": 3.3982,
      "step": 795900
    },
    {
      "epoch": 170.6689536878216,
      "grad_norm": 5.950939178466797,
      "learning_rate": 1.9384653846153848e-05,
      "loss": 3.3865,
      "step": 796000
    },
    {
      "epoch": 170.69039451114924,
      "grad_norm": 5.958295822143555,
      "learning_rate": 1.9380807692307694e-05,
      "loss": 3.4097,
      "step": 796100
    },
    {
      "epoch": 170.71183533447683,
      "grad_norm": 5.582867622375488,
      "learning_rate": 1.937696153846154e-05,
      "loss": 3.3422,
      "step": 796200
    },
    {
      "epoch": 170.73327615780445,
      "grad_norm": 6.35844087600708,
      "learning_rate": 1.9373115384615388e-05,
      "loss": 3.3792,
      "step": 796300
    },
    {
      "epoch": 170.75471698113208,
      "grad_norm": 6.3394670486450195,
      "learning_rate": 1.9369269230769234e-05,
      "loss": 3.3861,
      "step": 796400
    },
    {
      "epoch": 170.7761578044597,
      "grad_norm": 5.806722640991211,
      "learning_rate": 1.936542307692308e-05,
      "loss": 3.3707,
      "step": 796500
    },
    {
      "epoch": 170.7975986277873,
      "grad_norm": 6.140207290649414,
      "learning_rate": 1.9361576923076925e-05,
      "loss": 3.4062,
      "step": 796600
    },
    {
      "epoch": 170.81903945111492,
      "grad_norm": 6.44914436340332,
      "learning_rate": 1.935773076923077e-05,
      "loss": 3.3968,
      "step": 796700
    },
    {
      "epoch": 170.84048027444254,
      "grad_norm": 6.83021879196167,
      "learning_rate": 1.9353884615384616e-05,
      "loss": 3.3949,
      "step": 796800
    },
    {
      "epoch": 170.86192109777016,
      "grad_norm": 6.1507768630981445,
      "learning_rate": 1.935003846153846e-05,
      "loss": 3.4715,
      "step": 796900
    },
    {
      "epoch": 170.88336192109776,
      "grad_norm": 5.815464496612549,
      "learning_rate": 1.934619230769231e-05,
      "loss": 3.3854,
      "step": 797000
    },
    {
      "epoch": 170.90480274442538,
      "grad_norm": 5.77767276763916,
      "learning_rate": 1.9342346153846156e-05,
      "loss": 3.3825,
      "step": 797100
    },
    {
      "epoch": 170.926243567753,
      "grad_norm": 5.958355903625488,
      "learning_rate": 1.93385e-05,
      "loss": 3.4126,
      "step": 797200
    },
    {
      "epoch": 170.94768439108063,
      "grad_norm": 5.810319423675537,
      "learning_rate": 1.9334653846153847e-05,
      "loss": 3.359,
      "step": 797300
    },
    {
      "epoch": 170.96912521440822,
      "grad_norm": 6.21483850479126,
      "learning_rate": 1.9330807692307692e-05,
      "loss": 3.4029,
      "step": 797400
    },
    {
      "epoch": 170.99056603773585,
      "grad_norm": 5.802081108093262,
      "learning_rate": 1.9326961538461538e-05,
      "loss": 3.3698,
      "step": 797500
    },
    {
      "epoch": 171.01200686106347,
      "grad_norm": 6.15260648727417,
      "learning_rate": 1.9323115384615387e-05,
      "loss": 3.4467,
      "step": 797600
    },
    {
      "epoch": 171.0334476843911,
      "grad_norm": 6.4093708992004395,
      "learning_rate": 1.9319269230769232e-05,
      "loss": 3.3285,
      "step": 797700
    },
    {
      "epoch": 171.0548885077187,
      "grad_norm": 6.501572132110596,
      "learning_rate": 1.9315423076923078e-05,
      "loss": 3.323,
      "step": 797800
    },
    {
      "epoch": 171.0763293310463,
      "grad_norm": 5.991115093231201,
      "learning_rate": 1.9311576923076923e-05,
      "loss": 3.4071,
      "step": 797900
    },
    {
      "epoch": 171.09777015437393,
      "grad_norm": 5.876631736755371,
      "learning_rate": 1.930773076923077e-05,
      "loss": 3.3684,
      "step": 798000
    },
    {
      "epoch": 171.11921097770156,
      "grad_norm": 5.943883895874023,
      "learning_rate": 1.9303884615384615e-05,
      "loss": 3.3792,
      "step": 798100
    },
    {
      "epoch": 171.14065180102915,
      "grad_norm": 6.67781925201416,
      "learning_rate": 1.9300038461538463e-05,
      "loss": 3.4508,
      "step": 798200
    },
    {
      "epoch": 171.16209262435677,
      "grad_norm": 6.238274574279785,
      "learning_rate": 1.929619230769231e-05,
      "loss": 3.4108,
      "step": 798300
    },
    {
      "epoch": 171.1835334476844,
      "grad_norm": 6.257320880889893,
      "learning_rate": 1.9292346153846155e-05,
      "loss": 3.3231,
      "step": 798400
    },
    {
      "epoch": 171.20497427101202,
      "grad_norm": 5.643147945404053,
      "learning_rate": 1.92885e-05,
      "loss": 3.3975,
      "step": 798500
    },
    {
      "epoch": 171.22641509433961,
      "grad_norm": 5.886311054229736,
      "learning_rate": 1.9284653846153846e-05,
      "loss": 3.3631,
      "step": 798600
    },
    {
      "epoch": 171.24785591766724,
      "grad_norm": 7.036262035369873,
      "learning_rate": 1.928080769230769e-05,
      "loss": 3.3952,
      "step": 798700
    },
    {
      "epoch": 171.26929674099486,
      "grad_norm": 5.002589225769043,
      "learning_rate": 1.9276961538461537e-05,
      "loss": 3.3584,
      "step": 798800
    },
    {
      "epoch": 171.29073756432248,
      "grad_norm": 5.813600540161133,
      "learning_rate": 1.9273115384615386e-05,
      "loss": 3.3587,
      "step": 798900
    },
    {
      "epoch": 171.31217838765008,
      "grad_norm": 6.255825042724609,
      "learning_rate": 1.926926923076923e-05,
      "loss": 3.3897,
      "step": 799000
    },
    {
      "epoch": 171.3336192109777,
      "grad_norm": 5.862257957458496,
      "learning_rate": 1.9265423076923077e-05,
      "loss": 3.4245,
      "step": 799100
    },
    {
      "epoch": 171.35506003430532,
      "grad_norm": 5.718631744384766,
      "learning_rate": 1.9261576923076922e-05,
      "loss": 3.425,
      "step": 799200
    },
    {
      "epoch": 171.37650085763292,
      "grad_norm": 6.101504802703857,
      "learning_rate": 1.925773076923077e-05,
      "loss": 3.371,
      "step": 799300
    },
    {
      "epoch": 171.39794168096054,
      "grad_norm": 5.967925071716309,
      "learning_rate": 1.9253884615384617e-05,
      "loss": 3.3946,
      "step": 799400
    },
    {
      "epoch": 171.41938250428817,
      "grad_norm": 6.072100639343262,
      "learning_rate": 1.9250038461538462e-05,
      "loss": 3.3717,
      "step": 799500
    },
    {
      "epoch": 171.4408233276158,
      "grad_norm": 6.160484313964844,
      "learning_rate": 1.924619230769231e-05,
      "loss": 3.3993,
      "step": 799600
    },
    {
      "epoch": 171.46226415094338,
      "grad_norm": 5.962489604949951,
      "learning_rate": 1.9242346153846157e-05,
      "loss": 3.4191,
      "step": 799700
    },
    {
      "epoch": 171.483704974271,
      "grad_norm": 6.280603408813477,
      "learning_rate": 1.9238500000000002e-05,
      "loss": 3.3899,
      "step": 799800
    },
    {
      "epoch": 171.50514579759863,
      "grad_norm": 6.79205846786499,
      "learning_rate": 1.9234653846153848e-05,
      "loss": 3.4307,
      "step": 799900
    },
    {
      "epoch": 171.52658662092625,
      "grad_norm": 6.3609938621521,
      "learning_rate": 1.9230807692307693e-05,
      "loss": 3.3817,
      "step": 800000
    },
    {
      "epoch": 171.54802744425385,
      "grad_norm": 6.020482540130615,
      "learning_rate": 1.922696153846154e-05,
      "loss": 3.417,
      "step": 800100
    },
    {
      "epoch": 171.56946826758147,
      "grad_norm": 6.850059509277344,
      "learning_rate": 1.9223115384615388e-05,
      "loss": 3.3871,
      "step": 800200
    },
    {
      "epoch": 171.5909090909091,
      "grad_norm": 6.038290023803711,
      "learning_rate": 1.9219269230769233e-05,
      "loss": 3.4023,
      "step": 800300
    },
    {
      "epoch": 171.61234991423672,
      "grad_norm": 6.7418437004089355,
      "learning_rate": 1.921542307692308e-05,
      "loss": 3.4183,
      "step": 800400
    },
    {
      "epoch": 171.6337907375643,
      "grad_norm": 6.105352401733398,
      "learning_rate": 1.9211576923076924e-05,
      "loss": 3.3764,
      "step": 800500
    },
    {
      "epoch": 171.65523156089193,
      "grad_norm": 5.740312576293945,
      "learning_rate": 1.920773076923077e-05,
      "loss": 3.3671,
      "step": 800600
    },
    {
      "epoch": 171.67667238421956,
      "grad_norm": 5.784158229827881,
      "learning_rate": 1.9203884615384615e-05,
      "loss": 3.4025,
      "step": 800700
    },
    {
      "epoch": 171.69811320754718,
      "grad_norm": 5.980556011199951,
      "learning_rate": 1.9200038461538464e-05,
      "loss": 3.3832,
      "step": 800800
    },
    {
      "epoch": 171.71955403087478,
      "grad_norm": 6.5243048667907715,
      "learning_rate": 1.919619230769231e-05,
      "loss": 3.4293,
      "step": 800900
    },
    {
      "epoch": 171.7409948542024,
      "grad_norm": 6.073472023010254,
      "learning_rate": 1.9192346153846155e-05,
      "loss": 3.4209,
      "step": 801000
    },
    {
      "epoch": 171.76243567753002,
      "grad_norm": 6.601781368255615,
      "learning_rate": 1.91885e-05,
      "loss": 3.4123,
      "step": 801100
    },
    {
      "epoch": 171.78387650085764,
      "grad_norm": 6.820971488952637,
      "learning_rate": 1.9184653846153847e-05,
      "loss": 3.4151,
      "step": 801200
    },
    {
      "epoch": 171.80531732418524,
      "grad_norm": 6.650876045227051,
      "learning_rate": 1.9180807692307692e-05,
      "loss": 3.324,
      "step": 801300
    },
    {
      "epoch": 171.82675814751286,
      "grad_norm": 5.894394874572754,
      "learning_rate": 1.9176961538461538e-05,
      "loss": 3.4239,
      "step": 801400
    },
    {
      "epoch": 171.84819897084049,
      "grad_norm": 5.882047653198242,
      "learning_rate": 1.9173115384615387e-05,
      "loss": 3.3656,
      "step": 801500
    },
    {
      "epoch": 171.8696397941681,
      "grad_norm": 5.94090461730957,
      "learning_rate": 1.9169269230769232e-05,
      "loss": 3.3599,
      "step": 801600
    },
    {
      "epoch": 171.8910806174957,
      "grad_norm": 6.051925182342529,
      "learning_rate": 1.9165423076923078e-05,
      "loss": 3.4008,
      "step": 801700
    },
    {
      "epoch": 171.91252144082333,
      "grad_norm": 6.099043369293213,
      "learning_rate": 1.9161576923076923e-05,
      "loss": 3.3864,
      "step": 801800
    },
    {
      "epoch": 171.93396226415095,
      "grad_norm": 6.362381458282471,
      "learning_rate": 1.915773076923077e-05,
      "loss": 3.3532,
      "step": 801900
    },
    {
      "epoch": 171.95540308747857,
      "grad_norm": 6.093866348266602,
      "learning_rate": 1.9153884615384614e-05,
      "loss": 3.4117,
      "step": 802000
    },
    {
      "epoch": 171.97684391080617,
      "grad_norm": 6.403669834136963,
      "learning_rate": 1.9150038461538463e-05,
      "loss": 3.368,
      "step": 802100
    },
    {
      "epoch": 171.9982847341338,
      "grad_norm": 6.0625457763671875,
      "learning_rate": 1.914619230769231e-05,
      "loss": 3.3943,
      "step": 802200
    },
    {
      "epoch": 172.0197255574614,
      "grad_norm": 5.79284143447876,
      "learning_rate": 1.9142346153846154e-05,
      "loss": 3.3656,
      "step": 802300
    },
    {
      "epoch": 172.04116638078904,
      "grad_norm": 6.327040672302246,
      "learning_rate": 1.91385e-05,
      "loss": 3.3392,
      "step": 802400
    },
    {
      "epoch": 172.06260720411663,
      "grad_norm": 5.985842704772949,
      "learning_rate": 1.9134653846153845e-05,
      "loss": 3.4098,
      "step": 802500
    },
    {
      "epoch": 172.08404802744425,
      "grad_norm": 6.425050735473633,
      "learning_rate": 1.913080769230769e-05,
      "loss": 3.4133,
      "step": 802600
    },
    {
      "epoch": 172.10548885077188,
      "grad_norm": 6.2947211265563965,
      "learning_rate": 1.9126961538461536e-05,
      "loss": 3.2755,
      "step": 802700
    },
    {
      "epoch": 172.12692967409947,
      "grad_norm": 6.0635552406311035,
      "learning_rate": 1.9123115384615385e-05,
      "loss": 3.3813,
      "step": 802800
    },
    {
      "epoch": 172.1483704974271,
      "grad_norm": 6.067217826843262,
      "learning_rate": 1.911926923076923e-05,
      "loss": 3.3471,
      "step": 802900
    },
    {
      "epoch": 172.16981132075472,
      "grad_norm": 6.171365261077881,
      "learning_rate": 1.9115423076923076e-05,
      "loss": 3.3482,
      "step": 803000
    },
    {
      "epoch": 172.19125214408234,
      "grad_norm": 5.974328517913818,
      "learning_rate": 1.9111576923076925e-05,
      "loss": 3.3628,
      "step": 803100
    },
    {
      "epoch": 172.21269296740994,
      "grad_norm": 6.485208988189697,
      "learning_rate": 1.910773076923077e-05,
      "loss": 3.3507,
      "step": 803200
    },
    {
      "epoch": 172.23413379073756,
      "grad_norm": 6.3701653480529785,
      "learning_rate": 1.9103884615384616e-05,
      "loss": 3.4263,
      "step": 803300
    },
    {
      "epoch": 172.25557461406518,
      "grad_norm": 6.293495178222656,
      "learning_rate": 1.9100038461538462e-05,
      "loss": 3.3823,
      "step": 803400
    },
    {
      "epoch": 172.2770154373928,
      "grad_norm": 6.124203205108643,
      "learning_rate": 1.909619230769231e-05,
      "loss": 3.3628,
      "step": 803500
    },
    {
      "epoch": 172.2984562607204,
      "grad_norm": 6.546017646789551,
      "learning_rate": 1.9092346153846156e-05,
      "loss": 3.343,
      "step": 803600
    },
    {
      "epoch": 172.31989708404802,
      "grad_norm": 6.187298774719238,
      "learning_rate": 1.9088500000000002e-05,
      "loss": 3.3963,
      "step": 803700
    },
    {
      "epoch": 172.34133790737565,
      "grad_norm": 5.964930057525635,
      "learning_rate": 1.9084653846153848e-05,
      "loss": 3.4312,
      "step": 803800
    },
    {
      "epoch": 172.36277873070327,
      "grad_norm": 5.568482875823975,
      "learning_rate": 1.9080807692307693e-05,
      "loss": 3.3058,
      "step": 803900
    },
    {
      "epoch": 172.38421955403086,
      "grad_norm": 5.844305038452148,
      "learning_rate": 1.907696153846154e-05,
      "loss": 3.3409,
      "step": 804000
    },
    {
      "epoch": 172.4056603773585,
      "grad_norm": 6.017956733703613,
      "learning_rate": 1.9073115384615388e-05,
      "loss": 3.3588,
      "step": 804100
    },
    {
      "epoch": 172.4271012006861,
      "grad_norm": 5.988408088684082,
      "learning_rate": 1.9069269230769233e-05,
      "loss": 3.4592,
      "step": 804200
    },
    {
      "epoch": 172.44854202401373,
      "grad_norm": 6.900913715362549,
      "learning_rate": 1.906542307692308e-05,
      "loss": 3.324,
      "step": 804300
    },
    {
      "epoch": 172.46998284734133,
      "grad_norm": 5.432024955749512,
      "learning_rate": 1.9061576923076924e-05,
      "loss": 3.3744,
      "step": 804400
    },
    {
      "epoch": 172.49142367066895,
      "grad_norm": 6.041919708251953,
      "learning_rate": 1.905773076923077e-05,
      "loss": 3.3972,
      "step": 804500
    },
    {
      "epoch": 172.51286449399657,
      "grad_norm": 6.018036842346191,
      "learning_rate": 1.9053884615384615e-05,
      "loss": 3.3411,
      "step": 804600
    },
    {
      "epoch": 172.5343053173242,
      "grad_norm": 6.242475986480713,
      "learning_rate": 1.9050038461538464e-05,
      "loss": 3.3956,
      "step": 804700
    },
    {
      "epoch": 172.5557461406518,
      "grad_norm": 6.216066360473633,
      "learning_rate": 1.904619230769231e-05,
      "loss": 3.3748,
      "step": 804800
    },
    {
      "epoch": 172.57718696397941,
      "grad_norm": 6.110999584197998,
      "learning_rate": 1.9042346153846155e-05,
      "loss": 3.4256,
      "step": 804900
    },
    {
      "epoch": 172.59862778730704,
      "grad_norm": 5.9816718101501465,
      "learning_rate": 1.90385e-05,
      "loss": 3.3469,
      "step": 805000
    },
    {
      "epoch": 172.62006861063466,
      "grad_norm": 6.049947261810303,
      "learning_rate": 1.9034653846153846e-05,
      "loss": 3.4543,
      "step": 805100
    },
    {
      "epoch": 172.64150943396226,
      "grad_norm": 6.158288478851318,
      "learning_rate": 1.9030807692307692e-05,
      "loss": 3.391,
      "step": 805200
    },
    {
      "epoch": 172.66295025728988,
      "grad_norm": 5.868420600891113,
      "learning_rate": 1.902696153846154e-05,
      "loss": 3.3945,
      "step": 805300
    },
    {
      "epoch": 172.6843910806175,
      "grad_norm": 6.130550861358643,
      "learning_rate": 1.9023115384615386e-05,
      "loss": 3.3681,
      "step": 805400
    },
    {
      "epoch": 172.70583190394512,
      "grad_norm": 5.569737911224365,
      "learning_rate": 1.9019269230769232e-05,
      "loss": 3.3631,
      "step": 805500
    },
    {
      "epoch": 172.72727272727272,
      "grad_norm": 6.120644569396973,
      "learning_rate": 1.9015423076923077e-05,
      "loss": 3.3954,
      "step": 805600
    },
    {
      "epoch": 172.74871355060034,
      "grad_norm": 5.913097858428955,
      "learning_rate": 1.9011576923076923e-05,
      "loss": 3.401,
      "step": 805700
    },
    {
      "epoch": 172.77015437392797,
      "grad_norm": 5.953763008117676,
      "learning_rate": 1.900773076923077e-05,
      "loss": 3.3417,
      "step": 805800
    },
    {
      "epoch": 172.79159519725556,
      "grad_norm": 5.810882568359375,
      "learning_rate": 1.9003884615384614e-05,
      "loss": 3.3788,
      "step": 805900
    },
    {
      "epoch": 172.81303602058318,
      "grad_norm": 6.678932189941406,
      "learning_rate": 1.9000038461538463e-05,
      "loss": 3.3451,
      "step": 806000
    },
    {
      "epoch": 172.8344768439108,
      "grad_norm": 5.696866989135742,
      "learning_rate": 1.899619230769231e-05,
      "loss": 3.3984,
      "step": 806100
    },
    {
      "epoch": 172.85591766723843,
      "grad_norm": 6.443333625793457,
      "learning_rate": 1.8992346153846154e-05,
      "loss": 3.3968,
      "step": 806200
    },
    {
      "epoch": 172.87735849056602,
      "grad_norm": 6.545702934265137,
      "learning_rate": 1.89885e-05,
      "loss": 3.3484,
      "step": 806300
    },
    {
      "epoch": 172.89879931389365,
      "grad_norm": 5.836438179016113,
      "learning_rate": 1.8984653846153845e-05,
      "loss": 3.3868,
      "step": 806400
    },
    {
      "epoch": 172.92024013722127,
      "grad_norm": 5.625497817993164,
      "learning_rate": 1.898080769230769e-05,
      "loss": 3.3788,
      "step": 806500
    },
    {
      "epoch": 172.9416809605489,
      "grad_norm": 6.222513675689697,
      "learning_rate": 1.897696153846154e-05,
      "loss": 3.3336,
      "step": 806600
    },
    {
      "epoch": 172.9631217838765,
      "grad_norm": 5.315945148468018,
      "learning_rate": 1.8973115384615385e-05,
      "loss": 3.3621,
      "step": 806700
    },
    {
      "epoch": 172.9845626072041,
      "grad_norm": 5.9534783363342285,
      "learning_rate": 1.896926923076923e-05,
      "loss": 3.4316,
      "step": 806800
    },
    {
      "epoch": 173.00600343053173,
      "grad_norm": 5.894905090332031,
      "learning_rate": 1.8965423076923076e-05,
      "loss": 3.3348,
      "step": 806900
    },
    {
      "epoch": 173.02744425385936,
      "grad_norm": 5.944998264312744,
      "learning_rate": 1.8961576923076925e-05,
      "loss": 3.2828,
      "step": 807000
    },
    {
      "epoch": 173.04888507718695,
      "grad_norm": 6.07935094833374,
      "learning_rate": 1.895773076923077e-05,
      "loss": 3.3267,
      "step": 807100
    },
    {
      "epoch": 173.07032590051458,
      "grad_norm": 6.171850681304932,
      "learning_rate": 1.8953884615384616e-05,
      "loss": 3.3291,
      "step": 807200
    },
    {
      "epoch": 173.0917667238422,
      "grad_norm": 5.976039886474609,
      "learning_rate": 1.8950038461538465e-05,
      "loss": 3.3583,
      "step": 807300
    },
    {
      "epoch": 173.11320754716982,
      "grad_norm": 5.739546775817871,
      "learning_rate": 1.894619230769231e-05,
      "loss": 3.3448,
      "step": 807400
    },
    {
      "epoch": 173.13464837049742,
      "grad_norm": 5.776181221008301,
      "learning_rate": 1.8942346153846156e-05,
      "loss": 3.3269,
      "step": 807500
    },
    {
      "epoch": 173.15608919382504,
      "grad_norm": 6.04610013961792,
      "learning_rate": 1.8938500000000002e-05,
      "loss": 3.3311,
      "step": 807600
    },
    {
      "epoch": 173.17753001715266,
      "grad_norm": 6.383821964263916,
      "learning_rate": 1.8934653846153847e-05,
      "loss": 3.3674,
      "step": 807700
    },
    {
      "epoch": 173.19897084048029,
      "grad_norm": 6.1868720054626465,
      "learning_rate": 1.8930807692307693e-05,
      "loss": 3.3628,
      "step": 807800
    },
    {
      "epoch": 173.22041166380788,
      "grad_norm": 6.518783092498779,
      "learning_rate": 1.8926961538461542e-05,
      "loss": 3.3509,
      "step": 807900
    },
    {
      "epoch": 173.2418524871355,
      "grad_norm": 5.728198528289795,
      "learning_rate": 1.8923115384615387e-05,
      "loss": 3.3505,
      "step": 808000
    },
    {
      "epoch": 173.26329331046313,
      "grad_norm": 6.263362407684326,
      "learning_rate": 1.8919269230769233e-05,
      "loss": 3.3525,
      "step": 808100
    },
    {
      "epoch": 173.28473413379075,
      "grad_norm": 5.3358659744262695,
      "learning_rate": 1.891542307692308e-05,
      "loss": 3.3408,
      "step": 808200
    },
    {
      "epoch": 173.30617495711834,
      "grad_norm": 5.97701358795166,
      "learning_rate": 1.8911576923076924e-05,
      "loss": 3.3914,
      "step": 808300
    },
    {
      "epoch": 173.32761578044597,
      "grad_norm": 6.3945159912109375,
      "learning_rate": 1.890773076923077e-05,
      "loss": 3.3567,
      "step": 808400
    },
    {
      "epoch": 173.3490566037736,
      "grad_norm": 5.537901401519775,
      "learning_rate": 1.8903884615384615e-05,
      "loss": 3.3199,
      "step": 808500
    },
    {
      "epoch": 173.3704974271012,
      "grad_norm": 6.075817584991455,
      "learning_rate": 1.8900038461538464e-05,
      "loss": 3.4157,
      "step": 808600
    },
    {
      "epoch": 173.3919382504288,
      "grad_norm": 6.0986456871032715,
      "learning_rate": 1.889619230769231e-05,
      "loss": 3.4208,
      "step": 808700
    },
    {
      "epoch": 173.41337907375643,
      "grad_norm": 6.208083152770996,
      "learning_rate": 1.8892346153846155e-05,
      "loss": 3.3245,
      "step": 808800
    },
    {
      "epoch": 173.43481989708405,
      "grad_norm": 5.849940299987793,
      "learning_rate": 1.88885e-05,
      "loss": 3.3476,
      "step": 808900
    },
    {
      "epoch": 173.45626072041168,
      "grad_norm": 5.665141582489014,
      "learning_rate": 1.8884653846153846e-05,
      "loss": 3.4028,
      "step": 809000
    },
    {
      "epoch": 173.47770154373927,
      "grad_norm": 6.2396626472473145,
      "learning_rate": 1.888080769230769e-05,
      "loss": 3.3538,
      "step": 809100
    },
    {
      "epoch": 173.4991423670669,
      "grad_norm": 6.547184467315674,
      "learning_rate": 1.887696153846154e-05,
      "loss": 3.3278,
      "step": 809200
    },
    {
      "epoch": 173.52058319039452,
      "grad_norm": 5.8462910652160645,
      "learning_rate": 1.8873115384615386e-05,
      "loss": 3.3286,
      "step": 809300
    },
    {
      "epoch": 173.5420240137221,
      "grad_norm": 6.9156365394592285,
      "learning_rate": 1.886926923076923e-05,
      "loss": 3.3486,
      "step": 809400
    },
    {
      "epoch": 173.56346483704974,
      "grad_norm": 6.152253150939941,
      "learning_rate": 1.8865423076923077e-05,
      "loss": 3.3524,
      "step": 809500
    },
    {
      "epoch": 173.58490566037736,
      "grad_norm": 6.155186653137207,
      "learning_rate": 1.8861576923076923e-05,
      "loss": 3.3903,
      "step": 809600
    },
    {
      "epoch": 173.60634648370498,
      "grad_norm": 5.970670223236084,
      "learning_rate": 1.8857730769230768e-05,
      "loss": 3.3561,
      "step": 809700
    },
    {
      "epoch": 173.62778730703258,
      "grad_norm": 5.484672546386719,
      "learning_rate": 1.8853884615384614e-05,
      "loss": 3.4091,
      "step": 809800
    },
    {
      "epoch": 173.6492281303602,
      "grad_norm": 5.5259785652160645,
      "learning_rate": 1.8850038461538463e-05,
      "loss": 3.4091,
      "step": 809900
    },
    {
      "epoch": 173.67066895368782,
      "grad_norm": 6.250857830047607,
      "learning_rate": 1.8846192307692308e-05,
      "loss": 3.3417,
      "step": 810000
    },
    {
      "epoch": 173.69210977701545,
      "grad_norm": 5.971270561218262,
      "learning_rate": 1.8842346153846154e-05,
      "loss": 3.3784,
      "step": 810100
    },
    {
      "epoch": 173.71355060034304,
      "grad_norm": 5.887545585632324,
      "learning_rate": 1.88385e-05,
      "loss": 3.3736,
      "step": 810200
    },
    {
      "epoch": 173.73499142367066,
      "grad_norm": 5.893820285797119,
      "learning_rate": 1.8834653846153845e-05,
      "loss": 3.352,
      "step": 810300
    },
    {
      "epoch": 173.7564322469983,
      "grad_norm": 5.7830681800842285,
      "learning_rate": 1.883080769230769e-05,
      "loss": 3.3605,
      "step": 810400
    },
    {
      "epoch": 173.7778730703259,
      "grad_norm": 5.800474643707275,
      "learning_rate": 1.882696153846154e-05,
      "loss": 3.3943,
      "step": 810500
    },
    {
      "epoch": 173.7993138936535,
      "grad_norm": 6.000798225402832,
      "learning_rate": 1.8823115384615385e-05,
      "loss": 3.3719,
      "step": 810600
    },
    {
      "epoch": 173.82075471698113,
      "grad_norm": 5.9531965255737305,
      "learning_rate": 1.881926923076923e-05,
      "loss": 3.403,
      "step": 810700
    },
    {
      "epoch": 173.84219554030875,
      "grad_norm": 6.011720657348633,
      "learning_rate": 1.881542307692308e-05,
      "loss": 3.3557,
      "step": 810800
    },
    {
      "epoch": 173.86363636363637,
      "grad_norm": 6.149764060974121,
      "learning_rate": 1.8811576923076925e-05,
      "loss": 3.3982,
      "step": 810900
    },
    {
      "epoch": 173.88507718696397,
      "grad_norm": 6.54710054397583,
      "learning_rate": 1.880773076923077e-05,
      "loss": 3.4266,
      "step": 811000
    },
    {
      "epoch": 173.9065180102916,
      "grad_norm": 6.329131603240967,
      "learning_rate": 1.880388461538462e-05,
      "loss": 3.325,
      "step": 811100
    },
    {
      "epoch": 173.92795883361921,
      "grad_norm": 6.1673736572265625,
      "learning_rate": 1.8800038461538465e-05,
      "loss": 3.3271,
      "step": 811200
    },
    {
      "epoch": 173.94939965694684,
      "grad_norm": 6.080141067504883,
      "learning_rate": 1.879619230769231e-05,
      "loss": 3.3492,
      "step": 811300
    },
    {
      "epoch": 173.97084048027443,
      "grad_norm": 5.638858318328857,
      "learning_rate": 1.8792346153846156e-05,
      "loss": 3.3485,
      "step": 811400
    },
    {
      "epoch": 173.99228130360206,
      "grad_norm": 6.132531642913818,
      "learning_rate": 1.87885e-05,
      "loss": 3.4037,
      "step": 811500
    },
    {
      "epoch": 174.01372212692968,
      "grad_norm": 5.604452133178711,
      "learning_rate": 1.8784653846153847e-05,
      "loss": 3.3085,
      "step": 811600
    },
    {
      "epoch": 174.0351629502573,
      "grad_norm": 6.3368706703186035,
      "learning_rate": 1.8780807692307693e-05,
      "loss": 3.3449,
      "step": 811700
    },
    {
      "epoch": 174.0566037735849,
      "grad_norm": 5.951077461242676,
      "learning_rate": 1.877696153846154e-05,
      "loss": 3.296,
      "step": 811800
    },
    {
      "epoch": 174.07804459691252,
      "grad_norm": 6.106147289276123,
      "learning_rate": 1.8773115384615387e-05,
      "loss": 3.2953,
      "step": 811900
    },
    {
      "epoch": 174.09948542024014,
      "grad_norm": 5.644272327423096,
      "learning_rate": 1.8769269230769233e-05,
      "loss": 3.3168,
      "step": 812000
    },
    {
      "epoch": 174.12092624356777,
      "grad_norm": 6.055395126342773,
      "learning_rate": 1.8765423076923078e-05,
      "loss": 3.3473,
      "step": 812100
    },
    {
      "epoch": 174.14236706689536,
      "grad_norm": 6.288458347320557,
      "learning_rate": 1.8761576923076924e-05,
      "loss": 3.3329,
      "step": 812200
    },
    {
      "epoch": 174.16380789022298,
      "grad_norm": 6.164731025695801,
      "learning_rate": 1.875773076923077e-05,
      "loss": 3.3521,
      "step": 812300
    },
    {
      "epoch": 174.1852487135506,
      "grad_norm": 5.606257915496826,
      "learning_rate": 1.8753884615384618e-05,
      "loss": 3.3513,
      "step": 812400
    },
    {
      "epoch": 174.20668953687823,
      "grad_norm": 6.3496904373168945,
      "learning_rate": 1.8750038461538464e-05,
      "loss": 3.3692,
      "step": 812500
    },
    {
      "epoch": 174.22813036020582,
      "grad_norm": 5.883225440979004,
      "learning_rate": 1.874619230769231e-05,
      "loss": 3.3669,
      "step": 812600
    },
    {
      "epoch": 174.24957118353345,
      "grad_norm": 6.108564853668213,
      "learning_rate": 1.8742346153846155e-05,
      "loss": 3.311,
      "step": 812700
    },
    {
      "epoch": 174.27101200686107,
      "grad_norm": 5.771904468536377,
      "learning_rate": 1.87385e-05,
      "loss": 3.343,
      "step": 812800
    },
    {
      "epoch": 174.29245283018867,
      "grad_norm": 6.00883674621582,
      "learning_rate": 1.8734653846153846e-05,
      "loss": 3.4051,
      "step": 812900
    },
    {
      "epoch": 174.3138936535163,
      "grad_norm": 5.876998424530029,
      "learning_rate": 1.873080769230769e-05,
      "loss": 3.3246,
      "step": 813000
    },
    {
      "epoch": 174.3353344768439,
      "grad_norm": 6.418129920959473,
      "learning_rate": 1.872696153846154e-05,
      "loss": 3.2953,
      "step": 813100
    },
    {
      "epoch": 174.35677530017153,
      "grad_norm": 6.091102600097656,
      "learning_rate": 1.8723115384615386e-05,
      "loss": 3.3753,
      "step": 813200
    },
    {
      "epoch": 174.37821612349913,
      "grad_norm": 5.547195911407471,
      "learning_rate": 1.871926923076923e-05,
      "loss": 3.3411,
      "step": 813300
    },
    {
      "epoch": 174.39965694682675,
      "grad_norm": 6.338653564453125,
      "learning_rate": 1.8715423076923077e-05,
      "loss": 3.3286,
      "step": 813400
    },
    {
      "epoch": 174.42109777015438,
      "grad_norm": 6.375935077667236,
      "learning_rate": 1.8711576923076922e-05,
      "loss": 3.3699,
      "step": 813500
    },
    {
      "epoch": 174.442538593482,
      "grad_norm": 5.810459613800049,
      "learning_rate": 1.8707730769230768e-05,
      "loss": 3.3443,
      "step": 813600
    },
    {
      "epoch": 174.4639794168096,
      "grad_norm": 5.820827007293701,
      "learning_rate": 1.8703884615384617e-05,
      "loss": 3.3421,
      "step": 813700
    },
    {
      "epoch": 174.48542024013722,
      "grad_norm": 6.166592121124268,
      "learning_rate": 1.8700038461538462e-05,
      "loss": 3.3586,
      "step": 813800
    },
    {
      "epoch": 174.50686106346484,
      "grad_norm": 6.687958717346191,
      "learning_rate": 1.8696192307692308e-05,
      "loss": 3.3979,
      "step": 813900
    },
    {
      "epoch": 174.52830188679246,
      "grad_norm": 6.712215900421143,
      "learning_rate": 1.8692346153846153e-05,
      "loss": 3.3304,
      "step": 814000
    },
    {
      "epoch": 174.54974271012006,
      "grad_norm": 5.986889362335205,
      "learning_rate": 1.86885e-05,
      "loss": 3.3447,
      "step": 814100
    },
    {
      "epoch": 174.57118353344768,
      "grad_norm": 5.910480499267578,
      "learning_rate": 1.8684653846153845e-05,
      "loss": 3.4195,
      "step": 814200
    },
    {
      "epoch": 174.5926243567753,
      "grad_norm": 5.596201419830322,
      "learning_rate": 1.8680807692307693e-05,
      "loss": 3.354,
      "step": 814300
    },
    {
      "epoch": 174.61406518010293,
      "grad_norm": 5.907419681549072,
      "learning_rate": 1.867696153846154e-05,
      "loss": 3.3563,
      "step": 814400
    },
    {
      "epoch": 174.63550600343052,
      "grad_norm": 5.677302837371826,
      "learning_rate": 1.8673115384615385e-05,
      "loss": 3.3637,
      "step": 814500
    },
    {
      "epoch": 174.65694682675814,
      "grad_norm": 5.971920967102051,
      "learning_rate": 1.8669269230769234e-05,
      "loss": 3.3482,
      "step": 814600
    },
    {
      "epoch": 174.67838765008577,
      "grad_norm": 6.0608038902282715,
      "learning_rate": 1.866542307692308e-05,
      "loss": 3.3316,
      "step": 814700
    },
    {
      "epoch": 174.6998284734134,
      "grad_norm": 6.0993499755859375,
      "learning_rate": 1.8661576923076925e-05,
      "loss": 3.2918,
      "step": 814800
    },
    {
      "epoch": 174.72126929674099,
      "grad_norm": 6.6587724685668945,
      "learning_rate": 1.865773076923077e-05,
      "loss": 3.3605,
      "step": 814900
    },
    {
      "epoch": 174.7427101200686,
      "grad_norm": 6.132938861846924,
      "learning_rate": 1.865388461538462e-05,
      "loss": 3.3853,
      "step": 815000
    },
    {
      "epoch": 174.76415094339623,
      "grad_norm": 5.722092151641846,
      "learning_rate": 1.8650038461538465e-05,
      "loss": 3.3354,
      "step": 815100
    },
    {
      "epoch": 174.78559176672385,
      "grad_norm": 5.9589524269104,
      "learning_rate": 1.864619230769231e-05,
      "loss": 3.3683,
      "step": 815200
    },
    {
      "epoch": 174.80703259005145,
      "grad_norm": 6.242835998535156,
      "learning_rate": 1.8642346153846156e-05,
      "loss": 3.3709,
      "step": 815300
    },
    {
      "epoch": 174.82847341337907,
      "grad_norm": 6.036092758178711,
      "learning_rate": 1.86385e-05,
      "loss": 3.3359,
      "step": 815400
    },
    {
      "epoch": 174.8499142367067,
      "grad_norm": 6.100151062011719,
      "learning_rate": 1.8634653846153847e-05,
      "loss": 3.3758,
      "step": 815500
    },
    {
      "epoch": 174.87135506003432,
      "grad_norm": 6.08457088470459,
      "learning_rate": 1.8630807692307692e-05,
      "loss": 3.3493,
      "step": 815600
    },
    {
      "epoch": 174.8927958833619,
      "grad_norm": 6.303419589996338,
      "learning_rate": 1.862696153846154e-05,
      "loss": 3.341,
      "step": 815700
    },
    {
      "epoch": 174.91423670668954,
      "grad_norm": 6.333103656768799,
      "learning_rate": 1.8623115384615387e-05,
      "loss": 3.3685,
      "step": 815800
    },
    {
      "epoch": 174.93567753001716,
      "grad_norm": 5.710964202880859,
      "learning_rate": 1.8619269230769232e-05,
      "loss": 3.3128,
      "step": 815900
    },
    {
      "epoch": 174.95711835334478,
      "grad_norm": 6.0994367599487305,
      "learning_rate": 1.8615423076923078e-05,
      "loss": 3.3698,
      "step": 816000
    },
    {
      "epoch": 174.97855917667238,
      "grad_norm": 5.724812984466553,
      "learning_rate": 1.8611576923076923e-05,
      "loss": 3.337,
      "step": 816100
    },
    {
      "epoch": 175.0,
      "grad_norm": 8.69240951538086,
      "learning_rate": 1.860773076923077e-05,
      "loss": 3.3356,
      "step": 816200
    },
    {
      "epoch": 175.02144082332762,
      "grad_norm": 6.065133094787598,
      "learning_rate": 1.8603884615384618e-05,
      "loss": 3.2736,
      "step": 816300
    },
    {
      "epoch": 175.04288164665522,
      "grad_norm": 7.183989524841309,
      "learning_rate": 1.8600038461538463e-05,
      "loss": 3.3371,
      "step": 816400
    },
    {
      "epoch": 175.06432246998284,
      "grad_norm": 5.681873798370361,
      "learning_rate": 1.859619230769231e-05,
      "loss": 3.3546,
      "step": 816500
    },
    {
      "epoch": 175.08576329331046,
      "grad_norm": 5.498150825500488,
      "learning_rate": 1.8592346153846154e-05,
      "loss": 3.3097,
      "step": 816600
    },
    {
      "epoch": 175.1072041166381,
      "grad_norm": 6.096080303192139,
      "learning_rate": 1.85885e-05,
      "loss": 3.2993,
      "step": 816700
    },
    {
      "epoch": 175.12864493996568,
      "grad_norm": 5.937802791595459,
      "learning_rate": 1.8584653846153846e-05,
      "loss": 3.3597,
      "step": 816800
    },
    {
      "epoch": 175.1500857632933,
      "grad_norm": 5.8517632484436035,
      "learning_rate": 1.8580807692307694e-05,
      "loss": 3.352,
      "step": 816900
    },
    {
      "epoch": 175.17152658662093,
      "grad_norm": 6.264163017272949,
      "learning_rate": 1.857696153846154e-05,
      "loss": 3.342,
      "step": 817000
    },
    {
      "epoch": 175.19296740994855,
      "grad_norm": 6.169322967529297,
      "learning_rate": 1.8573115384615386e-05,
      "loss": 3.3676,
      "step": 817100
    },
    {
      "epoch": 175.21440823327615,
      "grad_norm": 6.125509738922119,
      "learning_rate": 1.856926923076923e-05,
      "loss": 3.3181,
      "step": 817200
    },
    {
      "epoch": 175.23584905660377,
      "grad_norm": 6.597287178039551,
      "learning_rate": 1.8565423076923077e-05,
      "loss": 3.3455,
      "step": 817300
    },
    {
      "epoch": 175.2572898799314,
      "grad_norm": 6.4986701011657715,
      "learning_rate": 1.8561576923076922e-05,
      "loss": 3.3214,
      "step": 817400
    },
    {
      "epoch": 175.27873070325901,
      "grad_norm": 6.215001583099365,
      "learning_rate": 1.8557730769230768e-05,
      "loss": 3.3704,
      "step": 817500
    },
    {
      "epoch": 175.3001715265866,
      "grad_norm": 5.332103252410889,
      "learning_rate": 1.8553884615384617e-05,
      "loss": 3.3262,
      "step": 817600
    },
    {
      "epoch": 175.32161234991423,
      "grad_norm": 6.082229137420654,
      "learning_rate": 1.8550038461538462e-05,
      "loss": 3.3075,
      "step": 817700
    },
    {
      "epoch": 175.34305317324186,
      "grad_norm": 6.942852020263672,
      "learning_rate": 1.8546192307692308e-05,
      "loss": 3.3018,
      "step": 817800
    },
    {
      "epoch": 175.36449399656948,
      "grad_norm": 7.044008255004883,
      "learning_rate": 1.8542346153846153e-05,
      "loss": 3.3522,
      "step": 817900
    },
    {
      "epoch": 175.38593481989707,
      "grad_norm": 5.6970086097717285,
      "learning_rate": 1.85385e-05,
      "loss": 3.3457,
      "step": 818000
    },
    {
      "epoch": 175.4073756432247,
      "grad_norm": 6.661842346191406,
      "learning_rate": 1.8534653846153848e-05,
      "loss": 3.3349,
      "step": 818100
    },
    {
      "epoch": 175.42881646655232,
      "grad_norm": 6.267511367797852,
      "learning_rate": 1.8530807692307693e-05,
      "loss": 3.3055,
      "step": 818200
    },
    {
      "epoch": 175.45025728987994,
      "grad_norm": 6.3927388191223145,
      "learning_rate": 1.852696153846154e-05,
      "loss": 3.3377,
      "step": 818300
    },
    {
      "epoch": 175.47169811320754,
      "grad_norm": 6.177441120147705,
      "learning_rate": 1.8523115384615384e-05,
      "loss": 3.3182,
      "step": 818400
    },
    {
      "epoch": 175.49313893653516,
      "grad_norm": 7.198418140411377,
      "learning_rate": 1.8519269230769233e-05,
      "loss": 3.3711,
      "step": 818500
    },
    {
      "epoch": 175.51457975986278,
      "grad_norm": 6.315569877624512,
      "learning_rate": 1.851542307692308e-05,
      "loss": 3.2887,
      "step": 818600
    },
    {
      "epoch": 175.5360205831904,
      "grad_norm": 5.216142177581787,
      "learning_rate": 1.8511576923076924e-05,
      "loss": 3.3173,
      "step": 818700
    },
    {
      "epoch": 175.557461406518,
      "grad_norm": 5.736163139343262,
      "learning_rate": 1.850773076923077e-05,
      "loss": 3.3468,
      "step": 818800
    },
    {
      "epoch": 175.57890222984562,
      "grad_norm": 6.065665245056152,
      "learning_rate": 1.850388461538462e-05,
      "loss": 3.3714,
      "step": 818900
    },
    {
      "epoch": 175.60034305317325,
      "grad_norm": 6.302549362182617,
      "learning_rate": 1.8500038461538464e-05,
      "loss": 3.3394,
      "step": 819000
    },
    {
      "epoch": 175.62178387650087,
      "grad_norm": 6.303348541259766,
      "learning_rate": 1.849619230769231e-05,
      "loss": 3.3199,
      "step": 819100
    },
    {
      "epoch": 175.64322469982847,
      "grad_norm": 6.124454021453857,
      "learning_rate": 1.8492346153846155e-05,
      "loss": 3.3339,
      "step": 819200
    },
    {
      "epoch": 175.6646655231561,
      "grad_norm": 6.006387710571289,
      "learning_rate": 1.84885e-05,
      "loss": 3.3053,
      "step": 819300
    },
    {
      "epoch": 175.6861063464837,
      "grad_norm": 6.348896503448486,
      "learning_rate": 1.8484653846153846e-05,
      "loss": 3.3578,
      "step": 819400
    },
    {
      "epoch": 175.70754716981133,
      "grad_norm": 5.962798595428467,
      "learning_rate": 1.8480807692307695e-05,
      "loss": 3.3766,
      "step": 819500
    },
    {
      "epoch": 175.72898799313893,
      "grad_norm": 5.91187047958374,
      "learning_rate": 1.847696153846154e-05,
      "loss": 3.287,
      "step": 819600
    },
    {
      "epoch": 175.75042881646655,
      "grad_norm": 6.141808986663818,
      "learning_rate": 1.8473115384615386e-05,
      "loss": 3.3475,
      "step": 819700
    },
    {
      "epoch": 175.77186963979418,
      "grad_norm": 6.444014549255371,
      "learning_rate": 1.8469269230769232e-05,
      "loss": 3.3452,
      "step": 819800
    },
    {
      "epoch": 175.79331046312177,
      "grad_norm": 6.220975875854492,
      "learning_rate": 1.8465423076923078e-05,
      "loss": 3.368,
      "step": 819900
    },
    {
      "epoch": 175.8147512864494,
      "grad_norm": 4.865876197814941,
      "learning_rate": 1.8461576923076923e-05,
      "loss": 3.3934,
      "step": 820000
    },
    {
      "epoch": 175.83619210977702,
      "grad_norm": 6.482146263122559,
      "learning_rate": 1.845773076923077e-05,
      "loss": 3.3272,
      "step": 820100
    },
    {
      "epoch": 175.85763293310464,
      "grad_norm": 5.5884270668029785,
      "learning_rate": 1.8453884615384618e-05,
      "loss": 3.2736,
      "step": 820200
    },
    {
      "epoch": 175.87907375643223,
      "grad_norm": 6.957272052764893,
      "learning_rate": 1.8450038461538463e-05,
      "loss": 3.3556,
      "step": 820300
    },
    {
      "epoch": 175.90051457975986,
      "grad_norm": 6.665835380554199,
      "learning_rate": 1.844619230769231e-05,
      "loss": 3.2733,
      "step": 820400
    },
    {
      "epoch": 175.92195540308748,
      "grad_norm": 6.249454498291016,
      "learning_rate": 1.8442346153846154e-05,
      "loss": 3.3904,
      "step": 820500
    },
    {
      "epoch": 175.9433962264151,
      "grad_norm": 6.393552303314209,
      "learning_rate": 1.84385e-05,
      "loss": 3.3584,
      "step": 820600
    },
    {
      "epoch": 175.9648370497427,
      "grad_norm": 6.459258556365967,
      "learning_rate": 1.8434653846153845e-05,
      "loss": 3.3256,
      "step": 820700
    },
    {
      "epoch": 175.98627787307032,
      "grad_norm": 5.68605899810791,
      "learning_rate": 1.8430807692307694e-05,
      "loss": 3.3301,
      "step": 820800
    },
    {
      "epoch": 176.00771869639794,
      "grad_norm": 5.767220973968506,
      "learning_rate": 1.842696153846154e-05,
      "loss": 3.3215,
      "step": 820900
    },
    {
      "epoch": 176.02915951972557,
      "grad_norm": 5.843137741088867,
      "learning_rate": 1.8423115384615385e-05,
      "loss": 3.3371,
      "step": 821000
    },
    {
      "epoch": 176.05060034305316,
      "grad_norm": 5.446926116943359,
      "learning_rate": 1.841926923076923e-05,
      "loss": 3.2833,
      "step": 821100
    },
    {
      "epoch": 176.07204116638079,
      "grad_norm": 6.2559638023376465,
      "learning_rate": 1.8415423076923076e-05,
      "loss": 3.3142,
      "step": 821200
    },
    {
      "epoch": 176.0934819897084,
      "grad_norm": 6.074775695800781,
      "learning_rate": 1.8411576923076922e-05,
      "loss": 3.2914,
      "step": 821300
    },
    {
      "epoch": 176.11492281303603,
      "grad_norm": 5.941838264465332,
      "learning_rate": 1.8407730769230767e-05,
      "loss": 3.3161,
      "step": 821400
    },
    {
      "epoch": 176.13636363636363,
      "grad_norm": 5.9040207862854,
      "learning_rate": 1.8403884615384616e-05,
      "loss": 3.3187,
      "step": 821500
    },
    {
      "epoch": 176.15780445969125,
      "grad_norm": 6.375113487243652,
      "learning_rate": 1.8400038461538462e-05,
      "loss": 3.3283,
      "step": 821600
    },
    {
      "epoch": 176.17924528301887,
      "grad_norm": 5.620992183685303,
      "learning_rate": 1.8396192307692307e-05,
      "loss": 3.3401,
      "step": 821700
    },
    {
      "epoch": 176.2006861063465,
      "grad_norm": 5.8345794677734375,
      "learning_rate": 1.8392346153846153e-05,
      "loss": 3.2829,
      "step": 821800
    },
    {
      "epoch": 176.2221269296741,
      "grad_norm": 6.306029796600342,
      "learning_rate": 1.83885e-05,
      "loss": 3.3462,
      "step": 821900
    },
    {
      "epoch": 176.2435677530017,
      "grad_norm": 6.1861724853515625,
      "learning_rate": 1.8384653846153847e-05,
      "loss": 3.2795,
      "step": 822000
    },
    {
      "epoch": 176.26500857632934,
      "grad_norm": 5.984336853027344,
      "learning_rate": 1.8380807692307693e-05,
      "loss": 3.2897,
      "step": 822100
    },
    {
      "epoch": 176.28644939965696,
      "grad_norm": 5.72479248046875,
      "learning_rate": 1.837696153846154e-05,
      "loss": 3.2998,
      "step": 822200
    },
    {
      "epoch": 176.30789022298455,
      "grad_norm": 6.272258281707764,
      "learning_rate": 1.8373115384615387e-05,
      "loss": 3.3621,
      "step": 822300
    },
    {
      "epoch": 176.32933104631218,
      "grad_norm": 6.281235694885254,
      "learning_rate": 1.8369269230769233e-05,
      "loss": 3.3821,
      "step": 822400
    },
    {
      "epoch": 176.3507718696398,
      "grad_norm": 6.469450950622559,
      "learning_rate": 1.836542307692308e-05,
      "loss": 3.3427,
      "step": 822500
    },
    {
      "epoch": 176.37221269296742,
      "grad_norm": 6.140540599822998,
      "learning_rate": 1.8361576923076924e-05,
      "loss": 3.3386,
      "step": 822600
    },
    {
      "epoch": 176.39365351629502,
      "grad_norm": 5.443873405456543,
      "learning_rate": 1.835773076923077e-05,
      "loss": 3.2951,
      "step": 822700
    },
    {
      "epoch": 176.41509433962264,
      "grad_norm": 5.9299540519714355,
      "learning_rate": 1.835388461538462e-05,
      "loss": 3.2998,
      "step": 822800
    },
    {
      "epoch": 176.43653516295026,
      "grad_norm": 5.928481101989746,
      "learning_rate": 1.8350038461538464e-05,
      "loss": 3.3233,
      "step": 822900
    },
    {
      "epoch": 176.4579759862779,
      "grad_norm": 6.401049613952637,
      "learning_rate": 1.834619230769231e-05,
      "loss": 3.3399,
      "step": 823000
    },
    {
      "epoch": 176.47941680960548,
      "grad_norm": 6.127445697784424,
      "learning_rate": 1.8342346153846155e-05,
      "loss": 3.3474,
      "step": 823100
    },
    {
      "epoch": 176.5008576329331,
      "grad_norm": 5.351053714752197,
      "learning_rate": 1.83385e-05,
      "loss": 3.3039,
      "step": 823200
    },
    {
      "epoch": 176.52229845626073,
      "grad_norm": 5.712692737579346,
      "learning_rate": 1.8334653846153846e-05,
      "loss": 3.3231,
      "step": 823300
    },
    {
      "epoch": 176.54373927958832,
      "grad_norm": 6.44167947769165,
      "learning_rate": 1.8330807692307695e-05,
      "loss": 3.3352,
      "step": 823400
    },
    {
      "epoch": 176.56518010291595,
      "grad_norm": 5.833208084106445,
      "learning_rate": 1.832696153846154e-05,
      "loss": 3.3187,
      "step": 823500
    },
    {
      "epoch": 176.58662092624357,
      "grad_norm": 6.4261155128479,
      "learning_rate": 1.8323115384615386e-05,
      "loss": 3.2839,
      "step": 823600
    },
    {
      "epoch": 176.6080617495712,
      "grad_norm": 6.202765464782715,
      "learning_rate": 1.8319269230769232e-05,
      "loss": 3.3148,
      "step": 823700
    },
    {
      "epoch": 176.6295025728988,
      "grad_norm": 6.1830902099609375,
      "learning_rate": 1.8315423076923077e-05,
      "loss": 3.3393,
      "step": 823800
    },
    {
      "epoch": 176.6509433962264,
      "grad_norm": 6.001361846923828,
      "learning_rate": 1.8311576923076923e-05,
      "loss": 3.3484,
      "step": 823900
    },
    {
      "epoch": 176.67238421955403,
      "grad_norm": 5.586812496185303,
      "learning_rate": 1.8307730769230772e-05,
      "loss": 3.4065,
      "step": 824000
    },
    {
      "epoch": 176.69382504288166,
      "grad_norm": 5.416773796081543,
      "learning_rate": 1.8303884615384617e-05,
      "loss": 3.3283,
      "step": 824100
    },
    {
      "epoch": 176.71526586620925,
      "grad_norm": 6.020354270935059,
      "learning_rate": 1.8300038461538463e-05,
      "loss": 3.2851,
      "step": 824200
    },
    {
      "epoch": 176.73670668953687,
      "grad_norm": 6.04190731048584,
      "learning_rate": 1.829619230769231e-05,
      "loss": 3.2939,
      "step": 824300
    },
    {
      "epoch": 176.7581475128645,
      "grad_norm": 6.143998146057129,
      "learning_rate": 1.8292346153846154e-05,
      "loss": 3.3308,
      "step": 824400
    },
    {
      "epoch": 176.77958833619212,
      "grad_norm": 5.831948280334473,
      "learning_rate": 1.82885e-05,
      "loss": 3.3165,
      "step": 824500
    },
    {
      "epoch": 176.80102915951971,
      "grad_norm": 6.662686824798584,
      "learning_rate": 1.8284653846153845e-05,
      "loss": 3.3452,
      "step": 824600
    },
    {
      "epoch": 176.82246998284734,
      "grad_norm": 6.192899703979492,
      "learning_rate": 1.8280807692307694e-05,
      "loss": 3.3152,
      "step": 824700
    },
    {
      "epoch": 176.84391080617496,
      "grad_norm": 6.677823543548584,
      "learning_rate": 1.827696153846154e-05,
      "loss": 3.3522,
      "step": 824800
    },
    {
      "epoch": 176.86535162950258,
      "grad_norm": 5.622873783111572,
      "learning_rate": 1.8273115384615385e-05,
      "loss": 3.2459,
      "step": 824900
    },
    {
      "epoch": 176.88679245283018,
      "grad_norm": 6.070965766906738,
      "learning_rate": 1.826926923076923e-05,
      "loss": 3.3127,
      "step": 825000
    },
    {
      "epoch": 176.9082332761578,
      "grad_norm": 6.864654541015625,
      "learning_rate": 1.8265423076923076e-05,
      "loss": 3.3125,
      "step": 825100
    },
    {
      "epoch": 176.92967409948542,
      "grad_norm": 7.176840305328369,
      "learning_rate": 1.826157692307692e-05,
      "loss": 3.3701,
      "step": 825200
    },
    {
      "epoch": 176.95111492281305,
      "grad_norm": 5.7493157386779785,
      "learning_rate": 1.825773076923077e-05,
      "loss": 3.282,
      "step": 825300
    },
    {
      "epoch": 176.97255574614064,
      "grad_norm": 6.221880912780762,
      "learning_rate": 1.8253884615384616e-05,
      "loss": 3.3523,
      "step": 825400
    },
    {
      "epoch": 176.99399656946827,
      "grad_norm": 6.717212677001953,
      "learning_rate": 1.825003846153846e-05,
      "loss": 3.3788,
      "step": 825500
    },
    {
      "epoch": 177.0154373927959,
      "grad_norm": 5.727997303009033,
      "learning_rate": 1.8246192307692307e-05,
      "loss": 3.2886,
      "step": 825600
    },
    {
      "epoch": 177.0368782161235,
      "grad_norm": 6.085115432739258,
      "learning_rate": 1.8242346153846153e-05,
      "loss": 3.3099,
      "step": 825700
    },
    {
      "epoch": 177.0583190394511,
      "grad_norm": 5.704812526702881,
      "learning_rate": 1.82385e-05,
      "loss": 3.3193,
      "step": 825800
    },
    {
      "epoch": 177.07975986277873,
      "grad_norm": 5.913252353668213,
      "learning_rate": 1.8234653846153847e-05,
      "loss": 3.3309,
      "step": 825900
    },
    {
      "epoch": 177.10120068610635,
      "grad_norm": 5.222377300262451,
      "learning_rate": 1.8230807692307693e-05,
      "loss": 3.2861,
      "step": 826000
    },
    {
      "epoch": 177.12264150943398,
      "grad_norm": 6.357458114624023,
      "learning_rate": 1.822696153846154e-05,
      "loss": 3.2815,
      "step": 826100
    },
    {
      "epoch": 177.14408233276157,
      "grad_norm": 5.733755588531494,
      "learning_rate": 1.8223115384615387e-05,
      "loss": 3.3157,
      "step": 826200
    },
    {
      "epoch": 177.1655231560892,
      "grad_norm": 5.703067779541016,
      "learning_rate": 1.8219269230769233e-05,
      "loss": 3.3256,
      "step": 826300
    },
    {
      "epoch": 177.18696397941682,
      "grad_norm": 5.802414417266846,
      "learning_rate": 1.8215423076923078e-05,
      "loss": 3.3313,
      "step": 826400
    },
    {
      "epoch": 177.2084048027444,
      "grad_norm": 5.510817527770996,
      "learning_rate": 1.8211576923076924e-05,
      "loss": 3.2623,
      "step": 826500
    },
    {
      "epoch": 177.22984562607203,
      "grad_norm": 5.627960681915283,
      "learning_rate": 1.8207730769230773e-05,
      "loss": 3.324,
      "step": 826600
    },
    {
      "epoch": 177.25128644939966,
      "grad_norm": 5.890316963195801,
      "learning_rate": 1.8203884615384618e-05,
      "loss": 3.3058,
      "step": 826700
    },
    {
      "epoch": 177.27272727272728,
      "grad_norm": 5.744313716888428,
      "learning_rate": 1.8200038461538464e-05,
      "loss": 3.3319,
      "step": 826800
    },
    {
      "epoch": 177.29416809605488,
      "grad_norm": 5.1774492263793945,
      "learning_rate": 1.819619230769231e-05,
      "loss": 3.2575,
      "step": 826900
    },
    {
      "epoch": 177.3156089193825,
      "grad_norm": 6.3387980461120605,
      "learning_rate": 1.8192346153846155e-05,
      "loss": 3.3513,
      "step": 827000
    },
    {
      "epoch": 177.33704974271012,
      "grad_norm": 6.306345462799072,
      "learning_rate": 1.81885e-05,
      "loss": 3.3928,
      "step": 827100
    },
    {
      "epoch": 177.35849056603774,
      "grad_norm": 5.579337120056152,
      "learning_rate": 1.8184653846153846e-05,
      "loss": 3.2955,
      "step": 827200
    },
    {
      "epoch": 177.37993138936534,
      "grad_norm": 6.211571216583252,
      "learning_rate": 1.8180807692307695e-05,
      "loss": 3.346,
      "step": 827300
    },
    {
      "epoch": 177.40137221269296,
      "grad_norm": 6.090977668762207,
      "learning_rate": 1.817696153846154e-05,
      "loss": 3.363,
      "step": 827400
    },
    {
      "epoch": 177.42281303602059,
      "grad_norm": 6.161773681640625,
      "learning_rate": 1.8173115384615386e-05,
      "loss": 3.3336,
      "step": 827500
    },
    {
      "epoch": 177.4442538593482,
      "grad_norm": 6.003795623779297,
      "learning_rate": 1.816926923076923e-05,
      "loss": 3.3044,
      "step": 827600
    },
    {
      "epoch": 177.4656946826758,
      "grad_norm": 6.074665546417236,
      "learning_rate": 1.8165423076923077e-05,
      "loss": 3.3394,
      "step": 827700
    },
    {
      "epoch": 177.48713550600343,
      "grad_norm": 5.770874500274658,
      "learning_rate": 1.8161576923076923e-05,
      "loss": 3.2762,
      "step": 827800
    },
    {
      "epoch": 177.50857632933105,
      "grad_norm": 5.806819438934326,
      "learning_rate": 1.815773076923077e-05,
      "loss": 3.2966,
      "step": 827900
    },
    {
      "epoch": 177.53001715265867,
      "grad_norm": 5.950934886932373,
      "learning_rate": 1.8153884615384617e-05,
      "loss": 3.2632,
      "step": 828000
    },
    {
      "epoch": 177.55145797598627,
      "grad_norm": 6.1888556480407715,
      "learning_rate": 1.8150038461538463e-05,
      "loss": 3.3167,
      "step": 828100
    },
    {
      "epoch": 177.5728987993139,
      "grad_norm": 5.747547626495361,
      "learning_rate": 1.8146192307692308e-05,
      "loss": 3.3133,
      "step": 828200
    },
    {
      "epoch": 177.5943396226415,
      "grad_norm": 5.89420747756958,
      "learning_rate": 1.8142346153846154e-05,
      "loss": 3.3516,
      "step": 828300
    },
    {
      "epoch": 177.61578044596914,
      "grad_norm": 5.9615373611450195,
      "learning_rate": 1.81385e-05,
      "loss": 3.3354,
      "step": 828400
    },
    {
      "epoch": 177.63722126929673,
      "grad_norm": 5.964547634124756,
      "learning_rate": 1.8134653846153845e-05,
      "loss": 3.355,
      "step": 828500
    },
    {
      "epoch": 177.65866209262435,
      "grad_norm": 5.639318943023682,
      "learning_rate": 1.8130807692307694e-05,
      "loss": 3.3489,
      "step": 828600
    },
    {
      "epoch": 177.68010291595198,
      "grad_norm": 5.770325183868408,
      "learning_rate": 1.812696153846154e-05,
      "loss": 3.3459,
      "step": 828700
    },
    {
      "epoch": 177.7015437392796,
      "grad_norm": 6.785284519195557,
      "learning_rate": 1.8123115384615385e-05,
      "loss": 3.3375,
      "step": 828800
    },
    {
      "epoch": 177.7229845626072,
      "grad_norm": 5.816317558288574,
      "learning_rate": 1.811926923076923e-05,
      "loss": 3.3026,
      "step": 828900
    },
    {
      "epoch": 177.74442538593482,
      "grad_norm": 5.765514373779297,
      "learning_rate": 1.8115423076923076e-05,
      "loss": 3.2696,
      "step": 829000
    },
    {
      "epoch": 177.76586620926244,
      "grad_norm": 6.111630439758301,
      "learning_rate": 1.811157692307692e-05,
      "loss": 3.3131,
      "step": 829100
    },
    {
      "epoch": 177.78730703259006,
      "grad_norm": 6.0692138671875,
      "learning_rate": 1.810773076923077e-05,
      "loss": 3.2518,
      "step": 829200
    },
    {
      "epoch": 177.80874785591766,
      "grad_norm": 5.881336688995361,
      "learning_rate": 1.8103884615384616e-05,
      "loss": 3.3317,
      "step": 829300
    },
    {
      "epoch": 177.83018867924528,
      "grad_norm": 5.845097064971924,
      "learning_rate": 1.810003846153846e-05,
      "loss": 3.2877,
      "step": 829400
    },
    {
      "epoch": 177.8516295025729,
      "grad_norm": 6.2306952476501465,
      "learning_rate": 1.8096192307692307e-05,
      "loss": 3.3045,
      "step": 829500
    },
    {
      "epoch": 177.87307032590053,
      "grad_norm": 5.9254350662231445,
      "learning_rate": 1.8092346153846156e-05,
      "loss": 3.261,
      "step": 829600
    },
    {
      "epoch": 177.89451114922812,
      "grad_norm": 6.013740062713623,
      "learning_rate": 1.80885e-05,
      "loss": 3.3449,
      "step": 829700
    },
    {
      "epoch": 177.91595197255575,
      "grad_norm": 5.539473056793213,
      "learning_rate": 1.8084653846153847e-05,
      "loss": 3.2915,
      "step": 829800
    },
    {
      "epoch": 177.93739279588337,
      "grad_norm": 6.558717727661133,
      "learning_rate": 1.8080807692307692e-05,
      "loss": 3.3011,
      "step": 829900
    },
    {
      "epoch": 177.95883361921096,
      "grad_norm": 5.64215612411499,
      "learning_rate": 1.807696153846154e-05,
      "loss": 3.2709,
      "step": 830000
    },
    {
      "epoch": 177.9802744425386,
      "grad_norm": 5.816754341125488,
      "learning_rate": 1.8073115384615387e-05,
      "loss": 3.294,
      "step": 830100
    },
    {
      "epoch": 178.0017152658662,
      "grad_norm": 5.978442668914795,
      "learning_rate": 1.8069269230769232e-05,
      "loss": 3.2534,
      "step": 830200
    },
    {
      "epoch": 178.02315608919383,
      "grad_norm": 6.506141185760498,
      "learning_rate": 1.8065423076923078e-05,
      "loss": 3.2866,
      "step": 830300
    },
    {
      "epoch": 178.04459691252143,
      "grad_norm": 5.739295482635498,
      "learning_rate": 1.8061576923076924e-05,
      "loss": 3.2585,
      "step": 830400
    },
    {
      "epoch": 178.06603773584905,
      "grad_norm": 5.988583087921143,
      "learning_rate": 1.8057730769230772e-05,
      "loss": 3.2488,
      "step": 830500
    },
    {
      "epoch": 178.08747855917667,
      "grad_norm": 6.011044979095459,
      "learning_rate": 1.8053884615384618e-05,
      "loss": 3.2969,
      "step": 830600
    },
    {
      "epoch": 178.1089193825043,
      "grad_norm": 5.190989971160889,
      "learning_rate": 1.8050038461538464e-05,
      "loss": 3.3426,
      "step": 830700
    },
    {
      "epoch": 178.1303602058319,
      "grad_norm": 5.883126735687256,
      "learning_rate": 1.804619230769231e-05,
      "loss": 3.3712,
      "step": 830800
    },
    {
      "epoch": 178.15180102915951,
      "grad_norm": 6.328507423400879,
      "learning_rate": 1.8042346153846155e-05,
      "loss": 3.274,
      "step": 830900
    },
    {
      "epoch": 178.17324185248714,
      "grad_norm": 5.5953264236450195,
      "learning_rate": 1.80385e-05,
      "loss": 3.2802,
      "step": 831000
    },
    {
      "epoch": 178.19468267581476,
      "grad_norm": 6.133884906768799,
      "learning_rate": 1.803465384615385e-05,
      "loss": 3.3313,
      "step": 831100
    },
    {
      "epoch": 178.21612349914236,
      "grad_norm": 5.513737201690674,
      "learning_rate": 1.8030807692307695e-05,
      "loss": 3.3392,
      "step": 831200
    },
    {
      "epoch": 178.23756432246998,
      "grad_norm": 5.950313091278076,
      "learning_rate": 1.802696153846154e-05,
      "loss": 3.2809,
      "step": 831300
    },
    {
      "epoch": 178.2590051457976,
      "grad_norm": 5.41133451461792,
      "learning_rate": 1.8023115384615386e-05,
      "loss": 3.265,
      "step": 831400
    },
    {
      "epoch": 178.28044596912522,
      "grad_norm": 5.910815715789795,
      "learning_rate": 1.801926923076923e-05,
      "loss": 3.314,
      "step": 831500
    },
    {
      "epoch": 178.30188679245282,
      "grad_norm": 5.98121452331543,
      "learning_rate": 1.8015423076923077e-05,
      "loss": 3.3019,
      "step": 831600
    },
    {
      "epoch": 178.32332761578044,
      "grad_norm": 6.44274377822876,
      "learning_rate": 1.8011576923076922e-05,
      "loss": 3.2845,
      "step": 831700
    },
    {
      "epoch": 178.34476843910807,
      "grad_norm": 6.086730480194092,
      "learning_rate": 1.800773076923077e-05,
      "loss": 3.2928,
      "step": 831800
    },
    {
      "epoch": 178.3662092624357,
      "grad_norm": 5.8357648849487305,
      "learning_rate": 1.8003884615384617e-05,
      "loss": 3.2772,
      "step": 831900
    },
    {
      "epoch": 178.38765008576328,
      "grad_norm": 6.775570392608643,
      "learning_rate": 1.8000038461538462e-05,
      "loss": 3.3429,
      "step": 832000
    },
    {
      "epoch": 178.4090909090909,
      "grad_norm": 5.822594165802002,
      "learning_rate": 1.7996192307692308e-05,
      "loss": 3.265,
      "step": 832100
    },
    {
      "epoch": 178.43053173241853,
      "grad_norm": 5.902990818023682,
      "learning_rate": 1.7992346153846153e-05,
      "loss": 3.2995,
      "step": 832200
    },
    {
      "epoch": 178.45197255574615,
      "grad_norm": 5.719759464263916,
      "learning_rate": 1.79885e-05,
      "loss": 3.2545,
      "step": 832300
    },
    {
      "epoch": 178.47341337907375,
      "grad_norm": 5.8805952072143555,
      "learning_rate": 1.7984653846153848e-05,
      "loss": 3.2891,
      "step": 832400
    },
    {
      "epoch": 178.49485420240137,
      "grad_norm": 5.962636947631836,
      "learning_rate": 1.7980807692307693e-05,
      "loss": 3.3109,
      "step": 832500
    },
    {
      "epoch": 178.516295025729,
      "grad_norm": 6.23073673248291,
      "learning_rate": 1.797696153846154e-05,
      "loss": 3.3027,
      "step": 832600
    },
    {
      "epoch": 178.53773584905662,
      "grad_norm": 5.854764938354492,
      "learning_rate": 1.7973115384615385e-05,
      "loss": 3.2686,
      "step": 832700
    },
    {
      "epoch": 178.5591766723842,
      "grad_norm": 5.966338634490967,
      "learning_rate": 1.796926923076923e-05,
      "loss": 3.2845,
      "step": 832800
    },
    {
      "epoch": 178.58061749571183,
      "grad_norm": 5.826059341430664,
      "learning_rate": 1.7965423076923076e-05,
      "loss": 3.273,
      "step": 832900
    },
    {
      "epoch": 178.60205831903946,
      "grad_norm": 6.553590297698975,
      "learning_rate": 1.796157692307692e-05,
      "loss": 3.2953,
      "step": 833000
    },
    {
      "epoch": 178.62349914236708,
      "grad_norm": 5.894711017608643,
      "learning_rate": 1.795773076923077e-05,
      "loss": 3.2919,
      "step": 833100
    },
    {
      "epoch": 178.64493996569468,
      "grad_norm": 5.975091934204102,
      "learning_rate": 1.7953884615384616e-05,
      "loss": 3.2561,
      "step": 833200
    },
    {
      "epoch": 178.6663807890223,
      "grad_norm": 6.084314823150635,
      "learning_rate": 1.795003846153846e-05,
      "loss": 3.3487,
      "step": 833300
    },
    {
      "epoch": 178.68782161234992,
      "grad_norm": 5.815151691436768,
      "learning_rate": 1.7946192307692307e-05,
      "loss": 3.3225,
      "step": 833400
    },
    {
      "epoch": 178.70926243567752,
      "grad_norm": 5.932291507720947,
      "learning_rate": 1.7942346153846156e-05,
      "loss": 3.266,
      "step": 833500
    },
    {
      "epoch": 178.73070325900514,
      "grad_norm": 5.86135196685791,
      "learning_rate": 1.79385e-05,
      "loss": 3.3326,
      "step": 833600
    },
    {
      "epoch": 178.75214408233276,
      "grad_norm": 5.961677551269531,
      "learning_rate": 1.7934653846153847e-05,
      "loss": 3.2949,
      "step": 833700
    },
    {
      "epoch": 178.77358490566039,
      "grad_norm": 5.663825988769531,
      "learning_rate": 1.7930807692307696e-05,
      "loss": 3.3048,
      "step": 833800
    },
    {
      "epoch": 178.79502572898798,
      "grad_norm": 5.807425498962402,
      "learning_rate": 1.792696153846154e-05,
      "loss": 3.318,
      "step": 833900
    },
    {
      "epoch": 178.8164665523156,
      "grad_norm": 5.491904258728027,
      "learning_rate": 1.7923115384615387e-05,
      "loss": 3.3105,
      "step": 834000
    },
    {
      "epoch": 178.83790737564323,
      "grad_norm": 5.797738075256348,
      "learning_rate": 1.7919269230769232e-05,
      "loss": 3.3231,
      "step": 834100
    },
    {
      "epoch": 178.85934819897085,
      "grad_norm": 5.950465679168701,
      "learning_rate": 1.7915423076923078e-05,
      "loss": 3.2741,
      "step": 834200
    },
    {
      "epoch": 178.88078902229844,
      "grad_norm": 5.909604549407959,
      "learning_rate": 1.7911576923076923e-05,
      "loss": 3.2957,
      "step": 834300
    },
    {
      "epoch": 178.90222984562607,
      "grad_norm": 5.961286544799805,
      "learning_rate": 1.7907730769230772e-05,
      "loss": 3.3711,
      "step": 834400
    },
    {
      "epoch": 178.9236706689537,
      "grad_norm": 5.623342514038086,
      "learning_rate": 1.7903884615384618e-05,
      "loss": 3.2795,
      "step": 834500
    },
    {
      "epoch": 178.9451114922813,
      "grad_norm": 5.802598476409912,
      "learning_rate": 1.7900038461538463e-05,
      "loss": 3.3175,
      "step": 834600
    },
    {
      "epoch": 178.9665523156089,
      "grad_norm": 5.842343807220459,
      "learning_rate": 1.789619230769231e-05,
      "loss": 3.3561,
      "step": 834700
    },
    {
      "epoch": 178.98799313893653,
      "grad_norm": 6.011582851409912,
      "learning_rate": 1.7892346153846154e-05,
      "loss": 3.2909,
      "step": 834800
    },
    {
      "epoch": 179.00943396226415,
      "grad_norm": 6.005711555480957,
      "learning_rate": 1.78885e-05,
      "loss": 3.2802,
      "step": 834900
    },
    {
      "epoch": 179.03087478559178,
      "grad_norm": 6.3703413009643555,
      "learning_rate": 1.788465384615385e-05,
      "loss": 3.3421,
      "step": 835000
    },
    {
      "epoch": 179.05231560891937,
      "grad_norm": 5.3469414710998535,
      "learning_rate": 1.7880807692307694e-05,
      "loss": 3.2411,
      "step": 835100
    },
    {
      "epoch": 179.073756432247,
      "grad_norm": 6.149041652679443,
      "learning_rate": 1.787696153846154e-05,
      "loss": 3.2677,
      "step": 835200
    },
    {
      "epoch": 179.09519725557462,
      "grad_norm": 5.637240886688232,
      "learning_rate": 1.7873115384615385e-05,
      "loss": 3.2469,
      "step": 835300
    },
    {
      "epoch": 179.11663807890224,
      "grad_norm": 5.910851955413818,
      "learning_rate": 1.786926923076923e-05,
      "loss": 3.2525,
      "step": 835400
    },
    {
      "epoch": 179.13807890222984,
      "grad_norm": 6.29507303237915,
      "learning_rate": 1.7865423076923077e-05,
      "loss": 3.2738,
      "step": 835500
    },
    {
      "epoch": 179.15951972555746,
      "grad_norm": 5.624278545379639,
      "learning_rate": 1.7861576923076925e-05,
      "loss": 3.2801,
      "step": 835600
    },
    {
      "epoch": 179.18096054888508,
      "grad_norm": 6.379484176635742,
      "learning_rate": 1.785773076923077e-05,
      "loss": 3.2394,
      "step": 835700
    },
    {
      "epoch": 179.2024013722127,
      "grad_norm": 6.259857177734375,
      "learning_rate": 1.7853884615384617e-05,
      "loss": 3.348,
      "step": 835800
    },
    {
      "epoch": 179.2238421955403,
      "grad_norm": 5.754189968109131,
      "learning_rate": 1.7850038461538462e-05,
      "loss": 3.2364,
      "step": 835900
    },
    {
      "epoch": 179.24528301886792,
      "grad_norm": 5.988415241241455,
      "learning_rate": 1.7846192307692308e-05,
      "loss": 3.2424,
      "step": 836000
    },
    {
      "epoch": 179.26672384219555,
      "grad_norm": 6.588991165161133,
      "learning_rate": 1.7842346153846153e-05,
      "loss": 3.2998,
      "step": 836100
    },
    {
      "epoch": 179.28816466552317,
      "grad_norm": 6.71632719039917,
      "learning_rate": 1.78385e-05,
      "loss": 3.2822,
      "step": 836200
    },
    {
      "epoch": 179.30960548885076,
      "grad_norm": 5.98244047164917,
      "learning_rate": 1.7834653846153848e-05,
      "loss": 3.3235,
      "step": 836300
    },
    {
      "epoch": 179.3310463121784,
      "grad_norm": 6.076296329498291,
      "learning_rate": 1.7830807692307693e-05,
      "loss": 3.2789,
      "step": 836400
    },
    {
      "epoch": 179.352487135506,
      "grad_norm": 5.855064392089844,
      "learning_rate": 1.782696153846154e-05,
      "loss": 3.2368,
      "step": 836500
    },
    {
      "epoch": 179.37392795883363,
      "grad_norm": 6.037792205810547,
      "learning_rate": 1.7823115384615384e-05,
      "loss": 3.3303,
      "step": 836600
    },
    {
      "epoch": 179.39536878216123,
      "grad_norm": 6.119100570678711,
      "learning_rate": 1.781926923076923e-05,
      "loss": 3.315,
      "step": 836700
    },
    {
      "epoch": 179.41680960548885,
      "grad_norm": 5.734038829803467,
      "learning_rate": 1.7815423076923075e-05,
      "loss": 3.3284,
      "step": 836800
    },
    {
      "epoch": 179.43825042881647,
      "grad_norm": 6.404068470001221,
      "learning_rate": 1.7811576923076924e-05,
      "loss": 3.3151,
      "step": 836900
    },
    {
      "epoch": 179.45969125214407,
      "grad_norm": 6.409414291381836,
      "learning_rate": 1.780773076923077e-05,
      "loss": 3.2692,
      "step": 837000
    },
    {
      "epoch": 179.4811320754717,
      "grad_norm": 5.646788597106934,
      "learning_rate": 1.7803884615384615e-05,
      "loss": 3.2535,
      "step": 837100
    },
    {
      "epoch": 179.50257289879931,
      "grad_norm": 6.498589992523193,
      "learning_rate": 1.780003846153846e-05,
      "loss": 3.2769,
      "step": 837200
    },
    {
      "epoch": 179.52401372212694,
      "grad_norm": 6.03632116317749,
      "learning_rate": 1.779619230769231e-05,
      "loss": 3.322,
      "step": 837300
    },
    {
      "epoch": 179.54545454545453,
      "grad_norm": 6.288480281829834,
      "learning_rate": 1.7792346153846155e-05,
      "loss": 3.3332,
      "step": 837400
    },
    {
      "epoch": 179.56689536878216,
      "grad_norm": 6.049345016479492,
      "learning_rate": 1.77885e-05,
      "loss": 3.3192,
      "step": 837500
    },
    {
      "epoch": 179.58833619210978,
      "grad_norm": 6.1860270500183105,
      "learning_rate": 1.778465384615385e-05,
      "loss": 3.2904,
      "step": 837600
    },
    {
      "epoch": 179.6097770154374,
      "grad_norm": 6.221695423126221,
      "learning_rate": 1.7780807692307695e-05,
      "loss": 3.3302,
      "step": 837700
    },
    {
      "epoch": 179.631217838765,
      "grad_norm": 6.584432125091553,
      "learning_rate": 1.777696153846154e-05,
      "loss": 3.3237,
      "step": 837800
    },
    {
      "epoch": 179.65265866209262,
      "grad_norm": 5.888129234313965,
      "learning_rate": 1.7773115384615386e-05,
      "loss": 3.3235,
      "step": 837900
    },
    {
      "epoch": 179.67409948542024,
      "grad_norm": 5.915542125701904,
      "learning_rate": 1.7769269230769232e-05,
      "loss": 3.3013,
      "step": 838000
    },
    {
      "epoch": 179.69554030874787,
      "grad_norm": 6.298670291900635,
      "learning_rate": 1.7765423076923077e-05,
      "loss": 3.2298,
      "step": 838100
    },
    {
      "epoch": 179.71698113207546,
      "grad_norm": 6.332353115081787,
      "learning_rate": 1.7761576923076926e-05,
      "loss": 3.294,
      "step": 838200
    },
    {
      "epoch": 179.73842195540308,
      "grad_norm": 6.44539213180542,
      "learning_rate": 1.7757730769230772e-05,
      "loss": 3.3135,
      "step": 838300
    },
    {
      "epoch": 179.7598627787307,
      "grad_norm": 6.627885341644287,
      "learning_rate": 1.7753884615384617e-05,
      "loss": 3.2934,
      "step": 838400
    },
    {
      "epoch": 179.78130360205833,
      "grad_norm": 5.453841686248779,
      "learning_rate": 1.7750038461538463e-05,
      "loss": 3.2946,
      "step": 838500
    },
    {
      "epoch": 179.80274442538592,
      "grad_norm": 6.16226053237915,
      "learning_rate": 1.774619230769231e-05,
      "loss": 3.3003,
      "step": 838600
    },
    {
      "epoch": 179.82418524871355,
      "grad_norm": 5.990797996520996,
      "learning_rate": 1.7742346153846154e-05,
      "loss": 3.2766,
      "step": 838700
    },
    {
      "epoch": 179.84562607204117,
      "grad_norm": 5.67318868637085,
      "learning_rate": 1.77385e-05,
      "loss": 3.2483,
      "step": 838800
    },
    {
      "epoch": 179.8670668953688,
      "grad_norm": 5.800247669219971,
      "learning_rate": 1.773465384615385e-05,
      "loss": 3.2726,
      "step": 838900
    },
    {
      "epoch": 179.8885077186964,
      "grad_norm": 5.852056980133057,
      "learning_rate": 1.7730807692307694e-05,
      "loss": 3.3018,
      "step": 839000
    },
    {
      "epoch": 179.909948542024,
      "grad_norm": 5.907946586608887,
      "learning_rate": 1.772696153846154e-05,
      "loss": 3.2538,
      "step": 839100
    },
    {
      "epoch": 179.93138936535163,
      "grad_norm": 6.250863075256348,
      "learning_rate": 1.7723115384615385e-05,
      "loss": 3.2944,
      "step": 839200
    },
    {
      "epoch": 179.95283018867926,
      "grad_norm": 5.976648807525635,
      "learning_rate": 1.771926923076923e-05,
      "loss": 3.3397,
      "step": 839300
    },
    {
      "epoch": 179.97427101200685,
      "grad_norm": 6.371530055999756,
      "learning_rate": 1.7715423076923076e-05,
      "loss": 3.3182,
      "step": 839400
    },
    {
      "epoch": 179.99571183533448,
      "grad_norm": 5.711058616638184,
      "learning_rate": 1.7711576923076925e-05,
      "loss": 3.2878,
      "step": 839500
    },
    {
      "epoch": 180.0171526586621,
      "grad_norm": 6.176540374755859,
      "learning_rate": 1.770773076923077e-05,
      "loss": 3.2322,
      "step": 839600
    },
    {
      "epoch": 180.03859348198972,
      "grad_norm": 6.15735387802124,
      "learning_rate": 1.7703884615384616e-05,
      "loss": 3.3095,
      "step": 839700
    },
    {
      "epoch": 180.06003430531732,
      "grad_norm": 6.098573684692383,
      "learning_rate": 1.7700038461538462e-05,
      "loss": 3.2966,
      "step": 839800
    },
    {
      "epoch": 180.08147512864494,
      "grad_norm": 6.548145294189453,
      "learning_rate": 1.7696192307692307e-05,
      "loss": 3.2375,
      "step": 839900
    },
    {
      "epoch": 180.10291595197256,
      "grad_norm": 6.11444616317749,
      "learning_rate": 1.7692346153846153e-05,
      "loss": 3.2651,
      "step": 840000
    },
    {
      "epoch": 180.12435677530019,
      "grad_norm": 6.406920909881592,
      "learning_rate": 1.76885e-05,
      "loss": 3.3057,
      "step": 840100
    },
    {
      "epoch": 180.14579759862778,
      "grad_norm": 6.310677528381348,
      "learning_rate": 1.7684653846153847e-05,
      "loss": 3.2689,
      "step": 840200
    },
    {
      "epoch": 180.1672384219554,
      "grad_norm": 5.843313217163086,
      "learning_rate": 1.7680807692307693e-05,
      "loss": 3.2823,
      "step": 840300
    },
    {
      "epoch": 180.18867924528303,
      "grad_norm": 6.27109956741333,
      "learning_rate": 1.767696153846154e-05,
      "loss": 3.2836,
      "step": 840400
    },
    {
      "epoch": 180.21012006861062,
      "grad_norm": 6.227497577667236,
      "learning_rate": 1.7673115384615384e-05,
      "loss": 3.2737,
      "step": 840500
    },
    {
      "epoch": 180.23156089193824,
      "grad_norm": 6.011959075927734,
      "learning_rate": 1.766926923076923e-05,
      "loss": 3.2717,
      "step": 840600
    },
    {
      "epoch": 180.25300171526587,
      "grad_norm": 5.539734840393066,
      "learning_rate": 1.7665423076923075e-05,
      "loss": 3.2371,
      "step": 840700
    },
    {
      "epoch": 180.2744425385935,
      "grad_norm": 6.016789436340332,
      "learning_rate": 1.7661576923076924e-05,
      "loss": 3.2798,
      "step": 840800
    },
    {
      "epoch": 180.29588336192108,
      "grad_norm": 5.786423683166504,
      "learning_rate": 1.765773076923077e-05,
      "loss": 3.2826,
      "step": 840900
    },
    {
      "epoch": 180.3173241852487,
      "grad_norm": 5.726032733917236,
      "learning_rate": 1.7653884615384615e-05,
      "loss": 3.2661,
      "step": 841000
    },
    {
      "epoch": 180.33876500857633,
      "grad_norm": 5.909639358520508,
      "learning_rate": 1.765003846153846e-05,
      "loss": 3.3156,
      "step": 841100
    },
    {
      "epoch": 180.36020583190395,
      "grad_norm": 6.580343723297119,
      "learning_rate": 1.764619230769231e-05,
      "loss": 3.2836,
      "step": 841200
    },
    {
      "epoch": 180.38164665523155,
      "grad_norm": 6.152039051055908,
      "learning_rate": 1.7642346153846155e-05,
      "loss": 3.2841,
      "step": 841300
    },
    {
      "epoch": 180.40308747855917,
      "grad_norm": 6.444100379943848,
      "learning_rate": 1.76385e-05,
      "loss": 3.2779,
      "step": 841400
    },
    {
      "epoch": 180.4245283018868,
      "grad_norm": 5.881994247436523,
      "learning_rate": 1.763465384615385e-05,
      "loss": 3.2722,
      "step": 841500
    },
    {
      "epoch": 180.44596912521442,
      "grad_norm": 6.0229926109313965,
      "learning_rate": 1.7630807692307695e-05,
      "loss": 3.2838,
      "step": 841600
    },
    {
      "epoch": 180.467409948542,
      "grad_norm": 5.859347820281982,
      "learning_rate": 1.762696153846154e-05,
      "loss": 3.269,
      "step": 841700
    },
    {
      "epoch": 180.48885077186964,
      "grad_norm": 6.0669050216674805,
      "learning_rate": 1.7623115384615386e-05,
      "loss": 3.2774,
      "step": 841800
    },
    {
      "epoch": 180.51029159519726,
      "grad_norm": 5.989767074584961,
      "learning_rate": 1.7619269230769232e-05,
      "loss": 3.2619,
      "step": 841900
    },
    {
      "epoch": 180.53173241852488,
      "grad_norm": 6.173493385314941,
      "learning_rate": 1.7615423076923077e-05,
      "loss": 3.2899,
      "step": 842000
    },
    {
      "epoch": 180.55317324185248,
      "grad_norm": 6.1170220375061035,
      "learning_rate": 1.7611576923076926e-05,
      "loss": 3.2775,
      "step": 842100
    },
    {
      "epoch": 180.5746140651801,
      "grad_norm": 6.477579116821289,
      "learning_rate": 1.7607730769230772e-05,
      "loss": 3.2908,
      "step": 842200
    },
    {
      "epoch": 180.59605488850772,
      "grad_norm": 5.682577610015869,
      "learning_rate": 1.7603884615384617e-05,
      "loss": 3.3011,
      "step": 842300
    },
    {
      "epoch": 180.61749571183535,
      "grad_norm": 6.043446063995361,
      "learning_rate": 1.7600038461538463e-05,
      "loss": 3.2831,
      "step": 842400
    },
    {
      "epoch": 180.63893653516294,
      "grad_norm": 6.197125434875488,
      "learning_rate": 1.759619230769231e-05,
      "loss": 3.2923,
      "step": 842500
    },
    {
      "epoch": 180.66037735849056,
      "grad_norm": 6.31699275970459,
      "learning_rate": 1.7592346153846154e-05,
      "loss": 3.2811,
      "step": 842600
    },
    {
      "epoch": 180.6818181818182,
      "grad_norm": 6.1607279777526855,
      "learning_rate": 1.7588500000000003e-05,
      "loss": 3.2634,
      "step": 842700
    },
    {
      "epoch": 180.7032590051458,
      "grad_norm": 5.905639171600342,
      "learning_rate": 1.758465384615385e-05,
      "loss": 3.2865,
      "step": 842800
    },
    {
      "epoch": 180.7246998284734,
      "grad_norm": 6.257211208343506,
      "learning_rate": 1.7580807692307694e-05,
      "loss": 3.2847,
      "step": 842900
    },
    {
      "epoch": 180.74614065180103,
      "grad_norm": 5.975307941436768,
      "learning_rate": 1.757696153846154e-05,
      "loss": 3.235,
      "step": 843000
    },
    {
      "epoch": 180.76758147512865,
      "grad_norm": 5.504462242126465,
      "learning_rate": 1.7573115384615385e-05,
      "loss": 3.3173,
      "step": 843100
    },
    {
      "epoch": 180.78902229845627,
      "grad_norm": 6.085970878601074,
      "learning_rate": 1.756926923076923e-05,
      "loss": 3.297,
      "step": 843200
    },
    {
      "epoch": 180.81046312178387,
      "grad_norm": 6.200652122497559,
      "learning_rate": 1.7565423076923076e-05,
      "loss": 3.2654,
      "step": 843300
    },
    {
      "epoch": 180.8319039451115,
      "grad_norm": 5.33500862121582,
      "learning_rate": 1.7561576923076925e-05,
      "loss": 3.275,
      "step": 843400
    },
    {
      "epoch": 180.85334476843911,
      "grad_norm": 5.8532185554504395,
      "learning_rate": 1.755773076923077e-05,
      "loss": 3.3034,
      "step": 843500
    },
    {
      "epoch": 180.87478559176674,
      "grad_norm": 6.069500923156738,
      "learning_rate": 1.7553884615384616e-05,
      "loss": 3.3222,
      "step": 843600
    },
    {
      "epoch": 180.89622641509433,
      "grad_norm": 7.291696071624756,
      "learning_rate": 1.755003846153846e-05,
      "loss": 3.2945,
      "step": 843700
    },
    {
      "epoch": 180.91766723842196,
      "grad_norm": 6.068445682525635,
      "learning_rate": 1.7546192307692307e-05,
      "loss": 3.2914,
      "step": 843800
    },
    {
      "epoch": 180.93910806174958,
      "grad_norm": 5.850568771362305,
      "learning_rate": 1.7542346153846153e-05,
      "loss": 3.2739,
      "step": 843900
    },
    {
      "epoch": 180.96054888507717,
      "grad_norm": 6.313804626464844,
      "learning_rate": 1.75385e-05,
      "loss": 3.296,
      "step": 844000
    },
    {
      "epoch": 180.9819897084048,
      "grad_norm": 6.276485443115234,
      "learning_rate": 1.7534653846153847e-05,
      "loss": 3.2488,
      "step": 844100
    },
    {
      "epoch": 181.00343053173242,
      "grad_norm": 5.645537376403809,
      "learning_rate": 1.7530807692307693e-05,
      "loss": 3.2489,
      "step": 844200
    },
    {
      "epoch": 181.02487135506004,
      "grad_norm": 6.32755184173584,
      "learning_rate": 1.7526961538461538e-05,
      "loss": 3.2156,
      "step": 844300
    },
    {
      "epoch": 181.04631217838764,
      "grad_norm": 5.998451232910156,
      "learning_rate": 1.7523115384615384e-05,
      "loss": 3.3113,
      "step": 844400
    },
    {
      "epoch": 181.06775300171526,
      "grad_norm": 6.324007511138916,
      "learning_rate": 1.751926923076923e-05,
      "loss": 3.2604,
      "step": 844500
    },
    {
      "epoch": 181.08919382504288,
      "grad_norm": 6.127198696136475,
      "learning_rate": 1.7515423076923075e-05,
      "loss": 3.2249,
      "step": 844600
    },
    {
      "epoch": 181.1106346483705,
      "grad_norm": 6.061883926391602,
      "learning_rate": 1.7511576923076924e-05,
      "loss": 3.2985,
      "step": 844700
    },
    {
      "epoch": 181.1320754716981,
      "grad_norm": 5.568134307861328,
      "learning_rate": 1.750773076923077e-05,
      "loss": 3.2286,
      "step": 844800
    },
    {
      "epoch": 181.15351629502572,
      "grad_norm": 5.681690216064453,
      "learning_rate": 1.7503884615384615e-05,
      "loss": 3.2478,
      "step": 844900
    },
    {
      "epoch": 181.17495711835335,
      "grad_norm": 5.660428524017334,
      "learning_rate": 1.7500038461538464e-05,
      "loss": 3.282,
      "step": 845000
    },
    {
      "epoch": 181.19639794168097,
      "grad_norm": 5.767916202545166,
      "learning_rate": 1.749619230769231e-05,
      "loss": 3.2393,
      "step": 845100
    },
    {
      "epoch": 181.21783876500857,
      "grad_norm": 5.928361415863037,
      "learning_rate": 1.7492346153846155e-05,
      "loss": 3.2638,
      "step": 845200
    },
    {
      "epoch": 181.2392795883362,
      "grad_norm": 5.47955322265625,
      "learning_rate": 1.7488500000000004e-05,
      "loss": 3.2309,
      "step": 845300
    },
    {
      "epoch": 181.2607204116638,
      "grad_norm": 5.496644973754883,
      "learning_rate": 1.748465384615385e-05,
      "loss": 3.2704,
      "step": 845400
    },
    {
      "epoch": 181.28216123499143,
      "grad_norm": 5.961612701416016,
      "learning_rate": 1.7480807692307695e-05,
      "loss": 3.261,
      "step": 845500
    },
    {
      "epoch": 181.30360205831903,
      "grad_norm": 5.713637351989746,
      "learning_rate": 1.747696153846154e-05,
      "loss": 3.2821,
      "step": 845600
    },
    {
      "epoch": 181.32504288164665,
      "grad_norm": 6.268186569213867,
      "learning_rate": 1.7473115384615386e-05,
      "loss": 3.2217,
      "step": 845700
    },
    {
      "epoch": 181.34648370497428,
      "grad_norm": 5.921077728271484,
      "learning_rate": 1.746926923076923e-05,
      "loss": 3.2579,
      "step": 845800
    },
    {
      "epoch": 181.3679245283019,
      "grad_norm": 6.106462001800537,
      "learning_rate": 1.7465423076923077e-05,
      "loss": 3.29,
      "step": 845900
    },
    {
      "epoch": 181.3893653516295,
      "grad_norm": 5.776066303253174,
      "learning_rate": 1.7461576923076926e-05,
      "loss": 3.2783,
      "step": 846000
    },
    {
      "epoch": 181.41080617495712,
      "grad_norm": 5.723459243774414,
      "learning_rate": 1.745773076923077e-05,
      "loss": 3.2952,
      "step": 846100
    },
    {
      "epoch": 181.43224699828474,
      "grad_norm": 5.958250522613525,
      "learning_rate": 1.7453884615384617e-05,
      "loss": 3.2874,
      "step": 846200
    },
    {
      "epoch": 181.45368782161236,
      "grad_norm": 6.050926208496094,
      "learning_rate": 1.7450038461538463e-05,
      "loss": 3.2618,
      "step": 846300
    },
    {
      "epoch": 181.47512864493996,
      "grad_norm": 6.196070194244385,
      "learning_rate": 1.7446192307692308e-05,
      "loss": 3.2372,
      "step": 846400
    },
    {
      "epoch": 181.49656946826758,
      "grad_norm": 6.2057204246521,
      "learning_rate": 1.7442346153846154e-05,
      "loss": 3.2977,
      "step": 846500
    },
    {
      "epoch": 181.5180102915952,
      "grad_norm": 6.119317531585693,
      "learning_rate": 1.7438500000000003e-05,
      "loss": 3.3197,
      "step": 846600
    },
    {
      "epoch": 181.53945111492283,
      "grad_norm": 6.485180377960205,
      "learning_rate": 1.7434653846153848e-05,
      "loss": 3.2524,
      "step": 846700
    },
    {
      "epoch": 181.56089193825042,
      "grad_norm": 6.221656799316406,
      "learning_rate": 1.7430807692307694e-05,
      "loss": 3.2801,
      "step": 846800
    },
    {
      "epoch": 181.58233276157804,
      "grad_norm": 5.933718681335449,
      "learning_rate": 1.742696153846154e-05,
      "loss": 3.2597,
      "step": 846900
    },
    {
      "epoch": 181.60377358490567,
      "grad_norm": 6.079260349273682,
      "learning_rate": 1.7423115384615385e-05,
      "loss": 3.299,
      "step": 847000
    },
    {
      "epoch": 181.62521440823326,
      "grad_norm": 5.316831111907959,
      "learning_rate": 1.741926923076923e-05,
      "loss": 3.3069,
      "step": 847100
    },
    {
      "epoch": 181.64665523156089,
      "grad_norm": 5.973588943481445,
      "learning_rate": 1.7415423076923076e-05,
      "loss": 3.2961,
      "step": 847200
    },
    {
      "epoch": 181.6680960548885,
      "grad_norm": 5.886597633361816,
      "learning_rate": 1.7411576923076925e-05,
      "loss": 3.3242,
      "step": 847300
    },
    {
      "epoch": 181.68953687821613,
      "grad_norm": 6.589069843292236,
      "learning_rate": 1.740773076923077e-05,
      "loss": 3.2225,
      "step": 847400
    },
    {
      "epoch": 181.71097770154373,
      "grad_norm": 5.853085041046143,
      "learning_rate": 1.7403884615384616e-05,
      "loss": 3.2969,
      "step": 847500
    },
    {
      "epoch": 181.73241852487135,
      "grad_norm": 5.751063823699951,
      "learning_rate": 1.740003846153846e-05,
      "loss": 3.2993,
      "step": 847600
    },
    {
      "epoch": 181.75385934819897,
      "grad_norm": 5.585477352142334,
      "learning_rate": 1.7396192307692307e-05,
      "loss": 3.2788,
      "step": 847700
    },
    {
      "epoch": 181.7753001715266,
      "grad_norm": 6.069107532501221,
      "learning_rate": 1.7392346153846152e-05,
      "loss": 3.2571,
      "step": 847800
    },
    {
      "epoch": 181.7967409948542,
      "grad_norm": 6.277515888214111,
      "learning_rate": 1.73885e-05,
      "loss": 3.2909,
      "step": 847900
    },
    {
      "epoch": 181.8181818181818,
      "grad_norm": 5.94654655456543,
      "learning_rate": 1.7384653846153847e-05,
      "loss": 3.3211,
      "step": 848000
    },
    {
      "epoch": 181.83962264150944,
      "grad_norm": 6.107674598693848,
      "learning_rate": 1.7380807692307692e-05,
      "loss": 3.2365,
      "step": 848100
    },
    {
      "epoch": 181.86106346483706,
      "grad_norm": 5.844445705413818,
      "learning_rate": 1.7376961538461538e-05,
      "loss": 3.2914,
      "step": 848200
    },
    {
      "epoch": 181.88250428816465,
      "grad_norm": 5.575656890869141,
      "learning_rate": 1.7373115384615383e-05,
      "loss": 3.2795,
      "step": 848300
    },
    {
      "epoch": 181.90394511149228,
      "grad_norm": 5.725667476654053,
      "learning_rate": 1.736926923076923e-05,
      "loss": 3.2267,
      "step": 848400
    },
    {
      "epoch": 181.9253859348199,
      "grad_norm": 6.337203025817871,
      "learning_rate": 1.7365423076923078e-05,
      "loss": 3.2226,
      "step": 848500
    },
    {
      "epoch": 181.94682675814752,
      "grad_norm": 5.722970485687256,
      "learning_rate": 1.7361576923076923e-05,
      "loss": 3.2783,
      "step": 848600
    },
    {
      "epoch": 181.96826758147512,
      "grad_norm": 6.087923049926758,
      "learning_rate": 1.735773076923077e-05,
      "loss": 3.2444,
      "step": 848700
    },
    {
      "epoch": 181.98970840480274,
      "grad_norm": 5.908874988555908,
      "learning_rate": 1.7353884615384618e-05,
      "loss": 3.3014,
      "step": 848800
    },
    {
      "epoch": 182.01114922813036,
      "grad_norm": 5.96238899230957,
      "learning_rate": 1.7350038461538463e-05,
      "loss": 3.2085,
      "step": 848900
    },
    {
      "epoch": 182.032590051458,
      "grad_norm": 6.110635757446289,
      "learning_rate": 1.734619230769231e-05,
      "loss": 3.2718,
      "step": 849000
    },
    {
      "epoch": 182.05403087478558,
      "grad_norm": 5.550546169281006,
      "learning_rate": 1.7342346153846155e-05,
      "loss": 3.23,
      "step": 849100
    },
    {
      "epoch": 182.0754716981132,
      "grad_norm": 6.038043022155762,
      "learning_rate": 1.7338500000000003e-05,
      "loss": 3.2943,
      "step": 849200
    },
    {
      "epoch": 182.09691252144083,
      "grad_norm": 6.316648006439209,
      "learning_rate": 1.733465384615385e-05,
      "loss": 3.2423,
      "step": 849300
    },
    {
      "epoch": 182.11835334476845,
      "grad_norm": 5.964853763580322,
      "learning_rate": 1.7330807692307695e-05,
      "loss": 3.2531,
      "step": 849400
    },
    {
      "epoch": 182.13979416809605,
      "grad_norm": 5.544079303741455,
      "learning_rate": 1.732696153846154e-05,
      "loss": 3.2195,
      "step": 849500
    },
    {
      "epoch": 182.16123499142367,
      "grad_norm": 5.7124433517456055,
      "learning_rate": 1.7323115384615386e-05,
      "loss": 3.2553,
      "step": 849600
    },
    {
      "epoch": 182.1826758147513,
      "grad_norm": 6.179737091064453,
      "learning_rate": 1.731926923076923e-05,
      "loss": 3.25,
      "step": 849700
    },
    {
      "epoch": 182.20411663807892,
      "grad_norm": 5.532526016235352,
      "learning_rate": 1.731542307692308e-05,
      "loss": 3.2187,
      "step": 849800
    },
    {
      "epoch": 182.2255574614065,
      "grad_norm": 6.794053554534912,
      "learning_rate": 1.7311576923076926e-05,
      "loss": 3.2475,
      "step": 849900
    },
    {
      "epoch": 182.24699828473413,
      "grad_norm": 6.30197811126709,
      "learning_rate": 1.730773076923077e-05,
      "loss": 3.2187,
      "step": 850000
    },
    {
      "epoch": 182.26843910806176,
      "grad_norm": 6.030337333679199,
      "learning_rate": 1.7303884615384617e-05,
      "loss": 3.2857,
      "step": 850100
    },
    {
      "epoch": 182.28987993138938,
      "grad_norm": 6.082995414733887,
      "learning_rate": 1.7300038461538462e-05,
      "loss": 3.2832,
      "step": 850200
    },
    {
      "epoch": 182.31132075471697,
      "grad_norm": 5.95083475112915,
      "learning_rate": 1.7296192307692308e-05,
      "loss": 3.2695,
      "step": 850300
    },
    {
      "epoch": 182.3327615780446,
      "grad_norm": 5.960071086883545,
      "learning_rate": 1.7292346153846153e-05,
      "loss": 3.209,
      "step": 850400
    },
    {
      "epoch": 182.35420240137222,
      "grad_norm": 5.972770690917969,
      "learning_rate": 1.7288500000000002e-05,
      "loss": 3.2418,
      "step": 850500
    },
    {
      "epoch": 182.37564322469981,
      "grad_norm": 5.803552150726318,
      "learning_rate": 1.7284653846153848e-05,
      "loss": 3.3028,
      "step": 850600
    },
    {
      "epoch": 182.39708404802744,
      "grad_norm": 6.161576747894287,
      "learning_rate": 1.7280807692307693e-05,
      "loss": 3.3108,
      "step": 850700
    },
    {
      "epoch": 182.41852487135506,
      "grad_norm": 6.016624927520752,
      "learning_rate": 1.727696153846154e-05,
      "loss": 3.2999,
      "step": 850800
    },
    {
      "epoch": 182.43996569468268,
      "grad_norm": 5.7082200050354,
      "learning_rate": 1.7273115384615384e-05,
      "loss": 3.2572,
      "step": 850900
    },
    {
      "epoch": 182.46140651801028,
      "grad_norm": 5.99069356918335,
      "learning_rate": 1.726926923076923e-05,
      "loss": 3.249,
      "step": 851000
    },
    {
      "epoch": 182.4828473413379,
      "grad_norm": 6.11470890045166,
      "learning_rate": 1.726542307692308e-05,
      "loss": 3.205,
      "step": 851100
    },
    {
      "epoch": 182.50428816466552,
      "grad_norm": 5.692904472351074,
      "learning_rate": 1.7261576923076924e-05,
      "loss": 3.2367,
      "step": 851200
    },
    {
      "epoch": 182.52572898799315,
      "grad_norm": 5.585867881774902,
      "learning_rate": 1.725773076923077e-05,
      "loss": 3.1841,
      "step": 851300
    },
    {
      "epoch": 182.54716981132074,
      "grad_norm": 6.18464469909668,
      "learning_rate": 1.7253884615384616e-05,
      "loss": 3.2279,
      "step": 851400
    },
    {
      "epoch": 182.56861063464837,
      "grad_norm": 6.1769514083862305,
      "learning_rate": 1.725003846153846e-05,
      "loss": 3.2575,
      "step": 851500
    },
    {
      "epoch": 182.590051457976,
      "grad_norm": 5.700836181640625,
      "learning_rate": 1.7246192307692307e-05,
      "loss": 3.2209,
      "step": 851600
    },
    {
      "epoch": 182.6114922813036,
      "grad_norm": 5.752185344696045,
      "learning_rate": 1.7242346153846152e-05,
      "loss": 3.2677,
      "step": 851700
    },
    {
      "epoch": 182.6329331046312,
      "grad_norm": 6.841894149780273,
      "learning_rate": 1.72385e-05,
      "loss": 3.2658,
      "step": 851800
    },
    {
      "epoch": 182.65437392795883,
      "grad_norm": 5.566398620605469,
      "learning_rate": 1.7234653846153847e-05,
      "loss": 3.2545,
      "step": 851900
    },
    {
      "epoch": 182.67581475128645,
      "grad_norm": 5.900341510772705,
      "learning_rate": 1.7230807692307692e-05,
      "loss": 3.3256,
      "step": 852000
    },
    {
      "epoch": 182.69725557461408,
      "grad_norm": 6.461555480957031,
      "learning_rate": 1.7226961538461538e-05,
      "loss": 3.2984,
      "step": 852100
    },
    {
      "epoch": 182.71869639794167,
      "grad_norm": 5.880878925323486,
      "learning_rate": 1.7223115384615383e-05,
      "loss": 3.2861,
      "step": 852200
    },
    {
      "epoch": 182.7401372212693,
      "grad_norm": 6.594012260437012,
      "learning_rate": 1.7219269230769232e-05,
      "loss": 3.2607,
      "step": 852300
    },
    {
      "epoch": 182.76157804459692,
      "grad_norm": 5.659570217132568,
      "learning_rate": 1.7215423076923078e-05,
      "loss": 3.2822,
      "step": 852400
    },
    {
      "epoch": 182.78301886792454,
      "grad_norm": 5.520137310028076,
      "learning_rate": 1.7211576923076923e-05,
      "loss": 3.291,
      "step": 852500
    },
    {
      "epoch": 182.80445969125213,
      "grad_norm": 6.35853385925293,
      "learning_rate": 1.720773076923077e-05,
      "loss": 3.2271,
      "step": 852600
    },
    {
      "epoch": 182.82590051457976,
      "grad_norm": 6.319601535797119,
      "learning_rate": 1.7203884615384618e-05,
      "loss": 3.2551,
      "step": 852700
    },
    {
      "epoch": 182.84734133790738,
      "grad_norm": 5.998010635375977,
      "learning_rate": 1.7200038461538463e-05,
      "loss": 3.2417,
      "step": 852800
    },
    {
      "epoch": 182.868782161235,
      "grad_norm": 5.874746799468994,
      "learning_rate": 1.719619230769231e-05,
      "loss": 3.2927,
      "step": 852900
    },
    {
      "epoch": 182.8902229845626,
      "grad_norm": 5.986566066741943,
      "learning_rate": 1.7192346153846154e-05,
      "loss": 3.3047,
      "step": 853000
    },
    {
      "epoch": 182.91166380789022,
      "grad_norm": 6.085625171661377,
      "learning_rate": 1.7188500000000003e-05,
      "loss": 3.3137,
      "step": 853100
    },
    {
      "epoch": 182.93310463121784,
      "grad_norm": 5.773239612579346,
      "learning_rate": 1.718465384615385e-05,
      "loss": 3.2913,
      "step": 853200
    },
    {
      "epoch": 182.95454545454547,
      "grad_norm": 5.5391621589660645,
      "learning_rate": 1.7180807692307694e-05,
      "loss": 3.2386,
      "step": 853300
    },
    {
      "epoch": 182.97598627787306,
      "grad_norm": 5.952091693878174,
      "learning_rate": 1.717696153846154e-05,
      "loss": 3.2524,
      "step": 853400
    },
    {
      "epoch": 182.99742710120069,
      "grad_norm": 6.089418411254883,
      "learning_rate": 1.7173115384615385e-05,
      "loss": 3.2652,
      "step": 853500
    },
    {
      "epoch": 183.0188679245283,
      "grad_norm": 6.165765762329102,
      "learning_rate": 1.716926923076923e-05,
      "loss": 3.2157,
      "step": 853600
    },
    {
      "epoch": 183.04030874785593,
      "grad_norm": 6.0036749839782715,
      "learning_rate": 1.716542307692308e-05,
      "loss": 3.2262,
      "step": 853700
    },
    {
      "epoch": 183.06174957118353,
      "grad_norm": 6.3441338539123535,
      "learning_rate": 1.7161576923076925e-05,
      "loss": 3.2632,
      "step": 853800
    },
    {
      "epoch": 183.08319039451115,
      "grad_norm": 5.902169704437256,
      "learning_rate": 1.715773076923077e-05,
      "loss": 3.2587,
      "step": 853900
    },
    {
      "epoch": 183.10463121783877,
      "grad_norm": 6.247959136962891,
      "learning_rate": 1.7153884615384616e-05,
      "loss": 3.2168,
      "step": 854000
    },
    {
      "epoch": 183.12607204116637,
      "grad_norm": 5.673214912414551,
      "learning_rate": 1.7150038461538462e-05,
      "loss": 3.2067,
      "step": 854100
    },
    {
      "epoch": 183.147512864494,
      "grad_norm": 6.149001598358154,
      "learning_rate": 1.7146192307692308e-05,
      "loss": 3.2574,
      "step": 854200
    },
    {
      "epoch": 183.1689536878216,
      "grad_norm": 5.437646389007568,
      "learning_rate": 1.7142346153846156e-05,
      "loss": 3.1978,
      "step": 854300
    },
    {
      "epoch": 183.19039451114924,
      "grad_norm": 5.834364414215088,
      "learning_rate": 1.7138500000000002e-05,
      "loss": 3.2426,
      "step": 854400
    },
    {
      "epoch": 183.21183533447683,
      "grad_norm": 5.722325801849365,
      "learning_rate": 1.7134653846153848e-05,
      "loss": 3.2056,
      "step": 854500
    },
    {
      "epoch": 183.23327615780445,
      "grad_norm": 6.018855571746826,
      "learning_rate": 1.7130807692307693e-05,
      "loss": 3.2605,
      "step": 854600
    },
    {
      "epoch": 183.25471698113208,
      "grad_norm": 6.057690620422363,
      "learning_rate": 1.712696153846154e-05,
      "loss": 3.2205,
      "step": 854700
    },
    {
      "epoch": 183.2761578044597,
      "grad_norm": 5.871840953826904,
      "learning_rate": 1.7123115384615384e-05,
      "loss": 3.2097,
      "step": 854800
    },
    {
      "epoch": 183.2975986277873,
      "grad_norm": 6.448391437530518,
      "learning_rate": 1.711926923076923e-05,
      "loss": 3.2749,
      "step": 854900
    },
    {
      "epoch": 183.31903945111492,
      "grad_norm": 6.102797985076904,
      "learning_rate": 1.711542307692308e-05,
      "loss": 3.2291,
      "step": 855000
    },
    {
      "epoch": 183.34048027444254,
      "grad_norm": 6.239814281463623,
      "learning_rate": 1.7111576923076924e-05,
      "loss": 3.2691,
      "step": 855100
    },
    {
      "epoch": 183.36192109777016,
      "grad_norm": 6.377331256866455,
      "learning_rate": 1.710773076923077e-05,
      "loss": 3.2766,
      "step": 855200
    },
    {
      "epoch": 183.38336192109776,
      "grad_norm": 5.414747714996338,
      "learning_rate": 1.7103884615384615e-05,
      "loss": 3.2219,
      "step": 855300
    },
    {
      "epoch": 183.40480274442538,
      "grad_norm": 5.770941734313965,
      "learning_rate": 1.710003846153846e-05,
      "loss": 3.2641,
      "step": 855400
    },
    {
      "epoch": 183.426243567753,
      "grad_norm": 5.6827473640441895,
      "learning_rate": 1.7096192307692306e-05,
      "loss": 3.2575,
      "step": 855500
    },
    {
      "epoch": 183.44768439108063,
      "grad_norm": 6.224225997924805,
      "learning_rate": 1.7092346153846155e-05,
      "loss": 3.2652,
      "step": 855600
    },
    {
      "epoch": 183.46912521440822,
      "grad_norm": 5.878400802612305,
      "learning_rate": 1.70885e-05,
      "loss": 3.2706,
      "step": 855700
    },
    {
      "epoch": 183.49056603773585,
      "grad_norm": 5.754587173461914,
      "learning_rate": 1.7084653846153846e-05,
      "loss": 3.2351,
      "step": 855800
    },
    {
      "epoch": 183.51200686106347,
      "grad_norm": 6.0615105628967285,
      "learning_rate": 1.7080807692307692e-05,
      "loss": 3.2076,
      "step": 855900
    },
    {
      "epoch": 183.5334476843911,
      "grad_norm": 5.412905693054199,
      "learning_rate": 1.7076961538461537e-05,
      "loss": 3.2439,
      "step": 856000
    },
    {
      "epoch": 183.5548885077187,
      "grad_norm": 6.0546875,
      "learning_rate": 1.7073115384615383e-05,
      "loss": 3.242,
      "step": 856100
    },
    {
      "epoch": 183.5763293310463,
      "grad_norm": 6.117181777954102,
      "learning_rate": 1.7069269230769232e-05,
      "loss": 3.2157,
      "step": 856200
    },
    {
      "epoch": 183.59777015437393,
      "grad_norm": 6.243173599243164,
      "learning_rate": 1.7065423076923077e-05,
      "loss": 3.214,
      "step": 856300
    },
    {
      "epoch": 183.61921097770156,
      "grad_norm": 5.222383499145508,
      "learning_rate": 1.7061576923076923e-05,
      "loss": 3.2835,
      "step": 856400
    },
    {
      "epoch": 183.64065180102915,
      "grad_norm": 5.985470294952393,
      "learning_rate": 1.7057730769230772e-05,
      "loss": 3.2575,
      "step": 856500
    },
    {
      "epoch": 183.66209262435677,
      "grad_norm": 7.167551040649414,
      "learning_rate": 1.7053884615384617e-05,
      "loss": 3.2805,
      "step": 856600
    },
    {
      "epoch": 183.6835334476844,
      "grad_norm": 5.856806755065918,
      "learning_rate": 1.7050038461538463e-05,
      "loss": 3.2714,
      "step": 856700
    },
    {
      "epoch": 183.70497427101202,
      "grad_norm": 5.699344635009766,
      "learning_rate": 1.704619230769231e-05,
      "loss": 3.2999,
      "step": 856800
    },
    {
      "epoch": 183.72641509433961,
      "grad_norm": 6.110971927642822,
      "learning_rate": 1.7042346153846157e-05,
      "loss": 3.2271,
      "step": 856900
    },
    {
      "epoch": 183.74785591766724,
      "grad_norm": 6.1432929039001465,
      "learning_rate": 1.7038500000000003e-05,
      "loss": 3.2816,
      "step": 857000
    },
    {
      "epoch": 183.76929674099486,
      "grad_norm": 6.119816303253174,
      "learning_rate": 1.703465384615385e-05,
      "loss": 3.2309,
      "step": 857100
    },
    {
      "epoch": 183.79073756432248,
      "grad_norm": 6.165647029876709,
      "learning_rate": 1.7030807692307694e-05,
      "loss": 3.3062,
      "step": 857200
    },
    {
      "epoch": 183.81217838765008,
      "grad_norm": 5.6423797607421875,
      "learning_rate": 1.702696153846154e-05,
      "loss": 3.2699,
      "step": 857300
    },
    {
      "epoch": 183.8336192109777,
      "grad_norm": 6.179776668548584,
      "learning_rate": 1.7023115384615385e-05,
      "loss": 3.225,
      "step": 857400
    },
    {
      "epoch": 183.85506003430532,
      "grad_norm": 5.987097263336182,
      "learning_rate": 1.701926923076923e-05,
      "loss": 3.2759,
      "step": 857500
    },
    {
      "epoch": 183.87650085763292,
      "grad_norm": 6.0725626945495605,
      "learning_rate": 1.701542307692308e-05,
      "loss": 3.2502,
      "step": 857600
    },
    {
      "epoch": 183.89794168096054,
      "grad_norm": 6.382688522338867,
      "learning_rate": 1.7011576923076925e-05,
      "loss": 3.3188,
      "step": 857700
    },
    {
      "epoch": 183.91938250428817,
      "grad_norm": 5.815374374389648,
      "learning_rate": 1.700773076923077e-05,
      "loss": 3.2261,
      "step": 857800
    },
    {
      "epoch": 183.9408233276158,
      "grad_norm": 5.854923248291016,
      "learning_rate": 1.7003884615384616e-05,
      "loss": 3.2985,
      "step": 857900
    },
    {
      "epoch": 183.96226415094338,
      "grad_norm": 6.409360885620117,
      "learning_rate": 1.7000038461538462e-05,
      "loss": 3.2785,
      "step": 858000
    },
    {
      "epoch": 183.983704974271,
      "grad_norm": 6.416647434234619,
      "learning_rate": 1.6996192307692307e-05,
      "loss": 3.2287,
      "step": 858100
    },
    {
      "epoch": 184.00514579759863,
      "grad_norm": 6.165995121002197,
      "learning_rate": 1.6992346153846156e-05,
      "loss": 3.269,
      "step": 858200
    },
    {
      "epoch": 184.02658662092625,
      "grad_norm": 6.117045879364014,
      "learning_rate": 1.6988500000000002e-05,
      "loss": 3.2773,
      "step": 858300
    },
    {
      "epoch": 184.04802744425385,
      "grad_norm": 5.988405704498291,
      "learning_rate": 1.6984653846153847e-05,
      "loss": 3.1949,
      "step": 858400
    },
    {
      "epoch": 184.06946826758147,
      "grad_norm": 5.942605972290039,
      "learning_rate": 1.6980807692307693e-05,
      "loss": 3.2389,
      "step": 858500
    },
    {
      "epoch": 184.0909090909091,
      "grad_norm": 6.793385982513428,
      "learning_rate": 1.697696153846154e-05,
      "loss": 3.2321,
      "step": 858600
    },
    {
      "epoch": 184.11234991423672,
      "grad_norm": 5.454989910125732,
      "learning_rate": 1.6973115384615384e-05,
      "loss": 3.2491,
      "step": 858700
    },
    {
      "epoch": 184.1337907375643,
      "grad_norm": 6.088388442993164,
      "learning_rate": 1.696926923076923e-05,
      "loss": 3.2307,
      "step": 858800
    },
    {
      "epoch": 184.15523156089193,
      "grad_norm": 5.956569671630859,
      "learning_rate": 1.696542307692308e-05,
      "loss": 3.2388,
      "step": 858900
    },
    {
      "epoch": 184.17667238421956,
      "grad_norm": 5.590498447418213,
      "learning_rate": 1.6961576923076924e-05,
      "loss": 3.2648,
      "step": 859000
    },
    {
      "epoch": 184.19811320754718,
      "grad_norm": 5.293303966522217,
      "learning_rate": 1.695773076923077e-05,
      "loss": 3.2155,
      "step": 859100
    },
    {
      "epoch": 184.21955403087478,
      "grad_norm": 5.813403606414795,
      "learning_rate": 1.6953884615384615e-05,
      "loss": 3.2626,
      "step": 859200
    },
    {
      "epoch": 184.2409948542024,
      "grad_norm": 6.037446975708008,
      "learning_rate": 1.695003846153846e-05,
      "loss": 3.2266,
      "step": 859300
    },
    {
      "epoch": 184.26243567753002,
      "grad_norm": 6.206850051879883,
      "learning_rate": 1.6946192307692306e-05,
      "loss": 3.2603,
      "step": 859400
    },
    {
      "epoch": 184.28387650085764,
      "grad_norm": 6.124607563018799,
      "learning_rate": 1.6942346153846155e-05,
      "loss": 3.2518,
      "step": 859500
    },
    {
      "epoch": 184.30531732418524,
      "grad_norm": 6.042580604553223,
      "learning_rate": 1.69385e-05,
      "loss": 3.2144,
      "step": 859600
    },
    {
      "epoch": 184.32675814751286,
      "grad_norm": 5.7854485511779785,
      "learning_rate": 1.6934653846153846e-05,
      "loss": 3.2696,
      "step": 859700
    },
    {
      "epoch": 184.34819897084049,
      "grad_norm": 5.872899055480957,
      "learning_rate": 1.693080769230769e-05,
      "loss": 3.2078,
      "step": 859800
    },
    {
      "epoch": 184.3696397941681,
      "grad_norm": 6.005081653594971,
      "learning_rate": 1.6926961538461537e-05,
      "loss": 3.2523,
      "step": 859900
    },
    {
      "epoch": 184.3910806174957,
      "grad_norm": 5.553260803222656,
      "learning_rate": 1.6923115384615386e-05,
      "loss": 3.2712,
      "step": 860000
    },
    {
      "epoch": 184.41252144082333,
      "grad_norm": 5.93950891494751,
      "learning_rate": 1.691926923076923e-05,
      "loss": 3.2259,
      "step": 860100
    },
    {
      "epoch": 184.43396226415095,
      "grad_norm": 6.030261516571045,
      "learning_rate": 1.6915423076923077e-05,
      "loss": 3.2413,
      "step": 860200
    },
    {
      "epoch": 184.45540308747857,
      "grad_norm": 5.664588451385498,
      "learning_rate": 1.6911576923076926e-05,
      "loss": 3.2242,
      "step": 860300
    },
    {
      "epoch": 184.47684391080617,
      "grad_norm": 6.00595760345459,
      "learning_rate": 1.690773076923077e-05,
      "loss": 3.2798,
      "step": 860400
    },
    {
      "epoch": 184.4982847341338,
      "grad_norm": 6.143095016479492,
      "learning_rate": 1.6903884615384617e-05,
      "loss": 3.2212,
      "step": 860500
    },
    {
      "epoch": 184.5197255574614,
      "grad_norm": 6.163722515106201,
      "learning_rate": 1.6900038461538463e-05,
      "loss": 3.2544,
      "step": 860600
    },
    {
      "epoch": 184.54116638078904,
      "grad_norm": 5.693629741668701,
      "learning_rate": 1.6896192307692308e-05,
      "loss": 3.1728,
      "step": 860700
    },
    {
      "epoch": 184.56260720411663,
      "grad_norm": 5.868861675262451,
      "learning_rate": 1.6892346153846157e-05,
      "loss": 3.2346,
      "step": 860800
    },
    {
      "epoch": 184.58404802744425,
      "grad_norm": 5.662700653076172,
      "learning_rate": 1.6888500000000003e-05,
      "loss": 3.2621,
      "step": 860900
    },
    {
      "epoch": 184.60548885077188,
      "grad_norm": 5.71437406539917,
      "learning_rate": 1.6884653846153848e-05,
      "loss": 3.198,
      "step": 861000
    },
    {
      "epoch": 184.62692967409947,
      "grad_norm": 6.069568157196045,
      "learning_rate": 1.6880807692307694e-05,
      "loss": 3.1855,
      "step": 861100
    },
    {
      "epoch": 184.6483704974271,
      "grad_norm": 6.136115550994873,
      "learning_rate": 1.687696153846154e-05,
      "loss": 3.2022,
      "step": 861200
    },
    {
      "epoch": 184.66981132075472,
      "grad_norm": 5.993172645568848,
      "learning_rate": 1.6873115384615385e-05,
      "loss": 3.3073,
      "step": 861300
    },
    {
      "epoch": 184.69125214408234,
      "grad_norm": 6.852382659912109,
      "learning_rate": 1.6869269230769234e-05,
      "loss": 3.248,
      "step": 861400
    },
    {
      "epoch": 184.71269296740994,
      "grad_norm": 6.1025848388671875,
      "learning_rate": 1.686542307692308e-05,
      "loss": 3.2809,
      "step": 861500
    },
    {
      "epoch": 184.73413379073756,
      "grad_norm": 5.90761661529541,
      "learning_rate": 1.6861576923076925e-05,
      "loss": 3.2484,
      "step": 861600
    },
    {
      "epoch": 184.75557461406518,
      "grad_norm": 5.730557441711426,
      "learning_rate": 1.685773076923077e-05,
      "loss": 3.2536,
      "step": 861700
    },
    {
      "epoch": 184.7770154373928,
      "grad_norm": 6.820138454437256,
      "learning_rate": 1.6853884615384616e-05,
      "loss": 3.2321,
      "step": 861800
    },
    {
      "epoch": 184.7984562607204,
      "grad_norm": 5.900138854980469,
      "learning_rate": 1.685003846153846e-05,
      "loss": 3.2364,
      "step": 861900
    },
    {
      "epoch": 184.81989708404802,
      "grad_norm": 5.47786808013916,
      "learning_rate": 1.6846192307692307e-05,
      "loss": 3.2858,
      "step": 862000
    },
    {
      "epoch": 184.84133790737565,
      "grad_norm": 6.254665851593018,
      "learning_rate": 1.6842346153846156e-05,
      "loss": 3.1987,
      "step": 862100
    },
    {
      "epoch": 184.86277873070327,
      "grad_norm": 5.500512599945068,
      "learning_rate": 1.68385e-05,
      "loss": 3.2201,
      "step": 862200
    },
    {
      "epoch": 184.88421955403086,
      "grad_norm": 6.192528247833252,
      "learning_rate": 1.6834653846153847e-05,
      "loss": 3.225,
      "step": 862300
    },
    {
      "epoch": 184.9056603773585,
      "grad_norm": 6.296097755432129,
      "learning_rate": 1.6830807692307693e-05,
      "loss": 3.3221,
      "step": 862400
    },
    {
      "epoch": 184.9271012006861,
      "grad_norm": 5.754288196563721,
      "learning_rate": 1.6826961538461538e-05,
      "loss": 3.2436,
      "step": 862500
    },
    {
      "epoch": 184.94854202401373,
      "grad_norm": 6.382750511169434,
      "learning_rate": 1.6823115384615384e-05,
      "loss": 3.2171,
      "step": 862600
    },
    {
      "epoch": 184.96998284734133,
      "grad_norm": 6.317629814147949,
      "learning_rate": 1.6819269230769233e-05,
      "loss": 3.2778,
      "step": 862700
    },
    {
      "epoch": 184.99142367066895,
      "grad_norm": 6.656224727630615,
      "learning_rate": 1.6815423076923078e-05,
      "loss": 3.2808,
      "step": 862800
    },
    {
      "epoch": 185.01286449399657,
      "grad_norm": 5.798364162445068,
      "learning_rate": 1.6811576923076924e-05,
      "loss": 3.2283,
      "step": 862900
    },
    {
      "epoch": 185.0343053173242,
      "grad_norm": 6.087501525878906,
      "learning_rate": 1.680773076923077e-05,
      "loss": 3.2086,
      "step": 863000
    },
    {
      "epoch": 185.0557461406518,
      "grad_norm": 6.057308197021484,
      "learning_rate": 1.6803884615384615e-05,
      "loss": 3.1916,
      "step": 863100
    },
    {
      "epoch": 185.07718696397941,
      "grad_norm": 5.947593688964844,
      "learning_rate": 1.680003846153846e-05,
      "loss": 3.2033,
      "step": 863200
    },
    {
      "epoch": 185.09862778730704,
      "grad_norm": 5.912086009979248,
      "learning_rate": 1.6796192307692306e-05,
      "loss": 3.2534,
      "step": 863300
    },
    {
      "epoch": 185.12006861063466,
      "grad_norm": 6.099925994873047,
      "learning_rate": 1.6792346153846155e-05,
      "loss": 3.2318,
      "step": 863400
    },
    {
      "epoch": 185.14150943396226,
      "grad_norm": 6.044529438018799,
      "learning_rate": 1.67885e-05,
      "loss": 3.2733,
      "step": 863500
    },
    {
      "epoch": 185.16295025728988,
      "grad_norm": 5.960051536560059,
      "learning_rate": 1.6784653846153846e-05,
      "loss": 3.248,
      "step": 863600
    },
    {
      "epoch": 185.1843910806175,
      "grad_norm": 5.893250465393066,
      "learning_rate": 1.678080769230769e-05,
      "loss": 3.2069,
      "step": 863700
    },
    {
      "epoch": 185.20583190394512,
      "grad_norm": 6.200315952301025,
      "learning_rate": 1.677696153846154e-05,
      "loss": 3.2153,
      "step": 863800
    },
    {
      "epoch": 185.22727272727272,
      "grad_norm": 6.406674861907959,
      "learning_rate": 1.6773115384615386e-05,
      "loss": 3.2447,
      "step": 863900
    },
    {
      "epoch": 185.24871355060034,
      "grad_norm": 6.448522090911865,
      "learning_rate": 1.676926923076923e-05,
      "loss": 3.2232,
      "step": 864000
    },
    {
      "epoch": 185.27015437392797,
      "grad_norm": 5.975727081298828,
      "learning_rate": 1.6765423076923077e-05,
      "loss": 3.2351,
      "step": 864100
    },
    {
      "epoch": 185.29159519725556,
      "grad_norm": 5.780733108520508,
      "learning_rate": 1.6761576923076926e-05,
      "loss": 3.2356,
      "step": 864200
    },
    {
      "epoch": 185.31303602058318,
      "grad_norm": 5.432823657989502,
      "learning_rate": 1.675773076923077e-05,
      "loss": 3.2149,
      "step": 864300
    },
    {
      "epoch": 185.3344768439108,
      "grad_norm": 5.7426581382751465,
      "learning_rate": 1.6753884615384617e-05,
      "loss": 3.2225,
      "step": 864400
    },
    {
      "epoch": 185.35591766723843,
      "grad_norm": 6.102722644805908,
      "learning_rate": 1.6750038461538462e-05,
      "loss": 3.2488,
      "step": 864500
    },
    {
      "epoch": 185.37735849056602,
      "grad_norm": 6.115590572357178,
      "learning_rate": 1.6746192307692308e-05,
      "loss": 3.27,
      "step": 864600
    },
    {
      "epoch": 185.39879931389365,
      "grad_norm": 6.082345962524414,
      "learning_rate": 1.6742346153846157e-05,
      "loss": 3.2389,
      "step": 864700
    },
    {
      "epoch": 185.42024013722127,
      "grad_norm": 6.5247650146484375,
      "learning_rate": 1.6738500000000002e-05,
      "loss": 3.1757,
      "step": 864800
    },
    {
      "epoch": 185.4416809605489,
      "grad_norm": 5.782588481903076,
      "learning_rate": 1.6734653846153848e-05,
      "loss": 3.2297,
      "step": 864900
    },
    {
      "epoch": 185.4631217838765,
      "grad_norm": 5.939264297485352,
      "learning_rate": 1.6730807692307694e-05,
      "loss": 3.2276,
      "step": 865000
    },
    {
      "epoch": 185.4845626072041,
      "grad_norm": 6.4902753829956055,
      "learning_rate": 1.672696153846154e-05,
      "loss": 3.1924,
      "step": 865100
    },
    {
      "epoch": 185.50600343053173,
      "grad_norm": 5.969719886779785,
      "learning_rate": 1.6723115384615385e-05,
      "loss": 3.2303,
      "step": 865200
    },
    {
      "epoch": 185.52744425385936,
      "grad_norm": 5.55759859085083,
      "learning_rate": 1.6719269230769234e-05,
      "loss": 3.2493,
      "step": 865300
    },
    {
      "epoch": 185.54888507718695,
      "grad_norm": 6.124976634979248,
      "learning_rate": 1.671542307692308e-05,
      "loss": 3.2603,
      "step": 865400
    },
    {
      "epoch": 185.57032590051458,
      "grad_norm": 6.198690891265869,
      "learning_rate": 1.6711576923076925e-05,
      "loss": 3.264,
      "step": 865500
    },
    {
      "epoch": 185.5917667238422,
      "grad_norm": 5.962198734283447,
      "learning_rate": 1.670773076923077e-05,
      "loss": 3.2741,
      "step": 865600
    },
    {
      "epoch": 185.61320754716982,
      "grad_norm": 6.34291934967041,
      "learning_rate": 1.6703884615384616e-05,
      "loss": 3.2111,
      "step": 865700
    },
    {
      "epoch": 185.63464837049742,
      "grad_norm": 5.8086347579956055,
      "learning_rate": 1.670003846153846e-05,
      "loss": 3.257,
      "step": 865800
    },
    {
      "epoch": 185.65608919382504,
      "grad_norm": 6.052178382873535,
      "learning_rate": 1.6696192307692307e-05,
      "loss": 3.2213,
      "step": 865900
    },
    {
      "epoch": 185.67753001715266,
      "grad_norm": 5.6852803230285645,
      "learning_rate": 1.6692346153846156e-05,
      "loss": 3.2047,
      "step": 866000
    },
    {
      "epoch": 185.69897084048029,
      "grad_norm": 5.879289627075195,
      "learning_rate": 1.66885e-05,
      "loss": 3.2471,
      "step": 866100
    },
    {
      "epoch": 185.72041166380788,
      "grad_norm": 6.311210632324219,
      "learning_rate": 1.6684653846153847e-05,
      "loss": 3.2739,
      "step": 866200
    },
    {
      "epoch": 185.7418524871355,
      "grad_norm": 5.795952796936035,
      "learning_rate": 1.6680807692307692e-05,
      "loss": 3.2573,
      "step": 866300
    },
    {
      "epoch": 185.76329331046313,
      "grad_norm": 5.730776309967041,
      "learning_rate": 1.6676961538461538e-05,
      "loss": 3.2258,
      "step": 866400
    },
    {
      "epoch": 185.78473413379075,
      "grad_norm": 5.4602227210998535,
      "learning_rate": 1.6673115384615383e-05,
      "loss": 3.2065,
      "step": 866500
    },
    {
      "epoch": 185.80617495711834,
      "grad_norm": 6.851008415222168,
      "learning_rate": 1.6669269230769232e-05,
      "loss": 3.2512,
      "step": 866600
    },
    {
      "epoch": 185.82761578044597,
      "grad_norm": 6.026920318603516,
      "learning_rate": 1.6665423076923078e-05,
      "loss": 3.251,
      "step": 866700
    },
    {
      "epoch": 185.8490566037736,
      "grad_norm": 6.397674560546875,
      "learning_rate": 1.6661576923076923e-05,
      "loss": 3.2247,
      "step": 866800
    },
    {
      "epoch": 185.8704974271012,
      "grad_norm": 5.810260772705078,
      "learning_rate": 1.665773076923077e-05,
      "loss": 3.2238,
      "step": 866900
    },
    {
      "epoch": 185.8919382504288,
      "grad_norm": 6.244442462921143,
      "learning_rate": 1.6653884615384614e-05,
      "loss": 3.2615,
      "step": 867000
    },
    {
      "epoch": 185.91337907375643,
      "grad_norm": 6.499228000640869,
      "learning_rate": 1.665003846153846e-05,
      "loss": 3.1755,
      "step": 867100
    },
    {
      "epoch": 185.93481989708405,
      "grad_norm": 6.211434364318848,
      "learning_rate": 1.664619230769231e-05,
      "loss": 3.2363,
      "step": 867200
    },
    {
      "epoch": 185.95626072041168,
      "grad_norm": 5.924498558044434,
      "learning_rate": 1.6642346153846154e-05,
      "loss": 3.277,
      "step": 867300
    },
    {
      "epoch": 185.97770154373927,
      "grad_norm": 6.141495227813721,
      "learning_rate": 1.66385e-05,
      "loss": 3.2144,
      "step": 867400
    },
    {
      "epoch": 185.9991423670669,
      "grad_norm": 5.396970748901367,
      "learning_rate": 1.6634653846153846e-05,
      "loss": 3.252,
      "step": 867500
    },
    {
      "epoch": 186.02058319039452,
      "grad_norm": 6.165437698364258,
      "learning_rate": 1.663080769230769e-05,
      "loss": 3.2126,
      "step": 867600
    },
    {
      "epoch": 186.0420240137221,
      "grad_norm": 5.959512710571289,
      "learning_rate": 1.662696153846154e-05,
      "loss": 3.2257,
      "step": 867700
    },
    {
      "epoch": 186.06346483704974,
      "grad_norm": 6.463181972503662,
      "learning_rate": 1.6623115384615386e-05,
      "loss": 3.1922,
      "step": 867800
    },
    {
      "epoch": 186.08490566037736,
      "grad_norm": 6.0208892822265625,
      "learning_rate": 1.661926923076923e-05,
      "loss": 3.2356,
      "step": 867900
    },
    {
      "epoch": 186.10634648370498,
      "grad_norm": 6.064216136932373,
      "learning_rate": 1.661542307692308e-05,
      "loss": 3.2606,
      "step": 868000
    },
    {
      "epoch": 186.12778730703258,
      "grad_norm": 5.325770854949951,
      "learning_rate": 1.6611576923076926e-05,
      "loss": 3.2504,
      "step": 868100
    },
    {
      "epoch": 186.1492281303602,
      "grad_norm": 6.798454284667969,
      "learning_rate": 1.660773076923077e-05,
      "loss": 3.1872,
      "step": 868200
    },
    {
      "epoch": 186.17066895368782,
      "grad_norm": 5.820725917816162,
      "learning_rate": 1.6603884615384617e-05,
      "loss": 3.2763,
      "step": 868300
    },
    {
      "epoch": 186.19210977701545,
      "grad_norm": 5.59514856338501,
      "learning_rate": 1.6600038461538462e-05,
      "loss": 3.2263,
      "step": 868400
    },
    {
      "epoch": 186.21355060034304,
      "grad_norm": 5.780966758728027,
      "learning_rate": 1.659619230769231e-05,
      "loss": 3.195,
      "step": 868500
    },
    {
      "epoch": 186.23499142367066,
      "grad_norm": 5.785341739654541,
      "learning_rate": 1.6592346153846157e-05,
      "loss": 3.2085,
      "step": 868600
    },
    {
      "epoch": 186.2564322469983,
      "grad_norm": 5.743597507476807,
      "learning_rate": 1.6588500000000002e-05,
      "loss": 3.2315,
      "step": 868700
    },
    {
      "epoch": 186.2778730703259,
      "grad_norm": 6.2165727615356445,
      "learning_rate": 1.6584653846153848e-05,
      "loss": 3.2239,
      "step": 868800
    },
    {
      "epoch": 186.2993138936535,
      "grad_norm": 5.592644214630127,
      "learning_rate": 1.6580807692307693e-05,
      "loss": 3.2394,
      "step": 868900
    },
    {
      "epoch": 186.32075471698113,
      "grad_norm": 6.3475022315979,
      "learning_rate": 1.657696153846154e-05,
      "loss": 3.2002,
      "step": 869000
    },
    {
      "epoch": 186.34219554030875,
      "grad_norm": 5.388920307159424,
      "learning_rate": 1.6573115384615384e-05,
      "loss": 3.2335,
      "step": 869100
    },
    {
      "epoch": 186.36363636363637,
      "grad_norm": 5.7154669761657715,
      "learning_rate": 1.6569269230769233e-05,
      "loss": 3.2682,
      "step": 869200
    },
    {
      "epoch": 186.38507718696397,
      "grad_norm": 6.293440818786621,
      "learning_rate": 1.656542307692308e-05,
      "loss": 3.2686,
      "step": 869300
    },
    {
      "epoch": 186.4065180102916,
      "grad_norm": 7.074215888977051,
      "learning_rate": 1.6561576923076924e-05,
      "loss": 3.2136,
      "step": 869400
    },
    {
      "epoch": 186.42795883361921,
      "grad_norm": 5.3807692527771,
      "learning_rate": 1.655773076923077e-05,
      "loss": 3.2544,
      "step": 869500
    },
    {
      "epoch": 186.44939965694684,
      "grad_norm": 5.9365386962890625,
      "learning_rate": 1.6553884615384615e-05,
      "loss": 3.2183,
      "step": 869600
    },
    {
      "epoch": 186.47084048027443,
      "grad_norm": 6.3055009841918945,
      "learning_rate": 1.655003846153846e-05,
      "loss": 3.1649,
      "step": 869700
    },
    {
      "epoch": 186.49228130360206,
      "grad_norm": 5.820730209350586,
      "learning_rate": 1.654619230769231e-05,
      "loss": 3.1751,
      "step": 869800
    },
    {
      "epoch": 186.51372212692968,
      "grad_norm": 5.949606895446777,
      "learning_rate": 1.6542346153846155e-05,
      "loss": 3.227,
      "step": 869900
    },
    {
      "epoch": 186.5351629502573,
      "grad_norm": 5.604736804962158,
      "learning_rate": 1.65385e-05,
      "loss": 3.2256,
      "step": 870000
    },
    {
      "epoch": 186.5566037735849,
      "grad_norm": 7.015135288238525,
      "learning_rate": 1.6534653846153847e-05,
      "loss": 3.1917,
      "step": 870100
    },
    {
      "epoch": 186.57804459691252,
      "grad_norm": 4.875865936279297,
      "learning_rate": 1.6530807692307692e-05,
      "loss": 3.2058,
      "step": 870200
    },
    {
      "epoch": 186.59948542024014,
      "grad_norm": 5.909588813781738,
      "learning_rate": 1.6526961538461538e-05,
      "loss": 3.1942,
      "step": 870300
    },
    {
      "epoch": 186.62092624356777,
      "grad_norm": 6.025161266326904,
      "learning_rate": 1.6523115384615383e-05,
      "loss": 3.1736,
      "step": 870400
    },
    {
      "epoch": 186.64236706689536,
      "grad_norm": 6.031039237976074,
      "learning_rate": 1.6519269230769232e-05,
      "loss": 3.2227,
      "step": 870500
    },
    {
      "epoch": 186.66380789022298,
      "grad_norm": 5.72477912902832,
      "learning_rate": 1.6515423076923078e-05,
      "loss": 3.1929,
      "step": 870600
    },
    {
      "epoch": 186.6852487135506,
      "grad_norm": 5.934604167938232,
      "learning_rate": 1.6511576923076923e-05,
      "loss": 3.2463,
      "step": 870700
    },
    {
      "epoch": 186.70668953687823,
      "grad_norm": 5.918269157409668,
      "learning_rate": 1.650773076923077e-05,
      "loss": 3.2318,
      "step": 870800
    },
    {
      "epoch": 186.72813036020582,
      "grad_norm": 6.0806427001953125,
      "learning_rate": 1.6503884615384614e-05,
      "loss": 3.2258,
      "step": 870900
    },
    {
      "epoch": 186.74957118353345,
      "grad_norm": 6.006855010986328,
      "learning_rate": 1.650003846153846e-05,
      "loss": 3.1692,
      "step": 871000
    },
    {
      "epoch": 186.77101200686107,
      "grad_norm": 6.137872219085693,
      "learning_rate": 1.649619230769231e-05,
      "loss": 3.2164,
      "step": 871100
    },
    {
      "epoch": 186.79245283018867,
      "grad_norm": 5.934755802154541,
      "learning_rate": 1.6492346153846154e-05,
      "loss": 3.2524,
      "step": 871200
    },
    {
      "epoch": 186.8138936535163,
      "grad_norm": 6.01163387298584,
      "learning_rate": 1.64885e-05,
      "loss": 3.2256,
      "step": 871300
    },
    {
      "epoch": 186.8353344768439,
      "grad_norm": 6.177252292633057,
      "learning_rate": 1.6484653846153845e-05,
      "loss": 3.245,
      "step": 871400
    },
    {
      "epoch": 186.85677530017153,
      "grad_norm": 6.193002700805664,
      "learning_rate": 1.6480807692307694e-05,
      "loss": 3.2512,
      "step": 871500
    },
    {
      "epoch": 186.87821612349913,
      "grad_norm": 6.37186861038208,
      "learning_rate": 1.647696153846154e-05,
      "loss": 3.2682,
      "step": 871600
    },
    {
      "epoch": 186.89965694682675,
      "grad_norm": 6.060973167419434,
      "learning_rate": 1.6473115384615385e-05,
      "loss": 3.2402,
      "step": 871700
    },
    {
      "epoch": 186.92109777015438,
      "grad_norm": 5.834306240081787,
      "learning_rate": 1.6469269230769234e-05,
      "loss": 3.2444,
      "step": 871800
    },
    {
      "epoch": 186.942538593482,
      "grad_norm": 6.7070465087890625,
      "learning_rate": 1.646542307692308e-05,
      "loss": 3.2072,
      "step": 871900
    },
    {
      "epoch": 186.9639794168096,
      "grad_norm": 5.453552722930908,
      "learning_rate": 1.6461576923076925e-05,
      "loss": 3.2338,
      "step": 872000
    },
    {
      "epoch": 186.98542024013722,
      "grad_norm": 5.799275875091553,
      "learning_rate": 1.645773076923077e-05,
      "loss": 3.2502,
      "step": 872100
    },
    {
      "epoch": 187.00686106346484,
      "grad_norm": 6.163858890533447,
      "learning_rate": 1.6453884615384616e-05,
      "loss": 3.2371,
      "step": 872200
    },
    {
      "epoch": 187.02830188679246,
      "grad_norm": 6.505345821380615,
      "learning_rate": 1.6450038461538462e-05,
      "loss": 3.2049,
      "step": 872300
    },
    {
      "epoch": 187.04974271012006,
      "grad_norm": 6.049615859985352,
      "learning_rate": 1.644619230769231e-05,
      "loss": 3.1886,
      "step": 872400
    },
    {
      "epoch": 187.07118353344768,
      "grad_norm": 6.000053882598877,
      "learning_rate": 1.6442346153846156e-05,
      "loss": 3.1837,
      "step": 872500
    },
    {
      "epoch": 187.0926243567753,
      "grad_norm": 6.729565143585205,
      "learning_rate": 1.6438500000000002e-05,
      "loss": 3.1982,
      "step": 872600
    },
    {
      "epoch": 187.11406518010293,
      "grad_norm": 5.852939605712891,
      "learning_rate": 1.6434653846153847e-05,
      "loss": 3.2406,
      "step": 872700
    },
    {
      "epoch": 187.13550600343052,
      "grad_norm": 5.96196985244751,
      "learning_rate": 1.6430807692307693e-05,
      "loss": 3.1941,
      "step": 872800
    },
    {
      "epoch": 187.15694682675814,
      "grad_norm": 5.979584693908691,
      "learning_rate": 1.642696153846154e-05,
      "loss": 3.224,
      "step": 872900
    },
    {
      "epoch": 187.17838765008577,
      "grad_norm": 5.941946506500244,
      "learning_rate": 1.6423115384615384e-05,
      "loss": 3.2145,
      "step": 873000
    },
    {
      "epoch": 187.1998284734134,
      "grad_norm": 6.236173629760742,
      "learning_rate": 1.6419269230769233e-05,
      "loss": 3.246,
      "step": 873100
    },
    {
      "epoch": 187.22126929674099,
      "grad_norm": 5.795766353607178,
      "learning_rate": 1.641542307692308e-05,
      "loss": 3.2127,
      "step": 873200
    },
    {
      "epoch": 187.2427101200686,
      "grad_norm": 6.049345970153809,
      "learning_rate": 1.6411576923076924e-05,
      "loss": 3.1767,
      "step": 873300
    },
    {
      "epoch": 187.26415094339623,
      "grad_norm": 5.558868885040283,
      "learning_rate": 1.640773076923077e-05,
      "loss": 3.2482,
      "step": 873400
    },
    {
      "epoch": 187.28559176672385,
      "grad_norm": 5.457306385040283,
      "learning_rate": 1.6403884615384615e-05,
      "loss": 3.2655,
      "step": 873500
    },
    {
      "epoch": 187.30703259005145,
      "grad_norm": 5.8891730308532715,
      "learning_rate": 1.640003846153846e-05,
      "loss": 3.2728,
      "step": 873600
    },
    {
      "epoch": 187.32847341337907,
      "grad_norm": 5.95687198638916,
      "learning_rate": 1.639619230769231e-05,
      "loss": 3.2335,
      "step": 873700
    },
    {
      "epoch": 187.3499142367067,
      "grad_norm": 6.064459800720215,
      "learning_rate": 1.6392346153846155e-05,
      "loss": 3.2115,
      "step": 873800
    },
    {
      "epoch": 187.37135506003432,
      "grad_norm": 6.386096000671387,
      "learning_rate": 1.63885e-05,
      "loss": 3.1876,
      "step": 873900
    },
    {
      "epoch": 187.3927958833619,
      "grad_norm": 5.733015060424805,
      "learning_rate": 1.6384653846153846e-05,
      "loss": 3.2111,
      "step": 874000
    },
    {
      "epoch": 187.41423670668954,
      "grad_norm": 5.929770469665527,
      "learning_rate": 1.6380807692307692e-05,
      "loss": 3.1916,
      "step": 874100
    },
    {
      "epoch": 187.43567753001716,
      "grad_norm": 5.978481292724609,
      "learning_rate": 1.6376961538461537e-05,
      "loss": 3.1501,
      "step": 874200
    },
    {
      "epoch": 187.45711835334478,
      "grad_norm": 5.104775905609131,
      "learning_rate": 1.6373115384615386e-05,
      "loss": 3.1605,
      "step": 874300
    },
    {
      "epoch": 187.47855917667238,
      "grad_norm": 5.409114360809326,
      "learning_rate": 1.6369269230769232e-05,
      "loss": 3.2769,
      "step": 874400
    },
    {
      "epoch": 187.5,
      "grad_norm": 6.276594638824463,
      "learning_rate": 1.6365423076923077e-05,
      "loss": 3.2351,
      "step": 874500
    },
    {
      "epoch": 187.52144082332762,
      "grad_norm": 6.295581340789795,
      "learning_rate": 1.6361576923076923e-05,
      "loss": 3.2582,
      "step": 874600
    },
    {
      "epoch": 187.54288164665522,
      "grad_norm": 5.441233158111572,
      "learning_rate": 1.635773076923077e-05,
      "loss": 3.2263,
      "step": 874700
    },
    {
      "epoch": 187.56432246998284,
      "grad_norm": 5.831089496612549,
      "learning_rate": 1.6353884615384614e-05,
      "loss": 3.2407,
      "step": 874800
    },
    {
      "epoch": 187.58576329331046,
      "grad_norm": 6.286087512969971,
      "learning_rate": 1.635003846153846e-05,
      "loss": 3.2418,
      "step": 874900
    },
    {
      "epoch": 187.6072041166381,
      "grad_norm": 5.74520206451416,
      "learning_rate": 1.634619230769231e-05,
      "loss": 3.2416,
      "step": 875000
    },
    {
      "epoch": 187.62864493996568,
      "grad_norm": 5.898694038391113,
      "learning_rate": 1.6342346153846154e-05,
      "loss": 3.1914,
      "step": 875100
    },
    {
      "epoch": 187.6500857632933,
      "grad_norm": 6.076498985290527,
      "learning_rate": 1.63385e-05,
      "loss": 3.2334,
      "step": 875200
    },
    {
      "epoch": 187.67152658662093,
      "grad_norm": 6.202884197235107,
      "learning_rate": 1.633465384615385e-05,
      "loss": 3.2503,
      "step": 875300
    },
    {
      "epoch": 187.69296740994855,
      "grad_norm": 6.065371036529541,
      "learning_rate": 1.6330807692307694e-05,
      "loss": 3.1945,
      "step": 875400
    },
    {
      "epoch": 187.71440823327615,
      "grad_norm": 5.96604061126709,
      "learning_rate": 1.632696153846154e-05,
      "loss": 3.2095,
      "step": 875500
    },
    {
      "epoch": 187.73584905660377,
      "grad_norm": 5.609729290008545,
      "learning_rate": 1.6323115384615385e-05,
      "loss": 3.2108,
      "step": 875600
    },
    {
      "epoch": 187.7572898799314,
      "grad_norm": 6.15504264831543,
      "learning_rate": 1.6319269230769234e-05,
      "loss": 3.1939,
      "step": 875700
    },
    {
      "epoch": 187.77873070325901,
      "grad_norm": 6.426243305206299,
      "learning_rate": 1.631542307692308e-05,
      "loss": 3.2257,
      "step": 875800
    },
    {
      "epoch": 187.8001715265866,
      "grad_norm": 6.18122673034668,
      "learning_rate": 1.6311576923076925e-05,
      "loss": 3.1843,
      "step": 875900
    },
    {
      "epoch": 187.82161234991423,
      "grad_norm": 6.239516735076904,
      "learning_rate": 1.630773076923077e-05,
      "loss": 3.2556,
      "step": 876000
    },
    {
      "epoch": 187.84305317324186,
      "grad_norm": 6.33793830871582,
      "learning_rate": 1.6303884615384616e-05,
      "loss": 3.1836,
      "step": 876100
    },
    {
      "epoch": 187.86449399656948,
      "grad_norm": 7.157633304595947,
      "learning_rate": 1.630003846153846e-05,
      "loss": 3.1806,
      "step": 876200
    },
    {
      "epoch": 187.88593481989707,
      "grad_norm": 5.892934799194336,
      "learning_rate": 1.629619230769231e-05,
      "loss": 3.2223,
      "step": 876300
    },
    {
      "epoch": 187.9073756432247,
      "grad_norm": 6.201083183288574,
      "learning_rate": 1.6292346153846156e-05,
      "loss": 3.2505,
      "step": 876400
    },
    {
      "epoch": 187.92881646655232,
      "grad_norm": 6.019430637359619,
      "learning_rate": 1.62885e-05,
      "loss": 3.2046,
      "step": 876500
    },
    {
      "epoch": 187.95025728987994,
      "grad_norm": 5.378351211547852,
      "learning_rate": 1.6284653846153847e-05,
      "loss": 3.2048,
      "step": 876600
    },
    {
      "epoch": 187.97169811320754,
      "grad_norm": 5.888986110687256,
      "learning_rate": 1.6280807692307693e-05,
      "loss": 3.1892,
      "step": 876700
    },
    {
      "epoch": 187.99313893653516,
      "grad_norm": 5.845395565032959,
      "learning_rate": 1.6276961538461538e-05,
      "loss": 3.2543,
      "step": 876800
    },
    {
      "epoch": 188.01457975986278,
      "grad_norm": 5.959691524505615,
      "learning_rate": 1.6273115384615387e-05,
      "loss": 3.2378,
      "step": 876900
    },
    {
      "epoch": 188.0360205831904,
      "grad_norm": 6.5068440437316895,
      "learning_rate": 1.6269269230769233e-05,
      "loss": 3.1977,
      "step": 877000
    },
    {
      "epoch": 188.057461406518,
      "grad_norm": 6.468109130859375,
      "learning_rate": 1.6265423076923078e-05,
      "loss": 3.1019,
      "step": 877100
    },
    {
      "epoch": 188.07890222984562,
      "grad_norm": 5.96546745300293,
      "learning_rate": 1.6261576923076924e-05,
      "loss": 3.2063,
      "step": 877200
    },
    {
      "epoch": 188.10034305317325,
      "grad_norm": 5.869087219238281,
      "learning_rate": 1.625773076923077e-05,
      "loss": 3.1477,
      "step": 877300
    },
    {
      "epoch": 188.12178387650087,
      "grad_norm": 5.5921430587768555,
      "learning_rate": 1.6253884615384615e-05,
      "loss": 3.1609,
      "step": 877400
    },
    {
      "epoch": 188.14322469982847,
      "grad_norm": 6.235785007476807,
      "learning_rate": 1.625003846153846e-05,
      "loss": 3.2373,
      "step": 877500
    },
    {
      "epoch": 188.1646655231561,
      "grad_norm": 5.640720844268799,
      "learning_rate": 1.624619230769231e-05,
      "loss": 3.2147,
      "step": 877600
    },
    {
      "epoch": 188.1861063464837,
      "grad_norm": 5.8076300621032715,
      "learning_rate": 1.6242346153846155e-05,
      "loss": 3.2101,
      "step": 877700
    },
    {
      "epoch": 188.20754716981133,
      "grad_norm": 5.368980884552002,
      "learning_rate": 1.62385e-05,
      "loss": 3.2022,
      "step": 877800
    },
    {
      "epoch": 188.22898799313893,
      "grad_norm": 5.893825054168701,
      "learning_rate": 1.6234653846153846e-05,
      "loss": 3.1844,
      "step": 877900
    },
    {
      "epoch": 188.25042881646655,
      "grad_norm": 5.728580951690674,
      "learning_rate": 1.623080769230769e-05,
      "loss": 3.2477,
      "step": 878000
    },
    {
      "epoch": 188.27186963979418,
      "grad_norm": 5.963316917419434,
      "learning_rate": 1.6226961538461537e-05,
      "loss": 3.2089,
      "step": 878100
    },
    {
      "epoch": 188.29331046312177,
      "grad_norm": 5.735794544219971,
      "learning_rate": 1.6223115384615386e-05,
      "loss": 3.2439,
      "step": 878200
    },
    {
      "epoch": 188.3147512864494,
      "grad_norm": 5.78550910949707,
      "learning_rate": 1.621926923076923e-05,
      "loss": 3.1923,
      "step": 878300
    },
    {
      "epoch": 188.33619210977702,
      "grad_norm": 6.356637477874756,
      "learning_rate": 1.6215423076923077e-05,
      "loss": 3.2269,
      "step": 878400
    },
    {
      "epoch": 188.35763293310464,
      "grad_norm": 5.800907611846924,
      "learning_rate": 1.6211576923076923e-05,
      "loss": 3.1663,
      "step": 878500
    },
    {
      "epoch": 188.37907375643223,
      "grad_norm": 6.083091735839844,
      "learning_rate": 1.6207730769230768e-05,
      "loss": 3.2531,
      "step": 878600
    },
    {
      "epoch": 188.40051457975986,
      "grad_norm": 5.495274066925049,
      "learning_rate": 1.6203884615384614e-05,
      "loss": 3.2092,
      "step": 878700
    },
    {
      "epoch": 188.42195540308748,
      "grad_norm": 5.596004962921143,
      "learning_rate": 1.6200038461538463e-05,
      "loss": 3.2086,
      "step": 878800
    },
    {
      "epoch": 188.4433962264151,
      "grad_norm": 6.321983337402344,
      "learning_rate": 1.6196192307692308e-05,
      "loss": 3.2358,
      "step": 878900
    },
    {
      "epoch": 188.4648370497427,
      "grad_norm": 6.254012584686279,
      "learning_rate": 1.6192346153846154e-05,
      "loss": 3.1804,
      "step": 879000
    },
    {
      "epoch": 188.48627787307032,
      "grad_norm": 6.090149402618408,
      "learning_rate": 1.61885e-05,
      "loss": 3.2112,
      "step": 879100
    },
    {
      "epoch": 188.50771869639794,
      "grad_norm": 5.531989097595215,
      "learning_rate": 1.6184653846153848e-05,
      "loss": 3.1912,
      "step": 879200
    },
    {
      "epoch": 188.52915951972557,
      "grad_norm": 6.359208583831787,
      "learning_rate": 1.6180807692307694e-05,
      "loss": 3.2304,
      "step": 879300
    },
    {
      "epoch": 188.55060034305316,
      "grad_norm": 5.825510501861572,
      "learning_rate": 1.617696153846154e-05,
      "loss": 3.2012,
      "step": 879400
    },
    {
      "epoch": 188.57204116638079,
      "grad_norm": 6.555271625518799,
      "learning_rate": 1.6173115384615388e-05,
      "loss": 3.3046,
      "step": 879500
    },
    {
      "epoch": 188.5934819897084,
      "grad_norm": 5.821519374847412,
      "learning_rate": 1.6169269230769234e-05,
      "loss": 3.17,
      "step": 879600
    },
    {
      "epoch": 188.61492281303603,
      "grad_norm": 5.447452545166016,
      "learning_rate": 1.616542307692308e-05,
      "loss": 3.2319,
      "step": 879700
    },
    {
      "epoch": 188.63636363636363,
      "grad_norm": 6.108701229095459,
      "learning_rate": 1.6161576923076925e-05,
      "loss": 3.209,
      "step": 879800
    },
    {
      "epoch": 188.65780445969125,
      "grad_norm": 6.428982734680176,
      "learning_rate": 1.615773076923077e-05,
      "loss": 3.2073,
      "step": 879900
    },
    {
      "epoch": 188.67924528301887,
      "grad_norm": 6.0993876457214355,
      "learning_rate": 1.6153884615384616e-05,
      "loss": 3.161,
      "step": 880000
    },
    {
      "epoch": 188.7006861063465,
      "grad_norm": 5.836090087890625,
      "learning_rate": 1.6150038461538465e-05,
      "loss": 3.2243,
      "step": 880100
    },
    {
      "epoch": 188.7221269296741,
      "grad_norm": 5.406834602355957,
      "learning_rate": 1.614619230769231e-05,
      "loss": 3.1695,
      "step": 880200
    },
    {
      "epoch": 188.7435677530017,
      "grad_norm": 5.946056842803955,
      "learning_rate": 1.6142346153846156e-05,
      "loss": 3.2262,
      "step": 880300
    },
    {
      "epoch": 188.76500857632934,
      "grad_norm": 5.753856658935547,
      "learning_rate": 1.61385e-05,
      "loss": 3.1983,
      "step": 880400
    },
    {
      "epoch": 188.78644939965696,
      "grad_norm": 5.894343376159668,
      "learning_rate": 1.6134653846153847e-05,
      "loss": 3.1721,
      "step": 880500
    },
    {
      "epoch": 188.80789022298455,
      "grad_norm": 5.849480628967285,
      "learning_rate": 1.6130807692307693e-05,
      "loss": 3.1903,
      "step": 880600
    },
    {
      "epoch": 188.82933104631218,
      "grad_norm": 6.095202922821045,
      "learning_rate": 1.6126961538461538e-05,
      "loss": 3.2259,
      "step": 880700
    },
    {
      "epoch": 188.8507718696398,
      "grad_norm": 5.770621299743652,
      "learning_rate": 1.6123115384615387e-05,
      "loss": 3.2295,
      "step": 880800
    },
    {
      "epoch": 188.87221269296742,
      "grad_norm": 5.927354335784912,
      "learning_rate": 1.6119269230769233e-05,
      "loss": 3.2547,
      "step": 880900
    },
    {
      "epoch": 188.89365351629502,
      "grad_norm": 6.1112518310546875,
      "learning_rate": 1.6115423076923078e-05,
      "loss": 3.1877,
      "step": 881000
    },
    {
      "epoch": 188.91509433962264,
      "grad_norm": 5.945371627807617,
      "learning_rate": 1.6111576923076924e-05,
      "loss": 3.2129,
      "step": 881100
    },
    {
      "epoch": 188.93653516295026,
      "grad_norm": 5.153692722320557,
      "learning_rate": 1.610773076923077e-05,
      "loss": 3.2482,
      "step": 881200
    },
    {
      "epoch": 188.9579759862779,
      "grad_norm": 6.3436503410339355,
      "learning_rate": 1.6103884615384615e-05,
      "loss": 3.2427,
      "step": 881300
    },
    {
      "epoch": 188.97941680960548,
      "grad_norm": 6.535051345825195,
      "learning_rate": 1.6100038461538464e-05,
      "loss": 3.2013,
      "step": 881400
    },
    {
      "epoch": 189.0008576329331,
      "grad_norm": 6.302496433258057,
      "learning_rate": 1.609619230769231e-05,
      "loss": 3.2507,
      "step": 881500
    },
    {
      "epoch": 189.02229845626073,
      "grad_norm": 6.20306921005249,
      "learning_rate": 1.6092346153846155e-05,
      "loss": 3.1504,
      "step": 881600
    },
    {
      "epoch": 189.04373927958832,
      "grad_norm": 6.529273986816406,
      "learning_rate": 1.60885e-05,
      "loss": 3.1823,
      "step": 881700
    },
    {
      "epoch": 189.06518010291595,
      "grad_norm": 6.62398624420166,
      "learning_rate": 1.6084653846153846e-05,
      "loss": 3.2011,
      "step": 881800
    },
    {
      "epoch": 189.08662092624357,
      "grad_norm": 6.086941719055176,
      "learning_rate": 1.608080769230769e-05,
      "loss": 3.1732,
      "step": 881900
    },
    {
      "epoch": 189.1080617495712,
      "grad_norm": 5.592316150665283,
      "learning_rate": 1.6076961538461537e-05,
      "loss": 3.1844,
      "step": 882000
    },
    {
      "epoch": 189.1295025728988,
      "grad_norm": 6.690093040466309,
      "learning_rate": 1.6073115384615386e-05,
      "loss": 3.1233,
      "step": 882100
    },
    {
      "epoch": 189.1509433962264,
      "grad_norm": 5.653260231018066,
      "learning_rate": 1.606926923076923e-05,
      "loss": 3.2642,
      "step": 882200
    },
    {
      "epoch": 189.17238421955403,
      "grad_norm": 5.700191020965576,
      "learning_rate": 1.6065423076923077e-05,
      "loss": 3.2264,
      "step": 882300
    },
    {
      "epoch": 189.19382504288166,
      "grad_norm": 6.969322204589844,
      "learning_rate": 1.6061576923076922e-05,
      "loss": 3.2161,
      "step": 882400
    },
    {
      "epoch": 189.21526586620925,
      "grad_norm": 5.514984130859375,
      "learning_rate": 1.6057730769230768e-05,
      "loss": 3.2298,
      "step": 882500
    },
    {
      "epoch": 189.23670668953687,
      "grad_norm": 6.001215934753418,
      "learning_rate": 1.6053884615384613e-05,
      "loss": 3.2004,
      "step": 882600
    },
    {
      "epoch": 189.2581475128645,
      "grad_norm": 5.8111653327941895,
      "learning_rate": 1.6050038461538462e-05,
      "loss": 3.2136,
      "step": 882700
    },
    {
      "epoch": 189.27958833619212,
      "grad_norm": 5.97212553024292,
      "learning_rate": 1.6046192307692308e-05,
      "loss": 3.1617,
      "step": 882800
    },
    {
      "epoch": 189.30102915951971,
      "grad_norm": 6.291905879974365,
      "learning_rate": 1.6042346153846153e-05,
      "loss": 3.1763,
      "step": 882900
    },
    {
      "epoch": 189.32246998284734,
      "grad_norm": 6.2110209465026855,
      "learning_rate": 1.6038500000000002e-05,
      "loss": 3.152,
      "step": 883000
    },
    {
      "epoch": 189.34391080617496,
      "grad_norm": 5.967099189758301,
      "learning_rate": 1.6034653846153848e-05,
      "loss": 3.219,
      "step": 883100
    },
    {
      "epoch": 189.36535162950258,
      "grad_norm": 6.142419815063477,
      "learning_rate": 1.6030807692307693e-05,
      "loss": 3.2289,
      "step": 883200
    },
    {
      "epoch": 189.38679245283018,
      "grad_norm": 5.973901748657227,
      "learning_rate": 1.602696153846154e-05,
      "loss": 3.2339,
      "step": 883300
    },
    {
      "epoch": 189.4082332761578,
      "grad_norm": 5.804671287536621,
      "learning_rate": 1.6023115384615388e-05,
      "loss": 3.2395,
      "step": 883400
    },
    {
      "epoch": 189.42967409948542,
      "grad_norm": 5.615514278411865,
      "learning_rate": 1.6019269230769233e-05,
      "loss": 3.224,
      "step": 883500
    },
    {
      "epoch": 189.45111492281305,
      "grad_norm": 6.499717712402344,
      "learning_rate": 1.601542307692308e-05,
      "loss": 3.2439,
      "step": 883600
    },
    {
      "epoch": 189.47255574614064,
      "grad_norm": 6.043125629425049,
      "learning_rate": 1.6011576923076925e-05,
      "loss": 3.2071,
      "step": 883700
    },
    {
      "epoch": 189.49399656946827,
      "grad_norm": 5.7211761474609375,
      "learning_rate": 1.600773076923077e-05,
      "loss": 3.2014,
      "step": 883800
    },
    {
      "epoch": 189.5154373927959,
      "grad_norm": 5.440824508666992,
      "learning_rate": 1.6003884615384616e-05,
      "loss": 3.1844,
      "step": 883900
    },
    {
      "epoch": 189.5368782161235,
      "grad_norm": 5.50809907913208,
      "learning_rate": 1.6000038461538465e-05,
      "loss": 3.183,
      "step": 884000
    },
    {
      "epoch": 189.5583190394511,
      "grad_norm": 5.61529541015625,
      "learning_rate": 1.599619230769231e-05,
      "loss": 3.174,
      "step": 884100
    },
    {
      "epoch": 189.57975986277873,
      "grad_norm": 6.34769344329834,
      "learning_rate": 1.5992346153846156e-05,
      "loss": 3.2312,
      "step": 884200
    },
    {
      "epoch": 189.60120068610635,
      "grad_norm": 5.918078422546387,
      "learning_rate": 1.59885e-05,
      "loss": 3.1787,
      "step": 884300
    },
    {
      "epoch": 189.62264150943398,
      "grad_norm": 5.714808464050293,
      "learning_rate": 1.5984653846153847e-05,
      "loss": 3.2006,
      "step": 884400
    },
    {
      "epoch": 189.64408233276157,
      "grad_norm": 6.099809646606445,
      "learning_rate": 1.5980807692307692e-05,
      "loss": 3.2416,
      "step": 884500
    },
    {
      "epoch": 189.6655231560892,
      "grad_norm": 5.738819599151611,
      "learning_rate": 1.5976961538461538e-05,
      "loss": 3.2251,
      "step": 884600
    },
    {
      "epoch": 189.68696397941682,
      "grad_norm": 5.502865314483643,
      "learning_rate": 1.5973115384615387e-05,
      "loss": 3.1403,
      "step": 884700
    },
    {
      "epoch": 189.70840480274444,
      "grad_norm": 6.246745586395264,
      "learning_rate": 1.5969269230769232e-05,
      "loss": 3.2287,
      "step": 884800
    },
    {
      "epoch": 189.72984562607203,
      "grad_norm": 6.3207597732543945,
      "learning_rate": 1.5965423076923078e-05,
      "loss": 3.1683,
      "step": 884900
    },
    {
      "epoch": 189.75128644939966,
      "grad_norm": 5.768251419067383,
      "learning_rate": 1.5961576923076923e-05,
      "loss": 3.258,
      "step": 885000
    },
    {
      "epoch": 189.77272727272728,
      "grad_norm": 6.183671951293945,
      "learning_rate": 1.595773076923077e-05,
      "loss": 3.1948,
      "step": 885100
    },
    {
      "epoch": 189.79416809605488,
      "grad_norm": 5.808387279510498,
      "learning_rate": 1.5953884615384614e-05,
      "loss": 3.2512,
      "step": 885200
    },
    {
      "epoch": 189.8156089193825,
      "grad_norm": 5.820353984832764,
      "learning_rate": 1.5950038461538463e-05,
      "loss": 3.2053,
      "step": 885300
    },
    {
      "epoch": 189.83704974271012,
      "grad_norm": 6.154700756072998,
      "learning_rate": 1.594619230769231e-05,
      "loss": 3.1549,
      "step": 885400
    },
    {
      "epoch": 189.85849056603774,
      "grad_norm": 5.629305839538574,
      "learning_rate": 1.5942346153846154e-05,
      "loss": 3.2262,
      "step": 885500
    },
    {
      "epoch": 189.87993138936534,
      "grad_norm": 6.2032790184021,
      "learning_rate": 1.59385e-05,
      "loss": 3.2236,
      "step": 885600
    },
    {
      "epoch": 189.90137221269296,
      "grad_norm": 5.5907487869262695,
      "learning_rate": 1.5934653846153845e-05,
      "loss": 3.2154,
      "step": 885700
    },
    {
      "epoch": 189.92281303602059,
      "grad_norm": 5.59251594543457,
      "learning_rate": 1.593080769230769e-05,
      "loss": 3.1785,
      "step": 885800
    },
    {
      "epoch": 189.9442538593482,
      "grad_norm": 5.680363178253174,
      "learning_rate": 1.592696153846154e-05,
      "loss": 3.151,
      "step": 885900
    },
    {
      "epoch": 189.9656946826758,
      "grad_norm": 5.284270763397217,
      "learning_rate": 1.5923115384615385e-05,
      "loss": 3.1952,
      "step": 886000
    },
    {
      "epoch": 189.98713550600343,
      "grad_norm": 5.892685413360596,
      "learning_rate": 1.591926923076923e-05,
      "loss": 3.2294,
      "step": 886100
    },
    {
      "epoch": 190.00857632933105,
      "grad_norm": 6.322803020477295,
      "learning_rate": 1.5915423076923077e-05,
      "loss": 3.1831,
      "step": 886200
    },
    {
      "epoch": 190.03001715265867,
      "grad_norm": 6.458341121673584,
      "learning_rate": 1.5911576923076922e-05,
      "loss": 3.1939,
      "step": 886300
    },
    {
      "epoch": 190.05145797598627,
      "grad_norm": 5.771720886230469,
      "learning_rate": 1.5907730769230768e-05,
      "loss": 3.1435,
      "step": 886400
    },
    {
      "epoch": 190.0728987993139,
      "grad_norm": 5.67747688293457,
      "learning_rate": 1.5903884615384617e-05,
      "loss": 3.1309,
      "step": 886500
    },
    {
      "epoch": 190.0943396226415,
      "grad_norm": 5.981750011444092,
      "learning_rate": 1.5900038461538462e-05,
      "loss": 3.1398,
      "step": 886600
    },
    {
      "epoch": 190.11578044596914,
      "grad_norm": 5.986574172973633,
      "learning_rate": 1.5896192307692308e-05,
      "loss": 3.2325,
      "step": 886700
    },
    {
      "epoch": 190.13722126929673,
      "grad_norm": 6.192324161529541,
      "learning_rate": 1.5892346153846153e-05,
      "loss": 3.1586,
      "step": 886800
    },
    {
      "epoch": 190.15866209262435,
      "grad_norm": 6.398773670196533,
      "learning_rate": 1.5888500000000002e-05,
      "loss": 3.2425,
      "step": 886900
    },
    {
      "epoch": 190.18010291595198,
      "grad_norm": 5.90009069442749,
      "learning_rate": 1.5884653846153848e-05,
      "loss": 3.2001,
      "step": 887000
    },
    {
      "epoch": 190.2015437392796,
      "grad_norm": 5.581854820251465,
      "learning_rate": 1.5880807692307693e-05,
      "loss": 3.201,
      "step": 887100
    },
    {
      "epoch": 190.2229845626072,
      "grad_norm": 6.091917037963867,
      "learning_rate": 1.5876961538461542e-05,
      "loss": 3.1978,
      "step": 887200
    },
    {
      "epoch": 190.24442538593482,
      "grad_norm": 5.849497318267822,
      "learning_rate": 1.5873115384615388e-05,
      "loss": 3.1742,
      "step": 887300
    },
    {
      "epoch": 190.26586620926244,
      "grad_norm": 5.366924285888672,
      "learning_rate": 1.5869269230769233e-05,
      "loss": 3.1897,
      "step": 887400
    },
    {
      "epoch": 190.28730703259006,
      "grad_norm": 6.0653486251831055,
      "learning_rate": 1.586542307692308e-05,
      "loss": 3.1803,
      "step": 887500
    },
    {
      "epoch": 190.30874785591766,
      "grad_norm": 6.619417667388916,
      "learning_rate": 1.5861576923076924e-05,
      "loss": 3.237,
      "step": 887600
    },
    {
      "epoch": 190.33018867924528,
      "grad_norm": 6.247030735015869,
      "learning_rate": 1.585773076923077e-05,
      "loss": 3.191,
      "step": 887700
    },
    {
      "epoch": 190.3516295025729,
      "grad_norm": 6.377501487731934,
      "learning_rate": 1.5853884615384615e-05,
      "loss": 3.1709,
      "step": 887800
    },
    {
      "epoch": 190.37307032590053,
      "grad_norm": 6.281209468841553,
      "learning_rate": 1.5850038461538464e-05,
      "loss": 3.1712,
      "step": 887900
    },
    {
      "epoch": 190.39451114922812,
      "grad_norm": 6.092808723449707,
      "learning_rate": 1.584619230769231e-05,
      "loss": 3.1882,
      "step": 888000
    },
    {
      "epoch": 190.41595197255575,
      "grad_norm": 6.210538864135742,
      "learning_rate": 1.5842346153846155e-05,
      "loss": 3.1369,
      "step": 888100
    },
    {
      "epoch": 190.43739279588337,
      "grad_norm": 6.24336576461792,
      "learning_rate": 1.58385e-05,
      "loss": 3.2225,
      "step": 888200
    },
    {
      "epoch": 190.45883361921096,
      "grad_norm": 5.801507472991943,
      "learning_rate": 1.5834653846153846e-05,
      "loss": 3.1979,
      "step": 888300
    },
    {
      "epoch": 190.4802744425386,
      "grad_norm": 6.068260192871094,
      "learning_rate": 1.5830807692307692e-05,
      "loss": 3.1994,
      "step": 888400
    },
    {
      "epoch": 190.5017152658662,
      "grad_norm": 5.931301116943359,
      "learning_rate": 1.582696153846154e-05,
      "loss": 3.2283,
      "step": 888500
    },
    {
      "epoch": 190.52315608919383,
      "grad_norm": 6.130064964294434,
      "learning_rate": 1.5823115384615386e-05,
      "loss": 3.1656,
      "step": 888600
    },
    {
      "epoch": 190.54459691252143,
      "grad_norm": 5.436727523803711,
      "learning_rate": 1.5819269230769232e-05,
      "loss": 3.2258,
      "step": 888700
    },
    {
      "epoch": 190.56603773584905,
      "grad_norm": 5.985854625701904,
      "learning_rate": 1.5815423076923078e-05,
      "loss": 3.1924,
      "step": 888800
    },
    {
      "epoch": 190.58747855917667,
      "grad_norm": 5.695215702056885,
      "learning_rate": 1.5811576923076923e-05,
      "loss": 3.2313,
      "step": 888900
    },
    {
      "epoch": 190.6089193825043,
      "grad_norm": 6.2195868492126465,
      "learning_rate": 1.580773076923077e-05,
      "loss": 3.2061,
      "step": 889000
    },
    {
      "epoch": 190.6303602058319,
      "grad_norm": 6.188486576080322,
      "learning_rate": 1.5803884615384614e-05,
      "loss": 3.1737,
      "step": 889100
    },
    {
      "epoch": 190.65180102915951,
      "grad_norm": 6.373584747314453,
      "learning_rate": 1.5800038461538463e-05,
      "loss": 3.2186,
      "step": 889200
    },
    {
      "epoch": 190.67324185248714,
      "grad_norm": 6.056776523590088,
      "learning_rate": 1.579619230769231e-05,
      "loss": 3.1881,
      "step": 889300
    },
    {
      "epoch": 190.69468267581476,
      "grad_norm": 5.428921222686768,
      "learning_rate": 1.5792346153846154e-05,
      "loss": 3.1969,
      "step": 889400
    },
    {
      "epoch": 190.71612349914236,
      "grad_norm": 6.083266258239746,
      "learning_rate": 1.57885e-05,
      "loss": 3.1445,
      "step": 889500
    },
    {
      "epoch": 190.73756432246998,
      "grad_norm": 5.8490729331970215,
      "learning_rate": 1.5784653846153845e-05,
      "loss": 3.146,
      "step": 889600
    },
    {
      "epoch": 190.7590051457976,
      "grad_norm": 6.135469913482666,
      "learning_rate": 1.578080769230769e-05,
      "loss": 3.2024,
      "step": 889700
    },
    {
      "epoch": 190.78044596912522,
      "grad_norm": 6.08821439743042,
      "learning_rate": 1.577696153846154e-05,
      "loss": 3.1872,
      "step": 889800
    },
    {
      "epoch": 190.80188679245282,
      "grad_norm": 6.15861177444458,
      "learning_rate": 1.5773115384615385e-05,
      "loss": 3.1875,
      "step": 889900
    },
    {
      "epoch": 190.82332761578044,
      "grad_norm": 5.960591793060303,
      "learning_rate": 1.576926923076923e-05,
      "loss": 3.2419,
      "step": 890000
    },
    {
      "epoch": 190.84476843910807,
      "grad_norm": 6.470386028289795,
      "learning_rate": 1.5765423076923076e-05,
      "loss": 3.2264,
      "step": 890100
    },
    {
      "epoch": 190.8662092624357,
      "grad_norm": 6.113203048706055,
      "learning_rate": 1.5761576923076922e-05,
      "loss": 3.2067,
      "step": 890200
    },
    {
      "epoch": 190.88765008576328,
      "grad_norm": 6.29442024230957,
      "learning_rate": 1.5757730769230767e-05,
      "loss": 3.2093,
      "step": 890300
    },
    {
      "epoch": 190.9090909090909,
      "grad_norm": 5.95072078704834,
      "learning_rate": 1.5753884615384616e-05,
      "loss": 3.2055,
      "step": 890400
    },
    {
      "epoch": 190.93053173241853,
      "grad_norm": 5.864802837371826,
      "learning_rate": 1.5750038461538462e-05,
      "loss": 3.1968,
      "step": 890500
    },
    {
      "epoch": 190.95197255574615,
      "grad_norm": 6.148177623748779,
      "learning_rate": 1.5746192307692307e-05,
      "loss": 3.206,
      "step": 890600
    },
    {
      "epoch": 190.97341337907375,
      "grad_norm": 5.9521589279174805,
      "learning_rate": 1.5742346153846156e-05,
      "loss": 3.2224,
      "step": 890700
    },
    {
      "epoch": 190.99485420240137,
      "grad_norm": 6.092892646789551,
      "learning_rate": 1.5738500000000002e-05,
      "loss": 3.2114,
      "step": 890800
    },
    {
      "epoch": 191.016295025729,
      "grad_norm": 6.313392162322998,
      "learning_rate": 1.5734653846153847e-05,
      "loss": 3.188,
      "step": 890900
    },
    {
      "epoch": 191.03773584905662,
      "grad_norm": 5.842294216156006,
      "learning_rate": 1.5730807692307693e-05,
      "loss": 3.1783,
      "step": 891000
    },
    {
      "epoch": 191.0591766723842,
      "grad_norm": 6.347052097320557,
      "learning_rate": 1.5726961538461542e-05,
      "loss": 3.1924,
      "step": 891100
    },
    {
      "epoch": 191.08061749571183,
      "grad_norm": 5.522709846496582,
      "learning_rate": 1.5723115384615387e-05,
      "loss": 3.1341,
      "step": 891200
    },
    {
      "epoch": 191.10205831903946,
      "grad_norm": 6.188967227935791,
      "learning_rate": 1.5719269230769233e-05,
      "loss": 3.1529,
      "step": 891300
    },
    {
      "epoch": 191.12349914236708,
      "grad_norm": 6.042815208435059,
      "learning_rate": 1.571542307692308e-05,
      "loss": 3.152,
      "step": 891400
    },
    {
      "epoch": 191.14493996569468,
      "grad_norm": 6.051616668701172,
      "learning_rate": 1.5711576923076924e-05,
      "loss": 3.1981,
      "step": 891500
    },
    {
      "epoch": 191.1663807890223,
      "grad_norm": 6.319546222686768,
      "learning_rate": 1.570773076923077e-05,
      "loss": 3.1307,
      "step": 891600
    },
    {
      "epoch": 191.18782161234992,
      "grad_norm": 6.305793762207031,
      "learning_rate": 1.5703884615384615e-05,
      "loss": 3.144,
      "step": 891700
    },
    {
      "epoch": 191.20926243567752,
      "grad_norm": 5.964381217956543,
      "learning_rate": 1.5700038461538464e-05,
      "loss": 3.1892,
      "step": 891800
    },
    {
      "epoch": 191.23070325900514,
      "grad_norm": 5.590573310852051,
      "learning_rate": 1.569619230769231e-05,
      "loss": 3.2012,
      "step": 891900
    },
    {
      "epoch": 191.25214408233276,
      "grad_norm": 6.1329216957092285,
      "learning_rate": 1.5692346153846155e-05,
      "loss": 3.1946,
      "step": 892000
    },
    {
      "epoch": 191.27358490566039,
      "grad_norm": 6.304905891418457,
      "learning_rate": 1.56885e-05,
      "loss": 3.2239,
      "step": 892100
    },
    {
      "epoch": 191.29502572898798,
      "grad_norm": 6.188682556152344,
      "learning_rate": 1.5684653846153846e-05,
      "loss": 3.2534,
      "step": 892200
    },
    {
      "epoch": 191.3164665523156,
      "grad_norm": 6.0162248611450195,
      "learning_rate": 1.5680807692307692e-05,
      "loss": 3.2599,
      "step": 892300
    },
    {
      "epoch": 191.33790737564323,
      "grad_norm": 5.65094518661499,
      "learning_rate": 1.567696153846154e-05,
      "loss": 3.1675,
      "step": 892400
    },
    {
      "epoch": 191.35934819897085,
      "grad_norm": 6.625217914581299,
      "learning_rate": 1.5673115384615386e-05,
      "loss": 3.1301,
      "step": 892500
    },
    {
      "epoch": 191.38078902229844,
      "grad_norm": 6.092315196990967,
      "learning_rate": 1.5669269230769232e-05,
      "loss": 3.1941,
      "step": 892600
    },
    {
      "epoch": 191.40222984562607,
      "grad_norm": 6.632983684539795,
      "learning_rate": 1.5665423076923077e-05,
      "loss": 3.213,
      "step": 892700
    },
    {
      "epoch": 191.4236706689537,
      "grad_norm": 6.299194812774658,
      "learning_rate": 1.5661576923076923e-05,
      "loss": 3.2299,
      "step": 892800
    },
    {
      "epoch": 191.4451114922813,
      "grad_norm": 6.004166126251221,
      "learning_rate": 1.565773076923077e-05,
      "loss": 3.1267,
      "step": 892900
    },
    {
      "epoch": 191.4665523156089,
      "grad_norm": 6.096464157104492,
      "learning_rate": 1.5653884615384617e-05,
      "loss": 3.1786,
      "step": 893000
    },
    {
      "epoch": 191.48799313893653,
      "grad_norm": 6.329967021942139,
      "learning_rate": 1.5650038461538463e-05,
      "loss": 3.2153,
      "step": 893100
    },
    {
      "epoch": 191.50943396226415,
      "grad_norm": 5.773452281951904,
      "learning_rate": 1.564619230769231e-05,
      "loss": 3.2218,
      "step": 893200
    },
    {
      "epoch": 191.53087478559178,
      "grad_norm": 6.065275192260742,
      "learning_rate": 1.5642346153846154e-05,
      "loss": 3.1999,
      "step": 893300
    },
    {
      "epoch": 191.55231560891937,
      "grad_norm": 5.684970378875732,
      "learning_rate": 1.56385e-05,
      "loss": 3.1384,
      "step": 893400
    },
    {
      "epoch": 191.573756432247,
      "grad_norm": 6.46391487121582,
      "learning_rate": 1.5634653846153845e-05,
      "loss": 3.1748,
      "step": 893500
    },
    {
      "epoch": 191.59519725557462,
      "grad_norm": 6.212496280670166,
      "learning_rate": 1.563080769230769e-05,
      "loss": 3.2176,
      "step": 893600
    },
    {
      "epoch": 191.61663807890224,
      "grad_norm": 6.065554618835449,
      "learning_rate": 1.562696153846154e-05,
      "loss": 3.1955,
      "step": 893700
    },
    {
      "epoch": 191.63807890222984,
      "grad_norm": 5.7585625648498535,
      "learning_rate": 1.5623115384615385e-05,
      "loss": 3.1696,
      "step": 893800
    },
    {
      "epoch": 191.65951972555746,
      "grad_norm": 6.007742881774902,
      "learning_rate": 1.561926923076923e-05,
      "loss": 3.1899,
      "step": 893900
    },
    {
      "epoch": 191.68096054888508,
      "grad_norm": 6.343582630157471,
      "learning_rate": 1.5615423076923076e-05,
      "loss": 3.1939,
      "step": 894000
    },
    {
      "epoch": 191.7024013722127,
      "grad_norm": 5.9618682861328125,
      "learning_rate": 1.561157692307692e-05,
      "loss": 3.232,
      "step": 894100
    },
    {
      "epoch": 191.7238421955403,
      "grad_norm": 6.450671672821045,
      "learning_rate": 1.560773076923077e-05,
      "loss": 3.1809,
      "step": 894200
    },
    {
      "epoch": 191.74528301886792,
      "grad_norm": 6.000009059906006,
      "learning_rate": 1.5603884615384616e-05,
      "loss": 3.1926,
      "step": 894300
    },
    {
      "epoch": 191.76672384219555,
      "grad_norm": 6.382251262664795,
      "learning_rate": 1.560003846153846e-05,
      "loss": 3.1587,
      "step": 894400
    },
    {
      "epoch": 191.78816466552317,
      "grad_norm": 6.451305389404297,
      "learning_rate": 1.559619230769231e-05,
      "loss": 3.1467,
      "step": 894500
    },
    {
      "epoch": 191.80960548885076,
      "grad_norm": 5.335368633270264,
      "learning_rate": 1.5592346153846156e-05,
      "loss": 3.1812,
      "step": 894600
    },
    {
      "epoch": 191.8310463121784,
      "grad_norm": 6.15452241897583,
      "learning_rate": 1.55885e-05,
      "loss": 3.1726,
      "step": 894700
    },
    {
      "epoch": 191.852487135506,
      "grad_norm": 6.147531509399414,
      "learning_rate": 1.5584653846153847e-05,
      "loss": 3.2192,
      "step": 894800
    },
    {
      "epoch": 191.87392795883363,
      "grad_norm": 6.136075973510742,
      "learning_rate": 1.5580807692307693e-05,
      "loss": 3.174,
      "step": 894900
    },
    {
      "epoch": 191.89536878216123,
      "grad_norm": 6.037691116333008,
      "learning_rate": 1.557696153846154e-05,
      "loss": 3.2135,
      "step": 895000
    },
    {
      "epoch": 191.91680960548885,
      "grad_norm": 6.279125690460205,
      "learning_rate": 1.5573115384615387e-05,
      "loss": 3.1977,
      "step": 895100
    },
    {
      "epoch": 191.93825042881647,
      "grad_norm": 6.0152177810668945,
      "learning_rate": 1.5569269230769233e-05,
      "loss": 3.1229,
      "step": 895200
    },
    {
      "epoch": 191.95969125214407,
      "grad_norm": 6.212009429931641,
      "learning_rate": 1.5565423076923078e-05,
      "loss": 3.2492,
      "step": 895300
    },
    {
      "epoch": 191.9811320754717,
      "grad_norm": 5.835085391998291,
      "learning_rate": 1.5561576923076924e-05,
      "loss": 3.1773,
      "step": 895400
    },
    {
      "epoch": 192.00257289879931,
      "grad_norm": 5.953823566436768,
      "learning_rate": 1.555773076923077e-05,
      "loss": 3.2156,
      "step": 895500
    },
    {
      "epoch": 192.02401372212694,
      "grad_norm": 6.100346565246582,
      "learning_rate": 1.5553884615384618e-05,
      "loss": 3.175,
      "step": 895600
    },
    {
      "epoch": 192.04545454545453,
      "grad_norm": 5.898320198059082,
      "learning_rate": 1.5550038461538464e-05,
      "loss": 3.1685,
      "step": 895700
    },
    {
      "epoch": 192.06689536878216,
      "grad_norm": 6.002708911895752,
      "learning_rate": 1.554619230769231e-05,
      "loss": 3.1542,
      "step": 895800
    },
    {
      "epoch": 192.08833619210978,
      "grad_norm": 5.504125118255615,
      "learning_rate": 1.5542346153846155e-05,
      "loss": 3.1653,
      "step": 895900
    },
    {
      "epoch": 192.1097770154374,
      "grad_norm": 6.661520957946777,
      "learning_rate": 1.55385e-05,
      "loss": 3.1568,
      "step": 896000
    },
    {
      "epoch": 192.131217838765,
      "grad_norm": 5.838879108428955,
      "learning_rate": 1.5534653846153846e-05,
      "loss": 3.1389,
      "step": 896100
    },
    {
      "epoch": 192.15265866209262,
      "grad_norm": 5.777188777923584,
      "learning_rate": 1.553080769230769e-05,
      "loss": 3.2086,
      "step": 896200
    },
    {
      "epoch": 192.17409948542024,
      "grad_norm": 5.98423957824707,
      "learning_rate": 1.552696153846154e-05,
      "loss": 3.1514,
      "step": 896300
    },
    {
      "epoch": 192.19554030874787,
      "grad_norm": 6.020877361297607,
      "learning_rate": 1.5523115384615386e-05,
      "loss": 3.1663,
      "step": 896400
    },
    {
      "epoch": 192.21698113207546,
      "grad_norm": 5.535819053649902,
      "learning_rate": 1.551926923076923e-05,
      "loss": 3.1895,
      "step": 896500
    },
    {
      "epoch": 192.23842195540308,
      "grad_norm": 6.62772274017334,
      "learning_rate": 1.5515423076923077e-05,
      "loss": 3.172,
      "step": 896600
    },
    {
      "epoch": 192.2598627787307,
      "grad_norm": 6.431210994720459,
      "learning_rate": 1.5511576923076923e-05,
      "loss": 3.2132,
      "step": 896700
    },
    {
      "epoch": 192.28130360205833,
      "grad_norm": 5.9809489250183105,
      "learning_rate": 1.5507730769230768e-05,
      "loss": 3.1365,
      "step": 896800
    },
    {
      "epoch": 192.30274442538592,
      "grad_norm": 5.81169319152832,
      "learning_rate": 1.5503884615384617e-05,
      "loss": 3.193,
      "step": 896900
    },
    {
      "epoch": 192.32418524871355,
      "grad_norm": 6.0474467277526855,
      "learning_rate": 1.5500038461538463e-05,
      "loss": 3.1993,
      "step": 897000
    },
    {
      "epoch": 192.34562607204117,
      "grad_norm": 5.853635311126709,
      "learning_rate": 1.5496192307692308e-05,
      "loss": 3.2237,
      "step": 897100
    },
    {
      "epoch": 192.3670668953688,
      "grad_norm": 5.966807842254639,
      "learning_rate": 1.5492346153846154e-05,
      "loss": 3.1797,
      "step": 897200
    },
    {
      "epoch": 192.3885077186964,
      "grad_norm": 5.224214553833008,
      "learning_rate": 1.54885e-05,
      "loss": 3.1644,
      "step": 897300
    },
    {
      "epoch": 192.409948542024,
      "grad_norm": 6.13683557510376,
      "learning_rate": 1.5484653846153845e-05,
      "loss": 3.2037,
      "step": 897400
    },
    {
      "epoch": 192.43138936535163,
      "grad_norm": 5.930558681488037,
      "learning_rate": 1.548080769230769e-05,
      "loss": 3.2142,
      "step": 897500
    },
    {
      "epoch": 192.45283018867926,
      "grad_norm": 6.4448933601379395,
      "learning_rate": 1.547696153846154e-05,
      "loss": 3.186,
      "step": 897600
    },
    {
      "epoch": 192.47427101200685,
      "grad_norm": 5.88461446762085,
      "learning_rate": 1.5473115384615385e-05,
      "loss": 3.1872,
      "step": 897700
    },
    {
      "epoch": 192.49571183533448,
      "grad_norm": 6.133703231811523,
      "learning_rate": 1.546926923076923e-05,
      "loss": 3.1738,
      "step": 897800
    },
    {
      "epoch": 192.5171526586621,
      "grad_norm": 5.659904956817627,
      "learning_rate": 1.5465423076923076e-05,
      "loss": 3.1858,
      "step": 897900
    },
    {
      "epoch": 192.53859348198972,
      "grad_norm": 5.787307262420654,
      "learning_rate": 1.5461576923076925e-05,
      "loss": 3.1756,
      "step": 898000
    },
    {
      "epoch": 192.56003430531732,
      "grad_norm": 5.576302528381348,
      "learning_rate": 1.545773076923077e-05,
      "loss": 3.1722,
      "step": 898100
    },
    {
      "epoch": 192.58147512864494,
      "grad_norm": 5.859062671661377,
      "learning_rate": 1.5453884615384616e-05,
      "loss": 3.212,
      "step": 898200
    },
    {
      "epoch": 192.60291595197256,
      "grad_norm": 6.094759941101074,
      "learning_rate": 1.545003846153846e-05,
      "loss": 3.1763,
      "step": 898300
    },
    {
      "epoch": 192.62435677530019,
      "grad_norm": 6.099833965301514,
      "learning_rate": 1.544619230769231e-05,
      "loss": 3.1941,
      "step": 898400
    },
    {
      "epoch": 192.64579759862778,
      "grad_norm": 6.131392478942871,
      "learning_rate": 1.5442346153846156e-05,
      "loss": 3.2405,
      "step": 898500
    },
    {
      "epoch": 192.6672384219554,
      "grad_norm": 5.7134199142456055,
      "learning_rate": 1.54385e-05,
      "loss": 3.1765,
      "step": 898600
    },
    {
      "epoch": 192.68867924528303,
      "grad_norm": 5.603900909423828,
      "learning_rate": 1.5434653846153847e-05,
      "loss": 3.2064,
      "step": 898700
    },
    {
      "epoch": 192.71012006861062,
      "grad_norm": 6.5277628898620605,
      "learning_rate": 1.5430807692307696e-05,
      "loss": 3.1589,
      "step": 898800
    },
    {
      "epoch": 192.73156089193824,
      "grad_norm": 5.929705619812012,
      "learning_rate": 1.542696153846154e-05,
      "loss": 3.208,
      "step": 898900
    },
    {
      "epoch": 192.75300171526587,
      "grad_norm": 5.761474609375,
      "learning_rate": 1.5423115384615387e-05,
      "loss": 3.1726,
      "step": 899000
    },
    {
      "epoch": 192.7744425385935,
      "grad_norm": 5.633513450622559,
      "learning_rate": 1.5419269230769232e-05,
      "loss": 3.1481,
      "step": 899100
    },
    {
      "epoch": 192.79588336192108,
      "grad_norm": 6.533675670623779,
      "learning_rate": 1.5415423076923078e-05,
      "loss": 3.2033,
      "step": 899200
    },
    {
      "epoch": 192.8173241852487,
      "grad_norm": 6.168015956878662,
      "learning_rate": 1.5411576923076924e-05,
      "loss": 3.1875,
      "step": 899300
    },
    {
      "epoch": 192.83876500857633,
      "grad_norm": 5.910874366760254,
      "learning_rate": 1.540773076923077e-05,
      "loss": 3.1443,
      "step": 899400
    },
    {
      "epoch": 192.86020583190395,
      "grad_norm": 5.987448692321777,
      "learning_rate": 1.5403884615384618e-05,
      "loss": 3.1373,
      "step": 899500
    },
    {
      "epoch": 192.88164665523155,
      "grad_norm": 5.747546672821045,
      "learning_rate": 1.5400038461538464e-05,
      "loss": 3.1796,
      "step": 899600
    },
    {
      "epoch": 192.90308747855917,
      "grad_norm": 6.239789009094238,
      "learning_rate": 1.539619230769231e-05,
      "loss": 3.1648,
      "step": 899700
    },
    {
      "epoch": 192.9245283018868,
      "grad_norm": 5.702945709228516,
      "learning_rate": 1.5392346153846155e-05,
      "loss": 3.1336,
      "step": 899800
    },
    {
      "epoch": 192.94596912521442,
      "grad_norm": 5.967057228088379,
      "learning_rate": 1.53885e-05,
      "loss": 3.2369,
      "step": 899900
    },
    {
      "epoch": 192.967409948542,
      "grad_norm": 5.804928779602051,
      "learning_rate": 1.5384653846153846e-05,
      "loss": 3.1943,
      "step": 900000
    },
    {
      "epoch": 192.98885077186964,
      "grad_norm": 5.988771915435791,
      "learning_rate": 1.5380807692307695e-05,
      "loss": 3.17,
      "step": 900100
    },
    {
      "epoch": 193.01029159519726,
      "grad_norm": 6.226363658905029,
      "learning_rate": 1.537696153846154e-05,
      "loss": 3.1936,
      "step": 900200
    },
    {
      "epoch": 193.03173241852488,
      "grad_norm": 6.200239658355713,
      "learning_rate": 1.5373115384615386e-05,
      "loss": 3.1611,
      "step": 900300
    },
    {
      "epoch": 193.05317324185248,
      "grad_norm": 5.857816696166992,
      "learning_rate": 1.536926923076923e-05,
      "loss": 3.1646,
      "step": 900400
    },
    {
      "epoch": 193.0746140651801,
      "grad_norm": 6.003656387329102,
      "learning_rate": 1.5365423076923077e-05,
      "loss": 3.1523,
      "step": 900500
    },
    {
      "epoch": 193.09605488850772,
      "grad_norm": 5.959368705749512,
      "learning_rate": 1.5361576923076922e-05,
      "loss": 3.1921,
      "step": 900600
    },
    {
      "epoch": 193.11749571183535,
      "grad_norm": 5.774778366088867,
      "learning_rate": 1.5357730769230768e-05,
      "loss": 3.1843,
      "step": 900700
    },
    {
      "epoch": 193.13893653516294,
      "grad_norm": 6.204898357391357,
      "learning_rate": 1.5353884615384617e-05,
      "loss": 3.1903,
      "step": 900800
    },
    {
      "epoch": 193.16037735849056,
      "grad_norm": 5.903594970703125,
      "learning_rate": 1.5350038461538462e-05,
      "loss": 3.1993,
      "step": 900900
    },
    {
      "epoch": 193.1818181818182,
      "grad_norm": 5.64719820022583,
      "learning_rate": 1.5346192307692308e-05,
      "loss": 3.1144,
      "step": 901000
    },
    {
      "epoch": 193.2032590051458,
      "grad_norm": 6.0700860023498535,
      "learning_rate": 1.5342346153846153e-05,
      "loss": 3.1639,
      "step": 901100
    },
    {
      "epoch": 193.2246998284734,
      "grad_norm": 5.942883014678955,
      "learning_rate": 1.53385e-05,
      "loss": 3.0725,
      "step": 901200
    },
    {
      "epoch": 193.24614065180103,
      "grad_norm": 5.8535990715026855,
      "learning_rate": 1.5334653846153844e-05,
      "loss": 3.2094,
      "step": 901300
    },
    {
      "epoch": 193.26758147512865,
      "grad_norm": 6.0936384201049805,
      "learning_rate": 1.5330807692307693e-05,
      "loss": 3.1529,
      "step": 901400
    },
    {
      "epoch": 193.28902229845627,
      "grad_norm": 6.014559745788574,
      "learning_rate": 1.532696153846154e-05,
      "loss": 3.168,
      "step": 901500
    },
    {
      "epoch": 193.31046312178387,
      "grad_norm": 5.853957653045654,
      "learning_rate": 1.5323115384615384e-05,
      "loss": 3.2116,
      "step": 901600
    },
    {
      "epoch": 193.3319039451115,
      "grad_norm": 5.6702880859375,
      "learning_rate": 1.531926923076923e-05,
      "loss": 3.146,
      "step": 901700
    },
    {
      "epoch": 193.35334476843911,
      "grad_norm": 5.79725980758667,
      "learning_rate": 1.5315423076923076e-05,
      "loss": 3.2137,
      "step": 901800
    },
    {
      "epoch": 193.37478559176674,
      "grad_norm": 5.988157749176025,
      "learning_rate": 1.5311576923076924e-05,
      "loss": 3.1641,
      "step": 901900
    },
    {
      "epoch": 193.39622641509433,
      "grad_norm": 6.0816168785095215,
      "learning_rate": 1.530773076923077e-05,
      "loss": 3.147,
      "step": 902000
    },
    {
      "epoch": 193.41766723842196,
      "grad_norm": 5.837425231933594,
      "learning_rate": 1.5303884615384616e-05,
      "loss": 3.1565,
      "step": 902100
    },
    {
      "epoch": 193.43910806174958,
      "grad_norm": 5.807986736297607,
      "learning_rate": 1.5300038461538464e-05,
      "loss": 3.1523,
      "step": 902200
    },
    {
      "epoch": 193.46054888507717,
      "grad_norm": 5.867790222167969,
      "learning_rate": 1.529619230769231e-05,
      "loss": 3.1721,
      "step": 902300
    },
    {
      "epoch": 193.4819897084048,
      "grad_norm": 6.136956691741943,
      "learning_rate": 1.5292346153846156e-05,
      "loss": 3.1437,
      "step": 902400
    },
    {
      "epoch": 193.50343053173242,
      "grad_norm": 5.174736976623535,
      "learning_rate": 1.52885e-05,
      "loss": 3.1684,
      "step": 902500
    },
    {
      "epoch": 193.52487135506004,
      "grad_norm": 6.274820804595947,
      "learning_rate": 1.5284653846153847e-05,
      "loss": 3.2374,
      "step": 902600
    },
    {
      "epoch": 193.54631217838764,
      "grad_norm": 5.798028945922852,
      "learning_rate": 1.5280807692307696e-05,
      "loss": 3.1605,
      "step": 902700
    },
    {
      "epoch": 193.56775300171526,
      "grad_norm": 5.952403545379639,
      "learning_rate": 1.527696153846154e-05,
      "loss": 3.1621,
      "step": 902800
    },
    {
      "epoch": 193.58919382504288,
      "grad_norm": 6.117640972137451,
      "learning_rate": 1.5273115384615387e-05,
      "loss": 3.1594,
      "step": 902900
    },
    {
      "epoch": 193.6106346483705,
      "grad_norm": 6.013535499572754,
      "learning_rate": 1.5269269230769232e-05,
      "loss": 3.1875,
      "step": 903000
    },
    {
      "epoch": 193.6320754716981,
      "grad_norm": 6.57105016708374,
      "learning_rate": 1.5265423076923078e-05,
      "loss": 3.1831,
      "step": 903100
    },
    {
      "epoch": 193.65351629502572,
      "grad_norm": 5.878652572631836,
      "learning_rate": 1.5261576923076923e-05,
      "loss": 3.1677,
      "step": 903200
    },
    {
      "epoch": 193.67495711835335,
      "grad_norm": 5.565586090087891,
      "learning_rate": 1.5257730769230769e-05,
      "loss": 3.2069,
      "step": 903300
    },
    {
      "epoch": 193.69639794168097,
      "grad_norm": 5.908576488494873,
      "learning_rate": 1.5253884615384618e-05,
      "loss": 3.1766,
      "step": 903400
    },
    {
      "epoch": 193.71783876500857,
      "grad_norm": 5.589380264282227,
      "learning_rate": 1.5250038461538463e-05,
      "loss": 3.1772,
      "step": 903500
    },
    {
      "epoch": 193.7392795883362,
      "grad_norm": 5.836248397827148,
      "learning_rate": 1.5246192307692309e-05,
      "loss": 3.1658,
      "step": 903600
    },
    {
      "epoch": 193.7607204116638,
      "grad_norm": 5.966078281402588,
      "learning_rate": 1.5242346153846154e-05,
      "loss": 3.2151,
      "step": 903700
    },
    {
      "epoch": 193.78216123499143,
      "grad_norm": 6.620998859405518,
      "learning_rate": 1.52385e-05,
      "loss": 3.199,
      "step": 903800
    },
    {
      "epoch": 193.80360205831903,
      "grad_norm": 5.631345272064209,
      "learning_rate": 1.5234653846153845e-05,
      "loss": 3.1537,
      "step": 903900
    },
    {
      "epoch": 193.82504288164665,
      "grad_norm": 5.965747356414795,
      "learning_rate": 1.5230807692307694e-05,
      "loss": 3.206,
      "step": 904000
    },
    {
      "epoch": 193.84648370497428,
      "grad_norm": 5.7980523109436035,
      "learning_rate": 1.522696153846154e-05,
      "loss": 3.1459,
      "step": 904100
    },
    {
      "epoch": 193.8679245283019,
      "grad_norm": 5.590484142303467,
      "learning_rate": 1.5223115384615385e-05,
      "loss": 3.135,
      "step": 904200
    },
    {
      "epoch": 193.8893653516295,
      "grad_norm": 5.923845291137695,
      "learning_rate": 1.5219269230769231e-05,
      "loss": 3.1206,
      "step": 904300
    },
    {
      "epoch": 193.91080617495712,
      "grad_norm": 6.259693622589111,
      "learning_rate": 1.5215423076923076e-05,
      "loss": 3.1717,
      "step": 904400
    },
    {
      "epoch": 193.93224699828474,
      "grad_norm": 5.64912748336792,
      "learning_rate": 1.5211576923076922e-05,
      "loss": 3.2392,
      "step": 904500
    },
    {
      "epoch": 193.95368782161236,
      "grad_norm": 5.724786281585693,
      "learning_rate": 1.5207730769230771e-05,
      "loss": 3.2034,
      "step": 904600
    },
    {
      "epoch": 193.97512864493996,
      "grad_norm": 6.259174346923828,
      "learning_rate": 1.5203884615384617e-05,
      "loss": 3.1771,
      "step": 904700
    },
    {
      "epoch": 193.99656946826758,
      "grad_norm": 6.473464012145996,
      "learning_rate": 1.5200038461538462e-05,
      "loss": 3.1968,
      "step": 904800
    },
    {
      "epoch": 194.0180102915952,
      "grad_norm": 5.715144634246826,
      "learning_rate": 1.519619230769231e-05,
      "loss": 3.1755,
      "step": 904900
    },
    {
      "epoch": 194.03945111492283,
      "grad_norm": 5.762613773345947,
      "learning_rate": 1.5192346153846155e-05,
      "loss": 3.1207,
      "step": 905000
    },
    {
      "epoch": 194.06089193825042,
      "grad_norm": 5.75132417678833,
      "learning_rate": 1.51885e-05,
      "loss": 3.2064,
      "step": 905100
    },
    {
      "epoch": 194.08233276157804,
      "grad_norm": 5.817321300506592,
      "learning_rate": 1.5184653846153846e-05,
      "loss": 3.1212,
      "step": 905200
    },
    {
      "epoch": 194.10377358490567,
      "grad_norm": 5.670421600341797,
      "learning_rate": 1.5180807692307695e-05,
      "loss": 3.1629,
      "step": 905300
    },
    {
      "epoch": 194.12521440823326,
      "grad_norm": 6.06851863861084,
      "learning_rate": 1.517696153846154e-05,
      "loss": 3.1639,
      "step": 905400
    },
    {
      "epoch": 194.14665523156089,
      "grad_norm": 5.991945743560791,
      "learning_rate": 1.5173115384615386e-05,
      "loss": 3.1481,
      "step": 905500
    },
    {
      "epoch": 194.1680960548885,
      "grad_norm": 5.687984466552734,
      "learning_rate": 1.5169269230769231e-05,
      "loss": 3.1587,
      "step": 905600
    },
    {
      "epoch": 194.18953687821613,
      "grad_norm": 6.38016939163208,
      "learning_rate": 1.5165423076923077e-05,
      "loss": 3.0972,
      "step": 905700
    },
    {
      "epoch": 194.21097770154373,
      "grad_norm": 6.231084823608398,
      "learning_rate": 1.5161576923076923e-05,
      "loss": 3.1479,
      "step": 905800
    },
    {
      "epoch": 194.23241852487135,
      "grad_norm": 6.886394023895264,
      "learning_rate": 1.5157730769230771e-05,
      "loss": 3.1514,
      "step": 905900
    },
    {
      "epoch": 194.25385934819897,
      "grad_norm": 5.681446552276611,
      "learning_rate": 1.5153884615384617e-05,
      "loss": 3.1413,
      "step": 906000
    },
    {
      "epoch": 194.2753001715266,
      "grad_norm": 5.436365127563477,
      "learning_rate": 1.5150038461538463e-05,
      "loss": 3.1495,
      "step": 906100
    },
    {
      "epoch": 194.2967409948542,
      "grad_norm": 5.412380218505859,
      "learning_rate": 1.5146192307692308e-05,
      "loss": 3.2083,
      "step": 906200
    },
    {
      "epoch": 194.3181818181818,
      "grad_norm": 6.365941047668457,
      "learning_rate": 1.5142346153846154e-05,
      "loss": 3.1918,
      "step": 906300
    },
    {
      "epoch": 194.33962264150944,
      "grad_norm": 6.015538215637207,
      "learning_rate": 1.5138499999999999e-05,
      "loss": 3.168,
      "step": 906400
    },
    {
      "epoch": 194.36106346483706,
      "grad_norm": 6.8807878494262695,
      "learning_rate": 1.5134653846153846e-05,
      "loss": 3.1512,
      "step": 906500
    },
    {
      "epoch": 194.38250428816465,
      "grad_norm": 5.95187520980835,
      "learning_rate": 1.5130807692307694e-05,
      "loss": 3.2442,
      "step": 906600
    },
    {
      "epoch": 194.40394511149228,
      "grad_norm": 5.904108047485352,
      "learning_rate": 1.5126961538461539e-05,
      "loss": 3.1154,
      "step": 906700
    },
    {
      "epoch": 194.4253859348199,
      "grad_norm": 5.769737720489502,
      "learning_rate": 1.5123115384615386e-05,
      "loss": 3.1603,
      "step": 906800
    },
    {
      "epoch": 194.44682675814752,
      "grad_norm": 6.04276704788208,
      "learning_rate": 1.5119269230769232e-05,
      "loss": 3.1599,
      "step": 906900
    },
    {
      "epoch": 194.46826758147512,
      "grad_norm": 6.118391513824463,
      "learning_rate": 1.5115423076923077e-05,
      "loss": 3.1728,
      "step": 907000
    },
    {
      "epoch": 194.48970840480274,
      "grad_norm": 5.995633602142334,
      "learning_rate": 1.5111576923076923e-05,
      "loss": 3.194,
      "step": 907100
    },
    {
      "epoch": 194.51114922813036,
      "grad_norm": 5.390936851501465,
      "learning_rate": 1.5107730769230772e-05,
      "loss": 3.1482,
      "step": 907200
    },
    {
      "epoch": 194.532590051458,
      "grad_norm": 5.873982906341553,
      "learning_rate": 1.5103884615384617e-05,
      "loss": 3.1781,
      "step": 907300
    },
    {
      "epoch": 194.55403087478558,
      "grad_norm": 6.274390697479248,
      "learning_rate": 1.5100038461538463e-05,
      "loss": 3.1375,
      "step": 907400
    },
    {
      "epoch": 194.5754716981132,
      "grad_norm": 6.152289867401123,
      "learning_rate": 1.5096192307692309e-05,
      "loss": 3.112,
      "step": 907500
    },
    {
      "epoch": 194.59691252144083,
      "grad_norm": 5.473906517028809,
      "learning_rate": 1.5092346153846154e-05,
      "loss": 3.1525,
      "step": 907600
    },
    {
      "epoch": 194.61835334476845,
      "grad_norm": 5.847266674041748,
      "learning_rate": 1.50885e-05,
      "loss": 3.1838,
      "step": 907700
    },
    {
      "epoch": 194.63979416809605,
      "grad_norm": 5.731688499450684,
      "learning_rate": 1.5084653846153845e-05,
      "loss": 3.1388,
      "step": 907800
    },
    {
      "epoch": 194.66123499142367,
      "grad_norm": 5.795896053314209,
      "learning_rate": 1.5080807692307694e-05,
      "loss": 3.1564,
      "step": 907900
    },
    {
      "epoch": 194.6826758147513,
      "grad_norm": 5.819644927978516,
      "learning_rate": 1.507696153846154e-05,
      "loss": 3.1528,
      "step": 908000
    },
    {
      "epoch": 194.70411663807892,
      "grad_norm": 5.522323131561279,
      "learning_rate": 1.5073115384615385e-05,
      "loss": 3.1471,
      "step": 908100
    },
    {
      "epoch": 194.7255574614065,
      "grad_norm": 6.302776336669922,
      "learning_rate": 1.506926923076923e-05,
      "loss": 3.1614,
      "step": 908200
    },
    {
      "epoch": 194.74699828473413,
      "grad_norm": 6.474475860595703,
      "learning_rate": 1.5065423076923076e-05,
      "loss": 3.1572,
      "step": 908300
    },
    {
      "epoch": 194.76843910806176,
      "grad_norm": 6.269562244415283,
      "learning_rate": 1.5061576923076923e-05,
      "loss": 3.1732,
      "step": 908400
    },
    {
      "epoch": 194.78987993138938,
      "grad_norm": 5.53524923324585,
      "learning_rate": 1.505773076923077e-05,
      "loss": 3.1849,
      "step": 908500
    },
    {
      "epoch": 194.81132075471697,
      "grad_norm": 5.744730472564697,
      "learning_rate": 1.5053884615384616e-05,
      "loss": 3.2003,
      "step": 908600
    },
    {
      "epoch": 194.8327615780446,
      "grad_norm": 6.055459976196289,
      "learning_rate": 1.5050038461538463e-05,
      "loss": 3.2246,
      "step": 908700
    },
    {
      "epoch": 194.85420240137222,
      "grad_norm": 6.319347381591797,
      "learning_rate": 1.5046192307692309e-05,
      "loss": 3.2194,
      "step": 908800
    },
    {
      "epoch": 194.87564322469981,
      "grad_norm": 6.129609107971191,
      "learning_rate": 1.5042346153846155e-05,
      "loss": 3.1829,
      "step": 908900
    },
    {
      "epoch": 194.89708404802744,
      "grad_norm": 5.708398342132568,
      "learning_rate": 1.50385e-05,
      "loss": 3.2333,
      "step": 909000
    },
    {
      "epoch": 194.91852487135506,
      "grad_norm": 6.107649803161621,
      "learning_rate": 1.5034653846153846e-05,
      "loss": 3.1488,
      "step": 909100
    },
    {
      "epoch": 194.93996569468268,
      "grad_norm": 6.0807013511657715,
      "learning_rate": 1.5030807692307695e-05,
      "loss": 3.1839,
      "step": 909200
    },
    {
      "epoch": 194.96140651801028,
      "grad_norm": 6.154359340667725,
      "learning_rate": 1.502696153846154e-05,
      "loss": 3.1718,
      "step": 909300
    },
    {
      "epoch": 194.9828473413379,
      "grad_norm": 5.949069023132324,
      "learning_rate": 1.5023115384615386e-05,
      "loss": 3.2224,
      "step": 909400
    },
    {
      "epoch": 195.00428816466552,
      "grad_norm": 5.2818403244018555,
      "learning_rate": 1.5019269230769231e-05,
      "loss": 3.1643,
      "step": 909500
    },
    {
      "epoch": 195.02572898799315,
      "grad_norm": 6.472718238830566,
      "learning_rate": 1.5015423076923077e-05,
      "loss": 3.2047,
      "step": 909600
    },
    {
      "epoch": 195.04716981132074,
      "grad_norm": 5.994329929351807,
      "learning_rate": 1.5011576923076922e-05,
      "loss": 3.1535,
      "step": 909700
    },
    {
      "epoch": 195.06861063464837,
      "grad_norm": 6.221482753753662,
      "learning_rate": 1.5007730769230771e-05,
      "loss": 3.1167,
      "step": 909800
    },
    {
      "epoch": 195.090051457976,
      "grad_norm": 5.739551544189453,
      "learning_rate": 1.5003884615384617e-05,
      "loss": 3.1615,
      "step": 909900
    },
    {
      "epoch": 195.1114922813036,
      "grad_norm": 5.817452430725098,
      "learning_rate": 1.5000038461538462e-05,
      "loss": 3.1503,
      "step": 910000
    },
    {
      "epoch": 195.1329331046312,
      "grad_norm": 6.212490558624268,
      "learning_rate": 1.4996192307692308e-05,
      "loss": 3.1541,
      "step": 910100
    },
    {
      "epoch": 195.15437392795883,
      "grad_norm": 6.277976036071777,
      "learning_rate": 1.4992346153846153e-05,
      "loss": 3.1252,
      "step": 910200
    },
    {
      "epoch": 195.17581475128645,
      "grad_norm": 5.594788074493408,
      "learning_rate": 1.49885e-05,
      "loss": 3.1977,
      "step": 910300
    },
    {
      "epoch": 195.19725557461408,
      "grad_norm": 5.873576641082764,
      "learning_rate": 1.4984653846153846e-05,
      "loss": 3.1553,
      "step": 910400
    },
    {
      "epoch": 195.21869639794167,
      "grad_norm": 5.9229302406311035,
      "learning_rate": 1.4980807692307693e-05,
      "loss": 3.1072,
      "step": 910500
    },
    {
      "epoch": 195.2401372212693,
      "grad_norm": 6.1787261962890625,
      "learning_rate": 1.4976961538461539e-05,
      "loss": 3.1658,
      "step": 910600
    },
    {
      "epoch": 195.26157804459692,
      "grad_norm": 6.233509063720703,
      "learning_rate": 1.4973115384615386e-05,
      "loss": 3.1471,
      "step": 910700
    },
    {
      "epoch": 195.28301886792454,
      "grad_norm": 5.706848621368408,
      "learning_rate": 1.4969269230769232e-05,
      "loss": 3.1458,
      "step": 910800
    },
    {
      "epoch": 195.30445969125213,
      "grad_norm": 5.774992942810059,
      "learning_rate": 1.4965423076923077e-05,
      "loss": 3.1359,
      "step": 910900
    },
    {
      "epoch": 195.32590051457976,
      "grad_norm": 5.737361431121826,
      "learning_rate": 1.4961576923076923e-05,
      "loss": 3.1888,
      "step": 911000
    },
    {
      "epoch": 195.34734133790738,
      "grad_norm": 6.021194934844971,
      "learning_rate": 1.4957730769230772e-05,
      "loss": 3.1266,
      "step": 911100
    },
    {
      "epoch": 195.368782161235,
      "grad_norm": 6.744422912597656,
      "learning_rate": 1.4953884615384617e-05,
      "loss": 3.188,
      "step": 911200
    },
    {
      "epoch": 195.3902229845626,
      "grad_norm": 5.823272705078125,
      "learning_rate": 1.4950038461538463e-05,
      "loss": 3.1499,
      "step": 911300
    },
    {
      "epoch": 195.41166380789022,
      "grad_norm": 6.2900848388671875,
      "learning_rate": 1.4946192307692308e-05,
      "loss": 3.1403,
      "step": 911400
    },
    {
      "epoch": 195.43310463121784,
      "grad_norm": 6.105334758758545,
      "learning_rate": 1.4942346153846154e-05,
      "loss": 3.1259,
      "step": 911500
    },
    {
      "epoch": 195.45454545454547,
      "grad_norm": 5.904638767242432,
      "learning_rate": 1.49385e-05,
      "loss": 3.2046,
      "step": 911600
    },
    {
      "epoch": 195.47598627787306,
      "grad_norm": 6.573207378387451,
      "learning_rate": 1.4934653846153848e-05,
      "loss": 3.1881,
      "step": 911700
    },
    {
      "epoch": 195.49742710120069,
      "grad_norm": 6.493252754211426,
      "learning_rate": 1.4930807692307694e-05,
      "loss": 3.1525,
      "step": 911800
    },
    {
      "epoch": 195.5188679245283,
      "grad_norm": 6.013672828674316,
      "learning_rate": 1.492696153846154e-05,
      "loss": 3.1582,
      "step": 911900
    },
    {
      "epoch": 195.54030874785593,
      "grad_norm": 5.811559677124023,
      "learning_rate": 1.4923115384615385e-05,
      "loss": 3.1285,
      "step": 912000
    },
    {
      "epoch": 195.56174957118353,
      "grad_norm": 5.470832347869873,
      "learning_rate": 1.491926923076923e-05,
      "loss": 3.1778,
      "step": 912100
    },
    {
      "epoch": 195.58319039451115,
      "grad_norm": 6.253789901733398,
      "learning_rate": 1.4915423076923078e-05,
      "loss": 3.1051,
      "step": 912200
    },
    {
      "epoch": 195.60463121783877,
      "grad_norm": 5.681285858154297,
      "learning_rate": 1.4911576923076923e-05,
      "loss": 3.215,
      "step": 912300
    },
    {
      "epoch": 195.62607204116637,
      "grad_norm": 6.12205171585083,
      "learning_rate": 1.490773076923077e-05,
      "loss": 3.1698,
      "step": 912400
    },
    {
      "epoch": 195.647512864494,
      "grad_norm": 5.964907169342041,
      "learning_rate": 1.4903884615384616e-05,
      "loss": 3.1579,
      "step": 912500
    },
    {
      "epoch": 195.6689536878216,
      "grad_norm": 5.646248817443848,
      "learning_rate": 1.4900038461538463e-05,
      "loss": 3.1583,
      "step": 912600
    },
    {
      "epoch": 195.69039451114924,
      "grad_norm": 6.168918132781982,
      "learning_rate": 1.4896192307692309e-05,
      "loss": 3.1416,
      "step": 912700
    },
    {
      "epoch": 195.71183533447683,
      "grad_norm": 5.9498066902160645,
      "learning_rate": 1.4892346153846154e-05,
      "loss": 3.219,
      "step": 912800
    },
    {
      "epoch": 195.73327615780445,
      "grad_norm": 6.304244518280029,
      "learning_rate": 1.48885e-05,
      "loss": 3.1758,
      "step": 912900
    },
    {
      "epoch": 195.75471698113208,
      "grad_norm": 5.36489200592041,
      "learning_rate": 1.4884653846153849e-05,
      "loss": 3.1305,
      "step": 913000
    },
    {
      "epoch": 195.7761578044597,
      "grad_norm": 6.370334625244141,
      "learning_rate": 1.4880807692307694e-05,
      "loss": 3.1533,
      "step": 913100
    },
    {
      "epoch": 195.7975986277873,
      "grad_norm": 6.0403852462768555,
      "learning_rate": 1.487696153846154e-05,
      "loss": 3.1991,
      "step": 913200
    },
    {
      "epoch": 195.81903945111492,
      "grad_norm": 6.720089435577393,
      "learning_rate": 1.4873115384615385e-05,
      "loss": 3.1652,
      "step": 913300
    },
    {
      "epoch": 195.84048027444254,
      "grad_norm": 6.248708724975586,
      "learning_rate": 1.4869269230769231e-05,
      "loss": 3.1483,
      "step": 913400
    },
    {
      "epoch": 195.86192109777016,
      "grad_norm": 6.21718692779541,
      "learning_rate": 1.4865423076923076e-05,
      "loss": 3.2046,
      "step": 913500
    },
    {
      "epoch": 195.88336192109776,
      "grad_norm": 5.9616780281066895,
      "learning_rate": 1.4861576923076922e-05,
      "loss": 3.2029,
      "step": 913600
    },
    {
      "epoch": 195.90480274442538,
      "grad_norm": 5.924463748931885,
      "learning_rate": 1.4857730769230771e-05,
      "loss": 3.1767,
      "step": 913700
    },
    {
      "epoch": 195.926243567753,
      "grad_norm": 5.325628280639648,
      "learning_rate": 1.4853884615384616e-05,
      "loss": 3.1911,
      "step": 913800
    },
    {
      "epoch": 195.94768439108063,
      "grad_norm": 6.427597522735596,
      "learning_rate": 1.4850038461538462e-05,
      "loss": 3.1635,
      "step": 913900
    },
    {
      "epoch": 195.96912521440822,
      "grad_norm": 6.256950855255127,
      "learning_rate": 1.4846192307692308e-05,
      "loss": 3.1106,
      "step": 914000
    },
    {
      "epoch": 195.99056603773585,
      "grad_norm": 5.798529148101807,
      "learning_rate": 1.4842346153846153e-05,
      "loss": 3.1758,
      "step": 914100
    },
    {
      "epoch": 196.01200686106347,
      "grad_norm": 5.985104084014893,
      "learning_rate": 1.48385e-05,
      "loss": 3.1504,
      "step": 914200
    },
    {
      "epoch": 196.0334476843911,
      "grad_norm": 5.759666442871094,
      "learning_rate": 1.4834653846153848e-05,
      "loss": 3.1166,
      "step": 914300
    },
    {
      "epoch": 196.0548885077187,
      "grad_norm": 5.9167304039001465,
      "learning_rate": 1.4830807692307693e-05,
      "loss": 3.1097,
      "step": 914400
    },
    {
      "epoch": 196.0763293310463,
      "grad_norm": 6.217963695526123,
      "learning_rate": 1.482696153846154e-05,
      "loss": 3.2019,
      "step": 914500
    },
    {
      "epoch": 196.09777015437393,
      "grad_norm": 5.4020280838012695,
      "learning_rate": 1.4823115384615386e-05,
      "loss": 3.1437,
      "step": 914600
    },
    {
      "epoch": 196.11921097770156,
      "grad_norm": 4.845032691955566,
      "learning_rate": 1.4819269230769231e-05,
      "loss": 3.1884,
      "step": 914700
    },
    {
      "epoch": 196.14065180102915,
      "grad_norm": 6.280491352081299,
      "learning_rate": 1.4815423076923077e-05,
      "loss": 3.1692,
      "step": 914800
    },
    {
      "epoch": 196.16209262435677,
      "grad_norm": 6.162403583526611,
      "learning_rate": 1.4811576923076922e-05,
      "loss": 3.1802,
      "step": 914900
    },
    {
      "epoch": 196.1835334476844,
      "grad_norm": 6.245489597320557,
      "learning_rate": 1.4807730769230771e-05,
      "loss": 3.1904,
      "step": 915000
    },
    {
      "epoch": 196.20497427101202,
      "grad_norm": 6.0934062004089355,
      "learning_rate": 1.4803884615384617e-05,
      "loss": 3.2081,
      "step": 915100
    },
    {
      "epoch": 196.22641509433961,
      "grad_norm": 7.271245956420898,
      "learning_rate": 1.4800038461538462e-05,
      "loss": 3.158,
      "step": 915200
    },
    {
      "epoch": 196.24785591766724,
      "grad_norm": 6.571414947509766,
      "learning_rate": 1.4796192307692308e-05,
      "loss": 3.1529,
      "step": 915300
    },
    {
      "epoch": 196.26929674099486,
      "grad_norm": 5.68502140045166,
      "learning_rate": 1.4792346153846154e-05,
      "loss": 3.1503,
      "step": 915400
    },
    {
      "epoch": 196.29073756432248,
      "grad_norm": 6.309792518615723,
      "learning_rate": 1.4788499999999999e-05,
      "loss": 3.1317,
      "step": 915500
    },
    {
      "epoch": 196.31217838765008,
      "grad_norm": 5.759838581085205,
      "learning_rate": 1.4784653846153848e-05,
      "loss": 3.141,
      "step": 915600
    },
    {
      "epoch": 196.3336192109777,
      "grad_norm": 5.351002216339111,
      "learning_rate": 1.4780807692307694e-05,
      "loss": 3.1563,
      "step": 915700
    },
    {
      "epoch": 196.35506003430532,
      "grad_norm": 5.572056770324707,
      "learning_rate": 1.4776961538461539e-05,
      "loss": 3.1712,
      "step": 915800
    },
    {
      "epoch": 196.37650085763292,
      "grad_norm": 6.506530284881592,
      "learning_rate": 1.4773115384615385e-05,
      "loss": 3.1346,
      "step": 915900
    },
    {
      "epoch": 196.39794168096054,
      "grad_norm": 6.307765960693359,
      "learning_rate": 1.476926923076923e-05,
      "loss": 3.1407,
      "step": 916000
    },
    {
      "epoch": 196.41938250428817,
      "grad_norm": 6.241100788116455,
      "learning_rate": 1.4765423076923077e-05,
      "loss": 3.1702,
      "step": 916100
    },
    {
      "epoch": 196.4408233276158,
      "grad_norm": 6.409085750579834,
      "learning_rate": 1.4761576923076923e-05,
      "loss": 3.1498,
      "step": 916200
    },
    {
      "epoch": 196.46226415094338,
      "grad_norm": 5.550173759460449,
      "learning_rate": 1.475773076923077e-05,
      "loss": 3.1477,
      "step": 916300
    },
    {
      "epoch": 196.483704974271,
      "grad_norm": 6.284125804901123,
      "learning_rate": 1.4753884615384617e-05,
      "loss": 3.1873,
      "step": 916400
    },
    {
      "epoch": 196.50514579759863,
      "grad_norm": 5.286627769470215,
      "learning_rate": 1.4750038461538463e-05,
      "loss": 3.1206,
      "step": 916500
    },
    {
      "epoch": 196.52658662092625,
      "grad_norm": 5.96277379989624,
      "learning_rate": 1.4746192307692309e-05,
      "loss": 3.1444,
      "step": 916600
    },
    {
      "epoch": 196.54802744425385,
      "grad_norm": 5.962784290313721,
      "learning_rate": 1.4742346153846154e-05,
      "loss": 3.1268,
      "step": 916700
    },
    {
      "epoch": 196.56946826758147,
      "grad_norm": 6.152798175811768,
      "learning_rate": 1.47385e-05,
      "loss": 3.2044,
      "step": 916800
    },
    {
      "epoch": 196.5909090909091,
      "grad_norm": 6.154663562774658,
      "learning_rate": 1.4734653846153849e-05,
      "loss": 3.2299,
      "step": 916900
    },
    {
      "epoch": 196.61234991423672,
      "grad_norm": 6.644702434539795,
      "learning_rate": 1.4730807692307694e-05,
      "loss": 3.2314,
      "step": 917000
    },
    {
      "epoch": 196.6337907375643,
      "grad_norm": 5.773877143859863,
      "learning_rate": 1.472696153846154e-05,
      "loss": 3.1197,
      "step": 917100
    },
    {
      "epoch": 196.65523156089193,
      "grad_norm": 5.972235202789307,
      "learning_rate": 1.4723115384615385e-05,
      "loss": 3.1333,
      "step": 917200
    },
    {
      "epoch": 196.67667238421956,
      "grad_norm": 6.651852607727051,
      "learning_rate": 1.471926923076923e-05,
      "loss": 3.1699,
      "step": 917300
    },
    {
      "epoch": 196.69811320754718,
      "grad_norm": 5.38579797744751,
      "learning_rate": 1.4715423076923076e-05,
      "loss": 3.1368,
      "step": 917400
    },
    {
      "epoch": 196.71955403087478,
      "grad_norm": 6.032364845275879,
      "learning_rate": 1.4711576923076925e-05,
      "loss": 3.1624,
      "step": 917500
    },
    {
      "epoch": 196.7409948542024,
      "grad_norm": 6.0573225021362305,
      "learning_rate": 1.470773076923077e-05,
      "loss": 3.1427,
      "step": 917600
    },
    {
      "epoch": 196.76243567753002,
      "grad_norm": 5.973730564117432,
      "learning_rate": 1.4703884615384616e-05,
      "loss": 3.1349,
      "step": 917700
    },
    {
      "epoch": 196.78387650085764,
      "grad_norm": 5.696130752563477,
      "learning_rate": 1.4700038461538462e-05,
      "loss": 3.1001,
      "step": 917800
    },
    {
      "epoch": 196.80531732418524,
      "grad_norm": 6.383752346038818,
      "learning_rate": 1.4696192307692307e-05,
      "loss": 3.1546,
      "step": 917900
    },
    {
      "epoch": 196.82675814751286,
      "grad_norm": 6.089195251464844,
      "learning_rate": 1.4692346153846155e-05,
      "loss": 3.126,
      "step": 918000
    },
    {
      "epoch": 196.84819897084049,
      "grad_norm": 5.7606916427612305,
      "learning_rate": 1.46885e-05,
      "loss": 3.133,
      "step": 918100
    },
    {
      "epoch": 196.8696397941681,
      "grad_norm": 6.094522476196289,
      "learning_rate": 1.4684653846153847e-05,
      "loss": 3.1822,
      "step": 918200
    },
    {
      "epoch": 196.8910806174957,
      "grad_norm": 6.180664539337158,
      "learning_rate": 1.4680807692307695e-05,
      "loss": 3.1149,
      "step": 918300
    },
    {
      "epoch": 196.91252144082333,
      "grad_norm": 6.401766300201416,
      "learning_rate": 1.467696153846154e-05,
      "loss": 3.1543,
      "step": 918400
    },
    {
      "epoch": 196.93396226415095,
      "grad_norm": 5.928998947143555,
      "learning_rate": 1.4673115384615386e-05,
      "loss": 3.1149,
      "step": 918500
    },
    {
      "epoch": 196.95540308747857,
      "grad_norm": 7.456750869750977,
      "learning_rate": 1.4669269230769231e-05,
      "loss": 3.1581,
      "step": 918600
    },
    {
      "epoch": 196.97684391080617,
      "grad_norm": 5.841894626617432,
      "learning_rate": 1.4665423076923077e-05,
      "loss": 3.1479,
      "step": 918700
    },
    {
      "epoch": 196.9982847341338,
      "grad_norm": 5.862307548522949,
      "learning_rate": 1.4661576923076926e-05,
      "loss": 3.1661,
      "step": 918800
    },
    {
      "epoch": 197.0197255574614,
      "grad_norm": 6.178993225097656,
      "learning_rate": 1.4657730769230771e-05,
      "loss": 3.1293,
      "step": 918900
    },
    {
      "epoch": 197.04116638078904,
      "grad_norm": 5.944722652435303,
      "learning_rate": 1.4653884615384617e-05,
      "loss": 3.1313,
      "step": 919000
    },
    {
      "epoch": 197.06260720411663,
      "grad_norm": 5.5311970710754395,
      "learning_rate": 1.4650038461538462e-05,
      "loss": 3.1088,
      "step": 919100
    },
    {
      "epoch": 197.08404802744425,
      "grad_norm": 5.284236431121826,
      "learning_rate": 1.4646192307692308e-05,
      "loss": 3.1434,
      "step": 919200
    },
    {
      "epoch": 197.10548885077188,
      "grad_norm": 5.961784362792969,
      "learning_rate": 1.4642346153846153e-05,
      "loss": 3.1425,
      "step": 919300
    },
    {
      "epoch": 197.12692967409947,
      "grad_norm": 6.606875896453857,
      "learning_rate": 1.4638499999999999e-05,
      "loss": 3.1937,
      "step": 919400
    },
    {
      "epoch": 197.1483704974271,
      "grad_norm": 5.685489654541016,
      "learning_rate": 1.4634653846153848e-05,
      "loss": 3.1264,
      "step": 919500
    },
    {
      "epoch": 197.16981132075472,
      "grad_norm": 5.615463733673096,
      "learning_rate": 1.4630807692307693e-05,
      "loss": 3.1931,
      "step": 919600
    },
    {
      "epoch": 197.19125214408234,
      "grad_norm": 5.868171215057373,
      "learning_rate": 1.4626961538461539e-05,
      "loss": 3.1307,
      "step": 919700
    },
    {
      "epoch": 197.21269296740994,
      "grad_norm": 5.8932647705078125,
      "learning_rate": 1.4623115384615384e-05,
      "loss": 3.1441,
      "step": 919800
    },
    {
      "epoch": 197.23413379073756,
      "grad_norm": 5.8635969161987305,
      "learning_rate": 1.4619269230769232e-05,
      "loss": 3.15,
      "step": 919900
    },
    {
      "epoch": 197.25557461406518,
      "grad_norm": 6.3962249755859375,
      "learning_rate": 1.4615423076923077e-05,
      "loss": 3.1045,
      "step": 920000
    },
    {
      "epoch": 197.2770154373928,
      "grad_norm": 5.886024475097656,
      "learning_rate": 1.4611576923076924e-05,
      "loss": 3.1463,
      "step": 920100
    },
    {
      "epoch": 197.2984562607204,
      "grad_norm": 6.448492527008057,
      "learning_rate": 1.460773076923077e-05,
      "loss": 3.1519,
      "step": 920200
    },
    {
      "epoch": 197.31989708404802,
      "grad_norm": 5.795155048370361,
      "learning_rate": 1.4603884615384617e-05,
      "loss": 3.1072,
      "step": 920300
    },
    {
      "epoch": 197.34133790737565,
      "grad_norm": 6.1507720947265625,
      "learning_rate": 1.4600038461538463e-05,
      "loss": 3.1502,
      "step": 920400
    },
    {
      "epoch": 197.36277873070327,
      "grad_norm": 6.137340068817139,
      "learning_rate": 1.4596192307692308e-05,
      "loss": 3.1457,
      "step": 920500
    },
    {
      "epoch": 197.38421955403086,
      "grad_norm": 5.349883079528809,
      "learning_rate": 1.4592346153846154e-05,
      "loss": 3.1007,
      "step": 920600
    },
    {
      "epoch": 197.4056603773585,
      "grad_norm": 6.4111528396606445,
      "learning_rate": 1.45885e-05,
      "loss": 3.1668,
      "step": 920700
    },
    {
      "epoch": 197.4271012006861,
      "grad_norm": 5.958942413330078,
      "learning_rate": 1.4584653846153848e-05,
      "loss": 3.172,
      "step": 920800
    },
    {
      "epoch": 197.44854202401373,
      "grad_norm": 6.425817489624023,
      "learning_rate": 1.4580807692307694e-05,
      "loss": 3.0726,
      "step": 920900
    },
    {
      "epoch": 197.46998284734133,
      "grad_norm": 6.073519706726074,
      "learning_rate": 1.457696153846154e-05,
      "loss": 3.1216,
      "step": 921000
    },
    {
      "epoch": 197.49142367066895,
      "grad_norm": 5.9758195877075195,
      "learning_rate": 1.4573115384615385e-05,
      "loss": 3.1417,
      "step": 921100
    },
    {
      "epoch": 197.51286449399657,
      "grad_norm": 6.132601737976074,
      "learning_rate": 1.456926923076923e-05,
      "loss": 3.1309,
      "step": 921200
    },
    {
      "epoch": 197.5343053173242,
      "grad_norm": 6.627628803253174,
      "learning_rate": 1.4565423076923076e-05,
      "loss": 3.1365,
      "step": 921300
    },
    {
      "epoch": 197.5557461406518,
      "grad_norm": 6.012118339538574,
      "learning_rate": 1.4561576923076925e-05,
      "loss": 3.1225,
      "step": 921400
    },
    {
      "epoch": 197.57718696397941,
      "grad_norm": 6.738480091094971,
      "learning_rate": 1.455773076923077e-05,
      "loss": 3.1769,
      "step": 921500
    },
    {
      "epoch": 197.59862778730704,
      "grad_norm": 5.747297286987305,
      "learning_rate": 1.4553884615384616e-05,
      "loss": 3.1209,
      "step": 921600
    },
    {
      "epoch": 197.62006861063466,
      "grad_norm": 6.213197231292725,
      "learning_rate": 1.4550038461538461e-05,
      "loss": 3.1466,
      "step": 921700
    },
    {
      "epoch": 197.64150943396226,
      "grad_norm": 5.834273815155029,
      "learning_rate": 1.4546192307692309e-05,
      "loss": 3.1617,
      "step": 921800
    },
    {
      "epoch": 197.66295025728988,
      "grad_norm": 5.927262306213379,
      "learning_rate": 1.4542346153846154e-05,
      "loss": 3.1197,
      "step": 921900
    },
    {
      "epoch": 197.6843910806175,
      "grad_norm": 6.176844120025635,
      "learning_rate": 1.45385e-05,
      "loss": 3.1321,
      "step": 922000
    },
    {
      "epoch": 197.70583190394512,
      "grad_norm": 6.367885112762451,
      "learning_rate": 1.4534653846153847e-05,
      "loss": 3.154,
      "step": 922100
    },
    {
      "epoch": 197.72727272727272,
      "grad_norm": 6.156252861022949,
      "learning_rate": 1.4530807692307694e-05,
      "loss": 3.1726,
      "step": 922200
    },
    {
      "epoch": 197.74871355060034,
      "grad_norm": 6.034012794494629,
      "learning_rate": 1.452696153846154e-05,
      "loss": 3.1817,
      "step": 922300
    },
    {
      "epoch": 197.77015437392797,
      "grad_norm": 6.025606632232666,
      "learning_rate": 1.4523115384615385e-05,
      "loss": 3.1441,
      "step": 922400
    },
    {
      "epoch": 197.79159519725556,
      "grad_norm": 6.295785427093506,
      "learning_rate": 1.4519269230769231e-05,
      "loss": 3.185,
      "step": 922500
    },
    {
      "epoch": 197.81303602058318,
      "grad_norm": 6.105353832244873,
      "learning_rate": 1.4515423076923076e-05,
      "loss": 3.1346,
      "step": 922600
    },
    {
      "epoch": 197.8344768439108,
      "grad_norm": 6.041969299316406,
      "learning_rate": 1.4511576923076925e-05,
      "loss": 3.1812,
      "step": 922700
    },
    {
      "epoch": 197.85591766723843,
      "grad_norm": 6.4433393478393555,
      "learning_rate": 1.4507730769230771e-05,
      "loss": 3.1663,
      "step": 922800
    },
    {
      "epoch": 197.87735849056602,
      "grad_norm": 6.215394496917725,
      "learning_rate": 1.4503884615384616e-05,
      "loss": 3.2388,
      "step": 922900
    },
    {
      "epoch": 197.89879931389365,
      "grad_norm": 5.560944080352783,
      "learning_rate": 1.4500038461538462e-05,
      "loss": 3.159,
      "step": 923000
    },
    {
      "epoch": 197.92024013722127,
      "grad_norm": 6.263544082641602,
      "learning_rate": 1.4496192307692308e-05,
      "loss": 3.187,
      "step": 923100
    },
    {
      "epoch": 197.9416809605489,
      "grad_norm": 5.900908946990967,
      "learning_rate": 1.4492346153846153e-05,
      "loss": 3.173,
      "step": 923200
    },
    {
      "epoch": 197.9631217838765,
      "grad_norm": 5.835081577301025,
      "learning_rate": 1.4488500000000002e-05,
      "loss": 3.2107,
      "step": 923300
    },
    {
      "epoch": 197.9845626072041,
      "grad_norm": 5.646725654602051,
      "learning_rate": 1.4484653846153848e-05,
      "loss": 3.1538,
      "step": 923400
    },
    {
      "epoch": 198.00600343053173,
      "grad_norm": 5.971057891845703,
      "learning_rate": 1.4480807692307693e-05,
      "loss": 3.1382,
      "step": 923500
    },
    {
      "epoch": 198.02744425385936,
      "grad_norm": 6.018957138061523,
      "learning_rate": 1.4476961538461539e-05,
      "loss": 3.1467,
      "step": 923600
    },
    {
      "epoch": 198.04888507718695,
      "grad_norm": 6.1443963050842285,
      "learning_rate": 1.4473115384615384e-05,
      "loss": 3.1273,
      "step": 923700
    },
    {
      "epoch": 198.07032590051458,
      "grad_norm": 6.072055816650391,
      "learning_rate": 1.4469269230769231e-05,
      "loss": 3.162,
      "step": 923800
    },
    {
      "epoch": 198.0917667238422,
      "grad_norm": 6.060914039611816,
      "learning_rate": 1.4465423076923077e-05,
      "loss": 3.1342,
      "step": 923900
    },
    {
      "epoch": 198.11320754716982,
      "grad_norm": 6.134096622467041,
      "learning_rate": 1.4461576923076924e-05,
      "loss": 3.1448,
      "step": 924000
    },
    {
      "epoch": 198.13464837049742,
      "grad_norm": 5.9959611892700195,
      "learning_rate": 1.4457730769230771e-05,
      "loss": 3.1395,
      "step": 924100
    },
    {
      "epoch": 198.15608919382504,
      "grad_norm": 5.180776596069336,
      "learning_rate": 1.4453884615384617e-05,
      "loss": 3.1089,
      "step": 924200
    },
    {
      "epoch": 198.17753001715266,
      "grad_norm": 6.121769905090332,
      "learning_rate": 1.4450038461538462e-05,
      "loss": 3.1205,
      "step": 924300
    },
    {
      "epoch": 198.19897084048029,
      "grad_norm": 6.135359764099121,
      "learning_rate": 1.4446192307692308e-05,
      "loss": 3.0922,
      "step": 924400
    },
    {
      "epoch": 198.22041166380788,
      "grad_norm": 6.328393936157227,
      "learning_rate": 1.4442346153846154e-05,
      "loss": 3.18,
      "step": 924500
    },
    {
      "epoch": 198.2418524871355,
      "grad_norm": 5.961337089538574,
      "learning_rate": 1.4438500000000002e-05,
      "loss": 3.1619,
      "step": 924600
    },
    {
      "epoch": 198.26329331046313,
      "grad_norm": 5.676421165466309,
      "learning_rate": 1.4434653846153848e-05,
      "loss": 3.1729,
      "step": 924700
    },
    {
      "epoch": 198.28473413379075,
      "grad_norm": 6.31486701965332,
      "learning_rate": 1.4430807692307694e-05,
      "loss": 3.1484,
      "step": 924800
    },
    {
      "epoch": 198.30617495711834,
      "grad_norm": 5.466541767120361,
      "learning_rate": 1.4426961538461539e-05,
      "loss": 3.1402,
      "step": 924900
    },
    {
      "epoch": 198.32761578044597,
      "grad_norm": 6.279943943023682,
      "learning_rate": 1.4423115384615385e-05,
      "loss": 3.1471,
      "step": 925000
    },
    {
      "epoch": 198.3490566037736,
      "grad_norm": 5.651363372802734,
      "learning_rate": 1.441926923076923e-05,
      "loss": 3.0834,
      "step": 925100
    },
    {
      "epoch": 198.3704974271012,
      "grad_norm": 6.569727420806885,
      "learning_rate": 1.4415423076923076e-05,
      "loss": 3.171,
      "step": 925200
    },
    {
      "epoch": 198.3919382504288,
      "grad_norm": 6.00858736038208,
      "learning_rate": 1.4411576923076925e-05,
      "loss": 3.1156,
      "step": 925300
    },
    {
      "epoch": 198.41337907375643,
      "grad_norm": 5.953629493713379,
      "learning_rate": 1.440773076923077e-05,
      "loss": 3.1461,
      "step": 925400
    },
    {
      "epoch": 198.43481989708405,
      "grad_norm": 5.902010440826416,
      "learning_rate": 1.4403884615384616e-05,
      "loss": 3.1249,
      "step": 925500
    },
    {
      "epoch": 198.45626072041168,
      "grad_norm": 6.2067484855651855,
      "learning_rate": 1.4400038461538461e-05,
      "loss": 3.1679,
      "step": 925600
    },
    {
      "epoch": 198.47770154373927,
      "grad_norm": 5.945721626281738,
      "learning_rate": 1.4396192307692308e-05,
      "loss": 3.1313,
      "step": 925700
    },
    {
      "epoch": 198.4991423670669,
      "grad_norm": 5.705329895019531,
      "learning_rate": 1.4392346153846154e-05,
      "loss": 3.1351,
      "step": 925800
    },
    {
      "epoch": 198.52058319039452,
      "grad_norm": 5.341081142425537,
      "learning_rate": 1.4388500000000001e-05,
      "loss": 3.1592,
      "step": 925900
    },
    {
      "epoch": 198.5420240137221,
      "grad_norm": 5.752011299133301,
      "learning_rate": 1.4384653846153848e-05,
      "loss": 3.1798,
      "step": 926000
    },
    {
      "epoch": 198.56346483704974,
      "grad_norm": 5.805721282958984,
      "learning_rate": 1.4380807692307694e-05,
      "loss": 3.1346,
      "step": 926100
    },
    {
      "epoch": 198.58490566037736,
      "grad_norm": 6.298468589782715,
      "learning_rate": 1.437696153846154e-05,
      "loss": 3.1351,
      "step": 926200
    },
    {
      "epoch": 198.60634648370498,
      "grad_norm": 5.836025238037109,
      "learning_rate": 1.4373115384615385e-05,
      "loss": 3.1389,
      "step": 926300
    },
    {
      "epoch": 198.62778730703258,
      "grad_norm": 6.440474510192871,
      "learning_rate": 1.436926923076923e-05,
      "loss": 3.1853,
      "step": 926400
    },
    {
      "epoch": 198.6492281303602,
      "grad_norm": 6.0155253410339355,
      "learning_rate": 1.4365423076923076e-05,
      "loss": 3.1469,
      "step": 926500
    },
    {
      "epoch": 198.67066895368782,
      "grad_norm": 6.459075450897217,
      "learning_rate": 1.4361576923076925e-05,
      "loss": 3.1427,
      "step": 926600
    },
    {
      "epoch": 198.69210977701545,
      "grad_norm": 6.63651704788208,
      "learning_rate": 1.435773076923077e-05,
      "loss": 3.1534,
      "step": 926700
    },
    {
      "epoch": 198.71355060034304,
      "grad_norm": 6.4035797119140625,
      "learning_rate": 1.4353884615384616e-05,
      "loss": 3.1702,
      "step": 926800
    },
    {
      "epoch": 198.73499142367066,
      "grad_norm": 5.763186931610107,
      "learning_rate": 1.4350038461538462e-05,
      "loss": 3.176,
      "step": 926900
    },
    {
      "epoch": 198.7564322469983,
      "grad_norm": 5.525667190551758,
      "learning_rate": 1.4346192307692307e-05,
      "loss": 3.088,
      "step": 927000
    },
    {
      "epoch": 198.7778730703259,
      "grad_norm": 5.93289852142334,
      "learning_rate": 1.4342346153846153e-05,
      "loss": 3.1176,
      "step": 927100
    },
    {
      "epoch": 198.7993138936535,
      "grad_norm": 5.492753505706787,
      "learning_rate": 1.4338500000000002e-05,
      "loss": 3.1431,
      "step": 927200
    },
    {
      "epoch": 198.82075471698113,
      "grad_norm": 5.598371505737305,
      "learning_rate": 1.4334653846153847e-05,
      "loss": 3.0544,
      "step": 927300
    },
    {
      "epoch": 198.84219554030875,
      "grad_norm": 6.236652851104736,
      "learning_rate": 1.4330807692307693e-05,
      "loss": 3.1829,
      "step": 927400
    },
    {
      "epoch": 198.86363636363637,
      "grad_norm": 6.263121604919434,
      "learning_rate": 1.4326961538461538e-05,
      "loss": 3.1485,
      "step": 927500
    },
    {
      "epoch": 198.88507718696397,
      "grad_norm": 6.264618396759033,
      "learning_rate": 1.4323115384615386e-05,
      "loss": 3.121,
      "step": 927600
    },
    {
      "epoch": 198.9065180102916,
      "grad_norm": 5.760220050811768,
      "learning_rate": 1.4319269230769231e-05,
      "loss": 3.1215,
      "step": 927700
    },
    {
      "epoch": 198.92795883361921,
      "grad_norm": 5.564792156219482,
      "learning_rate": 1.4315423076923077e-05,
      "loss": 3.1938,
      "step": 927800
    },
    {
      "epoch": 198.94939965694684,
      "grad_norm": 5.987853050231934,
      "learning_rate": 1.4311576923076926e-05,
      "loss": 3.1554,
      "step": 927900
    },
    {
      "epoch": 198.97084048027443,
      "grad_norm": 6.844851970672607,
      "learning_rate": 1.4307730769230771e-05,
      "loss": 3.1455,
      "step": 928000
    },
    {
      "epoch": 198.99228130360206,
      "grad_norm": 6.432143211364746,
      "learning_rate": 1.4303884615384617e-05,
      "loss": 3.1741,
      "step": 928100
    },
    {
      "epoch": 199.01372212692968,
      "grad_norm": 6.141564846038818,
      "learning_rate": 1.4300038461538462e-05,
      "loss": 3.1512,
      "step": 928200
    },
    {
      "epoch": 199.0351629502573,
      "grad_norm": 6.109642028808594,
      "learning_rate": 1.4296192307692308e-05,
      "loss": 3.1017,
      "step": 928300
    },
    {
      "epoch": 199.0566037735849,
      "grad_norm": 5.924812316894531,
      "learning_rate": 1.4292346153846153e-05,
      "loss": 3.1013,
      "step": 928400
    },
    {
      "epoch": 199.07804459691252,
      "grad_norm": 5.666936874389648,
      "learning_rate": 1.4288500000000002e-05,
      "loss": 3.1152,
      "step": 928500
    },
    {
      "epoch": 199.09948542024014,
      "grad_norm": 6.187933921813965,
      "learning_rate": 1.4284653846153848e-05,
      "loss": 3.0845,
      "step": 928600
    },
    {
      "epoch": 199.12092624356777,
      "grad_norm": 6.137792587280273,
      "learning_rate": 1.4280807692307693e-05,
      "loss": 3.1141,
      "step": 928700
    },
    {
      "epoch": 199.14236706689536,
      "grad_norm": 6.397487163543701,
      "learning_rate": 1.4276961538461539e-05,
      "loss": 3.1266,
      "step": 928800
    },
    {
      "epoch": 199.16380789022298,
      "grad_norm": 6.12106466293335,
      "learning_rate": 1.4273115384615384e-05,
      "loss": 3.1263,
      "step": 928900
    },
    {
      "epoch": 199.1852487135506,
      "grad_norm": 5.938628196716309,
      "learning_rate": 1.426926923076923e-05,
      "loss": 3.1446,
      "step": 929000
    },
    {
      "epoch": 199.20668953687823,
      "grad_norm": 6.68428373336792,
      "learning_rate": 1.4265423076923075e-05,
      "loss": 3.1216,
      "step": 929100
    },
    {
      "epoch": 199.22813036020582,
      "grad_norm": 5.986534118652344,
      "learning_rate": 1.4261576923076924e-05,
      "loss": 3.1274,
      "step": 929200
    },
    {
      "epoch": 199.24957118353345,
      "grad_norm": 6.207222938537598,
      "learning_rate": 1.425773076923077e-05,
      "loss": 3.1376,
      "step": 929300
    },
    {
      "epoch": 199.27101200686107,
      "grad_norm": 5.999289035797119,
      "learning_rate": 1.4253884615384615e-05,
      "loss": 3.1536,
      "step": 929400
    },
    {
      "epoch": 199.29245283018867,
      "grad_norm": 5.867269515991211,
      "learning_rate": 1.4250038461538463e-05,
      "loss": 3.1426,
      "step": 929500
    },
    {
      "epoch": 199.3138936535163,
      "grad_norm": 6.002577304840088,
      "learning_rate": 1.4246192307692308e-05,
      "loss": 3.0726,
      "step": 929600
    },
    {
      "epoch": 199.3353344768439,
      "grad_norm": 6.041067123413086,
      "learning_rate": 1.4242346153846154e-05,
      "loss": 3.1253,
      "step": 929700
    },
    {
      "epoch": 199.35677530017153,
      "grad_norm": 5.998824119567871,
      "learning_rate": 1.4238500000000003e-05,
      "loss": 3.1813,
      "step": 929800
    },
    {
      "epoch": 199.37821612349913,
      "grad_norm": 6.367769718170166,
      "learning_rate": 1.4234653846153848e-05,
      "loss": 3.1741,
      "step": 929900
    },
    {
      "epoch": 199.39965694682675,
      "grad_norm": 6.076988697052002,
      "learning_rate": 1.4230807692307694e-05,
      "loss": 3.173,
      "step": 930000
    },
    {
      "epoch": 199.42109777015438,
      "grad_norm": 6.329408168792725,
      "learning_rate": 1.422696153846154e-05,
      "loss": 3.1632,
      "step": 930100
    },
    {
      "epoch": 199.442538593482,
      "grad_norm": 6.353123664855957,
      "learning_rate": 1.4223115384615385e-05,
      "loss": 3.1675,
      "step": 930200
    },
    {
      "epoch": 199.4639794168096,
      "grad_norm": 6.550217628479004,
      "learning_rate": 1.421926923076923e-05,
      "loss": 3.1457,
      "step": 930300
    },
    {
      "epoch": 199.48542024013722,
      "grad_norm": 5.999199390411377,
      "learning_rate": 1.421542307692308e-05,
      "loss": 3.1296,
      "step": 930400
    },
    {
      "epoch": 199.50686106346484,
      "grad_norm": 5.555961608886719,
      "learning_rate": 1.4211576923076925e-05,
      "loss": 3.0834,
      "step": 930500
    },
    {
      "epoch": 199.52830188679246,
      "grad_norm": 5.742764949798584,
      "learning_rate": 1.420773076923077e-05,
      "loss": 3.1659,
      "step": 930600
    },
    {
      "epoch": 199.54974271012006,
      "grad_norm": 6.085417747497559,
      "learning_rate": 1.4203884615384616e-05,
      "loss": 3.1513,
      "step": 930700
    },
    {
      "epoch": 199.57118353344768,
      "grad_norm": 6.809829235076904,
      "learning_rate": 1.4200038461538461e-05,
      "loss": 3.1808,
      "step": 930800
    },
    {
      "epoch": 199.5926243567753,
      "grad_norm": 6.2596564292907715,
      "learning_rate": 1.4196192307692307e-05,
      "loss": 3.1723,
      "step": 930900
    },
    {
      "epoch": 199.61406518010293,
      "grad_norm": 6.764795780181885,
      "learning_rate": 1.4192346153846153e-05,
      "loss": 3.1167,
      "step": 931000
    },
    {
      "epoch": 199.63550600343052,
      "grad_norm": 6.154170513153076,
      "learning_rate": 1.4188500000000001e-05,
      "loss": 3.1061,
      "step": 931100
    },
    {
      "epoch": 199.65694682675814,
      "grad_norm": 5.081169605255127,
      "learning_rate": 1.4184653846153847e-05,
      "loss": 3.1658,
      "step": 931200
    },
    {
      "epoch": 199.67838765008577,
      "grad_norm": 6.140967845916748,
      "learning_rate": 1.4180807692307693e-05,
      "loss": 3.1593,
      "step": 931300
    },
    {
      "epoch": 199.6998284734134,
      "grad_norm": 6.336280345916748,
      "learning_rate": 1.417696153846154e-05,
      "loss": 3.1058,
      "step": 931400
    },
    {
      "epoch": 199.72126929674099,
      "grad_norm": 5.918127536773682,
      "learning_rate": 1.4173115384615385e-05,
      "loss": 3.1353,
      "step": 931500
    },
    {
      "epoch": 199.7427101200686,
      "grad_norm": 5.993221759796143,
      "learning_rate": 1.4169269230769231e-05,
      "loss": 3.224,
      "step": 931600
    },
    {
      "epoch": 199.76415094339623,
      "grad_norm": 6.342145919799805,
      "learning_rate": 1.4165423076923078e-05,
      "loss": 3.1525,
      "step": 931700
    },
    {
      "epoch": 199.78559176672385,
      "grad_norm": 5.831296920776367,
      "learning_rate": 1.4161576923076925e-05,
      "loss": 3.0955,
      "step": 931800
    },
    {
      "epoch": 199.80703259005145,
      "grad_norm": 5.760934829711914,
      "learning_rate": 1.4157730769230771e-05,
      "loss": 3.1568,
      "step": 931900
    },
    {
      "epoch": 199.82847341337907,
      "grad_norm": 6.425160884857178,
      "learning_rate": 1.4153884615384616e-05,
      "loss": 3.1186,
      "step": 932000
    },
    {
      "epoch": 199.8499142367067,
      "grad_norm": 6.211525917053223,
      "learning_rate": 1.4150038461538462e-05,
      "loss": 3.1242,
      "step": 932100
    },
    {
      "epoch": 199.87135506003432,
      "grad_norm": 6.327877998352051,
      "learning_rate": 1.4146192307692307e-05,
      "loss": 3.1152,
      "step": 932200
    },
    {
      "epoch": 199.8927958833619,
      "grad_norm": 5.5952372550964355,
      "learning_rate": 1.4142346153846153e-05,
      "loss": 3.1222,
      "step": 932300
    },
    {
      "epoch": 199.91423670668954,
      "grad_norm": 6.237522602081299,
      "learning_rate": 1.4138500000000002e-05,
      "loss": 3.1276,
      "step": 932400
    },
    {
      "epoch": 199.93567753001716,
      "grad_norm": 6.36997127532959,
      "learning_rate": 1.4134653846153847e-05,
      "loss": 3.141,
      "step": 932500
    },
    {
      "epoch": 199.95711835334478,
      "grad_norm": 6.179108619689941,
      "learning_rate": 1.4130807692307693e-05,
      "loss": 3.144,
      "step": 932600
    },
    {
      "epoch": 199.97855917667238,
      "grad_norm": 5.917880535125732,
      "learning_rate": 1.4126961538461539e-05,
      "loss": 3.1678,
      "step": 932700
    },
    {
      "epoch": 200.0,
      "grad_norm": 8.776616096496582,
      "learning_rate": 1.4123115384615384e-05,
      "loss": 3.1493,
      "step": 932800
    },
    {
      "epoch": 200.02144082332762,
      "grad_norm": 5.643282890319824,
      "learning_rate": 1.411926923076923e-05,
      "loss": 3.1237,
      "step": 932900
    },
    {
      "epoch": 200.04288164665522,
      "grad_norm": 6.091075420379639,
      "learning_rate": 1.4115423076923079e-05,
      "loss": 3.1266,
      "step": 933000
    },
    {
      "epoch": 200.06432246998284,
      "grad_norm": 6.190855026245117,
      "learning_rate": 1.4111576923076924e-05,
      "loss": 3.1214,
      "step": 933100
    },
    {
      "epoch": 200.08576329331046,
      "grad_norm": 6.178175449371338,
      "learning_rate": 1.410773076923077e-05,
      "loss": 3.1141,
      "step": 933200
    },
    {
      "epoch": 200.1072041166381,
      "grad_norm": 6.08493709564209,
      "learning_rate": 1.4103884615384617e-05,
      "loss": 3.1678,
      "step": 933300
    },
    {
      "epoch": 200.12864493996568,
      "grad_norm": 6.122073650360107,
      "learning_rate": 1.4100038461538462e-05,
      "loss": 3.1334,
      "step": 933400
    },
    {
      "epoch": 200.1500857632933,
      "grad_norm": 6.1177473068237305,
      "learning_rate": 1.4096192307692308e-05,
      "loss": 3.0996,
      "step": 933500
    },
    {
      "epoch": 200.17152658662093,
      "grad_norm": 5.863956928253174,
      "learning_rate": 1.4092346153846153e-05,
      "loss": 3.1263,
      "step": 933600
    },
    {
      "epoch": 200.19296740994855,
      "grad_norm": 5.825820446014404,
      "learning_rate": 1.4088500000000002e-05,
      "loss": 3.1203,
      "step": 933700
    },
    {
      "epoch": 200.21440823327615,
      "grad_norm": 6.113274097442627,
      "learning_rate": 1.4084653846153848e-05,
      "loss": 3.1282,
      "step": 933800
    },
    {
      "epoch": 200.23584905660377,
      "grad_norm": 6.04001522064209,
      "learning_rate": 1.4080807692307693e-05,
      "loss": 3.177,
      "step": 933900
    },
    {
      "epoch": 200.2572898799314,
      "grad_norm": 6.459092617034912,
      "learning_rate": 1.4076961538461539e-05,
      "loss": 3.1356,
      "step": 934000
    },
    {
      "epoch": 200.27873070325901,
      "grad_norm": 5.920022487640381,
      "learning_rate": 1.4073115384615385e-05,
      "loss": 3.1066,
      "step": 934100
    },
    {
      "epoch": 200.3001715265866,
      "grad_norm": 6.50656270980835,
      "learning_rate": 1.406926923076923e-05,
      "loss": 3.1746,
      "step": 934200
    },
    {
      "epoch": 200.32161234991423,
      "grad_norm": 6.041079998016357,
      "learning_rate": 1.4065423076923079e-05,
      "loss": 3.1878,
      "step": 934300
    },
    {
      "epoch": 200.34305317324186,
      "grad_norm": 6.2352752685546875,
      "learning_rate": 1.4061576923076925e-05,
      "loss": 3.1535,
      "step": 934400
    },
    {
      "epoch": 200.36449399656948,
      "grad_norm": 6.033261775970459,
      "learning_rate": 1.405773076923077e-05,
      "loss": 3.122,
      "step": 934500
    },
    {
      "epoch": 200.38593481989707,
      "grad_norm": 5.597538471221924,
      "learning_rate": 1.4053884615384616e-05,
      "loss": 3.1096,
      "step": 934600
    },
    {
      "epoch": 200.4073756432247,
      "grad_norm": 5.782768249511719,
      "learning_rate": 1.4050038461538461e-05,
      "loss": 3.1707,
      "step": 934700
    },
    {
      "epoch": 200.42881646655232,
      "grad_norm": 5.989081382751465,
      "learning_rate": 1.4046192307692307e-05,
      "loss": 3.132,
      "step": 934800
    },
    {
      "epoch": 200.45025728987994,
      "grad_norm": 5.961245059967041,
      "learning_rate": 1.4042346153846154e-05,
      "loss": 3.0748,
      "step": 934900
    },
    {
      "epoch": 200.47169811320754,
      "grad_norm": 6.05185604095459,
      "learning_rate": 1.4038500000000001e-05,
      "loss": 3.0888,
      "step": 935000
    },
    {
      "epoch": 200.49313893653516,
      "grad_norm": 6.256704330444336,
      "learning_rate": 1.4034653846153847e-05,
      "loss": 3.1414,
      "step": 935100
    },
    {
      "epoch": 200.51457975986278,
      "grad_norm": 5.827268123626709,
      "learning_rate": 1.4030807692307692e-05,
      "loss": 3.0773,
      "step": 935200
    },
    {
      "epoch": 200.5360205831904,
      "grad_norm": 5.905527114868164,
      "learning_rate": 1.402696153846154e-05,
      "loss": 3.1462,
      "step": 935300
    },
    {
      "epoch": 200.557461406518,
      "grad_norm": 6.044863224029541,
      "learning_rate": 1.4023115384615385e-05,
      "loss": 3.1629,
      "step": 935400
    },
    {
      "epoch": 200.57890222984562,
      "grad_norm": 6.273323059082031,
      "learning_rate": 1.401926923076923e-05,
      "loss": 3.1485,
      "step": 935500
    },
    {
      "epoch": 200.60034305317325,
      "grad_norm": 6.1975579261779785,
      "learning_rate": 1.401542307692308e-05,
      "loss": 3.1583,
      "step": 935600
    },
    {
      "epoch": 200.62178387650087,
      "grad_norm": 5.811069011688232,
      "learning_rate": 1.4011576923076925e-05,
      "loss": 3.1358,
      "step": 935700
    },
    {
      "epoch": 200.64322469982847,
      "grad_norm": 5.771157741546631,
      "learning_rate": 1.400773076923077e-05,
      "loss": 3.1293,
      "step": 935800
    },
    {
      "epoch": 200.6646655231561,
      "grad_norm": 6.247461318969727,
      "learning_rate": 1.4003884615384616e-05,
      "loss": 3.1185,
      "step": 935900
    },
    {
      "epoch": 200.6861063464837,
      "grad_norm": 6.020179271697998,
      "learning_rate": 1.4000038461538462e-05,
      "loss": 3.141,
      "step": 936000
    },
    {
      "epoch": 200.70754716981133,
      "grad_norm": 6.090433597564697,
      "learning_rate": 1.3996192307692307e-05,
      "loss": 3.1062,
      "step": 936100
    },
    {
      "epoch": 200.72898799313893,
      "grad_norm": 6.44758939743042,
      "learning_rate": 1.3992346153846156e-05,
      "loss": 3.1173,
      "step": 936200
    },
    {
      "epoch": 200.75042881646655,
      "grad_norm": 6.614899158477783,
      "learning_rate": 1.3988500000000002e-05,
      "loss": 3.0999,
      "step": 936300
    },
    {
      "epoch": 200.77186963979418,
      "grad_norm": 5.899679183959961,
      "learning_rate": 1.3984653846153847e-05,
      "loss": 3.1272,
      "step": 936400
    },
    {
      "epoch": 200.79331046312177,
      "grad_norm": 6.690036773681641,
      "learning_rate": 1.3980807692307693e-05,
      "loss": 3.1564,
      "step": 936500
    },
    {
      "epoch": 200.8147512864494,
      "grad_norm": 6.288020133972168,
      "learning_rate": 1.3976961538461538e-05,
      "loss": 3.1775,
      "step": 936600
    },
    {
      "epoch": 200.83619210977702,
      "grad_norm": 6.075049877166748,
      "learning_rate": 1.3973115384615384e-05,
      "loss": 3.1486,
      "step": 936700
    },
    {
      "epoch": 200.85763293310464,
      "grad_norm": 5.986537933349609,
      "learning_rate": 1.396926923076923e-05,
      "loss": 3.1105,
      "step": 936800
    },
    {
      "epoch": 200.87907375643223,
      "grad_norm": 6.339738845825195,
      "learning_rate": 1.3965423076923078e-05,
      "loss": 3.0722,
      "step": 936900
    },
    {
      "epoch": 200.90051457975986,
      "grad_norm": 5.859577655792236,
      "learning_rate": 1.3961576923076924e-05,
      "loss": 3.1323,
      "step": 937000
    },
    {
      "epoch": 200.92195540308748,
      "grad_norm": 5.772145748138428,
      "learning_rate": 1.395773076923077e-05,
      "loss": 3.1565,
      "step": 937100
    },
    {
      "epoch": 200.9433962264151,
      "grad_norm": 6.260616779327393,
      "learning_rate": 1.3953884615384617e-05,
      "loss": 3.1181,
      "step": 937200
    },
    {
      "epoch": 200.9648370497427,
      "grad_norm": 5.638772487640381,
      "learning_rate": 1.3950038461538462e-05,
      "loss": 3.1248,
      "step": 937300
    },
    {
      "epoch": 200.98627787307032,
      "grad_norm": 5.641324520111084,
      "learning_rate": 1.3946192307692308e-05,
      "loss": 3.1115,
      "step": 937400
    },
    {
      "epoch": 201.00771869639794,
      "grad_norm": 6.595199108123779,
      "learning_rate": 1.3942346153846157e-05,
      "loss": 3.1408,
      "step": 937500
    },
    {
      "epoch": 201.02915951972557,
      "grad_norm": 5.800475597381592,
      "learning_rate": 1.3938500000000002e-05,
      "loss": 3.0945,
      "step": 937600
    },
    {
      "epoch": 201.05060034305316,
      "grad_norm": 5.90023946762085,
      "learning_rate": 1.3934653846153848e-05,
      "loss": 3.0853,
      "step": 937700
    },
    {
      "epoch": 201.07204116638079,
      "grad_norm": 5.701935768127441,
      "learning_rate": 1.3930807692307693e-05,
      "loss": 3.122,
      "step": 937800
    },
    {
      "epoch": 201.0934819897084,
      "grad_norm": 6.189184665679932,
      "learning_rate": 1.3926961538461539e-05,
      "loss": 3.1334,
      "step": 937900
    },
    {
      "epoch": 201.11492281303603,
      "grad_norm": 5.714151382446289,
      "learning_rate": 1.3923115384615384e-05,
      "loss": 3.0496,
      "step": 938000
    },
    {
      "epoch": 201.13636363636363,
      "grad_norm": 6.54824686050415,
      "learning_rate": 1.391926923076923e-05,
      "loss": 3.1444,
      "step": 938100
    },
    {
      "epoch": 201.15780445969125,
      "grad_norm": 5.375847339630127,
      "learning_rate": 1.3915423076923079e-05,
      "loss": 3.0604,
      "step": 938200
    },
    {
      "epoch": 201.17924528301887,
      "grad_norm": 6.544764041900635,
      "learning_rate": 1.3911576923076924e-05,
      "loss": 3.176,
      "step": 938300
    },
    {
      "epoch": 201.2006861063465,
      "grad_norm": 6.106045246124268,
      "learning_rate": 1.390773076923077e-05,
      "loss": 3.1279,
      "step": 938400
    },
    {
      "epoch": 201.2221269296741,
      "grad_norm": 5.57952356338501,
      "learning_rate": 1.3903884615384615e-05,
      "loss": 3.0926,
      "step": 938500
    },
    {
      "epoch": 201.2435677530017,
      "grad_norm": 5.921696662902832,
      "learning_rate": 1.3900038461538461e-05,
      "loss": 3.161,
      "step": 938600
    },
    {
      "epoch": 201.26500857632934,
      "grad_norm": 5.425915241241455,
      "learning_rate": 1.3896192307692306e-05,
      "loss": 3.1483,
      "step": 938700
    },
    {
      "epoch": 201.28644939965696,
      "grad_norm": 5.744997978210449,
      "learning_rate": 1.3892346153846155e-05,
      "loss": 3.106,
      "step": 938800
    },
    {
      "epoch": 201.30789022298455,
      "grad_norm": 6.009346961975098,
      "learning_rate": 1.3888500000000001e-05,
      "loss": 3.1537,
      "step": 938900
    },
    {
      "epoch": 201.32933104631218,
      "grad_norm": 6.001039505004883,
      "learning_rate": 1.3884653846153846e-05,
      "loss": 3.1012,
      "step": 939000
    },
    {
      "epoch": 201.3507718696398,
      "grad_norm": 6.059545516967773,
      "learning_rate": 1.3880807692307694e-05,
      "loss": 3.1561,
      "step": 939100
    },
    {
      "epoch": 201.37221269296742,
      "grad_norm": 6.819624900817871,
      "learning_rate": 1.387696153846154e-05,
      "loss": 3.1328,
      "step": 939200
    },
    {
      "epoch": 201.39365351629502,
      "grad_norm": 6.005161762237549,
      "learning_rate": 1.3873115384615385e-05,
      "loss": 3.1609,
      "step": 939300
    },
    {
      "epoch": 201.41509433962264,
      "grad_norm": 6.255731105804443,
      "learning_rate": 1.386926923076923e-05,
      "loss": 3.1774,
      "step": 939400
    },
    {
      "epoch": 201.43653516295026,
      "grad_norm": 5.643644332885742,
      "learning_rate": 1.386542307692308e-05,
      "loss": 3.1453,
      "step": 939500
    },
    {
      "epoch": 201.4579759862779,
      "grad_norm": 5.5739264488220215,
      "learning_rate": 1.3861576923076925e-05,
      "loss": 3.1421,
      "step": 939600
    },
    {
      "epoch": 201.47941680960548,
      "grad_norm": 5.561098098754883,
      "learning_rate": 1.385773076923077e-05,
      "loss": 3.2039,
      "step": 939700
    },
    {
      "epoch": 201.5008576329331,
      "grad_norm": 6.043164253234863,
      "learning_rate": 1.3853884615384616e-05,
      "loss": 3.1481,
      "step": 939800
    },
    {
      "epoch": 201.52229845626073,
      "grad_norm": 6.161447048187256,
      "learning_rate": 1.3850038461538461e-05,
      "loss": 3.145,
      "step": 939900
    },
    {
      "epoch": 201.54373927958832,
      "grad_norm": 6.300086975097656,
      "learning_rate": 1.3846192307692307e-05,
      "loss": 3.0921,
      "step": 940000
    },
    {
      "epoch": 201.56518010291595,
      "grad_norm": 5.4570441246032715,
      "learning_rate": 1.3842346153846156e-05,
      "loss": 3.145,
      "step": 940100
    },
    {
      "epoch": 201.58662092624357,
      "grad_norm": 6.227753162384033,
      "learning_rate": 1.3838500000000001e-05,
      "loss": 3.1598,
      "step": 940200
    },
    {
      "epoch": 201.6080617495712,
      "grad_norm": 6.338165283203125,
      "learning_rate": 1.3834653846153847e-05,
      "loss": 3.1457,
      "step": 940300
    },
    {
      "epoch": 201.6295025728988,
      "grad_norm": 5.6698479652404785,
      "learning_rate": 1.3830807692307692e-05,
      "loss": 3.1426,
      "step": 940400
    },
    {
      "epoch": 201.6509433962264,
      "grad_norm": 5.8611931800842285,
      "learning_rate": 1.3826961538461538e-05,
      "loss": 3.1482,
      "step": 940500
    },
    {
      "epoch": 201.67238421955403,
      "grad_norm": 6.769223213195801,
      "learning_rate": 1.3823115384615384e-05,
      "loss": 3.1638,
      "step": 940600
    },
    {
      "epoch": 201.69382504288166,
      "grad_norm": 5.914794445037842,
      "learning_rate": 1.381926923076923e-05,
      "loss": 3.1581,
      "step": 940700
    },
    {
      "epoch": 201.71526586620925,
      "grad_norm": 5.870627403259277,
      "learning_rate": 1.3815423076923078e-05,
      "loss": 3.0545,
      "step": 940800
    },
    {
      "epoch": 201.73670668953687,
      "grad_norm": 5.613034248352051,
      "learning_rate": 1.3811576923076924e-05,
      "loss": 3.1125,
      "step": 940900
    },
    {
      "epoch": 201.7581475128645,
      "grad_norm": 6.292640686035156,
      "learning_rate": 1.380773076923077e-05,
      "loss": 3.0617,
      "step": 941000
    },
    {
      "epoch": 201.77958833619212,
      "grad_norm": 5.96782922744751,
      "learning_rate": 1.3803884615384616e-05,
      "loss": 3.126,
      "step": 941100
    },
    {
      "epoch": 201.80102915951971,
      "grad_norm": 5.896081447601318,
      "learning_rate": 1.3800038461538462e-05,
      "loss": 3.1139,
      "step": 941200
    },
    {
      "epoch": 201.82246998284734,
      "grad_norm": 5.5009331703186035,
      "learning_rate": 1.3796192307692307e-05,
      "loss": 3.1085,
      "step": 941300
    },
    {
      "epoch": 201.84391080617496,
      "grad_norm": 6.176970958709717,
      "learning_rate": 1.3792346153846156e-05,
      "loss": 3.1215,
      "step": 941400
    },
    {
      "epoch": 201.86535162950258,
      "grad_norm": 5.916934490203857,
      "learning_rate": 1.3788500000000002e-05,
      "loss": 3.1587,
      "step": 941500
    },
    {
      "epoch": 201.88679245283018,
      "grad_norm": 5.290903568267822,
      "learning_rate": 1.3784653846153847e-05,
      "loss": 3.0792,
      "step": 941600
    },
    {
      "epoch": 201.9082332761578,
      "grad_norm": 6.493041038513184,
      "learning_rate": 1.3780807692307693e-05,
      "loss": 3.1126,
      "step": 941700
    },
    {
      "epoch": 201.92967409948542,
      "grad_norm": 5.738853931427002,
      "learning_rate": 1.3776961538461539e-05,
      "loss": 3.1066,
      "step": 941800
    },
    {
      "epoch": 201.95111492281305,
      "grad_norm": 6.593777656555176,
      "learning_rate": 1.3773115384615384e-05,
      "loss": 3.1026,
      "step": 941900
    },
    {
      "epoch": 201.97255574614064,
      "grad_norm": 5.6914472579956055,
      "learning_rate": 1.3769269230769233e-05,
      "loss": 3.1314,
      "step": 942000
    },
    {
      "epoch": 201.99399656946827,
      "grad_norm": 5.845846176147461,
      "learning_rate": 1.3765423076923079e-05,
      "loss": 3.1207,
      "step": 942100
    },
    {
      "epoch": 202.0154373927959,
      "grad_norm": 5.84351921081543,
      "learning_rate": 1.3761576923076924e-05,
      "loss": 3.1306,
      "step": 942200
    },
    {
      "epoch": 202.0368782161235,
      "grad_norm": 6.318748950958252,
      "learning_rate": 1.375773076923077e-05,
      "loss": 3.1072,
      "step": 942300
    },
    {
      "epoch": 202.0583190394511,
      "grad_norm": 6.557027339935303,
      "learning_rate": 1.3753884615384615e-05,
      "loss": 3.1187,
      "step": 942400
    },
    {
      "epoch": 202.07975986277873,
      "grad_norm": 6.690606117248535,
      "learning_rate": 1.375003846153846e-05,
      "loss": 3.1085,
      "step": 942500
    },
    {
      "epoch": 202.10120068610635,
      "grad_norm": 6.104161739349365,
      "learning_rate": 1.3746192307692308e-05,
      "loss": 3.0856,
      "step": 942600
    },
    {
      "epoch": 202.12264150943398,
      "grad_norm": 6.123163223266602,
      "learning_rate": 1.3742346153846155e-05,
      "loss": 3.1147,
      "step": 942700
    },
    {
      "epoch": 202.14408233276157,
      "grad_norm": 5.863657474517822,
      "learning_rate": 1.37385e-05,
      "loss": 3.0884,
      "step": 942800
    },
    {
      "epoch": 202.1655231560892,
      "grad_norm": 5.991958141326904,
      "learning_rate": 1.3734653846153848e-05,
      "loss": 3.1164,
      "step": 942900
    },
    {
      "epoch": 202.18696397941682,
      "grad_norm": 5.704007625579834,
      "learning_rate": 1.3730807692307693e-05,
      "loss": 3.1002,
      "step": 943000
    },
    {
      "epoch": 202.2084048027444,
      "grad_norm": 6.516507148742676,
      "learning_rate": 1.3726961538461539e-05,
      "loss": 3.0928,
      "step": 943100
    },
    {
      "epoch": 202.22984562607203,
      "grad_norm": 6.140811920166016,
      "learning_rate": 1.3723115384615385e-05,
      "loss": 3.1433,
      "step": 943200
    },
    {
      "epoch": 202.25128644939966,
      "grad_norm": 6.179141998291016,
      "learning_rate": 1.3719269230769233e-05,
      "loss": 3.08,
      "step": 943300
    },
    {
      "epoch": 202.27272727272728,
      "grad_norm": 6.063859462738037,
      "learning_rate": 1.3715423076923079e-05,
      "loss": 3.0928,
      "step": 943400
    },
    {
      "epoch": 202.29416809605488,
      "grad_norm": 5.939220905303955,
      "learning_rate": 1.3711576923076925e-05,
      "loss": 3.1341,
      "step": 943500
    },
    {
      "epoch": 202.3156089193825,
      "grad_norm": 6.295485973358154,
      "learning_rate": 1.370773076923077e-05,
      "loss": 3.1325,
      "step": 943600
    },
    {
      "epoch": 202.33704974271012,
      "grad_norm": 5.950259208679199,
      "learning_rate": 1.3703884615384616e-05,
      "loss": 3.1012,
      "step": 943700
    },
    {
      "epoch": 202.35849056603774,
      "grad_norm": 5.329981803894043,
      "learning_rate": 1.3700038461538461e-05,
      "loss": 3.1502,
      "step": 943800
    },
    {
      "epoch": 202.37993138936534,
      "grad_norm": 6.169680595397949,
      "learning_rate": 1.3696192307692307e-05,
      "loss": 3.1166,
      "step": 943900
    },
    {
      "epoch": 202.40137221269296,
      "grad_norm": 6.239531517028809,
      "learning_rate": 1.3692346153846156e-05,
      "loss": 3.0775,
      "step": 944000
    },
    {
      "epoch": 202.42281303602059,
      "grad_norm": 5.918970108032227,
      "learning_rate": 1.3688500000000001e-05,
      "loss": 3.0866,
      "step": 944100
    },
    {
      "epoch": 202.4442538593482,
      "grad_norm": 6.003381252288818,
      "learning_rate": 1.3684653846153847e-05,
      "loss": 3.143,
      "step": 944200
    },
    {
      "epoch": 202.4656946826758,
      "grad_norm": 6.1370768547058105,
      "learning_rate": 1.3680807692307692e-05,
      "loss": 3.162,
      "step": 944300
    },
    {
      "epoch": 202.48713550600343,
      "grad_norm": 5.885117530822754,
      "learning_rate": 1.3676961538461538e-05,
      "loss": 3.1431,
      "step": 944400
    },
    {
      "epoch": 202.50857632933105,
      "grad_norm": 6.390659809112549,
      "learning_rate": 1.3673115384615385e-05,
      "loss": 3.1318,
      "step": 944500
    },
    {
      "epoch": 202.53001715265867,
      "grad_norm": 6.319893836975098,
      "learning_rate": 1.3669269230769232e-05,
      "loss": 3.1693,
      "step": 944600
    },
    {
      "epoch": 202.55145797598627,
      "grad_norm": 6.067210674285889,
      "learning_rate": 1.3665423076923078e-05,
      "loss": 3.1202,
      "step": 944700
    },
    {
      "epoch": 202.5728987993139,
      "grad_norm": 6.149054527282715,
      "learning_rate": 1.3661576923076923e-05,
      "loss": 3.1648,
      "step": 944800
    },
    {
      "epoch": 202.5943396226415,
      "grad_norm": 6.170980453491211,
      "learning_rate": 1.365773076923077e-05,
      "loss": 3.1246,
      "step": 944900
    },
    {
      "epoch": 202.61578044596914,
      "grad_norm": 6.001840114593506,
      "learning_rate": 1.3653884615384616e-05,
      "loss": 3.1321,
      "step": 945000
    },
    {
      "epoch": 202.63722126929673,
      "grad_norm": 5.807462215423584,
      "learning_rate": 1.3650038461538462e-05,
      "loss": 3.0952,
      "step": 945100
    },
    {
      "epoch": 202.65866209262435,
      "grad_norm": 6.4173994064331055,
      "learning_rate": 1.3646192307692307e-05,
      "loss": 3.0648,
      "step": 945200
    },
    {
      "epoch": 202.68010291595198,
      "grad_norm": 5.979167461395264,
      "learning_rate": 1.3642346153846156e-05,
      "loss": 3.124,
      "step": 945300
    },
    {
      "epoch": 202.7015437392796,
      "grad_norm": 5.9060869216918945,
      "learning_rate": 1.3638500000000002e-05,
      "loss": 3.1092,
      "step": 945400
    },
    {
      "epoch": 202.7229845626072,
      "grad_norm": 6.046382427215576,
      "learning_rate": 1.3634653846153847e-05,
      "loss": 3.1687,
      "step": 945500
    },
    {
      "epoch": 202.74442538593482,
      "grad_norm": 6.085935115814209,
      "learning_rate": 1.3630807692307693e-05,
      "loss": 3.0919,
      "step": 945600
    },
    {
      "epoch": 202.76586620926244,
      "grad_norm": 5.977685451507568,
      "learning_rate": 1.3626961538461538e-05,
      "loss": 3.1402,
      "step": 945700
    },
    {
      "epoch": 202.78730703259006,
      "grad_norm": 6.224188327789307,
      "learning_rate": 1.3623115384615384e-05,
      "loss": 3.1017,
      "step": 945800
    },
    {
      "epoch": 202.80874785591766,
      "grad_norm": 5.993537425994873,
      "learning_rate": 1.3619269230769233e-05,
      "loss": 3.1706,
      "step": 945900
    },
    {
      "epoch": 202.83018867924528,
      "grad_norm": 6.438559055328369,
      "learning_rate": 1.3615423076923078e-05,
      "loss": 3.1603,
      "step": 946000
    },
    {
      "epoch": 202.8516295025729,
      "grad_norm": 6.393631458282471,
      "learning_rate": 1.3611576923076924e-05,
      "loss": 3.1273,
      "step": 946100
    },
    {
      "epoch": 202.87307032590053,
      "grad_norm": 5.474118709564209,
      "learning_rate": 1.360773076923077e-05,
      "loss": 3.1331,
      "step": 946200
    },
    {
      "epoch": 202.89451114922812,
      "grad_norm": 6.306342601776123,
      "learning_rate": 1.3603884615384615e-05,
      "loss": 3.1276,
      "step": 946300
    },
    {
      "epoch": 202.91595197255575,
      "grad_norm": 6.1419782638549805,
      "learning_rate": 1.3600038461538462e-05,
      "loss": 3.0971,
      "step": 946400
    },
    {
      "epoch": 202.93739279588337,
      "grad_norm": 5.670628547668457,
      "learning_rate": 1.3596192307692308e-05,
      "loss": 3.1628,
      "step": 946500
    },
    {
      "epoch": 202.95883361921096,
      "grad_norm": 6.068490982055664,
      "learning_rate": 1.3592346153846155e-05,
      "loss": 3.1294,
      "step": 946600
    },
    {
      "epoch": 202.9802744425386,
      "grad_norm": 5.502439022064209,
      "learning_rate": 1.35885e-05,
      "loss": 3.1474,
      "step": 946700
    },
    {
      "epoch": 203.0017152658662,
      "grad_norm": 6.33044958114624,
      "learning_rate": 1.3584653846153848e-05,
      "loss": 3.0608,
      "step": 946800
    },
    {
      "epoch": 203.02315608919383,
      "grad_norm": 6.128366947174072,
      "learning_rate": 1.3580807692307693e-05,
      "loss": 3.1343,
      "step": 946900
    },
    {
      "epoch": 203.04459691252143,
      "grad_norm": 6.093944072723389,
      "learning_rate": 1.3576961538461539e-05,
      "loss": 3.114,
      "step": 947000
    },
    {
      "epoch": 203.06603773584905,
      "grad_norm": 6.022919654846191,
      "learning_rate": 1.3573115384615384e-05,
      "loss": 3.1196,
      "step": 947100
    },
    {
      "epoch": 203.08747855917667,
      "grad_norm": 5.748508453369141,
      "learning_rate": 1.3569269230769233e-05,
      "loss": 3.1538,
      "step": 947200
    },
    {
      "epoch": 203.1089193825043,
      "grad_norm": 5.89538049697876,
      "learning_rate": 1.3565423076923079e-05,
      "loss": 3.0723,
      "step": 947300
    },
    {
      "epoch": 203.1303602058319,
      "grad_norm": 5.982250213623047,
      "learning_rate": 1.3561576923076924e-05,
      "loss": 3.0765,
      "step": 947400
    },
    {
      "epoch": 203.15180102915951,
      "grad_norm": 6.30890417098999,
      "learning_rate": 1.355773076923077e-05,
      "loss": 3.1146,
      "step": 947500
    },
    {
      "epoch": 203.17324185248714,
      "grad_norm": 6.662854194641113,
      "learning_rate": 1.3553884615384615e-05,
      "loss": 3.0934,
      "step": 947600
    },
    {
      "epoch": 203.19468267581476,
      "grad_norm": 6.034578800201416,
      "learning_rate": 1.3550038461538461e-05,
      "loss": 3.0586,
      "step": 947700
    },
    {
      "epoch": 203.21612349914236,
      "grad_norm": 5.772982120513916,
      "learning_rate": 1.3546192307692306e-05,
      "loss": 3.0713,
      "step": 947800
    },
    {
      "epoch": 203.23756432246998,
      "grad_norm": 6.005324840545654,
      "learning_rate": 1.3542346153846155e-05,
      "loss": 3.1561,
      "step": 947900
    },
    {
      "epoch": 203.2590051457976,
      "grad_norm": 6.296672344207764,
      "learning_rate": 1.3538500000000001e-05,
      "loss": 3.1218,
      "step": 948000
    },
    {
      "epoch": 203.28044596912522,
      "grad_norm": 5.989146709442139,
      "learning_rate": 1.3534653846153846e-05,
      "loss": 3.1092,
      "step": 948100
    },
    {
      "epoch": 203.30188679245282,
      "grad_norm": 6.2280073165893555,
      "learning_rate": 1.3530807692307692e-05,
      "loss": 3.1247,
      "step": 948200
    },
    {
      "epoch": 203.32332761578044,
      "grad_norm": 5.9745659828186035,
      "learning_rate": 1.3526961538461538e-05,
      "loss": 3.0995,
      "step": 948300
    },
    {
      "epoch": 203.34476843910807,
      "grad_norm": 6.117207050323486,
      "learning_rate": 1.3523115384615385e-05,
      "loss": 3.0942,
      "step": 948400
    },
    {
      "epoch": 203.3662092624357,
      "grad_norm": 6.153316497802734,
      "learning_rate": 1.3519269230769232e-05,
      "loss": 3.1359,
      "step": 948500
    },
    {
      "epoch": 203.38765008576328,
      "grad_norm": 5.841828346252441,
      "learning_rate": 1.3515423076923078e-05,
      "loss": 3.1743,
      "step": 948600
    },
    {
      "epoch": 203.4090909090909,
      "grad_norm": 5.777772426605225,
      "learning_rate": 1.3511576923076925e-05,
      "loss": 3.0506,
      "step": 948700
    },
    {
      "epoch": 203.43053173241853,
      "grad_norm": 5.53434419631958,
      "learning_rate": 1.350773076923077e-05,
      "loss": 3.1678,
      "step": 948800
    },
    {
      "epoch": 203.45197255574615,
      "grad_norm": 6.974806308746338,
      "learning_rate": 1.3503884615384616e-05,
      "loss": 3.1503,
      "step": 948900
    },
    {
      "epoch": 203.47341337907375,
      "grad_norm": 5.9344024658203125,
      "learning_rate": 1.3500038461538461e-05,
      "loss": 3.1188,
      "step": 949000
    },
    {
      "epoch": 203.49485420240137,
      "grad_norm": 6.5376877784729,
      "learning_rate": 1.349619230769231e-05,
      "loss": 3.0731,
      "step": 949100
    },
    {
      "epoch": 203.516295025729,
      "grad_norm": 6.312907695770264,
      "learning_rate": 1.3492346153846156e-05,
      "loss": 3.0734,
      "step": 949200
    },
    {
      "epoch": 203.53773584905662,
      "grad_norm": 6.584658622741699,
      "learning_rate": 1.3488500000000001e-05,
      "loss": 3.1012,
      "step": 949300
    },
    {
      "epoch": 203.5591766723842,
      "grad_norm": 5.926616668701172,
      "learning_rate": 1.3484653846153847e-05,
      "loss": 3.1447,
      "step": 949400
    },
    {
      "epoch": 203.58061749571183,
      "grad_norm": 5.682806968688965,
      "learning_rate": 1.3480807692307692e-05,
      "loss": 3.0737,
      "step": 949500
    },
    {
      "epoch": 203.60205831903946,
      "grad_norm": 6.030328273773193,
      "learning_rate": 1.3476961538461538e-05,
      "loss": 3.1195,
      "step": 949600
    },
    {
      "epoch": 203.62349914236708,
      "grad_norm": 5.674302577972412,
      "learning_rate": 1.3473115384615384e-05,
      "loss": 3.1347,
      "step": 949700
    },
    {
      "epoch": 203.64493996569468,
      "grad_norm": 6.454793453216553,
      "learning_rate": 1.3469269230769232e-05,
      "loss": 3.1051,
      "step": 949800
    },
    {
      "epoch": 203.6663807890223,
      "grad_norm": 5.980068683624268,
      "learning_rate": 1.3465423076923078e-05,
      "loss": 3.1049,
      "step": 949900
    },
    {
      "epoch": 203.68782161234992,
      "grad_norm": 5.767748832702637,
      "learning_rate": 1.3461576923076924e-05,
      "loss": 3.141,
      "step": 950000
    },
    {
      "epoch": 203.70926243567752,
      "grad_norm": 6.230434894561768,
      "learning_rate": 1.3457730769230769e-05,
      "loss": 3.1206,
      "step": 950100
    },
    {
      "epoch": 203.73070325900514,
      "grad_norm": 5.995338439941406,
      "learning_rate": 1.3453884615384615e-05,
      "loss": 3.0812,
      "step": 950200
    },
    {
      "epoch": 203.75214408233276,
      "grad_norm": 6.620663166046143,
      "learning_rate": 1.3450038461538462e-05,
      "loss": 3.1479,
      "step": 950300
    },
    {
      "epoch": 203.77358490566039,
      "grad_norm": 5.912876605987549,
      "learning_rate": 1.3446192307692309e-05,
      "loss": 3.1389,
      "step": 950400
    },
    {
      "epoch": 203.79502572898798,
      "grad_norm": 6.0867018699646,
      "learning_rate": 1.3442346153846155e-05,
      "loss": 3.124,
      "step": 950500
    },
    {
      "epoch": 203.8164665523156,
      "grad_norm": 6.0135321617126465,
      "learning_rate": 1.3438500000000002e-05,
      "loss": 3.0981,
      "step": 950600
    },
    {
      "epoch": 203.83790737564323,
      "grad_norm": 5.994227409362793,
      "learning_rate": 1.3434653846153847e-05,
      "loss": 3.1526,
      "step": 950700
    },
    {
      "epoch": 203.85934819897085,
      "grad_norm": 5.896414279937744,
      "learning_rate": 1.3430807692307693e-05,
      "loss": 3.0953,
      "step": 950800
    },
    {
      "epoch": 203.88078902229844,
      "grad_norm": 5.801764011383057,
      "learning_rate": 1.3426961538461538e-05,
      "loss": 3.1549,
      "step": 950900
    },
    {
      "epoch": 203.90222984562607,
      "grad_norm": 6.42216682434082,
      "learning_rate": 1.3423115384615384e-05,
      "loss": 3.0908,
      "step": 951000
    },
    {
      "epoch": 203.9236706689537,
      "grad_norm": 6.423142433166504,
      "learning_rate": 1.3419269230769233e-05,
      "loss": 3.1372,
      "step": 951100
    },
    {
      "epoch": 203.9451114922813,
      "grad_norm": 7.100513458251953,
      "learning_rate": 1.3415423076923078e-05,
      "loss": 3.104,
      "step": 951200
    },
    {
      "epoch": 203.9665523156089,
      "grad_norm": 5.998702049255371,
      "learning_rate": 1.3411576923076924e-05,
      "loss": 3.1762,
      "step": 951300
    },
    {
      "epoch": 203.98799313893653,
      "grad_norm": 5.936634063720703,
      "learning_rate": 1.340773076923077e-05,
      "loss": 3.1158,
      "step": 951400
    },
    {
      "epoch": 204.00943396226415,
      "grad_norm": 5.924116134643555,
      "learning_rate": 1.3403884615384615e-05,
      "loss": 3.128,
      "step": 951500
    },
    {
      "epoch": 204.03087478559178,
      "grad_norm": 6.824796676635742,
      "learning_rate": 1.340003846153846e-05,
      "loss": 3.0957,
      "step": 951600
    },
    {
      "epoch": 204.05231560891937,
      "grad_norm": 6.025521755218506,
      "learning_rate": 1.339619230769231e-05,
      "loss": 3.1087,
      "step": 951700
    },
    {
      "epoch": 204.073756432247,
      "grad_norm": 5.932432651519775,
      "learning_rate": 1.3392346153846155e-05,
      "loss": 3.1275,
      "step": 951800
    },
    {
      "epoch": 204.09519725557462,
      "grad_norm": 7.0318684577941895,
      "learning_rate": 1.33885e-05,
      "loss": 3.1547,
      "step": 951900
    },
    {
      "epoch": 204.11663807890224,
      "grad_norm": 6.5607805252075195,
      "learning_rate": 1.3384653846153846e-05,
      "loss": 3.083,
      "step": 952000
    },
    {
      "epoch": 204.13807890222984,
      "grad_norm": 6.595101356506348,
      "learning_rate": 1.3380807692307692e-05,
      "loss": 3.0676,
      "step": 952100
    },
    {
      "epoch": 204.15951972555746,
      "grad_norm": 5.905858039855957,
      "learning_rate": 1.3376961538461539e-05,
      "loss": 3.0625,
      "step": 952200
    },
    {
      "epoch": 204.18096054888508,
      "grad_norm": 6.244011878967285,
      "learning_rate": 1.3373115384615384e-05,
      "loss": 3.0907,
      "step": 952300
    },
    {
      "epoch": 204.2024013722127,
      "grad_norm": 6.2703375816345215,
      "learning_rate": 1.3369269230769232e-05,
      "loss": 3.1334,
      "step": 952400
    },
    {
      "epoch": 204.2238421955403,
      "grad_norm": 5.695896625518799,
      "learning_rate": 1.3365423076923079e-05,
      "loss": 3.0743,
      "step": 952500
    },
    {
      "epoch": 204.24528301886792,
      "grad_norm": 5.9847493171691895,
      "learning_rate": 1.3361576923076925e-05,
      "loss": 3.1074,
      "step": 952600
    },
    {
      "epoch": 204.26672384219555,
      "grad_norm": 6.0919318199157715,
      "learning_rate": 1.335773076923077e-05,
      "loss": 3.0638,
      "step": 952700
    },
    {
      "epoch": 204.28816466552317,
      "grad_norm": 6.269159317016602,
      "learning_rate": 1.3353884615384616e-05,
      "loss": 3.0937,
      "step": 952800
    },
    {
      "epoch": 204.30960548885076,
      "grad_norm": 6.368541240692139,
      "learning_rate": 1.3350038461538461e-05,
      "loss": 3.0724,
      "step": 952900
    },
    {
      "epoch": 204.3310463121784,
      "grad_norm": 6.1544294357299805,
      "learning_rate": 1.334619230769231e-05,
      "loss": 3.1417,
      "step": 953000
    },
    {
      "epoch": 204.352487135506,
      "grad_norm": 5.477907180786133,
      "learning_rate": 1.3342346153846156e-05,
      "loss": 3.1362,
      "step": 953100
    },
    {
      "epoch": 204.37392795883363,
      "grad_norm": 5.661945343017578,
      "learning_rate": 1.3338500000000001e-05,
      "loss": 3.1384,
      "step": 953200
    },
    {
      "epoch": 204.39536878216123,
      "grad_norm": 5.404852867126465,
      "learning_rate": 1.3334653846153847e-05,
      "loss": 3.0902,
      "step": 953300
    },
    {
      "epoch": 204.41680960548885,
      "grad_norm": 5.67797327041626,
      "learning_rate": 1.3330807692307692e-05,
      "loss": 3.0493,
      "step": 953400
    },
    {
      "epoch": 204.43825042881647,
      "grad_norm": 6.073130130767822,
      "learning_rate": 1.3326961538461538e-05,
      "loss": 3.0955,
      "step": 953500
    },
    {
      "epoch": 204.45969125214407,
      "grad_norm": 5.917500972747803,
      "learning_rate": 1.3323115384615383e-05,
      "loss": 3.1336,
      "step": 953600
    },
    {
      "epoch": 204.4811320754717,
      "grad_norm": 6.4082183837890625,
      "learning_rate": 1.3319269230769232e-05,
      "loss": 3.1418,
      "step": 953700
    },
    {
      "epoch": 204.50257289879931,
      "grad_norm": 5.802816390991211,
      "learning_rate": 1.3315423076923078e-05,
      "loss": 3.0808,
      "step": 953800
    },
    {
      "epoch": 204.52401372212694,
      "grad_norm": 7.086203575134277,
      "learning_rate": 1.3311576923076923e-05,
      "loss": 3.0733,
      "step": 953900
    },
    {
      "epoch": 204.54545454545453,
      "grad_norm": 6.170534133911133,
      "learning_rate": 1.3307730769230769e-05,
      "loss": 3.0982,
      "step": 954000
    },
    {
      "epoch": 204.56689536878216,
      "grad_norm": 5.697969436645508,
      "learning_rate": 1.3303884615384616e-05,
      "loss": 3.147,
      "step": 954100
    },
    {
      "epoch": 204.58833619210978,
      "grad_norm": 5.959427833557129,
      "learning_rate": 1.3300038461538462e-05,
      "loss": 3.1167,
      "step": 954200
    },
    {
      "epoch": 204.6097770154374,
      "grad_norm": 6.673192977905273,
      "learning_rate": 1.3296192307692309e-05,
      "loss": 3.0979,
      "step": 954300
    },
    {
      "epoch": 204.631217838765,
      "grad_norm": 5.761841773986816,
      "learning_rate": 1.3292346153846156e-05,
      "loss": 3.0681,
      "step": 954400
    },
    {
      "epoch": 204.65265866209262,
      "grad_norm": 6.075597763061523,
      "learning_rate": 1.3288500000000002e-05,
      "loss": 3.0597,
      "step": 954500
    },
    {
      "epoch": 204.67409948542024,
      "grad_norm": 6.299708366394043,
      "learning_rate": 1.3284653846153847e-05,
      "loss": 3.1309,
      "step": 954600
    },
    {
      "epoch": 204.69554030874787,
      "grad_norm": 6.169573783874512,
      "learning_rate": 1.3280807692307693e-05,
      "loss": 3.1314,
      "step": 954700
    },
    {
      "epoch": 204.71698113207546,
      "grad_norm": 6.088251113891602,
      "learning_rate": 1.3276961538461538e-05,
      "loss": 3.1245,
      "step": 954800
    },
    {
      "epoch": 204.73842195540308,
      "grad_norm": 5.665140151977539,
      "learning_rate": 1.3273115384615387e-05,
      "loss": 3.1184,
      "step": 954900
    },
    {
      "epoch": 204.7598627787307,
      "grad_norm": 6.349010467529297,
      "learning_rate": 1.3269269230769233e-05,
      "loss": 3.1151,
      "step": 955000
    },
    {
      "epoch": 204.78130360205833,
      "grad_norm": 6.749194145202637,
      "learning_rate": 1.3265423076923078e-05,
      "loss": 3.1067,
      "step": 955100
    },
    {
      "epoch": 204.80274442538592,
      "grad_norm": 5.655755043029785,
      "learning_rate": 1.3261576923076924e-05,
      "loss": 3.1379,
      "step": 955200
    },
    {
      "epoch": 204.82418524871355,
      "grad_norm": 6.223911762237549,
      "learning_rate": 1.325773076923077e-05,
      "loss": 3.1456,
      "step": 955300
    },
    {
      "epoch": 204.84562607204117,
      "grad_norm": 5.509035110473633,
      "learning_rate": 1.3253884615384615e-05,
      "loss": 3.0622,
      "step": 955400
    },
    {
      "epoch": 204.8670668953688,
      "grad_norm": 6.831982612609863,
      "learning_rate": 1.325003846153846e-05,
      "loss": 3.1234,
      "step": 955500
    },
    {
      "epoch": 204.8885077186964,
      "grad_norm": 6.013849258422852,
      "learning_rate": 1.324619230769231e-05,
      "loss": 3.1625,
      "step": 955600
    },
    {
      "epoch": 204.909948542024,
      "grad_norm": 6.053552150726318,
      "learning_rate": 1.3242346153846155e-05,
      "loss": 3.193,
      "step": 955700
    },
    {
      "epoch": 204.93138936535163,
      "grad_norm": 6.1525092124938965,
      "learning_rate": 1.32385e-05,
      "loss": 3.1044,
      "step": 955800
    },
    {
      "epoch": 204.95283018867926,
      "grad_norm": 6.3845014572143555,
      "learning_rate": 1.3234653846153846e-05,
      "loss": 3.1493,
      "step": 955900
    },
    {
      "epoch": 204.97427101200685,
      "grad_norm": 6.1083197593688965,
      "learning_rate": 1.3230807692307693e-05,
      "loss": 3.1372,
      "step": 956000
    },
    {
      "epoch": 204.99571183533448,
      "grad_norm": 6.244259357452393,
      "learning_rate": 1.3226961538461539e-05,
      "loss": 3.1824,
      "step": 956100
    },
    {
      "epoch": 205.0171526586621,
      "grad_norm": 6.114078521728516,
      "learning_rate": 1.3223115384615386e-05,
      "loss": 3.0913,
      "step": 956200
    },
    {
      "epoch": 205.03859348198972,
      "grad_norm": 6.315749168395996,
      "learning_rate": 1.3219269230769231e-05,
      "loss": 3.1111,
      "step": 956300
    },
    {
      "epoch": 205.06003430531732,
      "grad_norm": 5.717809200286865,
      "learning_rate": 1.3215423076923079e-05,
      "loss": 3.134,
      "step": 956400
    },
    {
      "epoch": 205.08147512864494,
      "grad_norm": 6.360565185546875,
      "learning_rate": 1.3211576923076924e-05,
      "loss": 3.0657,
      "step": 956500
    },
    {
      "epoch": 205.10291595197256,
      "grad_norm": 5.89591646194458,
      "learning_rate": 1.320773076923077e-05,
      "loss": 3.1033,
      "step": 956600
    },
    {
      "epoch": 205.12435677530019,
      "grad_norm": 5.881804943084717,
      "learning_rate": 1.3203884615384615e-05,
      "loss": 3.0981,
      "step": 956700
    },
    {
      "epoch": 205.14579759862778,
      "grad_norm": 5.870753765106201,
      "learning_rate": 1.3200038461538461e-05,
      "loss": 3.1202,
      "step": 956800
    },
    {
      "epoch": 205.1672384219554,
      "grad_norm": 5.834028720855713,
      "learning_rate": 1.319619230769231e-05,
      "loss": 3.0782,
      "step": 956900
    },
    {
      "epoch": 205.18867924528303,
      "grad_norm": 7.712250232696533,
      "learning_rate": 1.3192346153846155e-05,
      "loss": 3.1283,
      "step": 957000
    },
    {
      "epoch": 205.21012006861062,
      "grad_norm": 6.384862422943115,
      "learning_rate": 1.3188500000000001e-05,
      "loss": 3.1147,
      "step": 957100
    },
    {
      "epoch": 205.23156089193824,
      "grad_norm": 6.449181079864502,
      "learning_rate": 1.3184653846153846e-05,
      "loss": 3.118,
      "step": 957200
    },
    {
      "epoch": 205.25300171526587,
      "grad_norm": 5.870306015014648,
      "learning_rate": 1.3180807692307692e-05,
      "loss": 3.0691,
      "step": 957300
    },
    {
      "epoch": 205.2744425385935,
      "grad_norm": 6.163967609405518,
      "learning_rate": 1.3176961538461537e-05,
      "loss": 3.1747,
      "step": 957400
    },
    {
      "epoch": 205.29588336192108,
      "grad_norm": 6.202095985412598,
      "learning_rate": 1.3173115384615386e-05,
      "loss": 3.0931,
      "step": 957500
    },
    {
      "epoch": 205.3173241852487,
      "grad_norm": 5.897762775421143,
      "learning_rate": 1.3169269230769232e-05,
      "loss": 3.0792,
      "step": 957600
    },
    {
      "epoch": 205.33876500857633,
      "grad_norm": 7.061646938323975,
      "learning_rate": 1.3165423076923077e-05,
      "loss": 3.1038,
      "step": 957700
    },
    {
      "epoch": 205.36020583190395,
      "grad_norm": 5.974334716796875,
      "learning_rate": 1.3161576923076923e-05,
      "loss": 3.1224,
      "step": 957800
    },
    {
      "epoch": 205.38164665523155,
      "grad_norm": 6.0264153480529785,
      "learning_rate": 1.315773076923077e-05,
      "loss": 3.0484,
      "step": 957900
    },
    {
      "epoch": 205.40308747855917,
      "grad_norm": 5.829052448272705,
      "learning_rate": 1.3153884615384616e-05,
      "loss": 3.1162,
      "step": 958000
    },
    {
      "epoch": 205.4245283018868,
      "grad_norm": 5.968623638153076,
      "learning_rate": 1.3150038461538461e-05,
      "loss": 3.1346,
      "step": 958100
    },
    {
      "epoch": 205.44596912521442,
      "grad_norm": 6.32721471786499,
      "learning_rate": 1.3146192307692309e-05,
      "loss": 3.106,
      "step": 958200
    },
    {
      "epoch": 205.467409948542,
      "grad_norm": 5.927478313446045,
      "learning_rate": 1.3142346153846156e-05,
      "loss": 3.1211,
      "step": 958300
    },
    {
      "epoch": 205.48885077186964,
      "grad_norm": 6.1857218742370605,
      "learning_rate": 1.3138500000000001e-05,
      "loss": 3.059,
      "step": 958400
    },
    {
      "epoch": 205.51029159519726,
      "grad_norm": 6.474048614501953,
      "learning_rate": 1.3134653846153847e-05,
      "loss": 3.0855,
      "step": 958500
    },
    {
      "epoch": 205.53173241852488,
      "grad_norm": 6.193339824676514,
      "learning_rate": 1.3130807692307692e-05,
      "loss": 3.101,
      "step": 958600
    },
    {
      "epoch": 205.55317324185248,
      "grad_norm": 5.962225437164307,
      "learning_rate": 1.3126961538461538e-05,
      "loss": 3.1433,
      "step": 958700
    },
    {
      "epoch": 205.5746140651801,
      "grad_norm": 6.002309322357178,
      "learning_rate": 1.3123115384615387e-05,
      "loss": 3.1082,
      "step": 958800
    },
    {
      "epoch": 205.59605488850772,
      "grad_norm": 6.267081260681152,
      "learning_rate": 1.3119269230769232e-05,
      "loss": 3.0626,
      "step": 958900
    },
    {
      "epoch": 205.61749571183535,
      "grad_norm": 6.096574783325195,
      "learning_rate": 1.3115423076923078e-05,
      "loss": 3.0536,
      "step": 959000
    },
    {
      "epoch": 205.63893653516294,
      "grad_norm": 5.736009120941162,
      "learning_rate": 1.3111576923076924e-05,
      "loss": 3.065,
      "step": 959100
    },
    {
      "epoch": 205.66037735849056,
      "grad_norm": 6.5240302085876465,
      "learning_rate": 1.3107730769230769e-05,
      "loss": 3.1276,
      "step": 959200
    },
    {
      "epoch": 205.6818181818182,
      "grad_norm": 6.714863300323486,
      "learning_rate": 1.3103884615384615e-05,
      "loss": 3.0944,
      "step": 959300
    },
    {
      "epoch": 205.7032590051458,
      "grad_norm": 5.76061487197876,
      "learning_rate": 1.310003846153846e-05,
      "loss": 3.1061,
      "step": 959400
    },
    {
      "epoch": 205.7246998284734,
      "grad_norm": 6.585148334503174,
      "learning_rate": 1.3096192307692309e-05,
      "loss": 3.1443,
      "step": 959500
    },
    {
      "epoch": 205.74614065180103,
      "grad_norm": 5.953412055969238,
      "learning_rate": 1.3092346153846155e-05,
      "loss": 3.0996,
      "step": 959600
    },
    {
      "epoch": 205.76758147512865,
      "grad_norm": 6.059044361114502,
      "learning_rate": 1.30885e-05,
      "loss": 3.1309,
      "step": 959700
    },
    {
      "epoch": 205.78902229845627,
      "grad_norm": 6.247211456298828,
      "learning_rate": 1.3084653846153846e-05,
      "loss": 3.0703,
      "step": 959800
    },
    {
      "epoch": 205.81046312178387,
      "grad_norm": 6.622715473175049,
      "learning_rate": 1.3080807692307693e-05,
      "loss": 3.1054,
      "step": 959900
    },
    {
      "epoch": 205.8319039451115,
      "grad_norm": 5.86025857925415,
      "learning_rate": 1.3076961538461538e-05,
      "loss": 3.1656,
      "step": 960000
    },
    {
      "epoch": 205.85334476843911,
      "grad_norm": 6.325499057769775,
      "learning_rate": 1.3073115384615386e-05,
      "loss": 3.0992,
      "step": 960100
    },
    {
      "epoch": 205.87478559176674,
      "grad_norm": 5.9374470710754395,
      "learning_rate": 1.3069269230769233e-05,
      "loss": 3.1382,
      "step": 960200
    },
    {
      "epoch": 205.89622641509433,
      "grad_norm": 6.293362140655518,
      "learning_rate": 1.3065423076923078e-05,
      "loss": 3.1327,
      "step": 960300
    },
    {
      "epoch": 205.91766723842196,
      "grad_norm": 6.314485549926758,
      "learning_rate": 1.3061576923076924e-05,
      "loss": 3.0963,
      "step": 960400
    },
    {
      "epoch": 205.93910806174958,
      "grad_norm": 5.800870895385742,
      "learning_rate": 1.305773076923077e-05,
      "loss": 3.1332,
      "step": 960500
    },
    {
      "epoch": 205.96054888507717,
      "grad_norm": 5.142507076263428,
      "learning_rate": 1.3053884615384615e-05,
      "loss": 3.0793,
      "step": 960600
    },
    {
      "epoch": 205.9819897084048,
      "grad_norm": 6.462085247039795,
      "learning_rate": 1.3050038461538464e-05,
      "loss": 3.1374,
      "step": 960700
    },
    {
      "epoch": 206.00343053173242,
      "grad_norm": 5.653555870056152,
      "learning_rate": 1.304619230769231e-05,
      "loss": 3.1488,
      "step": 960800
    },
    {
      "epoch": 206.02487135506004,
      "grad_norm": 6.2410688400268555,
      "learning_rate": 1.3042346153846155e-05,
      "loss": 3.1045,
      "step": 960900
    },
    {
      "epoch": 206.04631217838764,
      "grad_norm": 6.232418060302734,
      "learning_rate": 1.30385e-05,
      "loss": 3.054,
      "step": 961000
    },
    {
      "epoch": 206.06775300171526,
      "grad_norm": 6.544890880584717,
      "learning_rate": 1.3034653846153846e-05,
      "loss": 3.1062,
      "step": 961100
    },
    {
      "epoch": 206.08919382504288,
      "grad_norm": 6.364253044128418,
      "learning_rate": 1.3030807692307692e-05,
      "loss": 3.0696,
      "step": 961200
    },
    {
      "epoch": 206.1106346483705,
      "grad_norm": 5.784057140350342,
      "learning_rate": 1.3026961538461537e-05,
      "loss": 3.0788,
      "step": 961300
    },
    {
      "epoch": 206.1320754716981,
      "grad_norm": 5.996083736419678,
      "learning_rate": 1.3023115384615386e-05,
      "loss": 3.103,
      "step": 961400
    },
    {
      "epoch": 206.15351629502572,
      "grad_norm": 6.0756940841674805,
      "learning_rate": 1.3019269230769232e-05,
      "loss": 3.0935,
      "step": 961500
    },
    {
      "epoch": 206.17495711835335,
      "grad_norm": 6.511779308319092,
      "learning_rate": 1.3015423076923077e-05,
      "loss": 3.044,
      "step": 961600
    },
    {
      "epoch": 206.19639794168097,
      "grad_norm": 6.0575690269470215,
      "learning_rate": 1.3011576923076923e-05,
      "loss": 3.175,
      "step": 961700
    },
    {
      "epoch": 206.21783876500857,
      "grad_norm": 5.901292324066162,
      "learning_rate": 1.300773076923077e-05,
      "loss": 3.0961,
      "step": 961800
    },
    {
      "epoch": 206.2392795883362,
      "grad_norm": 6.103648662567139,
      "learning_rate": 1.3003884615384616e-05,
      "loss": 3.0675,
      "step": 961900
    },
    {
      "epoch": 206.2607204116638,
      "grad_norm": 5.976578235626221,
      "learning_rate": 1.3000038461538463e-05,
      "loss": 3.1065,
      "step": 962000
    },
    {
      "epoch": 206.28216123499143,
      "grad_norm": 6.1970295906066895,
      "learning_rate": 1.299619230769231e-05,
      "loss": 3.0739,
      "step": 962100
    },
    {
      "epoch": 206.30360205831903,
      "grad_norm": 6.267029285430908,
      "learning_rate": 1.2992346153846156e-05,
      "loss": 3.1282,
      "step": 962200
    },
    {
      "epoch": 206.32504288164665,
      "grad_norm": 6.100236892700195,
      "learning_rate": 1.2988500000000001e-05,
      "loss": 3.0689,
      "step": 962300
    },
    {
      "epoch": 206.34648370497428,
      "grad_norm": 6.6269121170043945,
      "learning_rate": 1.2984653846153847e-05,
      "loss": 3.0907,
      "step": 962400
    },
    {
      "epoch": 206.3679245283019,
      "grad_norm": 5.652973651885986,
      "learning_rate": 1.2980807692307692e-05,
      "loss": 3.1009,
      "step": 962500
    },
    {
      "epoch": 206.3893653516295,
      "grad_norm": 6.108801364898682,
      "learning_rate": 1.2976961538461538e-05,
      "loss": 3.1042,
      "step": 962600
    },
    {
      "epoch": 206.41080617495712,
      "grad_norm": 5.781947135925293,
      "learning_rate": 1.2973115384615387e-05,
      "loss": 3.0908,
      "step": 962700
    },
    {
      "epoch": 206.43224699828474,
      "grad_norm": 5.921993255615234,
      "learning_rate": 1.2969269230769232e-05,
      "loss": 3.1427,
      "step": 962800
    },
    {
      "epoch": 206.45368782161236,
      "grad_norm": 6.535279273986816,
      "learning_rate": 1.2965423076923078e-05,
      "loss": 3.0827,
      "step": 962900
    },
    {
      "epoch": 206.47512864493996,
      "grad_norm": 6.011748790740967,
      "learning_rate": 1.2961576923076923e-05,
      "loss": 3.1488,
      "step": 963000
    },
    {
      "epoch": 206.49656946826758,
      "grad_norm": 5.732401371002197,
      "learning_rate": 1.2957730769230769e-05,
      "loss": 3.0838,
      "step": 963100
    },
    {
      "epoch": 206.5180102915952,
      "grad_norm": 5.8267412185668945,
      "learning_rate": 1.2953884615384614e-05,
      "loss": 3.1112,
      "step": 963200
    },
    {
      "epoch": 206.53945111492283,
      "grad_norm": 6.077887535095215,
      "learning_rate": 1.2950038461538463e-05,
      "loss": 3.0672,
      "step": 963300
    },
    {
      "epoch": 206.56089193825042,
      "grad_norm": 6.73377799987793,
      "learning_rate": 1.2946192307692309e-05,
      "loss": 3.0385,
      "step": 963400
    },
    {
      "epoch": 206.58233276157804,
      "grad_norm": 6.167273998260498,
      "learning_rate": 1.2942346153846154e-05,
      "loss": 3.0642,
      "step": 963500
    },
    {
      "epoch": 206.60377358490567,
      "grad_norm": 5.996910572052002,
      "learning_rate": 1.29385e-05,
      "loss": 3.0727,
      "step": 963600
    },
    {
      "epoch": 206.62521440823326,
      "grad_norm": 6.611654281616211,
      "learning_rate": 1.2934653846153847e-05,
      "loss": 3.1392,
      "step": 963700
    },
    {
      "epoch": 206.64665523156089,
      "grad_norm": 7.14926290512085,
      "learning_rate": 1.2930807692307693e-05,
      "loss": 3.0771,
      "step": 963800
    },
    {
      "epoch": 206.6680960548885,
      "grad_norm": 6.233090400695801,
      "learning_rate": 1.2926961538461538e-05,
      "loss": 3.1116,
      "step": 963900
    },
    {
      "epoch": 206.68953687821613,
      "grad_norm": 5.885977745056152,
      "learning_rate": 1.2923115384615387e-05,
      "loss": 3.0969,
      "step": 964000
    },
    {
      "epoch": 206.71097770154373,
      "grad_norm": 5.758917808532715,
      "learning_rate": 1.2919269230769233e-05,
      "loss": 3.1563,
      "step": 964100
    },
    {
      "epoch": 206.73241852487135,
      "grad_norm": 6.242980480194092,
      "learning_rate": 1.2915423076923078e-05,
      "loss": 3.1248,
      "step": 964200
    },
    {
      "epoch": 206.75385934819897,
      "grad_norm": 5.772700786590576,
      "learning_rate": 1.2911576923076924e-05,
      "loss": 3.1244,
      "step": 964300
    },
    {
      "epoch": 206.7753001715266,
      "grad_norm": 5.817431449890137,
      "learning_rate": 1.290773076923077e-05,
      "loss": 3.1369,
      "step": 964400
    },
    {
      "epoch": 206.7967409948542,
      "grad_norm": 6.6126627922058105,
      "learning_rate": 1.2903884615384615e-05,
      "loss": 3.0761,
      "step": 964500
    },
    {
      "epoch": 206.8181818181818,
      "grad_norm": 5.915508270263672,
      "learning_rate": 1.2900038461538464e-05,
      "loss": 3.1004,
      "step": 964600
    },
    {
      "epoch": 206.83962264150944,
      "grad_norm": 6.665125370025635,
      "learning_rate": 1.289619230769231e-05,
      "loss": 3.1281,
      "step": 964700
    },
    {
      "epoch": 206.86106346483706,
      "grad_norm": 6.231383323669434,
      "learning_rate": 1.2892346153846155e-05,
      "loss": 3.1207,
      "step": 964800
    },
    {
      "epoch": 206.88250428816465,
      "grad_norm": 5.9917097091674805,
      "learning_rate": 1.28885e-05,
      "loss": 3.1437,
      "step": 964900
    },
    {
      "epoch": 206.90394511149228,
      "grad_norm": 5.803936004638672,
      "learning_rate": 1.2884653846153846e-05,
      "loss": 3.1125,
      "step": 965000
    },
    {
      "epoch": 206.9253859348199,
      "grad_norm": 6.231072425842285,
      "learning_rate": 1.2880807692307691e-05,
      "loss": 3.111,
      "step": 965100
    },
    {
      "epoch": 206.94682675814752,
      "grad_norm": 6.106760501861572,
      "learning_rate": 1.2876961538461537e-05,
      "loss": 3.1232,
      "step": 965200
    },
    {
      "epoch": 206.96826758147512,
      "grad_norm": 5.999234676361084,
      "learning_rate": 1.2873115384615386e-05,
      "loss": 3.1002,
      "step": 965300
    },
    {
      "epoch": 206.98970840480274,
      "grad_norm": 5.9566426277160645,
      "learning_rate": 1.2869269230769231e-05,
      "loss": 3.103,
      "step": 965400
    },
    {
      "epoch": 207.01114922813036,
      "grad_norm": 6.194759368896484,
      "learning_rate": 1.2865423076923077e-05,
      "loss": 3.0968,
      "step": 965500
    },
    {
      "epoch": 207.032590051458,
      "grad_norm": 6.652068614959717,
      "learning_rate": 1.2861576923076924e-05,
      "loss": 3.0649,
      "step": 965600
    },
    {
      "epoch": 207.05403087478558,
      "grad_norm": 5.743891716003418,
      "learning_rate": 1.285773076923077e-05,
      "loss": 3.1375,
      "step": 965700
    },
    {
      "epoch": 207.0754716981132,
      "grad_norm": 6.69261360168457,
      "learning_rate": 1.2853884615384615e-05,
      "loss": 3.0376,
      "step": 965800
    },
    {
      "epoch": 207.09691252144083,
      "grad_norm": 5.947878837585449,
      "learning_rate": 1.2850038461538463e-05,
      "loss": 3.1207,
      "step": 965900
    },
    {
      "epoch": 207.11835334476845,
      "grad_norm": 6.519969940185547,
      "learning_rate": 1.284619230769231e-05,
      "loss": 3.0882,
      "step": 966000
    },
    {
      "epoch": 207.13979416809605,
      "grad_norm": 6.174295902252197,
      "learning_rate": 1.2842346153846155e-05,
      "loss": 3.0726,
      "step": 966100
    },
    {
      "epoch": 207.16123499142367,
      "grad_norm": 6.6300554275512695,
      "learning_rate": 1.28385e-05,
      "loss": 3.0903,
      "step": 966200
    },
    {
      "epoch": 207.1826758147513,
      "grad_norm": 6.062168598175049,
      "learning_rate": 1.2834653846153846e-05,
      "loss": 3.1325,
      "step": 966300
    },
    {
      "epoch": 207.20411663807892,
      "grad_norm": 5.439315319061279,
      "learning_rate": 1.2830807692307692e-05,
      "loss": 3.0703,
      "step": 966400
    },
    {
      "epoch": 207.2255574614065,
      "grad_norm": 5.805065155029297,
      "learning_rate": 1.2826961538461537e-05,
      "loss": 3.0866,
      "step": 966500
    },
    {
      "epoch": 207.24699828473413,
      "grad_norm": 5.818276882171631,
      "learning_rate": 1.2823115384615386e-05,
      "loss": 3.09,
      "step": 966600
    },
    {
      "epoch": 207.26843910806176,
      "grad_norm": 6.097415924072266,
      "learning_rate": 1.2819269230769232e-05,
      "loss": 3.0907,
      "step": 966700
    },
    {
      "epoch": 207.28987993138938,
      "grad_norm": 5.749650001525879,
      "learning_rate": 1.2815423076923077e-05,
      "loss": 3.1005,
      "step": 966800
    },
    {
      "epoch": 207.31132075471697,
      "grad_norm": 5.951841831207275,
      "learning_rate": 1.2811576923076923e-05,
      "loss": 3.0526,
      "step": 966900
    },
    {
      "epoch": 207.3327615780446,
      "grad_norm": 5.777307033538818,
      "learning_rate": 1.2807730769230769e-05,
      "loss": 3.121,
      "step": 967000
    },
    {
      "epoch": 207.35420240137222,
      "grad_norm": 6.093665599822998,
      "learning_rate": 1.2803884615384614e-05,
      "loss": 3.1292,
      "step": 967100
    },
    {
      "epoch": 207.37564322469981,
      "grad_norm": 6.150989532470703,
      "learning_rate": 1.2800038461538463e-05,
      "loss": 3.0946,
      "step": 967200
    },
    {
      "epoch": 207.39708404802744,
      "grad_norm": 6.265773773193359,
      "learning_rate": 1.2796192307692309e-05,
      "loss": 3.092,
      "step": 967300
    },
    {
      "epoch": 207.41852487135506,
      "grad_norm": 6.13992166519165,
      "learning_rate": 1.2792346153846154e-05,
      "loss": 3.0533,
      "step": 967400
    },
    {
      "epoch": 207.43996569468268,
      "grad_norm": 5.943477630615234,
      "learning_rate": 1.2788500000000001e-05,
      "loss": 3.0797,
      "step": 967500
    },
    {
      "epoch": 207.46140651801028,
      "grad_norm": 5.6073784828186035,
      "learning_rate": 1.2784653846153847e-05,
      "loss": 3.0992,
      "step": 967600
    },
    {
      "epoch": 207.4828473413379,
      "grad_norm": 6.215243816375732,
      "learning_rate": 1.2780807692307692e-05,
      "loss": 3.1423,
      "step": 967700
    },
    {
      "epoch": 207.50428816466552,
      "grad_norm": 6.524597644805908,
      "learning_rate": 1.277696153846154e-05,
      "loss": 3.071,
      "step": 967800
    },
    {
      "epoch": 207.52572898799315,
      "grad_norm": 6.387928009033203,
      "learning_rate": 1.2773115384615387e-05,
      "loss": 3.0981,
      "step": 967900
    },
    {
      "epoch": 207.54716981132074,
      "grad_norm": 5.791119575500488,
      "learning_rate": 1.2769269230769232e-05,
      "loss": 3.0908,
      "step": 968000
    },
    {
      "epoch": 207.56861063464837,
      "grad_norm": 5.922179222106934,
      "learning_rate": 1.2765423076923078e-05,
      "loss": 3.0917,
      "step": 968100
    },
    {
      "epoch": 207.590051457976,
      "grad_norm": 7.044528484344482,
      "learning_rate": 1.2761576923076923e-05,
      "loss": 3.0982,
      "step": 968200
    },
    {
      "epoch": 207.6114922813036,
      "grad_norm": 6.406816482543945,
      "learning_rate": 1.2757730769230769e-05,
      "loss": 3.1091,
      "step": 968300
    },
    {
      "epoch": 207.6329331046312,
      "grad_norm": 6.237978935241699,
      "learning_rate": 1.2753884615384615e-05,
      "loss": 3.1443,
      "step": 968400
    },
    {
      "epoch": 207.65437392795883,
      "grad_norm": 6.051202774047852,
      "learning_rate": 1.2750038461538463e-05,
      "loss": 3.1371,
      "step": 968500
    },
    {
      "epoch": 207.67581475128645,
      "grad_norm": 6.360748767852783,
      "learning_rate": 1.2746192307692309e-05,
      "loss": 3.1294,
      "step": 968600
    },
    {
      "epoch": 207.69725557461408,
      "grad_norm": 5.794680595397949,
      "learning_rate": 1.2742346153846155e-05,
      "loss": 3.1215,
      "step": 968700
    },
    {
      "epoch": 207.71869639794167,
      "grad_norm": 6.857793807983398,
      "learning_rate": 1.27385e-05,
      "loss": 3.1401,
      "step": 968800
    },
    {
      "epoch": 207.7401372212693,
      "grad_norm": 6.201658725738525,
      "learning_rate": 1.2734653846153846e-05,
      "loss": 3.1602,
      "step": 968900
    },
    {
      "epoch": 207.76157804459692,
      "grad_norm": 6.319109916687012,
      "learning_rate": 1.2730807692307691e-05,
      "loss": 3.0449,
      "step": 969000
    },
    {
      "epoch": 207.78301886792454,
      "grad_norm": 5.933251857757568,
      "learning_rate": 1.272696153846154e-05,
      "loss": 3.0763,
      "step": 969100
    },
    {
      "epoch": 207.80445969125213,
      "grad_norm": 6.204875946044922,
      "learning_rate": 1.2723115384615386e-05,
      "loss": 3.1232,
      "step": 969200
    },
    {
      "epoch": 207.82590051457976,
      "grad_norm": 6.061189651489258,
      "learning_rate": 1.2719269230769231e-05,
      "loss": 3.085,
      "step": 969300
    },
    {
      "epoch": 207.84734133790738,
      "grad_norm": 6.3794660568237305,
      "learning_rate": 1.2715423076923077e-05,
      "loss": 3.1139,
      "step": 969400
    },
    {
      "epoch": 207.868782161235,
      "grad_norm": 6.91254997253418,
      "learning_rate": 1.2711576923076924e-05,
      "loss": 3.1046,
      "step": 969500
    },
    {
      "epoch": 207.8902229845626,
      "grad_norm": 5.796311855316162,
      "learning_rate": 1.270773076923077e-05,
      "loss": 3.027,
      "step": 969600
    },
    {
      "epoch": 207.91166380789022,
      "grad_norm": 6.203293800354004,
      "learning_rate": 1.2703884615384615e-05,
      "loss": 3.0628,
      "step": 969700
    },
    {
      "epoch": 207.93310463121784,
      "grad_norm": 5.661152362823486,
      "learning_rate": 1.2700038461538464e-05,
      "loss": 3.0754,
      "step": 969800
    },
    {
      "epoch": 207.95454545454547,
      "grad_norm": 5.471236228942871,
      "learning_rate": 1.269619230769231e-05,
      "loss": 3.1366,
      "step": 969900
    },
    {
      "epoch": 207.97598627787306,
      "grad_norm": 5.7597551345825195,
      "learning_rate": 1.2692346153846155e-05,
      "loss": 3.0894,
      "step": 970000
    },
    {
      "epoch": 207.99742710120069,
      "grad_norm": 6.231387615203857,
      "learning_rate": 1.26885e-05,
      "loss": 3.118,
      "step": 970100
    },
    {
      "epoch": 208.0188679245283,
      "grad_norm": 5.982918739318848,
      "learning_rate": 1.2684653846153846e-05,
      "loss": 3.0807,
      "step": 970200
    },
    {
      "epoch": 208.04030874785593,
      "grad_norm": 5.510321140289307,
      "learning_rate": 1.2680807692307692e-05,
      "loss": 3.0369,
      "step": 970300
    },
    {
      "epoch": 208.06174957118353,
      "grad_norm": 6.1060075759887695,
      "learning_rate": 1.267696153846154e-05,
      "loss": 3.0634,
      "step": 970400
    },
    {
      "epoch": 208.08319039451115,
      "grad_norm": 6.457093238830566,
      "learning_rate": 1.2673115384615386e-05,
      "loss": 3.0787,
      "step": 970500
    },
    {
      "epoch": 208.10463121783877,
      "grad_norm": 6.342074871063232,
      "learning_rate": 1.2669269230769232e-05,
      "loss": 3.0688,
      "step": 970600
    },
    {
      "epoch": 208.12607204116637,
      "grad_norm": 5.602424144744873,
      "learning_rate": 1.2665423076923077e-05,
      "loss": 3.073,
      "step": 970700
    },
    {
      "epoch": 208.147512864494,
      "grad_norm": 6.260310173034668,
      "learning_rate": 1.2661576923076923e-05,
      "loss": 3.0717,
      "step": 970800
    },
    {
      "epoch": 208.1689536878216,
      "grad_norm": 6.263488292694092,
      "learning_rate": 1.2657730769230768e-05,
      "loss": 3.1059,
      "step": 970900
    },
    {
      "epoch": 208.19039451114924,
      "grad_norm": 5.911708354949951,
      "learning_rate": 1.2653884615384616e-05,
      "loss": 3.1063,
      "step": 971000
    },
    {
      "epoch": 208.21183533447683,
      "grad_norm": 6.05045747756958,
      "learning_rate": 1.2650038461538463e-05,
      "loss": 3.1054,
      "step": 971100
    },
    {
      "epoch": 208.23327615780445,
      "grad_norm": 5.70166540145874,
      "learning_rate": 1.2646192307692308e-05,
      "loss": 3.0884,
      "step": 971200
    },
    {
      "epoch": 208.25471698113208,
      "grad_norm": 5.962253570556641,
      "learning_rate": 1.2642346153846154e-05,
      "loss": 3.0994,
      "step": 971300
    },
    {
      "epoch": 208.2761578044597,
      "grad_norm": 6.334475040435791,
      "learning_rate": 1.2638500000000001e-05,
      "loss": 3.0869,
      "step": 971400
    },
    {
      "epoch": 208.2975986277873,
      "grad_norm": 6.18394660949707,
      "learning_rate": 1.2634653846153847e-05,
      "loss": 3.0427,
      "step": 971500
    },
    {
      "epoch": 208.31903945111492,
      "grad_norm": 6.711122035980225,
      "learning_rate": 1.2630807692307692e-05,
      "loss": 3.0637,
      "step": 971600
    },
    {
      "epoch": 208.34048027444254,
      "grad_norm": 6.549464702606201,
      "learning_rate": 1.2626961538461541e-05,
      "loss": 3.0738,
      "step": 971700
    },
    {
      "epoch": 208.36192109777016,
      "grad_norm": 6.098081111907959,
      "learning_rate": 1.2623115384615387e-05,
      "loss": 3.133,
      "step": 971800
    },
    {
      "epoch": 208.38336192109776,
      "grad_norm": 6.070542812347412,
      "learning_rate": 1.2619269230769232e-05,
      "loss": 3.1134,
      "step": 971900
    },
    {
      "epoch": 208.40480274442538,
      "grad_norm": 6.88508939743042,
      "learning_rate": 1.2615423076923078e-05,
      "loss": 3.1216,
      "step": 972000
    },
    {
      "epoch": 208.426243567753,
      "grad_norm": 6.2638678550720215,
      "learning_rate": 1.2611576923076923e-05,
      "loss": 3.0502,
      "step": 972100
    },
    {
      "epoch": 208.44768439108063,
      "grad_norm": 6.246516704559326,
      "learning_rate": 1.2607730769230769e-05,
      "loss": 3.1175,
      "step": 972200
    },
    {
      "epoch": 208.46912521440822,
      "grad_norm": 5.8836750984191895,
      "learning_rate": 1.2603884615384614e-05,
      "loss": 3.0956,
      "step": 972300
    },
    {
      "epoch": 208.49056603773585,
      "grad_norm": 6.083804607391357,
      "learning_rate": 1.2600038461538463e-05,
      "loss": 3.089,
      "step": 972400
    },
    {
      "epoch": 208.51200686106347,
      "grad_norm": 5.873319625854492,
      "learning_rate": 1.2596192307692309e-05,
      "loss": 3.118,
      "step": 972500
    },
    {
      "epoch": 208.5334476843911,
      "grad_norm": 5.755184173583984,
      "learning_rate": 1.2592346153846154e-05,
      "loss": 3.097,
      "step": 972600
    },
    {
      "epoch": 208.5548885077187,
      "grad_norm": 6.221948623657227,
      "learning_rate": 1.25885e-05,
      "loss": 3.06,
      "step": 972700
    },
    {
      "epoch": 208.5763293310463,
      "grad_norm": 6.11707067489624,
      "learning_rate": 1.2584653846153845e-05,
      "loss": 3.0849,
      "step": 972800
    },
    {
      "epoch": 208.59777015437393,
      "grad_norm": 6.145607948303223,
      "learning_rate": 1.2580807692307691e-05,
      "loss": 3.0715,
      "step": 972900
    },
    {
      "epoch": 208.61921097770156,
      "grad_norm": 5.967463493347168,
      "learning_rate": 1.257696153846154e-05,
      "loss": 3.1279,
      "step": 973000
    },
    {
      "epoch": 208.64065180102915,
      "grad_norm": 5.996923446655273,
      "learning_rate": 1.2573115384615385e-05,
      "loss": 3.1128,
      "step": 973100
    },
    {
      "epoch": 208.66209262435677,
      "grad_norm": 5.9353508949279785,
      "learning_rate": 1.2569269230769231e-05,
      "loss": 3.1128,
      "step": 973200
    },
    {
      "epoch": 208.6835334476844,
      "grad_norm": 6.165850639343262,
      "learning_rate": 1.2565423076923078e-05,
      "loss": 3.0708,
      "step": 973300
    },
    {
      "epoch": 208.70497427101202,
      "grad_norm": 5.683374881744385,
      "learning_rate": 1.2561576923076924e-05,
      "loss": 3.0827,
      "step": 973400
    },
    {
      "epoch": 208.72641509433961,
      "grad_norm": 6.071604251861572,
      "learning_rate": 1.255773076923077e-05,
      "loss": 3.0879,
      "step": 973500
    },
    {
      "epoch": 208.74785591766724,
      "grad_norm": 5.655173301696777,
      "learning_rate": 1.2553884615384618e-05,
      "loss": 3.1505,
      "step": 973600
    },
    {
      "epoch": 208.76929674099486,
      "grad_norm": 5.417512893676758,
      "learning_rate": 1.2550038461538464e-05,
      "loss": 3.091,
      "step": 973700
    },
    {
      "epoch": 208.79073756432248,
      "grad_norm": 5.954891204833984,
      "learning_rate": 1.254619230769231e-05,
      "loss": 3.0522,
      "step": 973800
    },
    {
      "epoch": 208.81217838765008,
      "grad_norm": 6.220382213592529,
      "learning_rate": 1.2542346153846155e-05,
      "loss": 3.0611,
      "step": 973900
    },
    {
      "epoch": 208.8336192109777,
      "grad_norm": 5.9771857261657715,
      "learning_rate": 1.25385e-05,
      "loss": 3.1022,
      "step": 974000
    },
    {
      "epoch": 208.85506003430532,
      "grad_norm": 5.7593278884887695,
      "learning_rate": 1.2534653846153846e-05,
      "loss": 3.0698,
      "step": 974100
    },
    {
      "epoch": 208.87650085763292,
      "grad_norm": 5.942200183868408,
      "learning_rate": 1.2530807692307691e-05,
      "loss": 3.0619,
      "step": 974200
    },
    {
      "epoch": 208.89794168096054,
      "grad_norm": 6.491703510284424,
      "learning_rate": 1.252696153846154e-05,
      "loss": 3.103,
      "step": 974300
    },
    {
      "epoch": 208.91938250428817,
      "grad_norm": 6.025759696960449,
      "learning_rate": 1.2523115384615386e-05,
      "loss": 3.1433,
      "step": 974400
    },
    {
      "epoch": 208.9408233276158,
      "grad_norm": 6.396307468414307,
      "learning_rate": 1.2519269230769231e-05,
      "loss": 3.1131,
      "step": 974500
    },
    {
      "epoch": 208.96226415094338,
      "grad_norm": 6.227680683135986,
      "learning_rate": 1.2515423076923077e-05,
      "loss": 3.1377,
      "step": 974600
    },
    {
      "epoch": 208.983704974271,
      "grad_norm": 6.6377129554748535,
      "learning_rate": 1.2511576923076922e-05,
      "loss": 3.1453,
      "step": 974700
    },
    {
      "epoch": 209.00514579759863,
      "grad_norm": 6.115683078765869,
      "learning_rate": 1.2507730769230768e-05,
      "loss": 3.1369,
      "step": 974800
    },
    {
      "epoch": 209.02658662092625,
      "grad_norm": 6.059501647949219,
      "learning_rate": 1.2503884615384617e-05,
      "loss": 3.0825,
      "step": 974900
    },
    {
      "epoch": 209.04802744425385,
      "grad_norm": 5.5827226638793945,
      "learning_rate": 1.2500038461538462e-05,
      "loss": 3.0297,
      "step": 975000
    },
    {
      "epoch": 209.06946826758147,
      "grad_norm": 6.35316276550293,
      "learning_rate": 1.2496192307692308e-05,
      "loss": 3.0765,
      "step": 975100
    },
    {
      "epoch": 209.0909090909091,
      "grad_norm": 6.350492000579834,
      "learning_rate": 1.2492346153846155e-05,
      "loss": 3.0843,
      "step": 975200
    },
    {
      "epoch": 209.11234991423672,
      "grad_norm": 6.229730129241943,
      "learning_rate": 1.24885e-05,
      "loss": 3.0747,
      "step": 975300
    },
    {
      "epoch": 209.1337907375643,
      "grad_norm": 5.642086982727051,
      "learning_rate": 1.2484653846153848e-05,
      "loss": 3.1025,
      "step": 975400
    },
    {
      "epoch": 209.15523156089193,
      "grad_norm": 6.0149760246276855,
      "learning_rate": 1.2480807692307694e-05,
      "loss": 3.0824,
      "step": 975500
    },
    {
      "epoch": 209.17667238421956,
      "grad_norm": 5.714032173156738,
      "learning_rate": 1.2476961538461539e-05,
      "loss": 3.0387,
      "step": 975600
    },
    {
      "epoch": 209.19811320754718,
      "grad_norm": 6.507876396179199,
      "learning_rate": 1.2473115384615386e-05,
      "loss": 3.1075,
      "step": 975700
    },
    {
      "epoch": 209.21955403087478,
      "grad_norm": 6.085207462310791,
      "learning_rate": 1.2469269230769232e-05,
      "loss": 3.0388,
      "step": 975800
    },
    {
      "epoch": 209.2409948542024,
      "grad_norm": 6.004812240600586,
      "learning_rate": 1.2465423076923077e-05,
      "loss": 3.0517,
      "step": 975900
    },
    {
      "epoch": 209.26243567753002,
      "grad_norm": 6.532888412475586,
      "learning_rate": 1.2461576923076923e-05,
      "loss": 3.1024,
      "step": 976000
    },
    {
      "epoch": 209.28387650085764,
      "grad_norm": 6.399246692657471,
      "learning_rate": 1.245773076923077e-05,
      "loss": 3.0924,
      "step": 976100
    },
    {
      "epoch": 209.30531732418524,
      "grad_norm": 6.194846153259277,
      "learning_rate": 1.2453884615384616e-05,
      "loss": 3.0634,
      "step": 976200
    },
    {
      "epoch": 209.32675814751286,
      "grad_norm": 5.654850959777832,
      "learning_rate": 1.2450038461538461e-05,
      "loss": 3.0432,
      "step": 976300
    },
    {
      "epoch": 209.34819897084049,
      "grad_norm": 6.322831153869629,
      "learning_rate": 1.2446192307692309e-05,
      "loss": 3.0599,
      "step": 976400
    },
    {
      "epoch": 209.3696397941681,
      "grad_norm": 6.1823859214782715,
      "learning_rate": 1.2442346153846154e-05,
      "loss": 3.1064,
      "step": 976500
    },
    {
      "epoch": 209.3910806174957,
      "grad_norm": 6.157406330108643,
      "learning_rate": 1.24385e-05,
      "loss": 3.1324,
      "step": 976600
    },
    {
      "epoch": 209.41252144082333,
      "grad_norm": 6.0991740226745605,
      "learning_rate": 1.2434653846153847e-05,
      "loss": 3.0823,
      "step": 976700
    },
    {
      "epoch": 209.43396226415095,
      "grad_norm": 5.957493305206299,
      "learning_rate": 1.2430807692307692e-05,
      "loss": 3.1124,
      "step": 976800
    },
    {
      "epoch": 209.45540308747857,
      "grad_norm": 6.108263969421387,
      "learning_rate": 1.2426961538461538e-05,
      "loss": 3.0607,
      "step": 976900
    },
    {
      "epoch": 209.47684391080617,
      "grad_norm": 6.220203399658203,
      "learning_rate": 1.2423115384615385e-05,
      "loss": 3.0718,
      "step": 977000
    },
    {
      "epoch": 209.4982847341338,
      "grad_norm": 6.144461154937744,
      "learning_rate": 1.2419269230769232e-05,
      "loss": 3.1304,
      "step": 977100
    },
    {
      "epoch": 209.5197255574614,
      "grad_norm": 6.567567348480225,
      "learning_rate": 1.2415423076923078e-05,
      "loss": 3.0584,
      "step": 977200
    },
    {
      "epoch": 209.54116638078904,
      "grad_norm": 6.089241027832031,
      "learning_rate": 1.2411576923076923e-05,
      "loss": 3.0963,
      "step": 977300
    },
    {
      "epoch": 209.56260720411663,
      "grad_norm": 5.992336750030518,
      "learning_rate": 1.240773076923077e-05,
      "loss": 3.0853,
      "step": 977400
    },
    {
      "epoch": 209.58404802744425,
      "grad_norm": 5.849329471588135,
      "learning_rate": 1.2403884615384616e-05,
      "loss": 3.1477,
      "step": 977500
    },
    {
      "epoch": 209.60548885077188,
      "grad_norm": 5.921555042266846,
      "learning_rate": 1.2400038461538462e-05,
      "loss": 3.1096,
      "step": 977600
    },
    {
      "epoch": 209.62692967409947,
      "grad_norm": 6.04002046585083,
      "learning_rate": 1.2396192307692309e-05,
      "loss": 3.1164,
      "step": 977700
    },
    {
      "epoch": 209.6483704974271,
      "grad_norm": 6.106428623199463,
      "learning_rate": 1.2392346153846155e-05,
      "loss": 3.066,
      "step": 977800
    },
    {
      "epoch": 209.66981132075472,
      "grad_norm": 6.31425142288208,
      "learning_rate": 1.23885e-05,
      "loss": 3.0925,
      "step": 977900
    },
    {
      "epoch": 209.69125214408234,
      "grad_norm": 5.932793617248535,
      "learning_rate": 1.2384653846153847e-05,
      "loss": 3.0523,
      "step": 978000
    },
    {
      "epoch": 209.71269296740994,
      "grad_norm": 6.638472557067871,
      "learning_rate": 1.2380807692307693e-05,
      "loss": 3.1335,
      "step": 978100
    },
    {
      "epoch": 209.73413379073756,
      "grad_norm": 6.280128002166748,
      "learning_rate": 1.2376961538461538e-05,
      "loss": 3.126,
      "step": 978200
    },
    {
      "epoch": 209.75557461406518,
      "grad_norm": 6.158353805541992,
      "learning_rate": 1.2373115384615386e-05,
      "loss": 3.1071,
      "step": 978300
    },
    {
      "epoch": 209.7770154373928,
      "grad_norm": 6.419689178466797,
      "learning_rate": 1.2369269230769231e-05,
      "loss": 3.0664,
      "step": 978400
    },
    {
      "epoch": 209.7984562607204,
      "grad_norm": 5.82193660736084,
      "learning_rate": 1.2365423076923077e-05,
      "loss": 3.0894,
      "step": 978500
    },
    {
      "epoch": 209.81989708404802,
      "grad_norm": 5.745514869689941,
      "learning_rate": 1.2361576923076924e-05,
      "loss": 3.1192,
      "step": 978600
    },
    {
      "epoch": 209.84133790737565,
      "grad_norm": 6.457249641418457,
      "learning_rate": 1.235773076923077e-05,
      "loss": 3.1248,
      "step": 978700
    },
    {
      "epoch": 209.86277873070327,
      "grad_norm": 6.119554042816162,
      "learning_rate": 1.2353884615384615e-05,
      "loss": 3.0987,
      "step": 978800
    },
    {
      "epoch": 209.88421955403086,
      "grad_norm": 6.087459087371826,
      "learning_rate": 1.2350038461538462e-05,
      "loss": 3.0973,
      "step": 978900
    },
    {
      "epoch": 209.9056603773585,
      "grad_norm": 5.997727394104004,
      "learning_rate": 1.234619230769231e-05,
      "loss": 3.1023,
      "step": 979000
    },
    {
      "epoch": 209.9271012006861,
      "grad_norm": 6.368207931518555,
      "learning_rate": 1.2342346153846155e-05,
      "loss": 3.0909,
      "step": 979100
    },
    {
      "epoch": 209.94854202401373,
      "grad_norm": 6.440852642059326,
      "learning_rate": 1.23385e-05,
      "loss": 3.0799,
      "step": 979200
    },
    {
      "epoch": 209.96998284734133,
      "grad_norm": 6.27400016784668,
      "learning_rate": 1.2334653846153848e-05,
      "loss": 3.063,
      "step": 979300
    },
    {
      "epoch": 209.99142367066895,
      "grad_norm": 6.382346153259277,
      "learning_rate": 1.2330807692307693e-05,
      "loss": 3.0727,
      "step": 979400
    },
    {
      "epoch": 210.01286449399657,
      "grad_norm": 6.380885124206543,
      "learning_rate": 1.2326961538461539e-05,
      "loss": 3.1056,
      "step": 979500
    },
    {
      "epoch": 210.0343053173242,
      "grad_norm": 5.686337471008301,
      "learning_rate": 1.2323115384615386e-05,
      "loss": 3.0853,
      "step": 979600
    },
    {
      "epoch": 210.0557461406518,
      "grad_norm": 6.189608573913574,
      "learning_rate": 1.2319269230769232e-05,
      "loss": 3.0913,
      "step": 979700
    },
    {
      "epoch": 210.07718696397941,
      "grad_norm": 5.775681972503662,
      "learning_rate": 1.2315423076923077e-05,
      "loss": 3.0832,
      "step": 979800
    },
    {
      "epoch": 210.09862778730704,
      "grad_norm": 5.9478960037231445,
      "learning_rate": 1.2311576923076924e-05,
      "loss": 3.0972,
      "step": 979900
    },
    {
      "epoch": 210.12006861063466,
      "grad_norm": 6.342309951782227,
      "learning_rate": 1.230773076923077e-05,
      "loss": 3.0304,
      "step": 980000
    },
    {
      "epoch": 210.14150943396226,
      "grad_norm": 5.552639007568359,
      "learning_rate": 1.2303884615384615e-05,
      "loss": 3.0717,
      "step": 980100
    },
    {
      "epoch": 210.16295025728988,
      "grad_norm": 6.580981731414795,
      "learning_rate": 1.2300038461538461e-05,
      "loss": 3.0846,
      "step": 980200
    },
    {
      "epoch": 210.1843910806175,
      "grad_norm": 6.534302711486816,
      "learning_rate": 1.2296192307692308e-05,
      "loss": 3.0995,
      "step": 980300
    },
    {
      "epoch": 210.20583190394512,
      "grad_norm": 6.186595916748047,
      "learning_rate": 1.2292346153846154e-05,
      "loss": 3.1105,
      "step": 980400
    },
    {
      "epoch": 210.22727272727272,
      "grad_norm": 6.018543243408203,
      "learning_rate": 1.22885e-05,
      "loss": 3.0673,
      "step": 980500
    },
    {
      "epoch": 210.24871355060034,
      "grad_norm": 5.869351863861084,
      "learning_rate": 1.2284653846153847e-05,
      "loss": 3.0546,
      "step": 980600
    },
    {
      "epoch": 210.27015437392797,
      "grad_norm": 6.199880123138428,
      "learning_rate": 1.2280807692307692e-05,
      "loss": 3.0544,
      "step": 980700
    },
    {
      "epoch": 210.29159519725556,
      "grad_norm": 5.503691673278809,
      "learning_rate": 1.227696153846154e-05,
      "loss": 3.118,
      "step": 980800
    },
    {
      "epoch": 210.31303602058318,
      "grad_norm": 6.097143650054932,
      "learning_rate": 1.2273115384615385e-05,
      "loss": 3.0663,
      "step": 980900
    },
    {
      "epoch": 210.3344768439108,
      "grad_norm": 6.213981628417969,
      "learning_rate": 1.2269269230769232e-05,
      "loss": 3.1329,
      "step": 981000
    },
    {
      "epoch": 210.35591766723843,
      "grad_norm": 6.186595916748047,
      "learning_rate": 1.2265423076923078e-05,
      "loss": 3.1113,
      "step": 981100
    },
    {
      "epoch": 210.37735849056602,
      "grad_norm": 5.479924201965332,
      "learning_rate": 1.2261576923076925e-05,
      "loss": 3.0324,
      "step": 981200
    },
    {
      "epoch": 210.39879931389365,
      "grad_norm": 6.4342122077941895,
      "learning_rate": 1.225773076923077e-05,
      "loss": 3.1018,
      "step": 981300
    },
    {
      "epoch": 210.42024013722127,
      "grad_norm": 6.70095682144165,
      "learning_rate": 1.2253884615384616e-05,
      "loss": 3.1255,
      "step": 981400
    },
    {
      "epoch": 210.4416809605489,
      "grad_norm": 7.034173965454102,
      "learning_rate": 1.2250038461538463e-05,
      "loss": 3.0993,
      "step": 981500
    },
    {
      "epoch": 210.4631217838765,
      "grad_norm": 5.9589924812316895,
      "learning_rate": 1.2246192307692309e-05,
      "loss": 3.0602,
      "step": 981600
    },
    {
      "epoch": 210.4845626072041,
      "grad_norm": 6.067188262939453,
      "learning_rate": 1.2242346153846154e-05,
      "loss": 3.0532,
      "step": 981700
    },
    {
      "epoch": 210.50600343053173,
      "grad_norm": 6.695509433746338,
      "learning_rate": 1.22385e-05,
      "loss": 3.0421,
      "step": 981800
    },
    {
      "epoch": 210.52744425385936,
      "grad_norm": 5.9512434005737305,
      "learning_rate": 1.2234653846153847e-05,
      "loss": 3.0862,
      "step": 981900
    },
    {
      "epoch": 210.54888507718695,
      "grad_norm": 6.172119617462158,
      "learning_rate": 1.2230807692307693e-05,
      "loss": 3.1241,
      "step": 982000
    },
    {
      "epoch": 210.57032590051458,
      "grad_norm": 6.501945972442627,
      "learning_rate": 1.2226961538461538e-05,
      "loss": 3.0649,
      "step": 982100
    },
    {
      "epoch": 210.5917667238422,
      "grad_norm": 6.124696254730225,
      "learning_rate": 1.2223115384615385e-05,
      "loss": 3.0778,
      "step": 982200
    },
    {
      "epoch": 210.61320754716982,
      "grad_norm": 6.06626033782959,
      "learning_rate": 1.2219269230769231e-05,
      "loss": 3.0822,
      "step": 982300
    },
    {
      "epoch": 210.63464837049742,
      "grad_norm": 5.891931533813477,
      "learning_rate": 1.2215423076923076e-05,
      "loss": 3.0467,
      "step": 982400
    },
    {
      "epoch": 210.65608919382504,
      "grad_norm": 6.26825475692749,
      "learning_rate": 1.2211576923076924e-05,
      "loss": 3.0447,
      "step": 982500
    },
    {
      "epoch": 210.67753001715266,
      "grad_norm": 5.929309368133545,
      "learning_rate": 1.220773076923077e-05,
      "loss": 3.1068,
      "step": 982600
    },
    {
      "epoch": 210.69897084048029,
      "grad_norm": 6.433073043823242,
      "learning_rate": 1.2203884615384616e-05,
      "loss": 3.083,
      "step": 982700
    },
    {
      "epoch": 210.72041166380788,
      "grad_norm": 6.227126598358154,
      "learning_rate": 1.2200038461538462e-05,
      "loss": 3.119,
      "step": 982800
    },
    {
      "epoch": 210.7418524871355,
      "grad_norm": 6.296176910400391,
      "learning_rate": 1.219619230769231e-05,
      "loss": 3.1119,
      "step": 982900
    },
    {
      "epoch": 210.76329331046313,
      "grad_norm": 5.978270053863525,
      "learning_rate": 1.2192346153846155e-05,
      "loss": 3.0558,
      "step": 983000
    },
    {
      "epoch": 210.78473413379075,
      "grad_norm": 6.584933280944824,
      "learning_rate": 1.21885e-05,
      "loss": 3.0916,
      "step": 983100
    },
    {
      "epoch": 210.80617495711834,
      "grad_norm": 6.046039581298828,
      "learning_rate": 1.2184653846153848e-05,
      "loss": 3.1009,
      "step": 983200
    },
    {
      "epoch": 210.82761578044597,
      "grad_norm": 5.904359340667725,
      "learning_rate": 1.2180807692307693e-05,
      "loss": 3.07,
      "step": 983300
    },
    {
      "epoch": 210.8490566037736,
      "grad_norm": 6.108788967132568,
      "learning_rate": 1.2176961538461539e-05,
      "loss": 3.1112,
      "step": 983400
    },
    {
      "epoch": 210.8704974271012,
      "grad_norm": 5.744112491607666,
      "learning_rate": 1.2173115384615386e-05,
      "loss": 3.0736,
      "step": 983500
    },
    {
      "epoch": 210.8919382504288,
      "grad_norm": 6.277872562408447,
      "learning_rate": 1.2169269230769231e-05,
      "loss": 3.0779,
      "step": 983600
    },
    {
      "epoch": 210.91337907375643,
      "grad_norm": 6.228427410125732,
      "learning_rate": 1.2165423076923077e-05,
      "loss": 3.0595,
      "step": 983700
    },
    {
      "epoch": 210.93481989708405,
      "grad_norm": 5.93264102935791,
      "learning_rate": 1.2161576923076924e-05,
      "loss": 3.1173,
      "step": 983800
    },
    {
      "epoch": 210.95626072041168,
      "grad_norm": 7.363760948181152,
      "learning_rate": 1.215773076923077e-05,
      "loss": 3.0812,
      "step": 983900
    },
    {
      "epoch": 210.97770154373927,
      "grad_norm": 5.845524311065674,
      "learning_rate": 1.2153884615384615e-05,
      "loss": 3.0641,
      "step": 984000
    },
    {
      "epoch": 210.9991423670669,
      "grad_norm": 6.140195369720459,
      "learning_rate": 1.2150038461538462e-05,
      "loss": 3.1104,
      "step": 984100
    },
    {
      "epoch": 211.02058319039452,
      "grad_norm": 5.942258358001709,
      "learning_rate": 1.2146192307692308e-05,
      "loss": 3.059,
      "step": 984200
    },
    {
      "epoch": 211.0420240137221,
      "grad_norm": 5.530230522155762,
      "learning_rate": 1.2142346153846154e-05,
      "loss": 3.0587,
      "step": 984300
    },
    {
      "epoch": 211.06346483704974,
      "grad_norm": 5.9803314208984375,
      "learning_rate": 1.21385e-05,
      "loss": 3.0409,
      "step": 984400
    },
    {
      "epoch": 211.08490566037736,
      "grad_norm": 5.860397815704346,
      "learning_rate": 1.2134653846153846e-05,
      "loss": 3.0298,
      "step": 984500
    },
    {
      "epoch": 211.10634648370498,
      "grad_norm": 5.971793174743652,
      "learning_rate": 1.2130807692307694e-05,
      "loss": 3.0431,
      "step": 984600
    },
    {
      "epoch": 211.12778730703258,
      "grad_norm": 5.918757915496826,
      "learning_rate": 1.2126961538461539e-05,
      "loss": 3.1005,
      "step": 984700
    },
    {
      "epoch": 211.1492281303602,
      "grad_norm": 6.523352146148682,
      "learning_rate": 1.2123115384615386e-05,
      "loss": 3.0593,
      "step": 984800
    },
    {
      "epoch": 211.17066895368782,
      "grad_norm": 6.279021263122559,
      "learning_rate": 1.2119269230769232e-05,
      "loss": 3.0357,
      "step": 984900
    },
    {
      "epoch": 211.19210977701545,
      "grad_norm": 6.4710235595703125,
      "learning_rate": 1.2115423076923077e-05,
      "loss": 3.1003,
      "step": 985000
    },
    {
      "epoch": 211.21355060034304,
      "grad_norm": 6.657196998596191,
      "learning_rate": 1.2111576923076925e-05,
      "loss": 3.0778,
      "step": 985100
    },
    {
      "epoch": 211.23499142367066,
      "grad_norm": 6.076913356781006,
      "learning_rate": 1.210773076923077e-05,
      "loss": 3.1062,
      "step": 985200
    },
    {
      "epoch": 211.2564322469983,
      "grad_norm": 5.561005115509033,
      "learning_rate": 1.2103884615384616e-05,
      "loss": 3.0542,
      "step": 985300
    },
    {
      "epoch": 211.2778730703259,
      "grad_norm": 5.905594825744629,
      "learning_rate": 1.2100038461538463e-05,
      "loss": 3.0836,
      "step": 985400
    },
    {
      "epoch": 211.2993138936535,
      "grad_norm": 6.810379505157471,
      "learning_rate": 1.2096192307692308e-05,
      "loss": 3.049,
      "step": 985500
    },
    {
      "epoch": 211.32075471698113,
      "grad_norm": 6.010997772216797,
      "learning_rate": 1.2092346153846154e-05,
      "loss": 3.0559,
      "step": 985600
    },
    {
      "epoch": 211.34219554030875,
      "grad_norm": 6.418290138244629,
      "learning_rate": 1.2088500000000001e-05,
      "loss": 3.1076,
      "step": 985700
    },
    {
      "epoch": 211.36363636363637,
      "grad_norm": 5.571899890899658,
      "learning_rate": 1.2084653846153847e-05,
      "loss": 3.1094,
      "step": 985800
    },
    {
      "epoch": 211.38507718696397,
      "grad_norm": 5.866926670074463,
      "learning_rate": 1.2080807692307692e-05,
      "loss": 3.0327,
      "step": 985900
    },
    {
      "epoch": 211.4065180102916,
      "grad_norm": 6.67807149887085,
      "learning_rate": 1.2076961538461538e-05,
      "loss": 3.0786,
      "step": 986000
    },
    {
      "epoch": 211.42795883361921,
      "grad_norm": 6.257274627685547,
      "learning_rate": 1.2073115384615385e-05,
      "loss": 3.0963,
      "step": 986100
    },
    {
      "epoch": 211.44939965694684,
      "grad_norm": 6.235113620758057,
      "learning_rate": 1.206926923076923e-05,
      "loss": 3.0924,
      "step": 986200
    },
    {
      "epoch": 211.47084048027443,
      "grad_norm": 5.4367876052856445,
      "learning_rate": 1.2065423076923076e-05,
      "loss": 3.0962,
      "step": 986300
    },
    {
      "epoch": 211.49228130360206,
      "grad_norm": 5.8722243309021,
      "learning_rate": 1.2061576923076923e-05,
      "loss": 3.0051,
      "step": 986400
    },
    {
      "epoch": 211.51372212692968,
      "grad_norm": 6.0890583992004395,
      "learning_rate": 1.2057730769230769e-05,
      "loss": 3.0866,
      "step": 986500
    },
    {
      "epoch": 211.5351629502573,
      "grad_norm": 5.932352066040039,
      "learning_rate": 1.2053884615384616e-05,
      "loss": 3.059,
      "step": 986600
    },
    {
      "epoch": 211.5566037735849,
      "grad_norm": 6.382481575012207,
      "learning_rate": 1.2050038461538463e-05,
      "loss": 3.0911,
      "step": 986700
    },
    {
      "epoch": 211.57804459691252,
      "grad_norm": 5.859710693359375,
      "learning_rate": 1.2046192307692309e-05,
      "loss": 3.0505,
      "step": 986800
    },
    {
      "epoch": 211.59948542024014,
      "grad_norm": 6.230597496032715,
      "learning_rate": 1.2042346153846154e-05,
      "loss": 3.0968,
      "step": 986900
    },
    {
      "epoch": 211.62092624356777,
      "grad_norm": 5.89201021194458,
      "learning_rate": 1.2038500000000002e-05,
      "loss": 3.1009,
      "step": 987000
    },
    {
      "epoch": 211.64236706689536,
      "grad_norm": 6.220143795013428,
      "learning_rate": 1.2034653846153847e-05,
      "loss": 3.1022,
      "step": 987100
    },
    {
      "epoch": 211.66380789022298,
      "grad_norm": 6.409757614135742,
      "learning_rate": 1.2030807692307693e-05,
      "loss": 3.0328,
      "step": 987200
    },
    {
      "epoch": 211.6852487135506,
      "grad_norm": 6.083891868591309,
      "learning_rate": 1.202696153846154e-05,
      "loss": 3.0812,
      "step": 987300
    },
    {
      "epoch": 211.70668953687823,
      "grad_norm": 6.128023147583008,
      "learning_rate": 1.2023115384615386e-05,
      "loss": 3.0657,
      "step": 987400
    },
    {
      "epoch": 211.72813036020582,
      "grad_norm": 6.429544448852539,
      "learning_rate": 1.2019269230769231e-05,
      "loss": 3.0882,
      "step": 987500
    },
    {
      "epoch": 211.74957118353345,
      "grad_norm": 6.6296467781066895,
      "learning_rate": 1.2015423076923077e-05,
      "loss": 3.1053,
      "step": 987600
    },
    {
      "epoch": 211.77101200686107,
      "grad_norm": 6.1102495193481445,
      "learning_rate": 1.2011576923076924e-05,
      "loss": 3.0523,
      "step": 987700
    },
    {
      "epoch": 211.79245283018867,
      "grad_norm": 6.3717451095581055,
      "learning_rate": 1.200773076923077e-05,
      "loss": 3.1221,
      "step": 987800
    },
    {
      "epoch": 211.8138936535163,
      "grad_norm": 6.326347827911377,
      "learning_rate": 1.2003884615384615e-05,
      "loss": 3.1127,
      "step": 987900
    },
    {
      "epoch": 211.8353344768439,
      "grad_norm": 6.44585657119751,
      "learning_rate": 1.2000038461538462e-05,
      "loss": 3.114,
      "step": 988000
    },
    {
      "epoch": 211.85677530017153,
      "grad_norm": 6.04290771484375,
      "learning_rate": 1.1996192307692308e-05,
      "loss": 3.0908,
      "step": 988100
    },
    {
      "epoch": 211.87821612349913,
      "grad_norm": 6.384746074676514,
      "learning_rate": 1.1992346153846153e-05,
      "loss": 3.1678,
      "step": 988200
    },
    {
      "epoch": 211.89965694682675,
      "grad_norm": 5.424674987792969,
      "learning_rate": 1.19885e-05,
      "loss": 3.0651,
      "step": 988300
    },
    {
      "epoch": 211.92109777015438,
      "grad_norm": 6.427616119384766,
      "learning_rate": 1.1984653846153846e-05,
      "loss": 3.0932,
      "step": 988400
    },
    {
      "epoch": 211.942538593482,
      "grad_norm": 6.206409454345703,
      "learning_rate": 1.1980807692307693e-05,
      "loss": 3.0918,
      "step": 988500
    },
    {
      "epoch": 211.9639794168096,
      "grad_norm": 6.421140670776367,
      "learning_rate": 1.197696153846154e-05,
      "loss": 3.1046,
      "step": 988600
    },
    {
      "epoch": 211.98542024013722,
      "grad_norm": 5.989225387573242,
      "learning_rate": 1.1973115384615386e-05,
      "loss": 3.1417,
      "step": 988700
    },
    {
      "epoch": 212.00686106346484,
      "grad_norm": 6.097490310668945,
      "learning_rate": 1.1969269230769232e-05,
      "loss": 3.0613,
      "step": 988800
    },
    {
      "epoch": 212.02830188679246,
      "grad_norm": 5.837460994720459,
      "learning_rate": 1.1965423076923077e-05,
      "loss": 3.095,
      "step": 988900
    },
    {
      "epoch": 212.04974271012006,
      "grad_norm": 5.730978965759277,
      "learning_rate": 1.1961576923076924e-05,
      "loss": 3.0618,
      "step": 989000
    },
    {
      "epoch": 212.07118353344768,
      "grad_norm": 6.134485721588135,
      "learning_rate": 1.195773076923077e-05,
      "loss": 3.0988,
      "step": 989100
    },
    {
      "epoch": 212.0926243567753,
      "grad_norm": 6.001671314239502,
      "learning_rate": 1.1953884615384615e-05,
      "loss": 3.0264,
      "step": 989200
    },
    {
      "epoch": 212.11406518010293,
      "grad_norm": 6.042026042938232,
      "learning_rate": 1.1950038461538463e-05,
      "loss": 3.0601,
      "step": 989300
    },
    {
      "epoch": 212.13550600343052,
      "grad_norm": 6.205669403076172,
      "learning_rate": 1.1946192307692308e-05,
      "loss": 3.0399,
      "step": 989400
    },
    {
      "epoch": 212.15694682675814,
      "grad_norm": 6.004255294799805,
      "learning_rate": 1.1942346153846154e-05,
      "loss": 3.0846,
      "step": 989500
    },
    {
      "epoch": 212.17838765008577,
      "grad_norm": 6.357394695281982,
      "learning_rate": 1.1938500000000001e-05,
      "loss": 3.0605,
      "step": 989600
    },
    {
      "epoch": 212.1998284734134,
      "grad_norm": 6.000129222869873,
      "learning_rate": 1.1934653846153847e-05,
      "loss": 3.0512,
      "step": 989700
    },
    {
      "epoch": 212.22126929674099,
      "grad_norm": 6.403316497802734,
      "learning_rate": 1.1930807692307692e-05,
      "loss": 3.0625,
      "step": 989800
    },
    {
      "epoch": 212.2427101200686,
      "grad_norm": 5.860400199890137,
      "learning_rate": 1.192696153846154e-05,
      "loss": 3.0766,
      "step": 989900
    },
    {
      "epoch": 212.26415094339623,
      "grad_norm": 6.770478248596191,
      "learning_rate": 1.1923115384615385e-05,
      "loss": 3.0738,
      "step": 990000
    },
    {
      "epoch": 212.28559176672385,
      "grad_norm": 6.224921703338623,
      "learning_rate": 1.191926923076923e-05,
      "loss": 3.0505,
      "step": 990100
    },
    {
      "epoch": 212.30703259005145,
      "grad_norm": 6.302350044250488,
      "learning_rate": 1.1915423076923078e-05,
      "loss": 3.0568,
      "step": 990200
    },
    {
      "epoch": 212.32847341337907,
      "grad_norm": 5.79597282409668,
      "learning_rate": 1.1911576923076923e-05,
      "loss": 3.0949,
      "step": 990300
    },
    {
      "epoch": 212.3499142367067,
      "grad_norm": 6.069925785064697,
      "learning_rate": 1.190773076923077e-05,
      "loss": 3.0489,
      "step": 990400
    },
    {
      "epoch": 212.37135506003432,
      "grad_norm": 6.2512407302856445,
      "learning_rate": 1.1903884615384616e-05,
      "loss": 3.0547,
      "step": 990500
    },
    {
      "epoch": 212.3927958833619,
      "grad_norm": 6.246253967285156,
      "learning_rate": 1.1900038461538463e-05,
      "loss": 3.097,
      "step": 990600
    },
    {
      "epoch": 212.41423670668954,
      "grad_norm": 5.810675144195557,
      "learning_rate": 1.1896192307692309e-05,
      "loss": 3.106,
      "step": 990700
    },
    {
      "epoch": 212.43567753001716,
      "grad_norm": 6.153273105621338,
      "learning_rate": 1.1892346153846154e-05,
      "loss": 3.0777,
      "step": 990800
    },
    {
      "epoch": 212.45711835334478,
      "grad_norm": 6.14508581161499,
      "learning_rate": 1.1888500000000001e-05,
      "loss": 3.1317,
      "step": 990900
    },
    {
      "epoch": 212.47855917667238,
      "grad_norm": 6.121760845184326,
      "learning_rate": 1.1884653846153847e-05,
      "loss": 3.0958,
      "step": 991000
    },
    {
      "epoch": 212.5,
      "grad_norm": 6.303399085998535,
      "learning_rate": 1.1880807692307693e-05,
      "loss": 3.081,
      "step": 991100
    },
    {
      "epoch": 212.52144082332762,
      "grad_norm": 6.157296657562256,
      "learning_rate": 1.187696153846154e-05,
      "loss": 3.0668,
      "step": 991200
    },
    {
      "epoch": 212.54288164665522,
      "grad_norm": 5.725255489349365,
      "learning_rate": 1.1873115384615385e-05,
      "loss": 3.0469,
      "step": 991300
    },
    {
      "epoch": 212.56432246998284,
      "grad_norm": 6.199834823608398,
      "learning_rate": 1.186926923076923e-05,
      "loss": 3.0714,
      "step": 991400
    },
    {
      "epoch": 212.58576329331046,
      "grad_norm": 6.685576438903809,
      "learning_rate": 1.1865423076923078e-05,
      "loss": 3.0769,
      "step": 991500
    },
    {
      "epoch": 212.6072041166381,
      "grad_norm": 6.218354225158691,
      "learning_rate": 1.1861576923076924e-05,
      "loss": 3.0771,
      "step": 991600
    },
    {
      "epoch": 212.62864493996568,
      "grad_norm": 6.615490436553955,
      "learning_rate": 1.185773076923077e-05,
      "loss": 3.0529,
      "step": 991700
    },
    {
      "epoch": 212.6500857632933,
      "grad_norm": 6.077969551086426,
      "learning_rate": 1.1853884615384615e-05,
      "loss": 3.0978,
      "step": 991800
    },
    {
      "epoch": 212.67152658662093,
      "grad_norm": 6.528713703155518,
      "learning_rate": 1.1850038461538462e-05,
      "loss": 3.0981,
      "step": 991900
    },
    {
      "epoch": 212.69296740994855,
      "grad_norm": 5.8008952140808105,
      "learning_rate": 1.1846192307692307e-05,
      "loss": 3.06,
      "step": 992000
    },
    {
      "epoch": 212.71440823327615,
      "grad_norm": 6.42068338394165,
      "learning_rate": 1.1842346153846155e-05,
      "loss": 3.1157,
      "step": 992100
    },
    {
      "epoch": 212.73584905660377,
      "grad_norm": 6.061607837677002,
      "learning_rate": 1.18385e-05,
      "loss": 3.1462,
      "step": 992200
    },
    {
      "epoch": 212.7572898799314,
      "grad_norm": 6.674988746643066,
      "learning_rate": 1.1834653846153847e-05,
      "loss": 3.0614,
      "step": 992300
    },
    {
      "epoch": 212.77873070325901,
      "grad_norm": 5.803606986999512,
      "learning_rate": 1.1830807692307693e-05,
      "loss": 3.0522,
      "step": 992400
    },
    {
      "epoch": 212.8001715265866,
      "grad_norm": 6.658585071563721,
      "learning_rate": 1.182696153846154e-05,
      "loss": 3.0728,
      "step": 992500
    },
    {
      "epoch": 212.82161234991423,
      "grad_norm": 6.321846008300781,
      "learning_rate": 1.1823115384615386e-05,
      "loss": 3.1099,
      "step": 992600
    },
    {
      "epoch": 212.84305317324186,
      "grad_norm": 6.180632591247559,
      "learning_rate": 1.1819269230769231e-05,
      "loss": 3.1198,
      "step": 992700
    },
    {
      "epoch": 212.86449399656948,
      "grad_norm": 7.5921244621276855,
      "learning_rate": 1.1815423076923079e-05,
      "loss": 3.0866,
      "step": 992800
    },
    {
      "epoch": 212.88593481989707,
      "grad_norm": 5.6334404945373535,
      "learning_rate": 1.1811576923076924e-05,
      "loss": 3.0798,
      "step": 992900
    },
    {
      "epoch": 212.9073756432247,
      "grad_norm": 6.470151424407959,
      "learning_rate": 1.180773076923077e-05,
      "loss": 3.082,
      "step": 993000
    },
    {
      "epoch": 212.92881646655232,
      "grad_norm": 6.413451671600342,
      "learning_rate": 1.1803884615384615e-05,
      "loss": 3.0963,
      "step": 993100
    },
    {
      "epoch": 212.95025728987994,
      "grad_norm": 6.570913791656494,
      "learning_rate": 1.1800038461538462e-05,
      "loss": 3.0352,
      "step": 993200
    },
    {
      "epoch": 212.97169811320754,
      "grad_norm": 6.280761241912842,
      "learning_rate": 1.1796192307692308e-05,
      "loss": 3.0583,
      "step": 993300
    },
    {
      "epoch": 212.99313893653516,
      "grad_norm": 5.731022834777832,
      "learning_rate": 1.1792346153846153e-05,
      "loss": 3.0405,
      "step": 993400
    },
    {
      "epoch": 213.01457975986278,
      "grad_norm": 6.2942399978637695,
      "learning_rate": 1.17885e-05,
      "loss": 3.074,
      "step": 993500
    },
    {
      "epoch": 213.0360205831904,
      "grad_norm": 6.400991439819336,
      "learning_rate": 1.1784653846153846e-05,
      "loss": 3.1429,
      "step": 993600
    },
    {
      "epoch": 213.057461406518,
      "grad_norm": 6.8670268058776855,
      "learning_rate": 1.1780807692307692e-05,
      "loss": 3.0706,
      "step": 993700
    },
    {
      "epoch": 213.07890222984562,
      "grad_norm": 6.145092487335205,
      "learning_rate": 1.1776961538461539e-05,
      "loss": 3.0656,
      "step": 993800
    },
    {
      "epoch": 213.10034305317325,
      "grad_norm": 6.088611602783203,
      "learning_rate": 1.1773115384615385e-05,
      "loss": 3.0514,
      "step": 993900
    },
    {
      "epoch": 213.12178387650087,
      "grad_norm": 5.742869853973389,
      "learning_rate": 1.176926923076923e-05,
      "loss": 3.0479,
      "step": 994000
    },
    {
      "epoch": 213.14322469982847,
      "grad_norm": 6.127570152282715,
      "learning_rate": 1.1765423076923077e-05,
      "loss": 3.0314,
      "step": 994100
    },
    {
      "epoch": 213.1646655231561,
      "grad_norm": 5.489351272583008,
      "learning_rate": 1.1761576923076925e-05,
      "loss": 3.041,
      "step": 994200
    },
    {
      "epoch": 213.1861063464837,
      "grad_norm": 6.087732791900635,
      "learning_rate": 1.175773076923077e-05,
      "loss": 3.0335,
      "step": 994300
    },
    {
      "epoch": 213.20754716981133,
      "grad_norm": 6.077146053314209,
      "learning_rate": 1.1753884615384617e-05,
      "loss": 3.0783,
      "step": 994400
    },
    {
      "epoch": 213.22898799313893,
      "grad_norm": 6.1301960945129395,
      "learning_rate": 1.1750038461538463e-05,
      "loss": 3.0716,
      "step": 994500
    },
    {
      "epoch": 213.25042881646655,
      "grad_norm": 6.1593523025512695,
      "learning_rate": 1.1746192307692308e-05,
      "loss": 3.0393,
      "step": 994600
    },
    {
      "epoch": 213.27186963979418,
      "grad_norm": 6.092033386230469,
      "learning_rate": 1.1742346153846154e-05,
      "loss": 3.0883,
      "step": 994700
    },
    {
      "epoch": 213.29331046312177,
      "grad_norm": 5.885085105895996,
      "learning_rate": 1.1738500000000001e-05,
      "loss": 3.0494,
      "step": 994800
    },
    {
      "epoch": 213.3147512864494,
      "grad_norm": 6.577704429626465,
      "learning_rate": 1.1734653846153847e-05,
      "loss": 3.0726,
      "step": 994900
    },
    {
      "epoch": 213.33619210977702,
      "grad_norm": 6.373409748077393,
      "learning_rate": 1.1730807692307692e-05,
      "loss": 3.0657,
      "step": 995000
    },
    {
      "epoch": 213.35763293310464,
      "grad_norm": 5.939637660980225,
      "learning_rate": 1.172696153846154e-05,
      "loss": 3.0399,
      "step": 995100
    },
    {
      "epoch": 213.37907375643223,
      "grad_norm": 6.545769214630127,
      "learning_rate": 1.1723115384615385e-05,
      "loss": 3.0571,
      "step": 995200
    },
    {
      "epoch": 213.40051457975986,
      "grad_norm": 5.890956401824951,
      "learning_rate": 1.171926923076923e-05,
      "loss": 3.0985,
      "step": 995300
    },
    {
      "epoch": 213.42195540308748,
      "grad_norm": 6.2223734855651855,
      "learning_rate": 1.1715423076923078e-05,
      "loss": 3.1309,
      "step": 995400
    },
    {
      "epoch": 213.4433962264151,
      "grad_norm": 6.054325103759766,
      "learning_rate": 1.1711576923076923e-05,
      "loss": 3.0632,
      "step": 995500
    },
    {
      "epoch": 213.4648370497427,
      "grad_norm": 6.223794937133789,
      "learning_rate": 1.1707730769230769e-05,
      "loss": 3.0532,
      "step": 995600
    },
    {
      "epoch": 213.48627787307032,
      "grad_norm": 5.707461357116699,
      "learning_rate": 1.1703884615384616e-05,
      "loss": 3.0854,
      "step": 995700
    },
    {
      "epoch": 213.50771869639794,
      "grad_norm": 6.02621603012085,
      "learning_rate": 1.1700038461538462e-05,
      "loss": 3.0856,
      "step": 995800
    },
    {
      "epoch": 213.52915951972557,
      "grad_norm": 6.549895763397217,
      "learning_rate": 1.1696192307692307e-05,
      "loss": 3.0865,
      "step": 995900
    },
    {
      "epoch": 213.55060034305316,
      "grad_norm": 6.385081768035889,
      "learning_rate": 1.1692346153846154e-05,
      "loss": 3.0618,
      "step": 996000
    },
    {
      "epoch": 213.57204116638079,
      "grad_norm": 5.960798740386963,
      "learning_rate": 1.1688500000000002e-05,
      "loss": 3.0729,
      "step": 996100
    },
    {
      "epoch": 213.5934819897084,
      "grad_norm": 6.607823848724365,
      "learning_rate": 1.1684653846153847e-05,
      "loss": 3.0323,
      "step": 996200
    },
    {
      "epoch": 213.61492281303603,
      "grad_norm": 5.7923197746276855,
      "learning_rate": 1.1680807692307693e-05,
      "loss": 3.1053,
      "step": 996300
    },
    {
      "epoch": 213.63636363636363,
      "grad_norm": 6.066485404968262,
      "learning_rate": 1.167696153846154e-05,
      "loss": 3.0749,
      "step": 996400
    },
    {
      "epoch": 213.65780445969125,
      "grad_norm": 5.925683498382568,
      "learning_rate": 1.1673115384615386e-05,
      "loss": 3.0557,
      "step": 996500
    },
    {
      "epoch": 213.67924528301887,
      "grad_norm": 6.576035022735596,
      "learning_rate": 1.1669269230769231e-05,
      "loss": 3.1,
      "step": 996600
    },
    {
      "epoch": 213.7006861063465,
      "grad_norm": 6.0381669998168945,
      "learning_rate": 1.1665423076923078e-05,
      "loss": 3.0771,
      "step": 996700
    },
    {
      "epoch": 213.7221269296741,
      "grad_norm": 6.082587718963623,
      "learning_rate": 1.1661576923076924e-05,
      "loss": 3.1039,
      "step": 996800
    },
    {
      "epoch": 213.7435677530017,
      "grad_norm": 6.638763904571533,
      "learning_rate": 1.165773076923077e-05,
      "loss": 3.1008,
      "step": 996900
    },
    {
      "epoch": 213.76500857632934,
      "grad_norm": 6.060217380523682,
      "learning_rate": 1.1653884615384617e-05,
      "loss": 3.0726,
      "step": 997000
    },
    {
      "epoch": 213.78644939965696,
      "grad_norm": 5.778170108795166,
      "learning_rate": 1.1650038461538462e-05,
      "loss": 3.0441,
      "step": 997100
    },
    {
      "epoch": 213.80789022298455,
      "grad_norm": 6.1403350830078125,
      "learning_rate": 1.1646192307692308e-05,
      "loss": 3.0936,
      "step": 997200
    },
    {
      "epoch": 213.82933104631218,
      "grad_norm": 6.312461853027344,
      "learning_rate": 1.1642346153846155e-05,
      "loss": 3.0612,
      "step": 997300
    },
    {
      "epoch": 213.8507718696398,
      "grad_norm": 6.33435583114624,
      "learning_rate": 1.16385e-05,
      "loss": 3.0567,
      "step": 997400
    },
    {
      "epoch": 213.87221269296742,
      "grad_norm": 6.65423059463501,
      "learning_rate": 1.1634653846153846e-05,
      "loss": 3.0961,
      "step": 997500
    },
    {
      "epoch": 213.89365351629502,
      "grad_norm": 6.340635776519775,
      "learning_rate": 1.1630807692307692e-05,
      "loss": 3.066,
      "step": 997600
    },
    {
      "epoch": 213.91509433962264,
      "grad_norm": 6.276333808898926,
      "learning_rate": 1.1626961538461539e-05,
      "loss": 3.0929,
      "step": 997700
    },
    {
      "epoch": 213.93653516295026,
      "grad_norm": 6.294902801513672,
      "learning_rate": 1.1623115384615384e-05,
      "loss": 3.0536,
      "step": 997800
    },
    {
      "epoch": 213.9579759862779,
      "grad_norm": 5.834089756011963,
      "learning_rate": 1.1619269230769232e-05,
      "loss": 3.082,
      "step": 997900
    },
    {
      "epoch": 213.97941680960548,
      "grad_norm": 6.544713973999023,
      "learning_rate": 1.1615423076923077e-05,
      "loss": 3.0889,
      "step": 998000
    },
    {
      "epoch": 214.0008576329331,
      "grad_norm": 5.821066379547119,
      "learning_rate": 1.1611576923076924e-05,
      "loss": 3.0514,
      "step": 998100
    },
    {
      "epoch": 214.02229845626073,
      "grad_norm": 5.7490715980529785,
      "learning_rate": 1.160773076923077e-05,
      "loss": 3.0364,
      "step": 998200
    },
    {
      "epoch": 214.04373927958832,
      "grad_norm": 6.212000370025635,
      "learning_rate": 1.1603884615384617e-05,
      "loss": 3.0475,
      "step": 998300
    },
    {
      "epoch": 214.06518010291595,
      "grad_norm": 6.110836029052734,
      "learning_rate": 1.1600038461538463e-05,
      "loss": 3.0596,
      "step": 998400
    },
    {
      "epoch": 214.08662092624357,
      "grad_norm": 5.8817138671875,
      "learning_rate": 1.1596192307692308e-05,
      "loss": 3.0374,
      "step": 998500
    },
    {
      "epoch": 214.1080617495712,
      "grad_norm": 6.21927547454834,
      "learning_rate": 1.1592346153846155e-05,
      "loss": 3.0502,
      "step": 998600
    },
    {
      "epoch": 214.1295025728988,
      "grad_norm": 6.156095504760742,
      "learning_rate": 1.1588500000000001e-05,
      "loss": 3.0945,
      "step": 998700
    },
    {
      "epoch": 214.1509433962264,
      "grad_norm": 6.205269813537598,
      "learning_rate": 1.1584653846153846e-05,
      "loss": 3.06,
      "step": 998800
    },
    {
      "epoch": 214.17238421955403,
      "grad_norm": 6.3335041999816895,
      "learning_rate": 1.1580807692307692e-05,
      "loss": 3.082,
      "step": 998900
    },
    {
      "epoch": 214.19382504288166,
      "grad_norm": 6.33330774307251,
      "learning_rate": 1.157696153846154e-05,
      "loss": 3.0462,
      "step": 999000
    },
    {
      "epoch": 214.21526586620925,
      "grad_norm": 6.323482513427734,
      "learning_rate": 1.1573115384615385e-05,
      "loss": 3.0752,
      "step": 999100
    },
    {
      "epoch": 214.23670668953687,
      "grad_norm": 5.831322193145752,
      "learning_rate": 1.156926923076923e-05,
      "loss": 3.0945,
      "step": 999200
    },
    {
      "epoch": 214.2581475128645,
      "grad_norm": 6.482926368713379,
      "learning_rate": 1.1565423076923078e-05,
      "loss": 3.0844,
      "step": 999300
    },
    {
      "epoch": 214.27958833619212,
      "grad_norm": 6.064988613128662,
      "learning_rate": 1.1561576923076923e-05,
      "loss": 3.0991,
      "step": 999400
    },
    {
      "epoch": 214.30102915951971,
      "grad_norm": 6.029490947723389,
      "learning_rate": 1.1557730769230769e-05,
      "loss": 3.0636,
      "step": 999500
    },
    {
      "epoch": 214.32246998284734,
      "grad_norm": 5.980589866638184,
      "learning_rate": 1.1553884615384616e-05,
      "loss": 3.0352,
      "step": 999600
    },
    {
      "epoch": 214.34391080617496,
      "grad_norm": 6.092403411865234,
      "learning_rate": 1.1550038461538461e-05,
      "loss": 3.0228,
      "step": 999700
    },
    {
      "epoch": 214.36535162950258,
      "grad_norm": 6.117059230804443,
      "learning_rate": 1.1546192307692309e-05,
      "loss": 3.0979,
      "step": 999800
    },
    {
      "epoch": 214.38679245283018,
      "grad_norm": 6.571574687957764,
      "learning_rate": 1.1542346153846154e-05,
      "loss": 3.1199,
      "step": 999900
    },
    {
      "epoch": 214.4082332761578,
      "grad_norm": 6.185457706451416,
      "learning_rate": 1.1538500000000001e-05,
      "loss": 3.0768,
      "step": 1000000
    },
    {
      "epoch": 214.42967409948542,
      "grad_norm": 6.110822677612305,
      "learning_rate": 1.1534653846153847e-05,
      "loss": 3.0139,
      "step": 1000100
    },
    {
      "epoch": 214.45111492281305,
      "grad_norm": 6.25421667098999,
      "learning_rate": 1.1530807692307694e-05,
      "loss": 3.0477,
      "step": 1000200
    },
    {
      "epoch": 214.47255574614064,
      "grad_norm": 5.762724876403809,
      "learning_rate": 1.152696153846154e-05,
      "loss": 3.08,
      "step": 1000300
    },
    {
      "epoch": 214.49399656946827,
      "grad_norm": 5.927839756011963,
      "learning_rate": 1.1523115384615385e-05,
      "loss": 3.0692,
      "step": 1000400
    },
    {
      "epoch": 214.5154373927959,
      "grad_norm": 6.101827144622803,
      "learning_rate": 1.151926923076923e-05,
      "loss": 3.0387,
      "step": 1000500
    },
    {
      "epoch": 214.5368782161235,
      "grad_norm": 5.897154808044434,
      "learning_rate": 1.1515423076923078e-05,
      "loss": 3.038,
      "step": 1000600
    },
    {
      "epoch": 214.5583190394511,
      "grad_norm": 6.750113487243652,
      "learning_rate": 1.1511576923076924e-05,
      "loss": 3.0149,
      "step": 1000700
    },
    {
      "epoch": 214.57975986277873,
      "grad_norm": 6.258925437927246,
      "learning_rate": 1.1507730769230769e-05,
      "loss": 3.0638,
      "step": 1000800
    },
    {
      "epoch": 214.60120068610635,
      "grad_norm": 6.3284010887146,
      "learning_rate": 1.1503884615384616e-05,
      "loss": 3.0499,
      "step": 1000900
    },
    {
      "epoch": 214.62264150943398,
      "grad_norm": 5.442476272583008,
      "learning_rate": 1.1500038461538462e-05,
      "loss": 3.0723,
      "step": 1001000
    },
    {
      "epoch": 214.64408233276157,
      "grad_norm": 5.99205207824707,
      "learning_rate": 1.1496192307692307e-05,
      "loss": 3.0676,
      "step": 1001100
    },
    {
      "epoch": 214.6655231560892,
      "grad_norm": 5.904644012451172,
      "learning_rate": 1.1492346153846155e-05,
      "loss": 3.1285,
      "step": 1001200
    },
    {
      "epoch": 214.68696397941682,
      "grad_norm": 6.800357341766357,
      "learning_rate": 1.14885e-05,
      "loss": 3.0528,
      "step": 1001300
    },
    {
      "epoch": 214.70840480274444,
      "grad_norm": 6.0967206954956055,
      "learning_rate": 1.1484653846153846e-05,
      "loss": 3.0968,
      "step": 1001400
    },
    {
      "epoch": 214.72984562607203,
      "grad_norm": 6.796754837036133,
      "learning_rate": 1.1480807692307693e-05,
      "loss": 3.107,
      "step": 1001500
    },
    {
      "epoch": 214.75128644939966,
      "grad_norm": 6.147134304046631,
      "learning_rate": 1.1476961538461539e-05,
      "loss": 3.0717,
      "step": 1001600
    },
    {
      "epoch": 214.77272727272728,
      "grad_norm": 6.097899436950684,
      "learning_rate": 1.1473115384615386e-05,
      "loss": 3.0719,
      "step": 1001700
    },
    {
      "epoch": 214.79416809605488,
      "grad_norm": 6.532488822937012,
      "learning_rate": 1.1469269230769231e-05,
      "loss": 3.0559,
      "step": 1001800
    },
    {
      "epoch": 214.8156089193825,
      "grad_norm": 5.596713542938232,
      "learning_rate": 1.1465423076923079e-05,
      "loss": 3.0779,
      "step": 1001900
    },
    {
      "epoch": 214.83704974271012,
      "grad_norm": 6.471926212310791,
      "learning_rate": 1.1461576923076924e-05,
      "loss": 3.0574,
      "step": 1002000
    },
    {
      "epoch": 214.85849056603774,
      "grad_norm": 6.249788284301758,
      "learning_rate": 1.145773076923077e-05,
      "loss": 3.021,
      "step": 1002100
    },
    {
      "epoch": 214.87993138936534,
      "grad_norm": 6.421308517456055,
      "learning_rate": 1.1453884615384617e-05,
      "loss": 3.0987,
      "step": 1002200
    },
    {
      "epoch": 214.90137221269296,
      "grad_norm": 5.9584808349609375,
      "learning_rate": 1.1450038461538462e-05,
      "loss": 3.0782,
      "step": 1002300
    },
    {
      "epoch": 214.92281303602059,
      "grad_norm": 6.402453899383545,
      "learning_rate": 1.1446192307692308e-05,
      "loss": 3.1178,
      "step": 1002400
    },
    {
      "epoch": 214.9442538593482,
      "grad_norm": 5.955925941467285,
      "learning_rate": 1.1442346153846155e-05,
      "loss": 3.0411,
      "step": 1002500
    },
    {
      "epoch": 214.9656946826758,
      "grad_norm": 5.917044639587402,
      "learning_rate": 1.14385e-05,
      "loss": 3.1188,
      "step": 1002600
    },
    {
      "epoch": 214.98713550600343,
      "grad_norm": 6.308122634887695,
      "learning_rate": 1.1434653846153846e-05,
      "loss": 3.0845,
      "step": 1002700
    },
    {
      "epoch": 215.00857632933105,
      "grad_norm": 6.30023717880249,
      "learning_rate": 1.1430807692307693e-05,
      "loss": 3.0943,
      "step": 1002800
    },
    {
      "epoch": 215.03001715265867,
      "grad_norm": 6.156936168670654,
      "learning_rate": 1.1426961538461539e-05,
      "loss": 3.0376,
      "step": 1002900
    },
    {
      "epoch": 215.05145797598627,
      "grad_norm": 6.4819183349609375,
      "learning_rate": 1.1423115384615385e-05,
      "loss": 3.0383,
      "step": 1003000
    },
    {
      "epoch": 215.0728987993139,
      "grad_norm": 6.245903968811035,
      "learning_rate": 1.1419269230769232e-05,
      "loss": 3.1022,
      "step": 1003100
    },
    {
      "epoch": 215.0943396226415,
      "grad_norm": 6.296399116516113,
      "learning_rate": 1.1415423076923077e-05,
      "loss": 3.0504,
      "step": 1003200
    },
    {
      "epoch": 215.11578044596914,
      "grad_norm": 6.217278957366943,
      "learning_rate": 1.1411576923076923e-05,
      "loss": 3.0499,
      "step": 1003300
    },
    {
      "epoch": 215.13722126929673,
      "grad_norm": 6.046844482421875,
      "learning_rate": 1.1407730769230768e-05,
      "loss": 3.0473,
      "step": 1003400
    },
    {
      "epoch": 215.15866209262435,
      "grad_norm": 6.5462493896484375,
      "learning_rate": 1.1403884615384616e-05,
      "loss": 3.1433,
      "step": 1003500
    },
    {
      "epoch": 215.18010291595198,
      "grad_norm": 5.935421466827393,
      "learning_rate": 1.1400038461538461e-05,
      "loss": 3.0664,
      "step": 1003600
    },
    {
      "epoch": 215.2015437392796,
      "grad_norm": 5.600144386291504,
      "learning_rate": 1.1396192307692308e-05,
      "loss": 3.0617,
      "step": 1003700
    },
    {
      "epoch": 215.2229845626072,
      "grad_norm": 5.83920431137085,
      "learning_rate": 1.1392346153846156e-05,
      "loss": 3.0045,
      "step": 1003800
    },
    {
      "epoch": 215.24442538593482,
      "grad_norm": 5.810780048370361,
      "learning_rate": 1.1388500000000001e-05,
      "loss": 3.055,
      "step": 1003900
    },
    {
      "epoch": 215.26586620926244,
      "grad_norm": 6.201728343963623,
      "learning_rate": 1.1384653846153847e-05,
      "loss": 3.0472,
      "step": 1004000
    },
    {
      "epoch": 215.28730703259006,
      "grad_norm": 5.761021614074707,
      "learning_rate": 1.1380807692307694e-05,
      "loss": 3.0679,
      "step": 1004100
    },
    {
      "epoch": 215.30874785591766,
      "grad_norm": 6.3063154220581055,
      "learning_rate": 1.137696153846154e-05,
      "loss": 3.0759,
      "step": 1004200
    },
    {
      "epoch": 215.33018867924528,
      "grad_norm": 5.815072536468506,
      "learning_rate": 1.1373115384615385e-05,
      "loss": 3.0458,
      "step": 1004300
    },
    {
      "epoch": 215.3516295025729,
      "grad_norm": 6.056726455688477,
      "learning_rate": 1.1369269230769232e-05,
      "loss": 3.0363,
      "step": 1004400
    },
    {
      "epoch": 215.37307032590053,
      "grad_norm": 6.06543493270874,
      "learning_rate": 1.1365423076923078e-05,
      "loss": 3.0378,
      "step": 1004500
    },
    {
      "epoch": 215.39451114922812,
      "grad_norm": 6.400965213775635,
      "learning_rate": 1.1361576923076923e-05,
      "loss": 3.1111,
      "step": 1004600
    },
    {
      "epoch": 215.41595197255575,
      "grad_norm": 5.9068169593811035,
      "learning_rate": 1.1357730769230769e-05,
      "loss": 3.0729,
      "step": 1004700
    },
    {
      "epoch": 215.43739279588337,
      "grad_norm": 6.231800556182861,
      "learning_rate": 1.1353884615384616e-05,
      "loss": 3.058,
      "step": 1004800
    },
    {
      "epoch": 215.45883361921096,
      "grad_norm": 6.360931396484375,
      "learning_rate": 1.1350038461538462e-05,
      "loss": 3.0649,
      "step": 1004900
    },
    {
      "epoch": 215.4802744425386,
      "grad_norm": 5.728534698486328,
      "learning_rate": 1.1346192307692307e-05,
      "loss": 3.0478,
      "step": 1005000
    },
    {
      "epoch": 215.5017152658662,
      "grad_norm": 6.369682788848877,
      "learning_rate": 1.1342346153846154e-05,
      "loss": 3.0584,
      "step": 1005100
    },
    {
      "epoch": 215.52315608919383,
      "grad_norm": 6.494111061096191,
      "learning_rate": 1.13385e-05,
      "loss": 3.0971,
      "step": 1005200
    },
    {
      "epoch": 215.54459691252143,
      "grad_norm": 5.884003162384033,
      "learning_rate": 1.1334653846153845e-05,
      "loss": 3.0814,
      "step": 1005300
    },
    {
      "epoch": 215.56603773584905,
      "grad_norm": 5.969605922698975,
      "learning_rate": 1.1330807692307693e-05,
      "loss": 3.1173,
      "step": 1005400
    },
    {
      "epoch": 215.58747855917667,
      "grad_norm": 6.197188377380371,
      "learning_rate": 1.1326961538461538e-05,
      "loss": 3.0609,
      "step": 1005500
    },
    {
      "epoch": 215.6089193825043,
      "grad_norm": 6.415266036987305,
      "learning_rate": 1.1323115384615385e-05,
      "loss": 3.0278,
      "step": 1005600
    },
    {
      "epoch": 215.6303602058319,
      "grad_norm": 5.98850154876709,
      "learning_rate": 1.1319269230769233e-05,
      "loss": 3.0888,
      "step": 1005700
    },
    {
      "epoch": 215.65180102915951,
      "grad_norm": 6.196114540100098,
      "learning_rate": 1.1315423076923078e-05,
      "loss": 3.0901,
      "step": 1005800
    },
    {
      "epoch": 215.67324185248714,
      "grad_norm": 6.317663192749023,
      "learning_rate": 1.1311576923076924e-05,
      "loss": 3.1019,
      "step": 1005900
    },
    {
      "epoch": 215.69468267581476,
      "grad_norm": 5.823972702026367,
      "learning_rate": 1.1307730769230771e-05,
      "loss": 3.0913,
      "step": 1006000
    },
    {
      "epoch": 215.71612349914236,
      "grad_norm": 6.579668045043945,
      "learning_rate": 1.1303884615384617e-05,
      "loss": 3.0526,
      "step": 1006100
    },
    {
      "epoch": 215.73756432246998,
      "grad_norm": 6.370328903198242,
      "learning_rate": 1.1300038461538462e-05,
      "loss": 3.0617,
      "step": 1006200
    },
    {
      "epoch": 215.7590051457976,
      "grad_norm": 6.045907497406006,
      "learning_rate": 1.1296192307692308e-05,
      "loss": 3.0381,
      "step": 1006300
    },
    {
      "epoch": 215.78044596912522,
      "grad_norm": 6.470372676849365,
      "learning_rate": 1.1292346153846155e-05,
      "loss": 3.0962,
      "step": 1006400
    },
    {
      "epoch": 215.80188679245282,
      "grad_norm": 5.967504501342773,
      "learning_rate": 1.12885e-05,
      "loss": 3.0849,
      "step": 1006500
    },
    {
      "epoch": 1123.4375,
      "grad_norm": 56.94472885131836,
      "learning_rate": 1.1284653846153846e-05,
      "loss": 6.9401,
      "step": 1006600
    },
    {
      "epoch": 1123.549107142857,
      "grad_norm": 59.68192672729492,
      "learning_rate": 1.1280807692307693e-05,
      "loss": 6.2661,
      "step": 1006700
    },
    {
      "epoch": 1123.6607142857142,
      "grad_norm": 49.957923889160156,
      "learning_rate": 1.1276961538461539e-05,
      "loss": 6.0122,
      "step": 1006800
    },
    {
      "epoch": 1123.7723214285713,
      "grad_norm": 43.603458404541016,
      "learning_rate": 1.1273115384615384e-05,
      "loss": 5.8418,
      "step": 1006900
    },
    {
      "epoch": 1123.8839285714287,
      "grad_norm": 39.08515167236328,
      "learning_rate": 1.1269269230769232e-05,
      "loss": 5.9567,
      "step": 1007000
    },
    {
      "epoch": 1123.9955357142858,
      "grad_norm": 42.415313720703125,
      "learning_rate": 1.1265423076923077e-05,
      "loss": 5.8633,
      "step": 1007100
    },
    {
      "epoch": 1124.107142857143,
      "grad_norm": 35.6027946472168,
      "learning_rate": 1.1261576923076923e-05,
      "loss": 5.8517,
      "step": 1007200
    },
    {
      "epoch": 1124.21875,
      "grad_norm": 37.07344055175781,
      "learning_rate": 1.125773076923077e-05,
      "loss": 5.7717,
      "step": 1007300
    },
    {
      "epoch": 1124.330357142857,
      "grad_norm": 35.57636260986328,
      "learning_rate": 1.1253884615384615e-05,
      "loss": 5.5367,
      "step": 1007400
    },
    {
      "epoch": 1124.4419642857142,
      "grad_norm": 34.137821197509766,
      "learning_rate": 1.1250038461538463e-05,
      "loss": 5.8238,
      "step": 1007500
    },
    {
      "epoch": 1124.5535714285713,
      "grad_norm": 29.726476669311523,
      "learning_rate": 1.1246192307692308e-05,
      "loss": 5.7553,
      "step": 1007600
    },
    {
      "epoch": 1124.6651785714287,
      "grad_norm": 33.98317337036133,
      "learning_rate": 1.1242346153846155e-05,
      "loss": 5.6834,
      "step": 1007700
    },
    {
      "epoch": 1124.7767857142858,
      "grad_norm": 34.79981994628906,
      "learning_rate": 1.1238500000000001e-05,
      "loss": 5.6114,
      "step": 1007800
    },
    {
      "epoch": 1124.888392857143,
      "grad_norm": 31.519054412841797,
      "learning_rate": 1.1234653846153846e-05,
      "loss": 5.724,
      "step": 1007900
    },
    {
      "epoch": 1125.0,
      "grad_norm": 34.477806091308594,
      "learning_rate": 1.1230807692307694e-05,
      "loss": 5.5706,
      "step": 1008000
    },
    {
      "epoch": 1125.111607142857,
      "grad_norm": 33.1868782043457,
      "learning_rate": 1.122696153846154e-05,
      "loss": 5.3023,
      "step": 1008100
    },
    {
      "epoch": 1125.2232142857142,
      "grad_norm": 32.105194091796875,
      "learning_rate": 1.1223115384615385e-05,
      "loss": 5.4111,
      "step": 1008200
    },
    {
      "epoch": 1125.3348214285713,
      "grad_norm": 31.537242889404297,
      "learning_rate": 1.1219269230769232e-05,
      "loss": 5.5846,
      "step": 1008300
    },
    {
      "epoch": 1125.4464285714287,
      "grad_norm": 32.08173751831055,
      "learning_rate": 1.1215423076923078e-05,
      "loss": 5.5683,
      "step": 1008400
    },
    {
      "epoch": 1125.5580357142858,
      "grad_norm": 30.56196403503418,
      "learning_rate": 1.1211576923076923e-05,
      "loss": 5.4211,
      "step": 1008500
    },
    {
      "epoch": 1125.669642857143,
      "grad_norm": 30.619503021240234,
      "learning_rate": 1.120773076923077e-05,
      "loss": 5.5217,
      "step": 1008600
    },
    {
      "epoch": 1125.78125,
      "grad_norm": 27.290084838867188,
      "learning_rate": 1.1203884615384616e-05,
      "loss": 5.4833,
      "step": 1008700
    },
    {
      "epoch": 1125.892857142857,
      "grad_norm": 29.37982749938965,
      "learning_rate": 1.1200038461538461e-05,
      "loss": 5.5107,
      "step": 1008800
    },
    {
      "epoch": 1126.0044642857142,
      "grad_norm": 27.99924659729004,
      "learning_rate": 1.1196192307692307e-05,
      "loss": 5.3749,
      "step": 1008900
    },
    {
      "epoch": 1126.1160714285713,
      "grad_norm": 27.692163467407227,
      "learning_rate": 1.1192346153846154e-05,
      "loss": 5.2079,
      "step": 1009000
    },
    {
      "epoch": 1126.2276785714287,
      "grad_norm": 34.87916946411133,
      "learning_rate": 1.11885e-05,
      "loss": 5.4024,
      "step": 1009100
    },
    {
      "epoch": 1126.3392857142858,
      "grad_norm": 30.459678649902344,
      "learning_rate": 1.1184653846153847e-05,
      "loss": 5.3345,
      "step": 1009200
    },
    {
      "epoch": 1126.450892857143,
      "grad_norm": 27.830551147460938,
      "learning_rate": 1.1180807692307692e-05,
      "loss": 5.4421,
      "step": 1009300
    },
    {
      "epoch": 1126.5625,
      "grad_norm": 24.43964385986328,
      "learning_rate": 1.117696153846154e-05,
      "loss": 5.3625,
      "step": 1009400
    },
    {
      "epoch": 1126.674107142857,
      "grad_norm": 26.38866424560547,
      "learning_rate": 1.1173115384615385e-05,
      "loss": 5.3738,
      "step": 1009500
    },
    {
      "epoch": 1126.7857142857142,
      "grad_norm": 27.054821014404297,
      "learning_rate": 1.1169269230769232e-05,
      "loss": 5.219,
      "step": 1009600
    },
    {
      "epoch": 1126.8973214285713,
      "grad_norm": 28.42287826538086,
      "learning_rate": 1.1165423076923078e-05,
      "loss": 5.3809,
      "step": 1009700
    },
    {
      "epoch": 1127.0089285714287,
      "grad_norm": 27.419883728027344,
      "learning_rate": 1.1161576923076924e-05,
      "loss": 5.3924,
      "step": 1009800
    },
    {
      "epoch": 1127.1205357142858,
      "grad_norm": 28.5760555267334,
      "learning_rate": 1.115773076923077e-05,
      "loss": 5.3186,
      "step": 1009900
    },
    {
      "epoch": 1127.232142857143,
      "grad_norm": 24.140844345092773,
      "learning_rate": 1.1153884615384616e-05,
      "loss": 5.3047,
      "step": 1010000
    },
    {
      "epoch": 1127.34375,
      "grad_norm": 25.812402725219727,
      "learning_rate": 1.1150038461538462e-05,
      "loss": 5.1849,
      "step": 1010100
    },
    {
      "epoch": 1127.455357142857,
      "grad_norm": 24.898639678955078,
      "learning_rate": 1.1146192307692309e-05,
      "loss": 5.1314,
      "step": 1010200
    },
    {
      "epoch": 1127.5669642857142,
      "grad_norm": 27.114896774291992,
      "learning_rate": 1.1142346153846155e-05,
      "loss": 5.1568,
      "step": 1010300
    },
    {
      "epoch": 1127.6785714285713,
      "grad_norm": 28.62980842590332,
      "learning_rate": 1.11385e-05,
      "loss": 5.2232,
      "step": 1010400
    },
    {
      "epoch": 1127.7901785714287,
      "grad_norm": 26.36983299255371,
      "learning_rate": 1.1134653846153846e-05,
      "loss": 5.4003,
      "step": 1010500
    },
    {
      "epoch": 1127.9017857142858,
      "grad_norm": 27.84506607055664,
      "learning_rate": 1.1130807692307693e-05,
      "loss": 5.2374,
      "step": 1010600
    },
    {
      "epoch": 1128.013392857143,
      "grad_norm": 26.662635803222656,
      "learning_rate": 1.1126961538461538e-05,
      "loss": 5.2585,
      "step": 1010700
    },
    {
      "epoch": 1128.125,
      "grad_norm": 25.32911491394043,
      "learning_rate": 1.1123115384615384e-05,
      "loss": 5.2158,
      "step": 1010800
    },
    {
      "epoch": 1128.236607142857,
      "grad_norm": 26.778564453125,
      "learning_rate": 1.1119269230769231e-05,
      "loss": 5.1853,
      "step": 1010900
    },
    {
      "epoch": 1128.3482142857142,
      "grad_norm": 24.55463218688965,
      "learning_rate": 1.1115423076923077e-05,
      "loss": 5.1797,
      "step": 1011000
    },
    {
      "epoch": 1128.4598214285713,
      "grad_norm": 24.766632080078125,
      "learning_rate": 1.1111576923076922e-05,
      "loss": 4.9906,
      "step": 1011100
    },
    {
      "epoch": 1128.5714285714287,
      "grad_norm": 24.62323760986328,
      "learning_rate": 1.110773076923077e-05,
      "loss": 5.1374,
      "step": 1011200
    },
    {
      "epoch": 1128.6830357142858,
      "grad_norm": 28.065744400024414,
      "learning_rate": 1.1103884615384617e-05,
      "loss": 5.2197,
      "step": 1011300
    },
    {
      "epoch": 1128.794642857143,
      "grad_norm": 27.34173011779785,
      "learning_rate": 1.1100038461538462e-05,
      "loss": 5.1196,
      "step": 1011400
    },
    {
      "epoch": 1128.90625,
      "grad_norm": 25.555986404418945,
      "learning_rate": 1.109619230769231e-05,
      "loss": 5.2148,
      "step": 1011500
    },
    {
      "epoch": 1129.017857142857,
      "grad_norm": 24.42475700378418,
      "learning_rate": 1.1092346153846155e-05,
      "loss": 5.1752,
      "step": 1011600
    },
    {
      "epoch": 1129.1294642857142,
      "grad_norm": 23.534021377563477,
      "learning_rate": 1.10885e-05,
      "loss": 5.1003,
      "step": 1011700
    },
    {
      "epoch": 1129.2410714285713,
      "grad_norm": 23.70734977722168,
      "learning_rate": 1.1084653846153846e-05,
      "loss": 5.0673,
      "step": 1011800
    },
    {
      "epoch": 1129.3526785714287,
      "grad_norm": 23.88711166381836,
      "learning_rate": 1.1080807692307693e-05,
      "loss": 5.1704,
      "step": 1011900
    },
    {
      "epoch": 1129.4642857142858,
      "grad_norm": 22.478410720825195,
      "learning_rate": 1.1076961538461539e-05,
      "loss": 5.0372,
      "step": 1012000
    },
    {
      "epoch": 1129.575892857143,
      "grad_norm": 27.339101791381836,
      "learning_rate": 1.1073115384615384e-05,
      "loss": 5.2082,
      "step": 1012100
    },
    {
      "epoch": 1129.6875,
      "grad_norm": 24.92356300354004,
      "learning_rate": 1.1069269230769232e-05,
      "loss": 5.1237,
      "step": 1012200
    },
    {
      "epoch": 1129.799107142857,
      "grad_norm": 24.920886993408203,
      "learning_rate": 1.1065423076923077e-05,
      "loss": 5.1523,
      "step": 1012300
    },
    {
      "epoch": 1129.9107142857142,
      "grad_norm": 23.19301414489746,
      "learning_rate": 1.1061576923076923e-05,
      "loss": 5.2483,
      "step": 1012400
    },
    {
      "epoch": 1130.0223214285713,
      "grad_norm": 23.861543655395508,
      "learning_rate": 1.105773076923077e-05,
      "loss": 5.0726,
      "step": 1012500
    },
    {
      "epoch": 1130.1339285714287,
      "grad_norm": 26.370616912841797,
      "learning_rate": 1.1053884615384616e-05,
      "loss": 5.1819,
      "step": 1012600
    },
    {
      "epoch": 1130.2455357142858,
      "grad_norm": 23.599342346191406,
      "learning_rate": 1.1050038461538461e-05,
      "loss": 5.1609,
      "step": 1012700
    },
    {
      "epoch": 1130.357142857143,
      "grad_norm": 24.733726501464844,
      "learning_rate": 1.1046192307692308e-05,
      "loss": 5.136,
      "step": 1012800
    },
    {
      "epoch": 1130.46875,
      "grad_norm": 22.744216918945312,
      "learning_rate": 1.1042346153846154e-05,
      "loss": 5.0547,
      "step": 1012900
    },
    {
      "epoch": 1130.580357142857,
      "grad_norm": 21.667654037475586,
      "learning_rate": 1.10385e-05,
      "loss": 5.0672,
      "step": 1013000
    },
    {
      "epoch": 1130.6919642857142,
      "grad_norm": 23.25306510925293,
      "learning_rate": 1.1034653846153847e-05,
      "loss": 5.0875,
      "step": 1013100
    },
    {
      "epoch": 1130.8035714285713,
      "grad_norm": 22.131433486938477,
      "learning_rate": 1.1030807692307694e-05,
      "loss": 5.1247,
      "step": 1013200
    },
    {
      "epoch": 1130.9151785714287,
      "grad_norm": 25.23044204711914,
      "learning_rate": 1.102696153846154e-05,
      "loss": 5.1121,
      "step": 1013300
    },
    {
      "epoch": 1131.0267857142858,
      "grad_norm": 23.24176597595215,
      "learning_rate": 1.1023115384615385e-05,
      "loss": 5.0713,
      "step": 1013400
    },
    {
      "epoch": 1131.138392857143,
      "grad_norm": 23.305156707763672,
      "learning_rate": 1.1019269230769232e-05,
      "loss": 5.1244,
      "step": 1013500
    },
    {
      "epoch": 2.3400777563327084,
      "grad_norm": 6.642641544342041,
      "learning_rate": 1.1015423076923078e-05,
      "loss": 6.7544,
      "step": 1013600
    },
    {
      "epoch": 2.3403086243039333,
      "grad_norm": 6.060852527618408,
      "learning_rate": 1.1011576923076923e-05,
      "loss": 5.6905,
      "step": 1013700
    },
    {
      "epoch": 2.3405394922751577,
      "grad_norm": 5.961709022521973,
      "learning_rate": 1.100773076923077e-05,
      "loss": 5.5411,
      "step": 1013800
    },
    {
      "epoch": 2.340770360246382,
      "grad_norm": 5.671079158782959,
      "learning_rate": 1.1003884615384616e-05,
      "loss": 5.5071,
      "step": 1013900
    },
    {
      "epoch": 2.341001228217607,
      "grad_norm": 5.210836887359619,
      "learning_rate": 1.1000038461538462e-05,
      "loss": 5.4585,
      "step": 1014000
    },
    {
      "epoch": 2.3412320961888318,
      "grad_norm": 7.882164478302002,
      "learning_rate": 1.0996192307692309e-05,
      "loss": 5.3903,
      "step": 1014100
    },
    {
      "epoch": 2.341462964160056,
      "grad_norm": 7.521362781524658,
      "learning_rate": 1.0992346153846154e-05,
      "loss": 5.4279,
      "step": 1014200
    },
    {
      "epoch": 2.3416938321312806,
      "grad_norm": 9.780169486999512,
      "learning_rate": 1.09885e-05,
      "loss": 5.3638,
      "step": 1014300
    },
    {
      "epoch": 2.3419247001025054,
      "grad_norm": 6.643026351928711,
      "learning_rate": 1.0984653846153847e-05,
      "loss": 5.4207,
      "step": 1014400
    },
    {
      "epoch": 2.34215556807373,
      "grad_norm": 5.51027774810791,
      "learning_rate": 1.0980807692307693e-05,
      "loss": 5.394,
      "step": 1014500
    },
    {
      "epoch": 2.3423864360449547,
      "grad_norm": 5.030416011810303,
      "learning_rate": 1.0976961538461538e-05,
      "loss": 5.3528,
      "step": 1014600
    },
    {
      "epoch": 2.342617304016179,
      "grad_norm": 5.315009117126465,
      "learning_rate": 1.0973115384615384e-05,
      "loss": 5.3853,
      "step": 1014700
    },
    {
      "epoch": 2.342848171987404,
      "grad_norm": 7.086097240447998,
      "learning_rate": 1.0969269230769231e-05,
      "loss": 5.2754,
      "step": 1014800
    },
    {
      "epoch": 2.3430790399586283,
      "grad_norm": 5.503729820251465,
      "learning_rate": 1.0965423076923077e-05,
      "loss": 5.2647,
      "step": 1014900
    },
    {
      "epoch": 2.343309907929853,
      "grad_norm": 5.325605392456055,
      "learning_rate": 1.0961576923076924e-05,
      "loss": 5.2895,
      "step": 1015000
    },
    {
      "epoch": 2.3435407759010776,
      "grad_norm": 6.335894584655762,
      "learning_rate": 1.095773076923077e-05,
      "loss": 5.2618,
      "step": 1015100
    },
    {
      "epoch": 2.3437716438723024,
      "grad_norm": 7.783001899719238,
      "learning_rate": 1.0953884615384617e-05,
      "loss": 5.2528,
      "step": 1015200
    },
    {
      "epoch": 2.344002511843527,
      "grad_norm": 7.915645122528076,
      "learning_rate": 1.0950038461538462e-05,
      "loss": 5.2715,
      "step": 1015300
    },
    {
      "epoch": 2.3442333798147517,
      "grad_norm": 6.406538009643555,
      "learning_rate": 1.094619230769231e-05,
      "loss": 5.2712,
      "step": 1015400
    },
    {
      "epoch": 2.344464247785976,
      "grad_norm": 5.002671241760254,
      "learning_rate": 1.0942346153846155e-05,
      "loss": 5.2763,
      "step": 1015500
    },
    {
      "epoch": 2.344695115757201,
      "grad_norm": 6.458869457244873,
      "learning_rate": 1.09385e-05,
      "loss": 5.2338,
      "step": 1015600
    },
    {
      "epoch": 2.3449259837284253,
      "grad_norm": 5.941689968109131,
      "learning_rate": 1.0934653846153848e-05,
      "loss": 5.2831,
      "step": 1015700
    },
    {
      "epoch": 2.34515685169965,
      "grad_norm": 6.109866619110107,
      "learning_rate": 1.0930807692307693e-05,
      "loss": 5.2105,
      "step": 1015800
    },
    {
      "epoch": 2.3453877196708746,
      "grad_norm": 7.1441802978515625,
      "learning_rate": 1.0926961538461539e-05,
      "loss": 5.2528,
      "step": 1015900
    },
    {
      "epoch": 2.3456185876420994,
      "grad_norm": 6.317363262176514,
      "learning_rate": 1.0923115384615386e-05,
      "loss": 5.2487,
      "step": 1016000
    },
    {
      "epoch": 2.345849455613324,
      "grad_norm": 6.015218734741211,
      "learning_rate": 1.0919269230769231e-05,
      "loss": 5.2808,
      "step": 1016100
    },
    {
      "epoch": 2.3460803235845487,
      "grad_norm": 5.323338985443115,
      "learning_rate": 1.0915423076923077e-05,
      "loss": 5.1916,
      "step": 1016200
    },
    {
      "epoch": 2.346311191555773,
      "grad_norm": 5.433508396148682,
      "learning_rate": 1.0911576923076923e-05,
      "loss": 5.2122,
      "step": 1016300
    },
    {
      "epoch": 2.346542059526998,
      "grad_norm": 4.915225982666016,
      "learning_rate": 1.090773076923077e-05,
      "loss": 5.2366,
      "step": 1016400
    },
    {
      "epoch": 2.3467729274982223,
      "grad_norm": 5.182816028594971,
      "learning_rate": 1.0903884615384615e-05,
      "loss": 5.2127,
      "step": 1016500
    },
    {
      "epoch": 2.3470037954694467,
      "grad_norm": 8.301321983337402,
      "learning_rate": 1.0900038461538461e-05,
      "loss": 5.2019,
      "step": 1016600
    },
    {
      "epoch": 2.3472346634406716,
      "grad_norm": 5.315529823303223,
      "learning_rate": 1.0896192307692308e-05,
      "loss": 5.1885,
      "step": 1016700
    },
    {
      "epoch": 2.3474655314118964,
      "grad_norm": 6.44758415222168,
      "learning_rate": 1.0892346153846154e-05,
      "loss": 5.1916,
      "step": 1016800
    },
    {
      "epoch": 2.347696399383121,
      "grad_norm": 5.709007263183594,
      "learning_rate": 1.0888500000000001e-05,
      "loss": 5.1719,
      "step": 1016900
    },
    {
      "epoch": 2.347927267354345,
      "grad_norm": 6.295651912689209,
      "learning_rate": 1.0884653846153846e-05,
      "loss": 5.1903,
      "step": 1017000
    },
    {
      "epoch": 2.34815813532557,
      "grad_norm": 7.665955066680908,
      "learning_rate": 1.0880807692307694e-05,
      "loss": 5.2128,
      "step": 1017100
    },
    {
      "epoch": 2.3483890032967945,
      "grad_norm": 7.094480037689209,
      "learning_rate": 1.087696153846154e-05,
      "loss": 5.2102,
      "step": 1017200
    },
    {
      "epoch": 2.3486198712680193,
      "grad_norm": 6.114221572875977,
      "learning_rate": 1.0873115384615386e-05,
      "loss": 5.1723,
      "step": 1017300
    },
    {
      "epoch": 2.3488507392392437,
      "grad_norm": 6.920480251312256,
      "learning_rate": 1.0869269230769232e-05,
      "loss": 5.204,
      "step": 1017400
    },
    {
      "epoch": 2.3490816072104685,
      "grad_norm": 5.717389106750488,
      "learning_rate": 1.0865423076923077e-05,
      "loss": 5.1295,
      "step": 1017500
    },
    {
      "epoch": 2.349312475181693,
      "grad_norm": 5.345824241638184,
      "learning_rate": 1.0861576923076923e-05,
      "loss": 5.1845,
      "step": 1017600
    },
    {
      "epoch": 2.349543343152918,
      "grad_norm": 6.20293664932251,
      "learning_rate": 1.085773076923077e-05,
      "loss": 5.2031,
      "step": 1017700
    },
    {
      "epoch": 2.349774211124142,
      "grad_norm": 6.4389472007751465,
      "learning_rate": 1.0853884615384616e-05,
      "loss": 5.1866,
      "step": 1017800
    },
    {
      "epoch": 2.350005079095367,
      "grad_norm": 6.522217273712158,
      "learning_rate": 1.0850038461538461e-05,
      "loss": 5.2032,
      "step": 1017900
    },
    {
      "epoch": 2.3502359470665914,
      "grad_norm": 6.209273815155029,
      "learning_rate": 1.0846192307692309e-05,
      "loss": 5.1541,
      "step": 1018000
    },
    {
      "epoch": 2.3504668150378163,
      "grad_norm": 5.345397472381592,
      "learning_rate": 1.0842346153846154e-05,
      "loss": 5.1405,
      "step": 1018100
    },
    {
      "epoch": 2.3506976830090407,
      "grad_norm": 5.235393524169922,
      "learning_rate": 1.08385e-05,
      "loss": 5.1341,
      "step": 1018200
    },
    {
      "epoch": 2.3509285509802655,
      "grad_norm": 6.4239501953125,
      "learning_rate": 1.0834653846153847e-05,
      "loss": 5.1907,
      "step": 1018300
    },
    {
      "epoch": 2.35115941895149,
      "grad_norm": 6.618278503417969,
      "learning_rate": 1.0830807692307692e-05,
      "loss": 5.1551,
      "step": 1018400
    },
    {
      "epoch": 2.351390286922715,
      "grad_norm": 7.309768199920654,
      "learning_rate": 1.0826961538461538e-05,
      "loss": 5.133,
      "step": 1018500
    },
    {
      "epoch": 2.351621154893939,
      "grad_norm": 6.825553894042969,
      "learning_rate": 1.0823115384615385e-05,
      "loss": 5.1436,
      "step": 1018600
    },
    {
      "epoch": 2.351852022865164,
      "grad_norm": 7.732450485229492,
      "learning_rate": 1.081926923076923e-05,
      "loss": 5.1428,
      "step": 1018700
    },
    {
      "epoch": 2.3520828908363884,
      "grad_norm": 5.154476165771484,
      "learning_rate": 1.0815423076923078e-05,
      "loss": 5.121,
      "step": 1018800
    },
    {
      "epoch": 2.3523137588076133,
      "grad_norm": 5.641148090362549,
      "learning_rate": 1.0811576923076924e-05,
      "loss": 5.1459,
      "step": 1018900
    },
    {
      "epoch": 2.3525446267788377,
      "grad_norm": 6.767922401428223,
      "learning_rate": 1.080773076923077e-05,
      "loss": 5.1684,
      "step": 1019000
    },
    {
      "epoch": 2.3527754947500625,
      "grad_norm": 6.220088005065918,
      "learning_rate": 1.0803884615384616e-05,
      "loss": 5.1558,
      "step": 1019100
    },
    {
      "epoch": 2.353006362721287,
      "grad_norm": 6.362011909484863,
      "learning_rate": 1.0800038461538462e-05,
      "loss": 5.1751,
      "step": 1019200
    },
    {
      "epoch": 2.3532372306925113,
      "grad_norm": 5.53366756439209,
      "learning_rate": 1.0796192307692309e-05,
      "loss": 5.1245,
      "step": 1019300
    },
    {
      "epoch": 2.353468098663736,
      "grad_norm": 5.602015495300293,
      "learning_rate": 1.0792346153846155e-05,
      "loss": 5.1245,
      "step": 1019400
    },
    {
      "epoch": 2.353698966634961,
      "grad_norm": 5.871676921844482,
      "learning_rate": 1.07885e-05,
      "loss": 5.1187,
      "step": 1019500
    },
    {
      "epoch": 2.3539298346061854,
      "grad_norm": 5.347317695617676,
      "learning_rate": 1.0784653846153847e-05,
      "loss": 5.0757,
      "step": 1019600
    },
    {
      "epoch": 2.35416070257741,
      "grad_norm": 5.297008037567139,
      "learning_rate": 1.0780807692307693e-05,
      "loss": 5.0819,
      "step": 1019700
    },
    {
      "epoch": 2.3543915705486347,
      "grad_norm": 6.9694647789001465,
      "learning_rate": 1.0776961538461538e-05,
      "loss": 5.1178,
      "step": 1019800
    },
    {
      "epoch": 2.354622438519859,
      "grad_norm": 5.575782775878906,
      "learning_rate": 1.0773115384615386e-05,
      "loss": 5.1529,
      "step": 1019900
    },
    {
      "epoch": 2.354853306491084,
      "grad_norm": 6.840855598449707,
      "learning_rate": 1.0769269230769231e-05,
      "loss": 5.0975,
      "step": 1020000
    },
    {
      "epoch": 2.3550841744623083,
      "grad_norm": 6.581533908843994,
      "learning_rate": 1.0765423076923077e-05,
      "loss": 5.1163,
      "step": 1020100
    },
    {
      "epoch": 2.355315042433533,
      "grad_norm": 5.603545188903809,
      "learning_rate": 1.0761576923076924e-05,
      "loss": 5.1065,
      "step": 1020200
    },
    {
      "epoch": 2.3555459104047576,
      "grad_norm": 7.072263240814209,
      "learning_rate": 1.075773076923077e-05,
      "loss": 5.1099,
      "step": 1020300
    },
    {
      "epoch": 2.3557767783759824,
      "grad_norm": 6.6744818687438965,
      "learning_rate": 1.0753884615384615e-05,
      "loss": 5.0511,
      "step": 1020400
    },
    {
      "epoch": 2.356007646347207,
      "grad_norm": 5.725602149963379,
      "learning_rate": 1.075003846153846e-05,
      "loss": 5.0775,
      "step": 1020500
    },
    {
      "epoch": 2.3562385143184317,
      "grad_norm": 5.273366451263428,
      "learning_rate": 1.0746192307692308e-05,
      "loss": 5.109,
      "step": 1020600
    },
    {
      "epoch": 2.356469382289656,
      "grad_norm": 4.702817916870117,
      "learning_rate": 1.0742346153846155e-05,
      "loss": 5.0931,
      "step": 1020700
    },
    {
      "epoch": 2.356700250260881,
      "grad_norm": 6.384431838989258,
      "learning_rate": 1.07385e-05,
      "loss": 5.0618,
      "step": 1020800
    },
    {
      "epoch": 2.3569311182321053,
      "grad_norm": 5.834715843200684,
      "learning_rate": 1.0734653846153848e-05,
      "loss": 5.0813,
      "step": 1020900
    },
    {
      "epoch": 2.35716198620333,
      "grad_norm": 4.864578723907471,
      "learning_rate": 1.0730807692307693e-05,
      "loss": 5.0979,
      "step": 1021000
    },
    {
      "epoch": 2.3573928541745546,
      "grad_norm": 6.570408821105957,
      "learning_rate": 1.0726961538461539e-05,
      "loss": 5.0717,
      "step": 1021100
    },
    {
      "epoch": 2.3576237221457794,
      "grad_norm": 8.317343711853027,
      "learning_rate": 1.0723115384615386e-05,
      "loss": 5.0531,
      "step": 1021200
    },
    {
      "epoch": 2.357854590117004,
      "grad_norm": 6.4042067527771,
      "learning_rate": 1.0719269230769232e-05,
      "loss": 5.0809,
      "step": 1021300
    },
    {
      "epoch": 2.3580854580882287,
      "grad_norm": 7.040582656860352,
      "learning_rate": 1.0715423076923077e-05,
      "loss": 5.0529,
      "step": 1021400
    },
    {
      "epoch": 2.358316326059453,
      "grad_norm": 7.314507484436035,
      "learning_rate": 1.0711576923076924e-05,
      "loss": 5.0664,
      "step": 1021500
    },
    {
      "epoch": 2.358547194030678,
      "grad_norm": 5.857795715332031,
      "learning_rate": 1.070773076923077e-05,
      "loss": 5.1003,
      "step": 1021600
    },
    {
      "epoch": 2.3587780620019023,
      "grad_norm": 6.143874168395996,
      "learning_rate": 1.0703884615384616e-05,
      "loss": 5.0931,
      "step": 1021700
    },
    {
      "epoch": 2.359008929973127,
      "grad_norm": 6.473800182342529,
      "learning_rate": 1.0700038461538463e-05,
      "loss": 4.9914,
      "step": 1021800
    },
    {
      "epoch": 2.3592397979443516,
      "grad_norm": 6.45278263092041,
      "learning_rate": 1.0696192307692308e-05,
      "loss": 5.0418,
      "step": 1021900
    },
    {
      "epoch": 2.359470665915576,
      "grad_norm": 7.278872966766357,
      "learning_rate": 1.0692346153846154e-05,
      "loss": 5.0558,
      "step": 1022000
    },
    {
      "epoch": 2.359701533886801,
      "grad_norm": 5.173380374908447,
      "learning_rate": 1.06885e-05,
      "loss": 5.073,
      "step": 1022100
    },
    {
      "epoch": 2.3599324018580257,
      "grad_norm": 6.02130651473999,
      "learning_rate": 1.0684653846153847e-05,
      "loss": 5.0363,
      "step": 1022200
    },
    {
      "epoch": 2.36016326982925,
      "grad_norm": 6.101856708526611,
      "learning_rate": 1.0680807692307692e-05,
      "loss": 5.0111,
      "step": 1022300
    },
    {
      "epoch": 2.3603941378004745,
      "grad_norm": 5.95285701751709,
      "learning_rate": 1.0676961538461538e-05,
      "loss": 5.0938,
      "step": 1022400
    },
    {
      "epoch": 2.3606250057716993,
      "grad_norm": 6.612963676452637,
      "learning_rate": 1.0673115384615385e-05,
      "loss": 5.0434,
      "step": 1022500
    },
    {
      "epoch": 2.3608558737429237,
      "grad_norm": 5.123377323150635,
      "learning_rate": 1.066926923076923e-05,
      "loss": 5.0436,
      "step": 1022600
    },
    {
      "epoch": 2.3610867417141486,
      "grad_norm": 5.287926197052002,
      "learning_rate": 1.0665423076923078e-05,
      "loss": 5.0525,
      "step": 1022700
    },
    {
      "epoch": 2.361317609685373,
      "grad_norm": 5.775644779205322,
      "learning_rate": 1.0661576923076925e-05,
      "loss": 5.1201,
      "step": 1022800
    },
    {
      "epoch": 2.361548477656598,
      "grad_norm": 4.968832492828369,
      "learning_rate": 1.065773076923077e-05,
      "loss": 5.0679,
      "step": 1022900
    },
    {
      "epoch": 2.361779345627822,
      "grad_norm": 6.560834884643555,
      "learning_rate": 1.0653884615384616e-05,
      "loss": 5.0034,
      "step": 1023000
    },
    {
      "epoch": 2.362010213599047,
      "grad_norm": 5.764579772949219,
      "learning_rate": 1.0650038461538463e-05,
      "loss": 5.0127,
      "step": 1023100
    },
    {
      "epoch": 2.3622410815702715,
      "grad_norm": 6.169491291046143,
      "learning_rate": 1.0646192307692309e-05,
      "loss": 5.0683,
      "step": 1023200
    },
    {
      "epoch": 2.3624719495414963,
      "grad_norm": 6.385393142700195,
      "learning_rate": 1.0642346153846154e-05,
      "loss": 5.0491,
      "step": 1023300
    },
    {
      "epoch": 2.3627028175127207,
      "grad_norm": 5.628724098205566,
      "learning_rate": 1.06385e-05,
      "loss": 5.0118,
      "step": 1023400
    },
    {
      "epoch": 2.3629336854839456,
      "grad_norm": 5.875647068023682,
      "learning_rate": 1.0634653846153847e-05,
      "loss": 4.9887,
      "step": 1023500
    },
    {
      "epoch": 2.36316455345517,
      "grad_norm": 7.110965728759766,
      "learning_rate": 1.0630807692307693e-05,
      "loss": 5.0218,
      "step": 1023600
    },
    {
      "epoch": 2.363395421426395,
      "grad_norm": 5.67033576965332,
      "learning_rate": 1.0626961538461538e-05,
      "loss": 4.9764,
      "step": 1023700
    },
    {
      "epoch": 2.363626289397619,
      "grad_norm": 8.461429595947266,
      "learning_rate": 1.0623115384615385e-05,
      "loss": 5.0347,
      "step": 1023800
    },
    {
      "epoch": 2.363857157368844,
      "grad_norm": 6.850661754608154,
      "learning_rate": 1.0619269230769231e-05,
      "loss": 5.055,
      "step": 1023900
    },
    {
      "epoch": 2.3640880253400685,
      "grad_norm": 7.155250549316406,
      "learning_rate": 1.0615423076923076e-05,
      "loss": 5.0233,
      "step": 1024000
    },
    {
      "epoch": 2.3643188933112933,
      "grad_norm": 6.2751970291137695,
      "learning_rate": 1.0611576923076924e-05,
      "loss": 5.0479,
      "step": 1024100
    },
    {
      "epoch": 2.3645497612825177,
      "grad_norm": 5.3851189613342285,
      "learning_rate": 1.060773076923077e-05,
      "loss": 5.0615,
      "step": 1024200
    },
    {
      "epoch": 2.3647806292537425,
      "grad_norm": 5.267460346221924,
      "learning_rate": 1.0603884615384615e-05,
      "loss": 5.0111,
      "step": 1024300
    },
    {
      "epoch": 2.365011497224967,
      "grad_norm": 5.3340911865234375,
      "learning_rate": 1.0600038461538462e-05,
      "loss": 5.034,
      "step": 1024400
    },
    {
      "epoch": 2.365242365196192,
      "grad_norm": 5.90596342086792,
      "learning_rate": 1.0596192307692308e-05,
      "loss": 5.0606,
      "step": 1024500
    },
    {
      "epoch": 2.365473233167416,
      "grad_norm": 5.631275177001953,
      "learning_rate": 1.0592346153846155e-05,
      "loss": 5.0233,
      "step": 1024600
    },
    {
      "epoch": 2.3657041011386406,
      "grad_norm": 5.257522106170654,
      "learning_rate": 1.05885e-05,
      "loss": 5.0035,
      "step": 1024700
    },
    {
      "epoch": 2.3659349691098654,
      "grad_norm": 4.925173282623291,
      "learning_rate": 1.0584653846153848e-05,
      "loss": 5.0407,
      "step": 1024800
    },
    {
      "epoch": 2.3661658370810903,
      "grad_norm": 6.019400596618652,
      "learning_rate": 1.0580807692307693e-05,
      "loss": 5.018,
      "step": 1024900
    },
    {
      "epoch": 2.3663967050523147,
      "grad_norm": 6.9921698570251465,
      "learning_rate": 1.0576961538461539e-05,
      "loss": 5.0309,
      "step": 1025000
    },
    {
      "epoch": 2.366627573023539,
      "grad_norm": 5.67456579208374,
      "learning_rate": 1.0573115384615386e-05,
      "loss": 5.0345,
      "step": 1025100
    },
    {
      "epoch": 2.366858440994764,
      "grad_norm": 5.218688011169434,
      "learning_rate": 1.0569269230769231e-05,
      "loss": 5.0344,
      "step": 1025200
    },
    {
      "epoch": 2.3670893089659883,
      "grad_norm": 6.800705909729004,
      "learning_rate": 1.0565423076923077e-05,
      "loss": 5.0019,
      "step": 1025300
    },
    {
      "epoch": 2.367320176937213,
      "grad_norm": 5.282451152801514,
      "learning_rate": 1.0561576923076924e-05,
      "loss": 4.9992,
      "step": 1025400
    },
    {
      "epoch": 2.3675510449084376,
      "grad_norm": 5.857972145080566,
      "learning_rate": 1.055773076923077e-05,
      "loss": 4.9835,
      "step": 1025500
    },
    {
      "epoch": 2.3677819128796624,
      "grad_norm": 5.2095417976379395,
      "learning_rate": 1.0553884615384615e-05,
      "loss": 4.9517,
      "step": 1025600
    },
    {
      "epoch": 2.368012780850887,
      "grad_norm": 5.347048759460449,
      "learning_rate": 1.0550038461538463e-05,
      "loss": 5.0189,
      "step": 1025700
    },
    {
      "epoch": 2.3682436488221117,
      "grad_norm": 5.198180198669434,
      "learning_rate": 1.0546192307692308e-05,
      "loss": 5.0338,
      "step": 1025800
    },
    {
      "epoch": 2.368474516793336,
      "grad_norm": 5.909748554229736,
      "learning_rate": 1.0542346153846154e-05,
      "loss": 4.966,
      "step": 1025900
    },
    {
      "epoch": 2.368705384764561,
      "grad_norm": 4.952630519866943,
      "learning_rate": 1.05385e-05,
      "loss": 5.0105,
      "step": 1026000
    },
    {
      "epoch": 2.3689362527357853,
      "grad_norm": 6.152337074279785,
      "learning_rate": 1.0534653846153846e-05,
      "loss": 4.9852,
      "step": 1026100
    },
    {
      "epoch": 2.36916712070701,
      "grad_norm": 5.111433029174805,
      "learning_rate": 1.0530807692307692e-05,
      "loss": 5.026,
      "step": 1026200
    },
    {
      "epoch": 2.3693979886782346,
      "grad_norm": 5.202280521392822,
      "learning_rate": 1.0526961538461539e-05,
      "loss": 5.0262,
      "step": 1026300
    },
    {
      "epoch": 2.3696288566494594,
      "grad_norm": 5.477046966552734,
      "learning_rate": 1.0523115384615385e-05,
      "loss": 5.0333,
      "step": 1026400
    },
    {
      "epoch": 2.369859724620684,
      "grad_norm": 5.584517478942871,
      "learning_rate": 1.0519269230769232e-05,
      "loss": 5.0021,
      "step": 1026500
    },
    {
      "epoch": 2.3700905925919087,
      "grad_norm": 6.760261535644531,
      "learning_rate": 1.0515423076923077e-05,
      "loss": 4.9671,
      "step": 1026600
    },
    {
      "epoch": 2.370321460563133,
      "grad_norm": 5.2293925285339355,
      "learning_rate": 1.0511576923076925e-05,
      "loss": 5.0,
      "step": 1026700
    },
    {
      "epoch": 2.370552328534358,
      "grad_norm": 6.000523567199707,
      "learning_rate": 1.050773076923077e-05,
      "loss": 4.9306,
      "step": 1026800
    },
    {
      "epoch": 2.3707831965055823,
      "grad_norm": 6.927600383758545,
      "learning_rate": 1.0503884615384616e-05,
      "loss": 5.024,
      "step": 1026900
    },
    {
      "epoch": 2.371014064476807,
      "grad_norm": 5.186995983123779,
      "learning_rate": 1.0500038461538463e-05,
      "loss": 5.0414,
      "step": 1027000
    },
    {
      "epoch": 2.3712449324480316,
      "grad_norm": 6.399526119232178,
      "learning_rate": 1.0496192307692309e-05,
      "loss": 4.9465,
      "step": 1027100
    },
    {
      "epoch": 2.3714758004192564,
      "grad_norm": 4.992422103881836,
      "learning_rate": 1.0492346153846154e-05,
      "loss": 4.954,
      "step": 1027200
    },
    {
      "epoch": 2.371706668390481,
      "grad_norm": 5.214709281921387,
      "learning_rate": 1.0488500000000001e-05,
      "loss": 4.9478,
      "step": 1027300
    },
    {
      "epoch": 2.3719375363617052,
      "grad_norm": 6.531644344329834,
      "learning_rate": 1.0484653846153847e-05,
      "loss": 4.9816,
      "step": 1027400
    },
    {
      "epoch": 2.37216840433293,
      "grad_norm": 6.760715961456299,
      "learning_rate": 1.0480807692307692e-05,
      "loss": 4.9594,
      "step": 1027500
    },
    {
      "epoch": 2.372399272304155,
      "grad_norm": 7.017304420471191,
      "learning_rate": 1.0476961538461538e-05,
      "loss": 5.0394,
      "step": 1027600
    },
    {
      "epoch": 2.3726301402753793,
      "grad_norm": 7.129474639892578,
      "learning_rate": 1.0473115384615385e-05,
      "loss": 5.017,
      "step": 1027700
    },
    {
      "epoch": 2.3728610082466037,
      "grad_norm": 6.319667339324951,
      "learning_rate": 1.046926923076923e-05,
      "loss": 4.9633,
      "step": 1027800
    },
    {
      "epoch": 2.3730918762178286,
      "grad_norm": 5.416443824768066,
      "learning_rate": 1.0465423076923076e-05,
      "loss": 4.9656,
      "step": 1027900
    },
    {
      "epoch": 2.373322744189053,
      "grad_norm": 6.576837539672852,
      "learning_rate": 1.0461576923076923e-05,
      "loss": 5.0115,
      "step": 1028000
    },
    {
      "epoch": 2.373553612160278,
      "grad_norm": 7.008008003234863,
      "learning_rate": 1.0457730769230769e-05,
      "loss": 4.9817,
      "step": 1028100
    },
    {
      "epoch": 2.3737844801315022,
      "grad_norm": 7.69577169418335,
      "learning_rate": 1.0453884615384615e-05,
      "loss": 4.9306,
      "step": 1028200
    },
    {
      "epoch": 2.374015348102727,
      "grad_norm": 6.141977787017822,
      "learning_rate": 1.0450038461538462e-05,
      "loss": 4.9991,
      "step": 1028300
    },
    {
      "epoch": 2.3742462160739515,
      "grad_norm": 6.538129806518555,
      "learning_rate": 1.0446192307692309e-05,
      "loss": 5.0031,
      "step": 1028400
    },
    {
      "epoch": 2.3744770840451763,
      "grad_norm": 5.93961238861084,
      "learning_rate": 1.0442346153846155e-05,
      "loss": 4.9847,
      "step": 1028500
    },
    {
      "epoch": 2.3747079520164007,
      "grad_norm": 5.189431190490723,
      "learning_rate": 1.0438500000000002e-05,
      "loss": 4.9422,
      "step": 1028600
    },
    {
      "epoch": 2.3749388199876256,
      "grad_norm": 7.941397190093994,
      "learning_rate": 1.0434653846153847e-05,
      "loss": 4.9947,
      "step": 1028700
    },
    {
      "epoch": 2.37516968795885,
      "grad_norm": 5.871037006378174,
      "learning_rate": 1.0430807692307693e-05,
      "loss": 4.9565,
      "step": 1028800
    },
    {
      "epoch": 2.375400555930075,
      "grad_norm": 6.391059875488281,
      "learning_rate": 1.042696153846154e-05,
      "loss": 4.9728,
      "step": 1028900
    },
    {
      "epoch": 2.375631423901299,
      "grad_norm": 5.857254981994629,
      "learning_rate": 1.0423115384615386e-05,
      "loss": 4.9753,
      "step": 1029000
    },
    {
      "epoch": 2.375862291872524,
      "grad_norm": 5.7226104736328125,
      "learning_rate": 1.0419269230769231e-05,
      "loss": 4.9112,
      "step": 1029100
    },
    {
      "epoch": 2.3760931598437485,
      "grad_norm": 5.597017765045166,
      "learning_rate": 1.0415423076923077e-05,
      "loss": 4.9476,
      "step": 1029200
    },
    {
      "epoch": 2.3763240278149733,
      "grad_norm": 6.384279727935791,
      "learning_rate": 1.0411576923076924e-05,
      "loss": 4.9031,
      "step": 1029300
    },
    {
      "epoch": 2.3765548957861977,
      "grad_norm": 6.31625509262085,
      "learning_rate": 1.040773076923077e-05,
      "loss": 4.9898,
      "step": 1029400
    },
    {
      "epoch": 2.3767857637574226,
      "grad_norm": 6.058571815490723,
      "learning_rate": 1.0403884615384615e-05,
      "loss": 4.988,
      "step": 1029500
    },
    {
      "epoch": 2.377016631728647,
      "grad_norm": 5.4303765296936035,
      "learning_rate": 1.0400038461538462e-05,
      "loss": 4.9423,
      "step": 1029600
    },
    {
      "epoch": 2.377247499699872,
      "grad_norm": 6.194243907928467,
      "learning_rate": 1.0396192307692308e-05,
      "loss": 4.9736,
      "step": 1029700
    },
    {
      "epoch": 2.377478367671096,
      "grad_norm": 5.154857635498047,
      "learning_rate": 1.0392346153846153e-05,
      "loss": 4.9899,
      "step": 1029800
    },
    {
      "epoch": 2.377709235642321,
      "grad_norm": 6.492337226867676,
      "learning_rate": 1.03885e-05,
      "loss": 4.9994,
      "step": 1029900
    },
    {
      "epoch": 2.3779401036135455,
      "grad_norm": 6.19028377532959,
      "learning_rate": 1.0384653846153846e-05,
      "loss": 4.977,
      "step": 1030000
    },
    {
      "epoch": 2.37817097158477,
      "grad_norm": 5.206808090209961,
      "learning_rate": 1.0380807692307692e-05,
      "loss": 4.9688,
      "step": 1030100
    },
    {
      "epoch": 2.3784018395559947,
      "grad_norm": 5.44404411315918,
      "learning_rate": 1.0376961538461539e-05,
      "loss": 4.9301,
      "step": 1030200
    },
    {
      "epoch": 2.3786327075272196,
      "grad_norm": 5.809498310089111,
      "learning_rate": 1.0373115384615386e-05,
      "loss": 4.9425,
      "step": 1030300
    },
    {
      "epoch": 2.378863575498444,
      "grad_norm": 5.4587225914001465,
      "learning_rate": 1.0369269230769232e-05,
      "loss": 4.9242,
      "step": 1030400
    },
    {
      "epoch": 2.3790944434696684,
      "grad_norm": 6.1809868812561035,
      "learning_rate": 1.0365423076923077e-05,
      "loss": 4.9145,
      "step": 1030500
    },
    {
      "epoch": 2.379325311440893,
      "grad_norm": 7.427559852600098,
      "learning_rate": 1.0361576923076924e-05,
      "loss": 4.9241,
      "step": 1030600
    },
    {
      "epoch": 2.3795561794121176,
      "grad_norm": 5.725305557250977,
      "learning_rate": 1.035773076923077e-05,
      "loss": 4.9601,
      "step": 1030700
    },
    {
      "epoch": 2.3797870473833425,
      "grad_norm": 6.854599952697754,
      "learning_rate": 1.0353884615384616e-05,
      "loss": 4.964,
      "step": 1030800
    },
    {
      "epoch": 2.380017915354567,
      "grad_norm": 6.126071453094482,
      "learning_rate": 1.0350038461538463e-05,
      "loss": 4.9609,
      "step": 1030900
    },
    {
      "epoch": 2.3802487833257917,
      "grad_norm": 6.5926055908203125,
      "learning_rate": 1.0346192307692308e-05,
      "loss": 5.0004,
      "step": 1031000
    },
    {
      "epoch": 2.380479651297016,
      "grad_norm": 5.2089362144470215,
      "learning_rate": 1.0342346153846154e-05,
      "loss": 4.9176,
      "step": 1031100
    },
    {
      "epoch": 2.380710519268241,
      "grad_norm": 5.818066120147705,
      "learning_rate": 1.0338500000000001e-05,
      "loss": 4.9178,
      "step": 1031200
    },
    {
      "epoch": 2.3809413872394654,
      "grad_norm": 6.964848518371582,
      "learning_rate": 1.0334653846153847e-05,
      "loss": 4.9234,
      "step": 1031300
    },
    {
      "epoch": 2.38117225521069,
      "grad_norm": 5.473431587219238,
      "learning_rate": 1.0330807692307692e-05,
      "loss": 4.9438,
      "step": 1031400
    },
    {
      "epoch": 2.3814031231819146,
      "grad_norm": 6.1731719970703125,
      "learning_rate": 1.032696153846154e-05,
      "loss": 4.9149,
      "step": 1031500
    },
    {
      "epoch": 2.3816339911531395,
      "grad_norm": 7.468325614929199,
      "learning_rate": 1.0323115384615385e-05,
      "loss": 4.9298,
      "step": 1031600
    },
    {
      "epoch": 2.381864859124364,
      "grad_norm": 5.255019664764404,
      "learning_rate": 1.031926923076923e-05,
      "loss": 4.9639,
      "step": 1031700
    },
    {
      "epoch": 2.3820957270955887,
      "grad_norm": 6.571645736694336,
      "learning_rate": 1.0315423076923078e-05,
      "loss": 4.9754,
      "step": 1031800
    },
    {
      "epoch": 2.382326595066813,
      "grad_norm": 6.639027118682861,
      "learning_rate": 1.0311576923076923e-05,
      "loss": 4.9073,
      "step": 1031900
    },
    {
      "epoch": 2.382557463038038,
      "grad_norm": 5.382055282592773,
      "learning_rate": 1.0307730769230769e-05,
      "loss": 4.917,
      "step": 1032000
    },
    {
      "epoch": 2.3827883310092624,
      "grad_norm": 6.472599983215332,
      "learning_rate": 1.0303884615384616e-05,
      "loss": 4.9755,
      "step": 1032100
    },
    {
      "epoch": 2.383019198980487,
      "grad_norm": 5.768996238708496,
      "learning_rate": 1.0300038461538462e-05,
      "loss": 4.9621,
      "step": 1032200
    },
    {
      "epoch": 2.3832500669517116,
      "grad_norm": 6.06899356842041,
      "learning_rate": 1.0296192307692309e-05,
      "loss": 4.9937,
      "step": 1032300
    },
    {
      "epoch": 2.3834809349229364,
      "grad_norm": 5.435807228088379,
      "learning_rate": 1.0292346153846154e-05,
      "loss": 4.9354,
      "step": 1032400
    },
    {
      "epoch": 2.383711802894161,
      "grad_norm": 6.109907627105713,
      "learning_rate": 1.0288500000000002e-05,
      "loss": 4.9073,
      "step": 1032500
    },
    {
      "epoch": 2.3839426708653857,
      "grad_norm": 5.930112838745117,
      "learning_rate": 1.0284653846153847e-05,
      "loss": 4.9751,
      "step": 1032600
    },
    {
      "epoch": 2.38417353883661,
      "grad_norm": 5.882584095001221,
      "learning_rate": 1.0280807692307693e-05,
      "loss": 4.9119,
      "step": 1032700
    },
    {
      "epoch": 2.3844044068078345,
      "grad_norm": 5.788487911224365,
      "learning_rate": 1.027696153846154e-05,
      "loss": 4.9275,
      "step": 1032800
    },
    {
      "epoch": 2.3846352747790593,
      "grad_norm": 6.541394233703613,
      "learning_rate": 1.0273115384615385e-05,
      "loss": 4.9539,
      "step": 1032900
    },
    {
      "epoch": 2.384866142750284,
      "grad_norm": 7.425163745880127,
      "learning_rate": 1.0269269230769231e-05,
      "loss": 4.9227,
      "step": 1033000
    },
    {
      "epoch": 2.3850970107215086,
      "grad_norm": 5.941156387329102,
      "learning_rate": 1.0265423076923078e-05,
      "loss": 4.8926,
      "step": 1033100
    },
    {
      "epoch": 2.385327878692733,
      "grad_norm": 6.121574401855469,
      "learning_rate": 1.0261576923076924e-05,
      "loss": 4.9446,
      "step": 1033200
    },
    {
      "epoch": 2.385558746663958,
      "grad_norm": 4.9865031242370605,
      "learning_rate": 1.025773076923077e-05,
      "loss": 4.9182,
      "step": 1033300
    },
    {
      "epoch": 2.3857896146351822,
      "grad_norm": 7.4956231117248535,
      "learning_rate": 1.0253884615384615e-05,
      "loss": 4.9327,
      "step": 1033400
    },
    {
      "epoch": 2.386020482606407,
      "grad_norm": 7.281765460968018,
      "learning_rate": 1.0250038461538462e-05,
      "loss": 4.8962,
      "step": 1033500
    },
    {
      "epoch": 2.3862513505776315,
      "grad_norm": 5.450728893280029,
      "learning_rate": 1.0246192307692308e-05,
      "loss": 4.9323,
      "step": 1033600
    },
    {
      "epoch": 2.3864822185488563,
      "grad_norm": 5.837100505828857,
      "learning_rate": 1.0242346153846153e-05,
      "loss": 4.9406,
      "step": 1033700
    },
    {
      "epoch": 2.3867130865200807,
      "grad_norm": 6.573256492614746,
      "learning_rate": 1.02385e-05,
      "loss": 4.9291,
      "step": 1033800
    },
    {
      "epoch": 2.3869439544913056,
      "grad_norm": 5.501009941101074,
      "learning_rate": 1.0234653846153846e-05,
      "loss": 4.9589,
      "step": 1033900
    },
    {
      "epoch": 2.38717482246253,
      "grad_norm": 6.592803001403809,
      "learning_rate": 1.0230807692307693e-05,
      "loss": 4.9329,
      "step": 1034000
    },
    {
      "epoch": 2.387405690433755,
      "grad_norm": 7.218626022338867,
      "learning_rate": 1.0226961538461539e-05,
      "loss": 4.9116,
      "step": 1034100
    },
    {
      "epoch": 2.3876365584049792,
      "grad_norm": 5.381909370422363,
      "learning_rate": 1.0223115384615386e-05,
      "loss": 4.9232,
      "step": 1034200
    },
    {
      "epoch": 2.387867426376204,
      "grad_norm": 7.2431135177612305,
      "learning_rate": 1.0219269230769231e-05,
      "loss": 4.9099,
      "step": 1034300
    },
    {
      "epoch": 2.3880982943474285,
      "grad_norm": 5.564589500427246,
      "learning_rate": 1.0215423076923079e-05,
      "loss": 4.9149,
      "step": 1034400
    },
    {
      "epoch": 2.3883291623186533,
      "grad_norm": 5.3693132400512695,
      "learning_rate": 1.0211576923076924e-05,
      "loss": 4.9358,
      "step": 1034500
    },
    {
      "epoch": 2.3885600302898777,
      "grad_norm": 5.538949489593506,
      "learning_rate": 1.020773076923077e-05,
      "loss": 4.9276,
      "step": 1034600
    },
    {
      "epoch": 2.3887908982611026,
      "grad_norm": 5.8301191329956055,
      "learning_rate": 1.0203884615384617e-05,
      "loss": 4.9077,
      "step": 1034700
    },
    {
      "epoch": 2.389021766232327,
      "grad_norm": 6.803781986236572,
      "learning_rate": 1.0200038461538462e-05,
      "loss": 4.9552,
      "step": 1034800
    },
    {
      "epoch": 2.389252634203552,
      "grad_norm": 5.783199310302734,
      "learning_rate": 1.0196192307692308e-05,
      "loss": 4.9252,
      "step": 1034900
    },
    {
      "epoch": 2.3894835021747762,
      "grad_norm": 7.5040812492370605,
      "learning_rate": 1.0192346153846154e-05,
      "loss": 4.9101,
      "step": 1035000
    },
    {
      "epoch": 2.389714370146001,
      "grad_norm": 8.969538688659668,
      "learning_rate": 1.01885e-05,
      "loss": 4.9202,
      "step": 1035100
    },
    {
      "epoch": 2.3899452381172255,
      "grad_norm": 6.328603744506836,
      "learning_rate": 1.0184653846153846e-05,
      "loss": 4.9337,
      "step": 1035200
    },
    {
      "epoch": 2.3901761060884503,
      "grad_norm": 9.762166976928711,
      "learning_rate": 1.0180807692307692e-05,
      "loss": 4.9139,
      "step": 1035300
    },
    {
      "epoch": 2.3904069740596747,
      "grad_norm": 6.7754387855529785,
      "learning_rate": 1.0176961538461539e-05,
      "loss": 4.9361,
      "step": 1035400
    },
    {
      "epoch": 2.390637842030899,
      "grad_norm": 6.093674659729004,
      "learning_rate": 1.0173115384615385e-05,
      "loss": 4.9161,
      "step": 1035500
    },
    {
      "epoch": 2.390868710002124,
      "grad_norm": 6.235572338104248,
      "learning_rate": 1.016926923076923e-05,
      "loss": 4.9175,
      "step": 1035600
    },
    {
      "epoch": 2.391099577973349,
      "grad_norm": 5.403674125671387,
      "learning_rate": 1.0165423076923077e-05,
      "loss": 4.8618,
      "step": 1035700
    },
    {
      "epoch": 2.3913304459445732,
      "grad_norm": 5.520505905151367,
      "learning_rate": 1.0161576923076923e-05,
      "loss": 4.8961,
      "step": 1035800
    },
    {
      "epoch": 2.3915613139157976,
      "grad_norm": 5.249807357788086,
      "learning_rate": 1.015773076923077e-05,
      "loss": 4.8802,
      "step": 1035900
    },
    {
      "epoch": 2.3917921818870225,
      "grad_norm": 6.787346363067627,
      "learning_rate": 1.0153884615384616e-05,
      "loss": 4.8924,
      "step": 1036000
    },
    {
      "epoch": 2.392023049858247,
      "grad_norm": 6.07487678527832,
      "learning_rate": 1.0150038461538463e-05,
      "loss": 4.8352,
      "step": 1036100
    },
    {
      "epoch": 2.3922539178294717,
      "grad_norm": 5.411674976348877,
      "learning_rate": 1.0146192307692308e-05,
      "loss": 4.8752,
      "step": 1036200
    },
    {
      "epoch": 2.392484785800696,
      "grad_norm": 5.259127140045166,
      "learning_rate": 1.0142346153846154e-05,
      "loss": 4.9026,
      "step": 1036300
    },
    {
      "epoch": 2.392715653771921,
      "grad_norm": 5.0465826988220215,
      "learning_rate": 1.0138500000000001e-05,
      "loss": 4.9055,
      "step": 1036400
    },
    {
      "epoch": 2.3929465217431454,
      "grad_norm": 7.09088659286499,
      "learning_rate": 1.0134653846153847e-05,
      "loss": 4.869,
      "step": 1036500
    },
    {
      "epoch": 2.39317738971437,
      "grad_norm": 5.7491774559021,
      "learning_rate": 1.0130807692307692e-05,
      "loss": 4.9131,
      "step": 1036600
    },
    {
      "epoch": 2.3934082576855946,
      "grad_norm": 5.98906946182251,
      "learning_rate": 1.012696153846154e-05,
      "loss": 4.8828,
      "step": 1036700
    },
    {
      "epoch": 2.3936391256568195,
      "grad_norm": 5.569483280181885,
      "learning_rate": 1.0123115384615385e-05,
      "loss": 4.9468,
      "step": 1036800
    },
    {
      "epoch": 2.393869993628044,
      "grad_norm": 6.230759143829346,
      "learning_rate": 1.011926923076923e-05,
      "loss": 4.9288,
      "step": 1036900
    },
    {
      "epoch": 2.3941008615992687,
      "grad_norm": 5.182190895080566,
      "learning_rate": 1.0115423076923078e-05,
      "loss": 4.9005,
      "step": 1037000
    },
    {
      "epoch": 2.394331729570493,
      "grad_norm": 6.3995184898376465,
      "learning_rate": 1.0111576923076923e-05,
      "loss": 4.8891,
      "step": 1037100
    },
    {
      "epoch": 2.394562597541718,
      "grad_norm": 6.945132255554199,
      "learning_rate": 1.0107730769230769e-05,
      "loss": 4.9195,
      "step": 1037200
    },
    {
      "epoch": 2.3947934655129424,
      "grad_norm": 5.114078044891357,
      "learning_rate": 1.0103884615384616e-05,
      "loss": 4.8921,
      "step": 1037300
    },
    {
      "epoch": 2.395024333484167,
      "grad_norm": 5.213975429534912,
      "learning_rate": 1.0100038461538462e-05,
      "loss": 4.9057,
      "step": 1037400
    },
    {
      "epoch": 2.3952552014553916,
      "grad_norm": 5.545079708099365,
      "learning_rate": 1.0096192307692307e-05,
      "loss": 4.8982,
      "step": 1037500
    },
    {
      "epoch": 2.3954860694266165,
      "grad_norm": 5.771769046783447,
      "learning_rate": 1.0092346153846155e-05,
      "loss": 4.8889,
      "step": 1037600
    },
    {
      "epoch": 2.395716937397841,
      "grad_norm": 8.02126693725586,
      "learning_rate": 1.00885e-05,
      "loss": 4.8891,
      "step": 1037700
    },
    {
      "epoch": 2.3959478053690657,
      "grad_norm": 6.039750576019287,
      "learning_rate": 1.0084653846153847e-05,
      "loss": 4.9214,
      "step": 1037800
    },
    {
      "epoch": 2.39617867334029,
      "grad_norm": 5.594930648803711,
      "learning_rate": 1.0080807692307693e-05,
      "loss": 4.8775,
      "step": 1037900
    },
    {
      "epoch": 2.396409541311515,
      "grad_norm": 5.799572467803955,
      "learning_rate": 1.007696153846154e-05,
      "loss": 4.8685,
      "step": 1038000
    },
    {
      "epoch": 2.3966404092827394,
      "grad_norm": 6.290966987609863,
      "learning_rate": 1.0073115384615386e-05,
      "loss": 4.8728,
      "step": 1038100
    },
    {
      "epoch": 2.3968712772539638,
      "grad_norm": 6.139772891998291,
      "learning_rate": 1.0069269230769231e-05,
      "loss": 4.8918,
      "step": 1038200
    },
    {
      "epoch": 2.3971021452251886,
      "grad_norm": 5.823053359985352,
      "learning_rate": 1.0065423076923078e-05,
      "loss": 4.8936,
      "step": 1038300
    },
    {
      "epoch": 2.3973330131964135,
      "grad_norm": 5.017282009124756,
      "learning_rate": 1.0061576923076924e-05,
      "loss": 4.8767,
      "step": 1038400
    },
    {
      "epoch": 2.397563881167638,
      "grad_norm": 5.333520412445068,
      "learning_rate": 1.005773076923077e-05,
      "loss": 4.907,
      "step": 1038500
    },
    {
      "epoch": 2.3977947491388623,
      "grad_norm": 6.552819728851318,
      "learning_rate": 1.0053884615384617e-05,
      "loss": 4.9003,
      "step": 1038600
    },
    {
      "epoch": 2.398025617110087,
      "grad_norm": 6.34102201461792,
      "learning_rate": 1.0050038461538462e-05,
      "loss": 4.9021,
      "step": 1038700
    },
    {
      "epoch": 2.3982564850813115,
      "grad_norm": 5.1097564697265625,
      "learning_rate": 1.0046192307692308e-05,
      "loss": 4.9493,
      "step": 1038800
    },
    {
      "epoch": 2.3984873530525364,
      "grad_norm": 5.036542892456055,
      "learning_rate": 1.0042346153846155e-05,
      "loss": 4.8717,
      "step": 1038900
    },
    {
      "epoch": 2.3987182210237608,
      "grad_norm": 6.424161911010742,
      "learning_rate": 1.00385e-05,
      "loss": 4.8903,
      "step": 1039000
    },
    {
      "epoch": 2.3989490889949856,
      "grad_norm": 5.303759574890137,
      "learning_rate": 1.0034653846153846e-05,
      "loss": 4.8832,
      "step": 1039100
    },
    {
      "epoch": 2.39917995696621,
      "grad_norm": 6.689347743988037,
      "learning_rate": 1.0030807692307692e-05,
      "loss": 4.8959,
      "step": 1039200
    },
    {
      "epoch": 2.399410824937435,
      "grad_norm": 5.995014190673828,
      "learning_rate": 1.0026961538461539e-05,
      "loss": 4.8884,
      "step": 1039300
    },
    {
      "epoch": 2.3996416929086593,
      "grad_norm": 6.8929123878479,
      "learning_rate": 1.0023115384615384e-05,
      "loss": 4.8792,
      "step": 1039400
    },
    {
      "epoch": 2.399872560879884,
      "grad_norm": 5.927372455596924,
      "learning_rate": 1.001926923076923e-05,
      "loss": 4.8709,
      "step": 1039500
    },
    {
      "epoch": 2.4001034288511085,
      "grad_norm": 5.329086780548096,
      "learning_rate": 1.0015423076923077e-05,
      "loss": 4.817,
      "step": 1039600
    },
    {
      "epoch": 2.4003342968223333,
      "grad_norm": 5.704623699188232,
      "learning_rate": 1.0011576923076923e-05,
      "loss": 4.8886,
      "step": 1039700
    },
    {
      "epoch": 2.4005651647935577,
      "grad_norm": 6.92939567565918,
      "learning_rate": 1.000773076923077e-05,
      "loss": 4.9281,
      "step": 1039800
    },
    {
      "epoch": 2.4007960327647826,
      "grad_norm": 6.356520652770996,
      "learning_rate": 1.0003884615384617e-05,
      "loss": 4.9288,
      "step": 1039900
    },
    {
      "epoch": 2.401026900736007,
      "grad_norm": 5.605195045471191,
      "learning_rate": 1.0000038461538463e-05,
      "loss": 4.9354,
      "step": 1040000
    },
    {
      "epoch": 2.401257768707232,
      "grad_norm": 5.988550186157227,
      "learning_rate": 9.996192307692308e-06,
      "loss": 4.8339,
      "step": 1040100
    },
    {
      "epoch": 2.4014886366784562,
      "grad_norm": 6.927086353302002,
      "learning_rate": 9.992346153846155e-06,
      "loss": 4.8577,
      "step": 1040200
    },
    {
      "epoch": 2.401719504649681,
      "grad_norm": 5.790696620941162,
      "learning_rate": 9.988500000000001e-06,
      "loss": 4.9165,
      "step": 1040300
    },
    {
      "epoch": 2.4019503726209055,
      "grad_norm": 5.4316887855529785,
      "learning_rate": 9.984653846153847e-06,
      "loss": 4.9153,
      "step": 1040400
    },
    {
      "epoch": 2.4021812405921303,
      "grad_norm": 6.681593894958496,
      "learning_rate": 9.980807692307694e-06,
      "loss": 4.8799,
      "step": 1040500
    },
    {
      "epoch": 2.4024121085633547,
      "grad_norm": 6.0047149658203125,
      "learning_rate": 9.97696153846154e-06,
      "loss": 4.8476,
      "step": 1040600
    },
    {
      "epoch": 2.4026429765345796,
      "grad_norm": 6.012073040008545,
      "learning_rate": 9.973115384615385e-06,
      "loss": 4.9336,
      "step": 1040700
    },
    {
      "epoch": 2.402873844505804,
      "grad_norm": 5.5425872802734375,
      "learning_rate": 9.96926923076923e-06,
      "loss": 4.9352,
      "step": 1040800
    },
    {
      "epoch": 2.4031047124770284,
      "grad_norm": 5.106247901916504,
      "learning_rate": 9.965423076923078e-06,
      "loss": 4.8844,
      "step": 1040900
    },
    {
      "epoch": 2.4033355804482532,
      "grad_norm": 5.612575531005859,
      "learning_rate": 9.961576923076923e-06,
      "loss": 4.8434,
      "step": 1041000
    },
    {
      "epoch": 2.403566448419478,
      "grad_norm": 7.189175605773926,
      "learning_rate": 9.957730769230769e-06,
      "loss": 4.8829,
      "step": 1041100
    },
    {
      "epoch": 2.4037973163907025,
      "grad_norm": 9.280025482177734,
      "learning_rate": 9.953884615384616e-06,
      "loss": 4.8856,
      "step": 1041200
    },
    {
      "epoch": 2.404028184361927,
      "grad_norm": 6.9215989112854,
      "learning_rate": 9.950038461538461e-06,
      "loss": 4.8701,
      "step": 1041300
    },
    {
      "epoch": 2.4042590523331517,
      "grad_norm": 6.112462997436523,
      "learning_rate": 9.946192307692307e-06,
      "loss": 4.8977,
      "step": 1041400
    },
    {
      "epoch": 2.4044899203043766,
      "grad_norm": 5.83574914932251,
      "learning_rate": 9.942346153846154e-06,
      "loss": 4.861,
      "step": 1041500
    },
    {
      "epoch": 2.404720788275601,
      "grad_norm": 5.911287784576416,
      "learning_rate": 9.9385e-06,
      "loss": 4.8718,
      "step": 1041600
    },
    {
      "epoch": 2.4049516562468254,
      "grad_norm": 5.435048580169678,
      "learning_rate": 9.934653846153847e-06,
      "loss": 4.8585,
      "step": 1041700
    },
    {
      "epoch": 2.4051825242180502,
      "grad_norm": 5.39851713180542,
      "learning_rate": 9.930807692307694e-06,
      "loss": 4.883,
      "step": 1041800
    },
    {
      "epoch": 2.4054133921892746,
      "grad_norm": 5.474602699279785,
      "learning_rate": 9.92696153846154e-06,
      "loss": 4.8607,
      "step": 1041900
    },
    {
      "epoch": 2.4056442601604995,
      "grad_norm": 6.492112159729004,
      "learning_rate": 9.923115384615385e-06,
      "loss": 4.845,
      "step": 1042000
    },
    {
      "epoch": 2.405875128131724,
      "grad_norm": 6.384956359863281,
      "learning_rate": 9.919269230769231e-06,
      "loss": 4.8632,
      "step": 1042100
    },
    {
      "epoch": 2.4061059961029487,
      "grad_norm": 6.713262557983398,
      "learning_rate": 9.915423076923078e-06,
      "loss": 4.855,
      "step": 1042200
    },
    {
      "epoch": 2.406336864074173,
      "grad_norm": 5.9431939125061035,
      "learning_rate": 9.911576923076924e-06,
      "loss": 4.8779,
      "step": 1042300
    },
    {
      "epoch": 2.406567732045398,
      "grad_norm": 5.164254188537598,
      "learning_rate": 9.90773076923077e-06,
      "loss": 4.85,
      "step": 1042400
    },
    {
      "epoch": 2.4067986000166224,
      "grad_norm": 5.490323543548584,
      "learning_rate": 9.903884615384616e-06,
      "loss": 4.8062,
      "step": 1042500
    },
    {
      "epoch": 2.4070294679878472,
      "grad_norm": 6.577178001403809,
      "learning_rate": 9.900038461538462e-06,
      "loss": 4.906,
      "step": 1042600
    },
    {
      "epoch": 2.4072603359590716,
      "grad_norm": 6.158863544464111,
      "learning_rate": 9.896192307692308e-06,
      "loss": 4.824,
      "step": 1042700
    },
    {
      "epoch": 2.4074912039302965,
      "grad_norm": 5.367504119873047,
      "learning_rate": 9.892346153846155e-06,
      "loss": 4.8178,
      "step": 1042800
    },
    {
      "epoch": 2.407722071901521,
      "grad_norm": 6.434636116027832,
      "learning_rate": 9.8885e-06,
      "loss": 4.8014,
      "step": 1042900
    },
    {
      "epoch": 2.4079529398727457,
      "grad_norm": 5.966330051422119,
      "learning_rate": 9.884653846153846e-06,
      "loss": 4.8644,
      "step": 1043000
    },
    {
      "epoch": 2.40818380784397,
      "grad_norm": 7.784609794616699,
      "learning_rate": 9.880807692307693e-06,
      "loss": 4.8595,
      "step": 1043100
    },
    {
      "epoch": 2.408414675815195,
      "grad_norm": 4.898784160614014,
      "learning_rate": 9.876961538461539e-06,
      "loss": 4.8988,
      "step": 1043200
    },
    {
      "epoch": 2.4086455437864194,
      "grad_norm": 5.819375038146973,
      "learning_rate": 9.873115384615384e-06,
      "loss": 4.8888,
      "step": 1043300
    },
    {
      "epoch": 2.408876411757644,
      "grad_norm": 7.113189220428467,
      "learning_rate": 9.869269230769231e-06,
      "loss": 4.8447,
      "step": 1043400
    },
    {
      "epoch": 2.4091072797288686,
      "grad_norm": 5.500197410583496,
      "learning_rate": 9.865423076923077e-06,
      "loss": 4.8429,
      "step": 1043500
    },
    {
      "epoch": 2.409338147700093,
      "grad_norm": 5.438135147094727,
      "learning_rate": 9.861576923076924e-06,
      "loss": 4.7971,
      "step": 1043600
    },
    {
      "epoch": 2.409569015671318,
      "grad_norm": 8.983659744262695,
      "learning_rate": 9.85773076923077e-06,
      "loss": 4.8475,
      "step": 1043700
    },
    {
      "epoch": 2.4097998836425427,
      "grad_norm": 4.877320289611816,
      "learning_rate": 9.853884615384617e-06,
      "loss": 4.867,
      "step": 1043800
    },
    {
      "epoch": 2.410030751613767,
      "grad_norm": 6.371154308319092,
      "learning_rate": 9.850038461538462e-06,
      "loss": 4.8387,
      "step": 1043900
    },
    {
      "epoch": 2.4102616195849915,
      "grad_norm": 6.59906005859375,
      "learning_rate": 9.846192307692308e-06,
      "loss": 4.814,
      "step": 1044000
    },
    {
      "epoch": 2.4104924875562164,
      "grad_norm": 6.063108921051025,
      "learning_rate": 9.842346153846155e-06,
      "loss": 4.8376,
      "step": 1044100
    },
    {
      "epoch": 2.410723355527441,
      "grad_norm": 8.04069709777832,
      "learning_rate": 9.8385e-06,
      "loss": 4.872,
      "step": 1044200
    },
    {
      "epoch": 2.4109542234986656,
      "grad_norm": 5.5552849769592285,
      "learning_rate": 9.834653846153846e-06,
      "loss": 4.8625,
      "step": 1044300
    },
    {
      "epoch": 2.41118509146989,
      "grad_norm": 5.339895725250244,
      "learning_rate": 9.830807692307694e-06,
      "loss": 4.8444,
      "step": 1044400
    },
    {
      "epoch": 2.411415959441115,
      "grad_norm": 6.533586502075195,
      "learning_rate": 9.826961538461539e-06,
      "loss": 4.8953,
      "step": 1044500
    },
    {
      "epoch": 2.4116468274123393,
      "grad_norm": 6.552513122558594,
      "learning_rate": 9.823115384615385e-06,
      "loss": 4.8631,
      "step": 1044600
    },
    {
      "epoch": 2.411877695383564,
      "grad_norm": 7.4066481590271,
      "learning_rate": 9.819269230769232e-06,
      "loss": 4.8291,
      "step": 1044700
    },
    {
      "epoch": 2.4121085633547885,
      "grad_norm": 5.247705936431885,
      "learning_rate": 9.815423076923077e-06,
      "loss": 4.861,
      "step": 1044800
    },
    {
      "epoch": 2.4123394313260134,
      "grad_norm": 6.291362285614014,
      "learning_rate": 9.811576923076923e-06,
      "loss": 4.8414,
      "step": 1044900
    },
    {
      "epoch": 2.4125702992972378,
      "grad_norm": 5.33541202545166,
      "learning_rate": 9.807730769230768e-06,
      "loss": 4.8459,
      "step": 1045000
    },
    {
      "epoch": 2.4128011672684626,
      "grad_norm": 5.941393852233887,
      "learning_rate": 9.803884615384616e-06,
      "loss": 4.8379,
      "step": 1045100
    },
    {
      "epoch": 2.413032035239687,
      "grad_norm": 5.400592803955078,
      "learning_rate": 9.800038461538461e-06,
      "loss": 4.874,
      "step": 1045200
    },
    {
      "epoch": 2.413262903210912,
      "grad_norm": 5.70086669921875,
      "learning_rate": 9.796192307692307e-06,
      "loss": 4.8385,
      "step": 1045300
    },
    {
      "epoch": 2.4134937711821363,
      "grad_norm": 6.179137706756592,
      "learning_rate": 9.792346153846154e-06,
      "loss": 4.8499,
      "step": 1045400
    },
    {
      "epoch": 2.413724639153361,
      "grad_norm": 5.763725280761719,
      "learning_rate": 9.788500000000001e-06,
      "loss": 4.8651,
      "step": 1045500
    },
    {
      "epoch": 2.4139555071245855,
      "grad_norm": 6.646202087402344,
      "learning_rate": 9.784653846153847e-06,
      "loss": 4.8756,
      "step": 1045600
    },
    {
      "epoch": 2.4141863750958104,
      "grad_norm": 6.228455543518066,
      "learning_rate": 9.780807692307694e-06,
      "loss": 4.8436,
      "step": 1045700
    },
    {
      "epoch": 2.4144172430670348,
      "grad_norm": 6.318739891052246,
      "learning_rate": 9.77696153846154e-06,
      "loss": 4.8207,
      "step": 1045800
    },
    {
      "epoch": 2.4146481110382596,
      "grad_norm": 7.498494625091553,
      "learning_rate": 9.773115384615385e-06,
      "loss": 4.8466,
      "step": 1045900
    },
    {
      "epoch": 2.414878979009484,
      "grad_norm": 5.519885063171387,
      "learning_rate": 9.769269230769232e-06,
      "loss": 4.8697,
      "step": 1046000
    },
    {
      "epoch": 2.415109846980709,
      "grad_norm": 5.581141948699951,
      "learning_rate": 9.765423076923078e-06,
      "loss": 4.8482,
      "step": 1046100
    },
    {
      "epoch": 2.4153407149519333,
      "grad_norm": 6.941680908203125,
      "learning_rate": 9.761576923076923e-06,
      "loss": 4.8199,
      "step": 1046200
    },
    {
      "epoch": 2.4155715829231577,
      "grad_norm": 5.921256065368652,
      "learning_rate": 9.757730769230769e-06,
      "loss": 4.8741,
      "step": 1046300
    },
    {
      "epoch": 2.4158024508943825,
      "grad_norm": 6.751009941101074,
      "learning_rate": 9.753884615384616e-06,
      "loss": 4.8373,
      "step": 1046400
    },
    {
      "epoch": 2.4160333188656073,
      "grad_norm": 6.431741714477539,
      "learning_rate": 9.750038461538462e-06,
      "loss": 4.8598,
      "step": 1046500
    },
    {
      "epoch": 2.4162641868368318,
      "grad_norm": 6.453866481781006,
      "learning_rate": 9.746192307692307e-06,
      "loss": 4.8108,
      "step": 1046600
    },
    {
      "epoch": 2.416495054808056,
      "grad_norm": 6.2272868156433105,
      "learning_rate": 9.742346153846154e-06,
      "loss": 4.8288,
      "step": 1046700
    },
    {
      "epoch": 2.416725922779281,
      "grad_norm": 6.034584999084473,
      "learning_rate": 9.7385e-06,
      "loss": 4.8897,
      "step": 1046800
    },
    {
      "epoch": 2.416956790750506,
      "grad_norm": 6.539680004119873,
      "learning_rate": 9.734653846153846e-06,
      "loss": 4.8079,
      "step": 1046900
    },
    {
      "epoch": 2.4171876587217302,
      "grad_norm": 5.971768856048584,
      "learning_rate": 9.730807692307693e-06,
      "loss": 4.8518,
      "step": 1047000
    },
    {
      "epoch": 2.4174185266929547,
      "grad_norm": 5.140800476074219,
      "learning_rate": 9.726961538461538e-06,
      "loss": 4.8866,
      "step": 1047100
    },
    {
      "epoch": 2.4176493946641795,
      "grad_norm": 6.382874011993408,
      "learning_rate": 9.723115384615384e-06,
      "loss": 4.8241,
      "step": 1047200
    },
    {
      "epoch": 2.417880262635404,
      "grad_norm": 6.103481769561768,
      "learning_rate": 9.719269230769231e-06,
      "loss": 4.8016,
      "step": 1047300
    },
    {
      "epoch": 2.4181111306066287,
      "grad_norm": 5.78071928024292,
      "learning_rate": 9.715423076923078e-06,
      "loss": 4.861,
      "step": 1047400
    },
    {
      "epoch": 2.418341998577853,
      "grad_norm": 5.326607704162598,
      "learning_rate": 9.711576923076924e-06,
      "loss": 4.8351,
      "step": 1047500
    },
    {
      "epoch": 2.418572866549078,
      "grad_norm": 5.836429119110107,
      "learning_rate": 9.707730769230771e-06,
      "loss": 4.8439,
      "step": 1047600
    },
    {
      "epoch": 2.4188037345203024,
      "grad_norm": 6.197480201721191,
      "learning_rate": 9.703884615384617e-06,
      "loss": 4.8223,
      "step": 1047700
    },
    {
      "epoch": 2.4190346024915272,
      "grad_norm": 7.061473846435547,
      "learning_rate": 9.700038461538462e-06,
      "loss": 4.8477,
      "step": 1047800
    },
    {
      "epoch": 2.4192654704627516,
      "grad_norm": 5.115202903747559,
      "learning_rate": 9.696192307692308e-06,
      "loss": 4.792,
      "step": 1047900
    },
    {
      "epoch": 2.4194963384339765,
      "grad_norm": 7.7317705154418945,
      "learning_rate": 9.692346153846155e-06,
      "loss": 4.7801,
      "step": 1048000
    },
    {
      "epoch": 2.419727206405201,
      "grad_norm": 6.554105281829834,
      "learning_rate": 9.6885e-06,
      "loss": 4.8508,
      "step": 1048100
    },
    {
      "epoch": 2.4199580743764257,
      "grad_norm": 5.887393474578857,
      "learning_rate": 9.684653846153846e-06,
      "loss": 4.861,
      "step": 1048200
    },
    {
      "epoch": 2.42018894234765,
      "grad_norm": 5.976950645446777,
      "learning_rate": 9.680807692307693e-06,
      "loss": 4.8851,
      "step": 1048300
    },
    {
      "epoch": 2.420419810318875,
      "grad_norm": 9.846113204956055,
      "learning_rate": 9.676961538461539e-06,
      "loss": 4.8658,
      "step": 1048400
    },
    {
      "epoch": 2.4206506782900994,
      "grad_norm": 5.99943208694458,
      "learning_rate": 9.673115384615384e-06,
      "loss": 4.8558,
      "step": 1048500
    },
    {
      "epoch": 2.4208815462613242,
      "grad_norm": 5.792778015136719,
      "learning_rate": 9.669269230769232e-06,
      "loss": 4.84,
      "step": 1048600
    },
    {
      "epoch": 2.4211124142325486,
      "grad_norm": 8.39609432220459,
      "learning_rate": 9.665423076923077e-06,
      "loss": 4.7841,
      "step": 1048700
    },
    {
      "epoch": 2.4213432822037735,
      "grad_norm": 6.003523349761963,
      "learning_rate": 9.661576923076923e-06,
      "loss": 4.848,
      "step": 1048800
    },
    {
      "epoch": 2.421574150174998,
      "grad_norm": 9.155313491821289,
      "learning_rate": 9.65773076923077e-06,
      "loss": 4.8693,
      "step": 1048900
    },
    {
      "epoch": 2.4218050181462227,
      "grad_norm": 6.085245609283447,
      "learning_rate": 9.653884615384615e-06,
      "loss": 4.8607,
      "step": 1049000
    },
    {
      "epoch": 2.422035886117447,
      "grad_norm": 6.889721393585205,
      "learning_rate": 9.650038461538461e-06,
      "loss": 4.835,
      "step": 1049100
    },
    {
      "epoch": 2.422266754088672,
      "grad_norm": 5.214367866516113,
      "learning_rate": 9.646192307692308e-06,
      "loss": 4.8012,
      "step": 1049200
    },
    {
      "epoch": 2.4224976220598964,
      "grad_norm": 5.823917865753174,
      "learning_rate": 9.642346153846154e-06,
      "loss": 4.8245,
      "step": 1049300
    },
    {
      "epoch": 2.422728490031121,
      "grad_norm": 6.3809943199157715,
      "learning_rate": 9.638500000000001e-06,
      "loss": 4.8465,
      "step": 1049400
    },
    {
      "epoch": 2.4229593580023456,
      "grad_norm": 5.820158958435059,
      "learning_rate": 9.634653846153847e-06,
      "loss": 4.8547,
      "step": 1049500
    },
    {
      "epoch": 2.4231902259735705,
      "grad_norm": 6.552731990814209,
      "learning_rate": 9.630807692307694e-06,
      "loss": 4.8707,
      "step": 1049600
    },
    {
      "epoch": 2.423421093944795,
      "grad_norm": 5.228915691375732,
      "learning_rate": 9.62696153846154e-06,
      "loss": 4.8497,
      "step": 1049700
    },
    {
      "epoch": 2.4236519619160193,
      "grad_norm": 6.951007843017578,
      "learning_rate": 9.623115384615385e-06,
      "loss": 4.8257,
      "step": 1049800
    },
    {
      "epoch": 2.423882829887244,
      "grad_norm": 5.470736026763916,
      "learning_rate": 9.619269230769232e-06,
      "loss": 4.8222,
      "step": 1049900
    },
    {
      "epoch": 2.4241136978584685,
      "grad_norm": 6.149827003479004,
      "learning_rate": 9.615423076923078e-06,
      "loss": 4.8496,
      "step": 1050000
    },
    {
      "epoch": 2.4243445658296934,
      "grad_norm": 6.2438883781433105,
      "learning_rate": 9.611576923076923e-06,
      "loss": 4.8174,
      "step": 1050100
    },
    {
      "epoch": 2.424575433800918,
      "grad_norm": 7.883223533630371,
      "learning_rate": 9.60773076923077e-06,
      "loss": 4.8117,
      "step": 1050200
    },
    {
      "epoch": 2.4248063017721426,
      "grad_norm": 7.518067359924316,
      "learning_rate": 9.603884615384616e-06,
      "loss": 4.765,
      "step": 1050300
    },
    {
      "epoch": 2.425037169743367,
      "grad_norm": 5.747649192810059,
      "learning_rate": 9.600038461538461e-06,
      "loss": 4.8077,
      "step": 1050400
    },
    {
      "epoch": 2.425268037714592,
      "grad_norm": 6.355113506317139,
      "learning_rate": 9.596192307692309e-06,
      "loss": 4.8544,
      "step": 1050500
    },
    {
      "epoch": 2.4254989056858163,
      "grad_norm": 6.899766445159912,
      "learning_rate": 9.592346153846154e-06,
      "loss": 4.8024,
      "step": 1050600
    },
    {
      "epoch": 2.425729773657041,
      "grad_norm": 6.659787654876709,
      "learning_rate": 9.5885e-06,
      "loss": 4.7442,
      "step": 1050700
    },
    {
      "epoch": 2.4259606416282655,
      "grad_norm": 6.1420674324035645,
      "learning_rate": 9.584653846153845e-06,
      "loss": 4.8058,
      "step": 1050800
    },
    {
      "epoch": 2.4261915095994904,
      "grad_norm": 6.079891204833984,
      "learning_rate": 9.580807692307693e-06,
      "loss": 4.8293,
      "step": 1050900
    },
    {
      "epoch": 2.4264223775707148,
      "grad_norm": 5.498045444488525,
      "learning_rate": 9.576961538461538e-06,
      "loss": 4.8398,
      "step": 1051000
    },
    {
      "epoch": 2.4266532455419396,
      "grad_norm": 5.35301399230957,
      "learning_rate": 9.573115384615385e-06,
      "loss": 4.7942,
      "step": 1051100
    },
    {
      "epoch": 2.426884113513164,
      "grad_norm": 5.56532096862793,
      "learning_rate": 9.56926923076923e-06,
      "loss": 4.7815,
      "step": 1051200
    },
    {
      "epoch": 2.427114981484389,
      "grad_norm": 5.5009446144104,
      "learning_rate": 9.565423076923078e-06,
      "loss": 4.841,
      "step": 1051300
    },
    {
      "epoch": 2.4273458494556133,
      "grad_norm": 8.058436393737793,
      "learning_rate": 9.561576923076924e-06,
      "loss": 4.824,
      "step": 1051400
    },
    {
      "epoch": 2.427576717426838,
      "grad_norm": 7.714717388153076,
      "learning_rate": 9.557730769230771e-06,
      "loss": 4.786,
      "step": 1051500
    },
    {
      "epoch": 2.4278075853980625,
      "grad_norm": 6.361771106719971,
      "learning_rate": 9.553884615384616e-06,
      "loss": 4.8136,
      "step": 1051600
    },
    {
      "epoch": 2.4280384533692874,
      "grad_norm": 5.140561580657959,
      "learning_rate": 9.550038461538462e-06,
      "loss": 4.8383,
      "step": 1051700
    },
    {
      "epoch": 2.4282693213405118,
      "grad_norm": 5.440267562866211,
      "learning_rate": 9.54619230769231e-06,
      "loss": 4.7588,
      "step": 1051800
    },
    {
      "epoch": 2.4285001893117366,
      "grad_norm": 6.47092342376709,
      "learning_rate": 9.542346153846155e-06,
      "loss": 4.8555,
      "step": 1051900
    },
    {
      "epoch": 2.428731057282961,
      "grad_norm": 4.836989879608154,
      "learning_rate": 9.5385e-06,
      "loss": 4.8364,
      "step": 1052000
    },
    {
      "epoch": 2.4289619252541854,
      "grad_norm": 5.454584121704102,
      "learning_rate": 9.534653846153846e-06,
      "loss": 4.8139,
      "step": 1052100
    },
    {
      "epoch": 2.4291927932254103,
      "grad_norm": 6.999950885772705,
      "learning_rate": 9.530807692307693e-06,
      "loss": 4.8687,
      "step": 1052200
    },
    {
      "epoch": 2.429423661196635,
      "grad_norm": 6.322723388671875,
      "learning_rate": 9.526961538461539e-06,
      "loss": 4.8034,
      "step": 1052300
    },
    {
      "epoch": 2.4296545291678595,
      "grad_norm": 6.303269863128662,
      "learning_rate": 9.523115384615384e-06,
      "loss": 4.7662,
      "step": 1052400
    },
    {
      "epoch": 2.429885397139084,
      "grad_norm": 5.86306619644165,
      "learning_rate": 9.519269230769231e-06,
      "loss": 4.8376,
      "step": 1052500
    },
    {
      "epoch": 2.4301162651103088,
      "grad_norm": 6.849788665771484,
      "learning_rate": 9.515423076923077e-06,
      "loss": 4.811,
      "step": 1052600
    },
    {
      "epoch": 2.430347133081533,
      "grad_norm": 5.714391231536865,
      "learning_rate": 9.511576923076922e-06,
      "loss": 4.8219,
      "step": 1052700
    },
    {
      "epoch": 2.430578001052758,
      "grad_norm": 5.210625648498535,
      "learning_rate": 9.50773076923077e-06,
      "loss": 4.8591,
      "step": 1052800
    },
    {
      "epoch": 2.4308088690239824,
      "grad_norm": 8.083600997924805,
      "learning_rate": 9.503884615384615e-06,
      "loss": 4.802,
      "step": 1052900
    },
    {
      "epoch": 2.4310397369952073,
      "grad_norm": 5.395679473876953,
      "learning_rate": 9.500038461538462e-06,
      "loss": 4.8179,
      "step": 1053000
    },
    {
      "epoch": 2.4312706049664317,
      "grad_norm": 5.209506511688232,
      "learning_rate": 9.496192307692308e-06,
      "loss": 4.78,
      "step": 1053100
    },
    {
      "epoch": 2.4315014729376565,
      "grad_norm": 5.1853928565979,
      "learning_rate": 9.492346153846155e-06,
      "loss": 4.8081,
      "step": 1053200
    },
    {
      "epoch": 2.431732340908881,
      "grad_norm": 5.890336513519287,
      "learning_rate": 9.4885e-06,
      "loss": 4.755,
      "step": 1053300
    },
    {
      "epoch": 2.4319632088801058,
      "grad_norm": 6.729462146759033,
      "learning_rate": 9.484653846153848e-06,
      "loss": 4.8053,
      "step": 1053400
    },
    {
      "epoch": 2.43219407685133,
      "grad_norm": 5.594569683074951,
      "learning_rate": 9.480807692307693e-06,
      "loss": 4.8141,
      "step": 1053500
    },
    {
      "epoch": 2.432424944822555,
      "grad_norm": 6.4278974533081055,
      "learning_rate": 9.476961538461539e-06,
      "loss": 4.7794,
      "step": 1053600
    },
    {
      "epoch": 2.4326558127937794,
      "grad_norm": 5.755687236785889,
      "learning_rate": 9.473115384615385e-06,
      "loss": 4.8106,
      "step": 1053700
    },
    {
      "epoch": 2.4328866807650043,
      "grad_norm": 5.307493686676025,
      "learning_rate": 9.469269230769232e-06,
      "loss": 4.7826,
      "step": 1053800
    },
    {
      "epoch": 2.4331175487362287,
      "grad_norm": 5.246915340423584,
      "learning_rate": 9.465423076923077e-06,
      "loss": 4.832,
      "step": 1053900
    },
    {
      "epoch": 2.4333484167074535,
      "grad_norm": 7.46438455581665,
      "learning_rate": 9.461576923076923e-06,
      "loss": 4.813,
      "step": 1054000
    },
    {
      "epoch": 2.433579284678678,
      "grad_norm": 6.0378336906433105,
      "learning_rate": 9.45773076923077e-06,
      "loss": 4.7413,
      "step": 1054100
    },
    {
      "epoch": 2.4338101526499027,
      "grad_norm": 5.768714427947998,
      "learning_rate": 9.453884615384616e-06,
      "loss": 4.7897,
      "step": 1054200
    },
    {
      "epoch": 2.434041020621127,
      "grad_norm": 8.746454238891602,
      "learning_rate": 9.450038461538461e-06,
      "loss": 4.8109,
      "step": 1054300
    },
    {
      "epoch": 2.434271888592352,
      "grad_norm": 5.585308074951172,
      "learning_rate": 9.446192307692308e-06,
      "loss": 4.7959,
      "step": 1054400
    },
    {
      "epoch": 2.4345027565635764,
      "grad_norm": 8.398807525634766,
      "learning_rate": 9.442346153846154e-06,
      "loss": 4.7831,
      "step": 1054500
    },
    {
      "epoch": 2.4347336245348012,
      "grad_norm": 5.404452323913574,
      "learning_rate": 9.4385e-06,
      "loss": 4.8001,
      "step": 1054600
    },
    {
      "epoch": 2.4349644925060256,
      "grad_norm": 5.880239009857178,
      "learning_rate": 9.434653846153847e-06,
      "loss": 4.7869,
      "step": 1054700
    },
    {
      "epoch": 2.43519536047725,
      "grad_norm": 6.018716335296631,
      "learning_rate": 9.430807692307692e-06,
      "loss": 4.7898,
      "step": 1054800
    },
    {
      "epoch": 2.435426228448475,
      "grad_norm": 4.943904876708984,
      "learning_rate": 9.42696153846154e-06,
      "loss": 4.8103,
      "step": 1054900
    },
    {
      "epoch": 2.4356570964196997,
      "grad_norm": 6.617527484893799,
      "learning_rate": 9.423115384615385e-06,
      "loss": 4.831,
      "step": 1055000
    },
    {
      "epoch": 2.435887964390924,
      "grad_norm": 6.366234302520752,
      "learning_rate": 9.419269230769232e-06,
      "loss": 4.7766,
      "step": 1055100
    },
    {
      "epoch": 2.4361188323621485,
      "grad_norm": 6.087441444396973,
      "learning_rate": 9.415423076923078e-06,
      "loss": 4.8171,
      "step": 1055200
    },
    {
      "epoch": 2.4363497003333734,
      "grad_norm": 6.241988182067871,
      "learning_rate": 9.411576923076923e-06,
      "loss": 4.8189,
      "step": 1055300
    },
    {
      "epoch": 2.436580568304598,
      "grad_norm": 6.8628997802734375,
      "learning_rate": 9.40773076923077e-06,
      "loss": 4.7758,
      "step": 1055400
    },
    {
      "epoch": 2.4368114362758226,
      "grad_norm": 6.203500747680664,
      "learning_rate": 9.403884615384616e-06,
      "loss": 4.8066,
      "step": 1055500
    },
    {
      "epoch": 2.437042304247047,
      "grad_norm": 6.069546699523926,
      "learning_rate": 9.400038461538462e-06,
      "loss": 4.7984,
      "step": 1055600
    },
    {
      "epoch": 2.437273172218272,
      "grad_norm": 6.082601547241211,
      "learning_rate": 9.396192307692309e-06,
      "loss": 4.7761,
      "step": 1055700
    },
    {
      "epoch": 2.4375040401894963,
      "grad_norm": 5.822194576263428,
      "learning_rate": 9.392346153846154e-06,
      "loss": 4.8108,
      "step": 1055800
    },
    {
      "epoch": 2.437734908160721,
      "grad_norm": 5.985411167144775,
      "learning_rate": 9.3885e-06,
      "loss": 4.7793,
      "step": 1055900
    },
    {
      "epoch": 2.4379657761319455,
      "grad_norm": 7.555365085601807,
      "learning_rate": 9.384653846153847e-06,
      "loss": 4.8393,
      "step": 1056000
    },
    {
      "epoch": 2.4381966441031704,
      "grad_norm": 5.226141929626465,
      "learning_rate": 9.380807692307693e-06,
      "loss": 4.7724,
      "step": 1056100
    },
    {
      "epoch": 2.438427512074395,
      "grad_norm": 6.028830051422119,
      "learning_rate": 9.376961538461538e-06,
      "loss": 4.7917,
      "step": 1056200
    },
    {
      "epoch": 2.4386583800456196,
      "grad_norm": 6.186074256896973,
      "learning_rate": 9.373115384615386e-06,
      "loss": 4.8304,
      "step": 1056300
    },
    {
      "epoch": 2.438889248016844,
      "grad_norm": 5.707233905792236,
      "learning_rate": 9.369269230769231e-06,
      "loss": 4.7947,
      "step": 1056400
    },
    {
      "epoch": 2.439120115988069,
      "grad_norm": 5.3573431968688965,
      "learning_rate": 9.365423076923077e-06,
      "loss": 4.7853,
      "step": 1056500
    },
    {
      "epoch": 2.4393509839592933,
      "grad_norm": 5.205114364624023,
      "learning_rate": 9.361576923076922e-06,
      "loss": 4.8017,
      "step": 1056600
    },
    {
      "epoch": 2.439581851930518,
      "grad_norm": 5.639374732971191,
      "learning_rate": 9.35773076923077e-06,
      "loss": 4.8113,
      "step": 1056700
    },
    {
      "epoch": 2.4398127199017425,
      "grad_norm": 6.204634666442871,
      "learning_rate": 9.353884615384615e-06,
      "loss": 4.7721,
      "step": 1056800
    },
    {
      "epoch": 2.4400435878729674,
      "grad_norm": 6.7901387214660645,
      "learning_rate": 9.350038461538462e-06,
      "loss": 4.8154,
      "step": 1056900
    },
    {
      "epoch": 2.440274455844192,
      "grad_norm": 5.9400715827941895,
      "learning_rate": 9.34619230769231e-06,
      "loss": 4.7633,
      "step": 1057000
    },
    {
      "epoch": 2.4405053238154166,
      "grad_norm": 6.209484100341797,
      "learning_rate": 9.342346153846155e-06,
      "loss": 4.7447,
      "step": 1057100
    },
    {
      "epoch": 2.440736191786641,
      "grad_norm": 6.08447790145874,
      "learning_rate": 9.3385e-06,
      "loss": 4.7961,
      "step": 1057200
    },
    {
      "epoch": 2.440967059757866,
      "grad_norm": 7.57997989654541,
      "learning_rate": 9.334653846153848e-06,
      "loss": 4.7962,
      "step": 1057300
    },
    {
      "epoch": 2.4411979277290903,
      "grad_norm": 4.990363597869873,
      "learning_rate": 9.330807692307693e-06,
      "loss": 4.7291,
      "step": 1057400
    },
    {
      "epoch": 2.4414287957003147,
      "grad_norm": 6.174269199371338,
      "learning_rate": 9.326961538461539e-06,
      "loss": 4.8189,
      "step": 1057500
    },
    {
      "epoch": 2.4416596636715395,
      "grad_norm": 6.6528496742248535,
      "learning_rate": 9.323115384615386e-06,
      "loss": 4.8041,
      "step": 1057600
    },
    {
      "epoch": 2.4418905316427644,
      "grad_norm": 4.975367069244385,
      "learning_rate": 9.319269230769232e-06,
      "loss": 4.7689,
      "step": 1057700
    },
    {
      "epoch": 2.4421213996139888,
      "grad_norm": 5.64628791809082,
      "learning_rate": 9.315423076923077e-06,
      "loss": 4.8005,
      "step": 1057800
    },
    {
      "epoch": 2.442352267585213,
      "grad_norm": 5.8347625732421875,
      "learning_rate": 9.311576923076923e-06,
      "loss": 4.7697,
      "step": 1057900
    },
    {
      "epoch": 2.442583135556438,
      "grad_norm": 5.513698101043701,
      "learning_rate": 9.30773076923077e-06,
      "loss": 4.7787,
      "step": 1058000
    },
    {
      "epoch": 2.4428140035276624,
      "grad_norm": 6.107320785522461,
      "learning_rate": 9.303884615384615e-06,
      "loss": 4.7831,
      "step": 1058100
    },
    {
      "epoch": 2.4430448714988873,
      "grad_norm": 5.811714172363281,
      "learning_rate": 9.300038461538461e-06,
      "loss": 4.7651,
      "step": 1058200
    },
    {
      "epoch": 2.4432757394701117,
      "grad_norm": 6.557050704956055,
      "learning_rate": 9.296192307692308e-06,
      "loss": 4.8147,
      "step": 1058300
    },
    {
      "epoch": 2.4435066074413365,
      "grad_norm": 5.799239158630371,
      "learning_rate": 9.292346153846154e-06,
      "loss": 4.7557,
      "step": 1058400
    },
    {
      "epoch": 2.443737475412561,
      "grad_norm": 6.063109874725342,
      "learning_rate": 9.2885e-06,
      "loss": 4.7552,
      "step": 1058500
    },
    {
      "epoch": 2.4439683433837858,
      "grad_norm": 6.020979881286621,
      "learning_rate": 9.284653846153846e-06,
      "loss": 4.8034,
      "step": 1058600
    },
    {
      "epoch": 2.44419921135501,
      "grad_norm": 5.400900363922119,
      "learning_rate": 9.280807692307692e-06,
      "loss": 4.8057,
      "step": 1058700
    },
    {
      "epoch": 2.444430079326235,
      "grad_norm": 5.353227615356445,
      "learning_rate": 9.27696153846154e-06,
      "loss": 4.8209,
      "step": 1058800
    },
    {
      "epoch": 2.4446609472974594,
      "grad_norm": 6.037652969360352,
      "learning_rate": 9.273115384615386e-06,
      "loss": 4.8131,
      "step": 1058900
    },
    {
      "epoch": 2.4448918152686843,
      "grad_norm": 4.982831001281738,
      "learning_rate": 9.269269230769232e-06,
      "loss": 4.8133,
      "step": 1059000
    },
    {
      "epoch": 2.4451226832399087,
      "grad_norm": 6.259589195251465,
      "learning_rate": 9.265423076923078e-06,
      "loss": 4.8161,
      "step": 1059100
    },
    {
      "epoch": 2.4453535512111335,
      "grad_norm": 5.492422580718994,
      "learning_rate": 9.261576923076925e-06,
      "loss": 4.7701,
      "step": 1059200
    },
    {
      "epoch": 2.445584419182358,
      "grad_norm": 6.039803504943848,
      "learning_rate": 9.25773076923077e-06,
      "loss": 4.8185,
      "step": 1059300
    },
    {
      "epoch": 2.4458152871535828,
      "grad_norm": 5.5901994705200195,
      "learning_rate": 9.253884615384616e-06,
      "loss": 4.7457,
      "step": 1059400
    },
    {
      "epoch": 2.446046155124807,
      "grad_norm": 5.190131187438965,
      "learning_rate": 9.250038461538461e-06,
      "loss": 4.735,
      "step": 1059500
    },
    {
      "epoch": 2.446277023096032,
      "grad_norm": 5.712218284606934,
      "learning_rate": 9.246192307692309e-06,
      "loss": 4.793,
      "step": 1059600
    },
    {
      "epoch": 2.4465078910672564,
      "grad_norm": 6.165919303894043,
      "learning_rate": 9.242346153846154e-06,
      "loss": 4.7572,
      "step": 1059700
    },
    {
      "epoch": 2.4467387590384813,
      "grad_norm": 5.698319911956787,
      "learning_rate": 9.2385e-06,
      "loss": 4.7721,
      "step": 1059800
    },
    {
      "epoch": 2.4469696270097057,
      "grad_norm": 5.796990394592285,
      "learning_rate": 9.234653846153847e-06,
      "loss": 4.8021,
      "step": 1059900
    },
    {
      "epoch": 2.4472004949809305,
      "grad_norm": 5.993238925933838,
      "learning_rate": 9.230807692307692e-06,
      "loss": 4.7654,
      "step": 1060000
    },
    {
      "epoch": 2.447431362952155,
      "grad_norm": 5.395268440246582,
      "learning_rate": 9.226961538461538e-06,
      "loss": 4.7689,
      "step": 1060100
    },
    {
      "epoch": 2.4476622309233793,
      "grad_norm": 6.0064473152160645,
      "learning_rate": 9.223115384615385e-06,
      "loss": 4.7765,
      "step": 1060200
    },
    {
      "epoch": 2.447893098894604,
      "grad_norm": 8.123083114624023,
      "learning_rate": 9.21926923076923e-06,
      "loss": 4.7717,
      "step": 1060300
    },
    {
      "epoch": 2.448123966865829,
      "grad_norm": 8.89566707611084,
      "learning_rate": 9.215423076923076e-06,
      "loss": 4.7334,
      "step": 1060400
    },
    {
      "epoch": 2.4483548348370534,
      "grad_norm": 6.1368560791015625,
      "learning_rate": 9.211576923076924e-06,
      "loss": 4.8195,
      "step": 1060500
    },
    {
      "epoch": 2.448585702808278,
      "grad_norm": 6.005471706390381,
      "learning_rate": 9.207730769230769e-06,
      "loss": 4.7479,
      "step": 1060600
    },
    {
      "epoch": 2.4488165707795027,
      "grad_norm": 8.117039680480957,
      "learning_rate": 9.203884615384616e-06,
      "loss": 4.7907,
      "step": 1060700
    },
    {
      "epoch": 2.449047438750727,
      "grad_norm": 6.556567668914795,
      "learning_rate": 9.200038461538462e-06,
      "loss": 4.8002,
      "step": 1060800
    },
    {
      "epoch": 2.449278306721952,
      "grad_norm": 5.840853691101074,
      "learning_rate": 9.196192307692309e-06,
      "loss": 4.7105,
      "step": 1060900
    },
    {
      "epoch": 2.4495091746931763,
      "grad_norm": 5.675083160400391,
      "learning_rate": 9.192346153846155e-06,
      "loss": 4.8161,
      "step": 1061000
    },
    {
      "epoch": 2.449740042664401,
      "grad_norm": 5.518039703369141,
      "learning_rate": 9.1885e-06,
      "loss": 4.7663,
      "step": 1061100
    },
    {
      "epoch": 2.4499709106356256,
      "grad_norm": 6.497396469116211,
      "learning_rate": 9.184653846153847e-06,
      "loss": 4.8229,
      "step": 1061200
    },
    {
      "epoch": 2.4502017786068504,
      "grad_norm": 6.48982048034668,
      "learning_rate": 9.180807692307693e-06,
      "loss": 4.77,
      "step": 1061300
    },
    {
      "epoch": 2.450432646578075,
      "grad_norm": 5.72626256942749,
      "learning_rate": 9.176961538461539e-06,
      "loss": 4.741,
      "step": 1061400
    },
    {
      "epoch": 2.4506635145492996,
      "grad_norm": 5.563344478607178,
      "learning_rate": 9.173115384615386e-06,
      "loss": 4.8355,
      "step": 1061500
    },
    {
      "epoch": 2.450894382520524,
      "grad_norm": 5.910210132598877,
      "learning_rate": 9.169269230769231e-06,
      "loss": 4.7069,
      "step": 1061600
    },
    {
      "epoch": 2.451125250491749,
      "grad_norm": 8.053778648376465,
      "learning_rate": 9.165423076923077e-06,
      "loss": 4.7762,
      "step": 1061700
    },
    {
      "epoch": 2.4513561184629733,
      "grad_norm": 6.263401031494141,
      "learning_rate": 9.161576923076924e-06,
      "loss": 4.7904,
      "step": 1061800
    },
    {
      "epoch": 2.451586986434198,
      "grad_norm": 6.008975505828857,
      "learning_rate": 9.15773076923077e-06,
      "loss": 4.7959,
      "step": 1061900
    },
    {
      "epoch": 2.4518178544054225,
      "grad_norm": 6.400514602661133,
      "learning_rate": 9.153884615384615e-06,
      "loss": 4.8135,
      "step": 1062000
    },
    {
      "epoch": 2.4520487223766474,
      "grad_norm": 8.393131256103516,
      "learning_rate": 9.150038461538462e-06,
      "loss": 4.775,
      "step": 1062100
    },
    {
      "epoch": 2.452279590347872,
      "grad_norm": 6.700349807739258,
      "learning_rate": 9.146192307692308e-06,
      "loss": 4.7775,
      "step": 1062200
    },
    {
      "epoch": 2.4525104583190966,
      "grad_norm": 5.376187801361084,
      "learning_rate": 9.142346153846153e-06,
      "loss": 4.7985,
      "step": 1062300
    },
    {
      "epoch": 2.452741326290321,
      "grad_norm": 5.925873756408691,
      "learning_rate": 9.1385e-06,
      "loss": 4.8022,
      "step": 1062400
    },
    {
      "epoch": 2.452972194261546,
      "grad_norm": 5.500069618225098,
      "learning_rate": 9.134653846153846e-06,
      "loss": 4.7753,
      "step": 1062500
    },
    {
      "epoch": 2.4532030622327703,
      "grad_norm": 5.0853271484375,
      "learning_rate": 9.130807692307693e-06,
      "loss": 4.7924,
      "step": 1062600
    },
    {
      "epoch": 2.453433930203995,
      "grad_norm": 7.726743698120117,
      "learning_rate": 9.126961538461539e-06,
      "loss": 4.7704,
      "step": 1062700
    },
    {
      "epoch": 2.4536647981752195,
      "grad_norm": 6.113760948181152,
      "learning_rate": 9.123115384615386e-06,
      "loss": 4.7668,
      "step": 1062800
    },
    {
      "epoch": 2.453895666146444,
      "grad_norm": 7.88129997253418,
      "learning_rate": 9.119269230769232e-06,
      "loss": 4.7911,
      "step": 1062900
    },
    {
      "epoch": 2.454126534117669,
      "grad_norm": 5.940326690673828,
      "learning_rate": 9.115423076923077e-06,
      "loss": 4.8282,
      "step": 1063000
    },
    {
      "epoch": 2.4543574020888936,
      "grad_norm": 4.9289116859436035,
      "learning_rate": 9.111576923076925e-06,
      "loss": 4.7736,
      "step": 1063100
    },
    {
      "epoch": 2.454588270060118,
      "grad_norm": 6.754950046539307,
      "learning_rate": 9.10773076923077e-06,
      "loss": 4.7695,
      "step": 1063200
    },
    {
      "epoch": 2.4548191380313424,
      "grad_norm": 5.932910919189453,
      "learning_rate": 9.103884615384616e-06,
      "loss": 4.7511,
      "step": 1063300
    },
    {
      "epoch": 2.4550500060025673,
      "grad_norm": 5.166890621185303,
      "learning_rate": 9.100038461538463e-06,
      "loss": 4.7853,
      "step": 1063400
    },
    {
      "epoch": 2.4552808739737917,
      "grad_norm": 6.440891742706299,
      "learning_rate": 9.096192307692308e-06,
      "loss": 4.8046,
      "step": 1063500
    },
    {
      "epoch": 2.4555117419450165,
      "grad_norm": 5.203956604003906,
      "learning_rate": 9.092346153846154e-06,
      "loss": 4.7526,
      "step": 1063600
    },
    {
      "epoch": 2.455742609916241,
      "grad_norm": 6.18884801864624,
      "learning_rate": 9.0885e-06,
      "loss": 4.7542,
      "step": 1063700
    },
    {
      "epoch": 2.455973477887466,
      "grad_norm": 5.568850994110107,
      "learning_rate": 9.084653846153847e-06,
      "loss": 4.7647,
      "step": 1063800
    },
    {
      "epoch": 2.45620434585869,
      "grad_norm": 5.450789928436279,
      "learning_rate": 9.080807692307692e-06,
      "loss": 4.7699,
      "step": 1063900
    },
    {
      "epoch": 2.456435213829915,
      "grad_norm": 5.475252151489258,
      "learning_rate": 9.076961538461538e-06,
      "loss": 4.7927,
      "step": 1064000
    },
    {
      "epoch": 2.4566660818011394,
      "grad_norm": 5.979197978973389,
      "learning_rate": 9.073115384615385e-06,
      "loss": 4.7473,
      "step": 1064100
    },
    {
      "epoch": 2.4568969497723643,
      "grad_norm": 5.081279277801514,
      "learning_rate": 9.06926923076923e-06,
      "loss": 4.7828,
      "step": 1064200
    },
    {
      "epoch": 2.4571278177435887,
      "grad_norm": 5.812636852264404,
      "learning_rate": 9.065423076923076e-06,
      "loss": 4.7664,
      "step": 1064300
    },
    {
      "epoch": 2.4573586857148135,
      "grad_norm": 5.533359050750732,
      "learning_rate": 9.061576923076923e-06,
      "loss": 4.7965,
      "step": 1064400
    },
    {
      "epoch": 2.457589553686038,
      "grad_norm": 6.262587547302246,
      "learning_rate": 9.05773076923077e-06,
      "loss": 4.7686,
      "step": 1064500
    },
    {
      "epoch": 2.4578204216572628,
      "grad_norm": 6.080981731414795,
      "learning_rate": 9.053884615384616e-06,
      "loss": 4.7347,
      "step": 1064600
    },
    {
      "epoch": 2.458051289628487,
      "grad_norm": 5.695040225982666,
      "learning_rate": 9.050038461538463e-06,
      "loss": 4.7807,
      "step": 1064700
    },
    {
      "epoch": 2.458282157599712,
      "grad_norm": 5.278221130371094,
      "learning_rate": 9.046192307692309e-06,
      "loss": 4.8063,
      "step": 1064800
    },
    {
      "epoch": 2.4585130255709364,
      "grad_norm": 5.6848530769348145,
      "learning_rate": 9.042346153846154e-06,
      "loss": 4.7593,
      "step": 1064900
    },
    {
      "epoch": 2.4587438935421613,
      "grad_norm": 7.810461044311523,
      "learning_rate": 9.0385e-06,
      "loss": 4.7166,
      "step": 1065000
    },
    {
      "epoch": 2.4589747615133857,
      "grad_norm": 6.962747097015381,
      "learning_rate": 9.034653846153847e-06,
      "loss": 4.7469,
      "step": 1065100
    },
    {
      "epoch": 2.4592056294846105,
      "grad_norm": 7.06786584854126,
      "learning_rate": 9.030807692307693e-06,
      "loss": 4.7591,
      "step": 1065200
    },
    {
      "epoch": 2.459436497455835,
      "grad_norm": 5.640719890594482,
      "learning_rate": 9.026961538461538e-06,
      "loss": 4.7467,
      "step": 1065300
    },
    {
      "epoch": 2.4596673654270598,
      "grad_norm": 5.969348907470703,
      "learning_rate": 9.023115384615385e-06,
      "loss": 4.7557,
      "step": 1065400
    },
    {
      "epoch": 2.459898233398284,
      "grad_norm": 5.6531829833984375,
      "learning_rate": 9.019269230769231e-06,
      "loss": 4.7488,
      "step": 1065500
    },
    {
      "epoch": 2.4601291013695086,
      "grad_norm": 5.094539642333984,
      "learning_rate": 9.015423076923077e-06,
      "loss": 4.7205,
      "step": 1065600
    },
    {
      "epoch": 2.4603599693407334,
      "grad_norm": 6.156853675842285,
      "learning_rate": 9.011576923076924e-06,
      "loss": 4.7511,
      "step": 1065700
    },
    {
      "epoch": 2.4605908373119583,
      "grad_norm": 7.794825553894043,
      "learning_rate": 9.00773076923077e-06,
      "loss": 4.7163,
      "step": 1065800
    },
    {
      "epoch": 2.4608217052831827,
      "grad_norm": 5.95583438873291,
      "learning_rate": 9.003884615384615e-06,
      "loss": 4.7528,
      "step": 1065900
    },
    {
      "epoch": 2.461052573254407,
      "grad_norm": 6.70700740814209,
      "learning_rate": 9.000038461538462e-06,
      "loss": 4.7302,
      "step": 1066000
    },
    {
      "epoch": 2.461283441225632,
      "grad_norm": 5.3391242027282715,
      "learning_rate": 8.996192307692308e-06,
      "loss": 4.7295,
      "step": 1066100
    },
    {
      "epoch": 2.4615143091968563,
      "grad_norm": 5.299208641052246,
      "learning_rate": 8.992346153846153e-06,
      "loss": 4.7431,
      "step": 1066200
    },
    {
      "epoch": 2.461745177168081,
      "grad_norm": 6.077012538909912,
      "learning_rate": 8.9885e-06,
      "loss": 4.7576,
      "step": 1066300
    },
    {
      "epoch": 2.4619760451393056,
      "grad_norm": 6.306588649749756,
      "learning_rate": 8.984653846153848e-06,
      "loss": 4.7694,
      "step": 1066400
    },
    {
      "epoch": 2.4622069131105304,
      "grad_norm": 5.933670520782471,
      "learning_rate": 8.980807692307693e-06,
      "loss": 4.7571,
      "step": 1066500
    },
    {
      "epoch": 2.462437781081755,
      "grad_norm": 6.673639297485352,
      "learning_rate": 8.976961538461539e-06,
      "loss": 4.7827,
      "step": 1066600
    },
    {
      "epoch": 2.4626686490529797,
      "grad_norm": 5.408440589904785,
      "learning_rate": 8.973115384615386e-06,
      "loss": 4.7425,
      "step": 1066700
    },
    {
      "epoch": 2.462899517024204,
      "grad_norm": 5.188779354095459,
      "learning_rate": 8.969269230769232e-06,
      "loss": 4.758,
      "step": 1066800
    },
    {
      "epoch": 2.463130384995429,
      "grad_norm": 5.653736114501953,
      "learning_rate": 8.965423076923077e-06,
      "loss": 4.7013,
      "step": 1066900
    },
    {
      "epoch": 2.4633612529666533,
      "grad_norm": 6.423812389373779,
      "learning_rate": 8.961576923076924e-06,
      "loss": 4.8013,
      "step": 1067000
    },
    {
      "epoch": 2.463592120937878,
      "grad_norm": 7.139511585235596,
      "learning_rate": 8.95773076923077e-06,
      "loss": 4.7469,
      "step": 1067100
    },
    {
      "epoch": 2.4638229889091026,
      "grad_norm": 6.0763163566589355,
      "learning_rate": 8.953884615384615e-06,
      "loss": 4.7213,
      "step": 1067200
    },
    {
      "epoch": 2.4640538568803274,
      "grad_norm": 5.089937686920166,
      "learning_rate": 8.950038461538463e-06,
      "loss": 4.7226,
      "step": 1067300
    },
    {
      "epoch": 2.464284724851552,
      "grad_norm": 6.223272800445557,
      "learning_rate": 8.946192307692308e-06,
      "loss": 4.7284,
      "step": 1067400
    },
    {
      "epoch": 2.4645155928227767,
      "grad_norm": 6.489206790924072,
      "learning_rate": 8.942346153846154e-06,
      "loss": 4.7566,
      "step": 1067500
    },
    {
      "epoch": 2.464746460794001,
      "grad_norm": 5.042544364929199,
      "learning_rate": 8.938500000000001e-06,
      "loss": 4.779,
      "step": 1067600
    },
    {
      "epoch": 2.464977328765226,
      "grad_norm": 5.588564872741699,
      "learning_rate": 8.934653846153846e-06,
      "loss": 4.7514,
      "step": 1067700
    },
    {
      "epoch": 2.4652081967364503,
      "grad_norm": 6.174625873565674,
      "learning_rate": 8.930807692307692e-06,
      "loss": 4.7419,
      "step": 1067800
    },
    {
      "epoch": 2.465439064707675,
      "grad_norm": 5.702315807342529,
      "learning_rate": 8.926961538461538e-06,
      "loss": 4.7596,
      "step": 1067900
    },
    {
      "epoch": 2.4656699326788996,
      "grad_norm": 7.288851737976074,
      "learning_rate": 8.923115384615385e-06,
      "loss": 4.7542,
      "step": 1068000
    },
    {
      "epoch": 2.4659008006501244,
      "grad_norm": 5.772502899169922,
      "learning_rate": 8.91926923076923e-06,
      "loss": 4.7754,
      "step": 1068100
    },
    {
      "epoch": 2.466131668621349,
      "grad_norm": 5.252157211303711,
      "learning_rate": 8.915423076923078e-06,
      "loss": 4.7453,
      "step": 1068200
    },
    {
      "epoch": 2.466362536592573,
      "grad_norm": 5.359470844268799,
      "learning_rate": 8.911576923076923e-06,
      "loss": 4.7195,
      "step": 1068300
    },
    {
      "epoch": 2.466593404563798,
      "grad_norm": 5.9288530349731445,
      "learning_rate": 8.90773076923077e-06,
      "loss": 4.744,
      "step": 1068400
    },
    {
      "epoch": 2.466824272535023,
      "grad_norm": 5.600829124450684,
      "learning_rate": 8.903884615384616e-06,
      "loss": 4.7535,
      "step": 1068500
    },
    {
      "epoch": 2.4670551405062473,
      "grad_norm": 7.157437324523926,
      "learning_rate": 8.900038461538463e-06,
      "loss": 4.8048,
      "step": 1068600
    },
    {
      "epoch": 2.4672860084774717,
      "grad_norm": 6.404023170471191,
      "learning_rate": 8.896192307692309e-06,
      "loss": 4.7731,
      "step": 1068700
    },
    {
      "epoch": 2.4675168764486966,
      "grad_norm": 6.901087760925293,
      "learning_rate": 8.892346153846154e-06,
      "loss": 4.7464,
      "step": 1068800
    },
    {
      "epoch": 2.467747744419921,
      "grad_norm": 17.084503173828125,
      "learning_rate": 8.888500000000001e-06,
      "loss": 4.7288,
      "step": 1068900
    },
    {
      "epoch": 2.467978612391146,
      "grad_norm": 7.793504238128662,
      "learning_rate": 8.884653846153847e-06,
      "loss": 4.7436,
      "step": 1069000
    },
    {
      "epoch": 2.46820948036237,
      "grad_norm": 9.258609771728516,
      "learning_rate": 8.880807692307692e-06,
      "loss": 4.7721,
      "step": 1069100
    },
    {
      "epoch": 2.468440348333595,
      "grad_norm": 7.309492111206055,
      "learning_rate": 8.87696153846154e-06,
      "loss": 4.7203,
      "step": 1069200
    },
    {
      "epoch": 2.4686712163048194,
      "grad_norm": 6.158595085144043,
      "learning_rate": 8.873115384615385e-06,
      "loss": 4.7728,
      "step": 1069300
    },
    {
      "epoch": 2.4689020842760443,
      "grad_norm": 5.4937849044799805,
      "learning_rate": 8.86926923076923e-06,
      "loss": 4.7303,
      "step": 1069400
    },
    {
      "epoch": 2.4691329522472687,
      "grad_norm": 5.783698558807373,
      "learning_rate": 8.865423076923076e-06,
      "loss": 4.7392,
      "step": 1069500
    },
    {
      "epoch": 2.4693638202184935,
      "grad_norm": 6.769099235534668,
      "learning_rate": 8.861576923076924e-06,
      "loss": 4.7727,
      "step": 1069600
    },
    {
      "epoch": 2.469594688189718,
      "grad_norm": 6.72163724899292,
      "learning_rate": 8.857730769230769e-06,
      "loss": 4.7827,
      "step": 1069700
    },
    {
      "epoch": 2.469825556160943,
      "grad_norm": 6.057662010192871,
      "learning_rate": 8.853884615384615e-06,
      "loss": 4.751,
      "step": 1069800
    },
    {
      "epoch": 2.470056424132167,
      "grad_norm": 6.508346080780029,
      "learning_rate": 8.850038461538462e-06,
      "loss": 4.7585,
      "step": 1069900
    },
    {
      "epoch": 2.470287292103392,
      "grad_norm": 7.577728271484375,
      "learning_rate": 8.846192307692307e-06,
      "loss": 4.7535,
      "step": 1070000
    },
    {
      "epoch": 2.4705181600746164,
      "grad_norm": 6.224982738494873,
      "learning_rate": 8.842346153846155e-06,
      "loss": 4.7637,
      "step": 1070100
    },
    {
      "epoch": 2.4707490280458413,
      "grad_norm": 5.860757827758789,
      "learning_rate": 8.8385e-06,
      "loss": 4.7401,
      "step": 1070200
    },
    {
      "epoch": 2.4709798960170657,
      "grad_norm": 6.571061134338379,
      "learning_rate": 8.834653846153847e-06,
      "loss": 4.7547,
      "step": 1070300
    },
    {
      "epoch": 2.4712107639882905,
      "grad_norm": 5.2775468826293945,
      "learning_rate": 8.830807692307693e-06,
      "loss": 4.7397,
      "step": 1070400
    },
    {
      "epoch": 2.471441631959515,
      "grad_norm": 6.163997650146484,
      "learning_rate": 8.82696153846154e-06,
      "loss": 4.7181,
      "step": 1070500
    },
    {
      "epoch": 2.47167249993074,
      "grad_norm": 5.522949695587158,
      "learning_rate": 8.823115384615386e-06,
      "loss": 4.7883,
      "step": 1070600
    },
    {
      "epoch": 2.471903367901964,
      "grad_norm": 5.1348772048950195,
      "learning_rate": 8.819269230769231e-06,
      "loss": 4.7317,
      "step": 1070700
    },
    {
      "epoch": 2.472134235873189,
      "grad_norm": 7.14382791519165,
      "learning_rate": 8.815423076923077e-06,
      "loss": 4.7372,
      "step": 1070800
    },
    {
      "epoch": 2.4723651038444134,
      "grad_norm": 6.512324810028076,
      "learning_rate": 8.811576923076924e-06,
      "loss": 4.7319,
      "step": 1070900
    },
    {
      "epoch": 2.472595971815638,
      "grad_norm": 6.410308361053467,
      "learning_rate": 8.80773076923077e-06,
      "loss": 4.7301,
      "step": 1071000
    },
    {
      "epoch": 2.4728268397868627,
      "grad_norm": 5.5156683921813965,
      "learning_rate": 8.803884615384615e-06,
      "loss": 4.7453,
      "step": 1071100
    },
    {
      "epoch": 2.4730577077580875,
      "grad_norm": 5.182865142822266,
      "learning_rate": 8.800038461538462e-06,
      "loss": 4.7606,
      "step": 1071200
    },
    {
      "epoch": 2.473288575729312,
      "grad_norm": 5.846747875213623,
      "learning_rate": 8.796192307692308e-06,
      "loss": 4.7759,
      "step": 1071300
    },
    {
      "epoch": 2.4735194437005363,
      "grad_norm": 5.055730819702148,
      "learning_rate": 8.792346153846153e-06,
      "loss": 4.738,
      "step": 1071400
    },
    {
      "epoch": 2.473750311671761,
      "grad_norm": 6.224419116973877,
      "learning_rate": 8.7885e-06,
      "loss": 4.7474,
      "step": 1071500
    },
    {
      "epoch": 2.4739811796429856,
      "grad_norm": 6.04971170425415,
      "learning_rate": 8.784653846153846e-06,
      "loss": 4.7912,
      "step": 1071600
    },
    {
      "epoch": 2.4742120476142104,
      "grad_norm": 6.101316452026367,
      "learning_rate": 8.780807692307692e-06,
      "loss": 4.7707,
      "step": 1071700
    },
    {
      "epoch": 2.474442915585435,
      "grad_norm": 5.3389506340026855,
      "learning_rate": 8.776961538461539e-06,
      "loss": 4.7452,
      "step": 1071800
    },
    {
      "epoch": 2.4746737835566597,
      "grad_norm": 6.007370471954346,
      "learning_rate": 8.773115384615384e-06,
      "loss": 4.7525,
      "step": 1071900
    },
    {
      "epoch": 2.474904651527884,
      "grad_norm": 6.242787837982178,
      "learning_rate": 8.769269230769232e-06,
      "loss": 4.7032,
      "step": 1072000
    },
    {
      "epoch": 2.475135519499109,
      "grad_norm": 5.7482171058654785,
      "learning_rate": 8.765423076923077e-06,
      "loss": 4.7067,
      "step": 1072100
    },
    {
      "epoch": 2.4753663874703333,
      "grad_norm": 5.038801193237305,
      "learning_rate": 8.761576923076924e-06,
      "loss": 4.7553,
      "step": 1072200
    },
    {
      "epoch": 2.475597255441558,
      "grad_norm": 7.314121246337891,
      "learning_rate": 8.75773076923077e-06,
      "loss": 4.7653,
      "step": 1072300
    },
    {
      "epoch": 2.4758281234127826,
      "grad_norm": 5.145784378051758,
      "learning_rate": 8.753884615384616e-06,
      "loss": 4.7234,
      "step": 1072400
    },
    {
      "epoch": 2.4760589913840074,
      "grad_norm": 5.511430740356445,
      "learning_rate": 8.750038461538463e-06,
      "loss": 4.7435,
      "step": 1072500
    },
    {
      "epoch": 2.476289859355232,
      "grad_norm": 6.59376859664917,
      "learning_rate": 8.746192307692308e-06,
      "loss": 4.7567,
      "step": 1072600
    },
    {
      "epoch": 2.4765207273264567,
      "grad_norm": 6.046679973602295,
      "learning_rate": 8.742346153846154e-06,
      "loss": 4.6951,
      "step": 1072700
    },
    {
      "epoch": 2.476751595297681,
      "grad_norm": 5.231808185577393,
      "learning_rate": 8.738500000000001e-06,
      "loss": 4.7204,
      "step": 1072800
    },
    {
      "epoch": 2.476982463268906,
      "grad_norm": 5.263259410858154,
      "learning_rate": 8.734653846153847e-06,
      "loss": 4.7389,
      "step": 1072900
    },
    {
      "epoch": 2.4772133312401303,
      "grad_norm": 5.466324806213379,
      "learning_rate": 8.730807692307692e-06,
      "loss": 4.6938,
      "step": 1073000
    },
    {
      "epoch": 2.477444199211355,
      "grad_norm": 6.569479942321777,
      "learning_rate": 8.72696153846154e-06,
      "loss": 4.6975,
      "step": 1073100
    },
    {
      "epoch": 2.4776750671825796,
      "grad_norm": 5.600597381591797,
      "learning_rate": 8.723115384615385e-06,
      "loss": 4.7441,
      "step": 1073200
    },
    {
      "epoch": 2.4779059351538044,
      "grad_norm": 5.866616249084473,
      "learning_rate": 8.71926923076923e-06,
      "loss": 4.7135,
      "step": 1073300
    },
    {
      "epoch": 2.478136803125029,
      "grad_norm": 4.991322040557861,
      "learning_rate": 8.715423076923078e-06,
      "loss": 4.6987,
      "step": 1073400
    },
    {
      "epoch": 2.4783676710962537,
      "grad_norm": 7.38168478012085,
      "learning_rate": 8.711576923076923e-06,
      "loss": 4.7381,
      "step": 1073500
    },
    {
      "epoch": 2.478598539067478,
      "grad_norm": 5.402482509613037,
      "learning_rate": 8.707730769230769e-06,
      "loss": 4.7301,
      "step": 1073600
    },
    {
      "epoch": 2.4788294070387025,
      "grad_norm": 5.706015586853027,
      "learning_rate": 8.703884615384614e-06,
      "loss": 4.7235,
      "step": 1073700
    },
    {
      "epoch": 2.4790602750099273,
      "grad_norm": 6.948280334472656,
      "learning_rate": 8.700038461538462e-06,
      "loss": 4.7329,
      "step": 1073800
    },
    {
      "epoch": 2.479291142981152,
      "grad_norm": 5.964980125427246,
      "learning_rate": 8.696192307692307e-06,
      "loss": 4.7346,
      "step": 1073900
    },
    {
      "epoch": 2.4795220109523766,
      "grad_norm": 5.2257890701293945,
      "learning_rate": 8.692346153846154e-06,
      "loss": 4.6912,
      "step": 1074000
    },
    {
      "epoch": 2.479752878923601,
      "grad_norm": 6.261260032653809,
      "learning_rate": 8.688500000000002e-06,
      "loss": 4.6989,
      "step": 1074100
    },
    {
      "epoch": 2.479983746894826,
      "grad_norm": 5.581818580627441,
      "learning_rate": 8.684653846153847e-06,
      "loss": 4.6658,
      "step": 1074200
    },
    {
      "epoch": 2.48021461486605,
      "grad_norm": 5.8465895652771,
      "learning_rate": 8.680807692307693e-06,
      "loss": 4.7507,
      "step": 1074300
    },
    {
      "epoch": 2.480445482837275,
      "grad_norm": 6.1730122566223145,
      "learning_rate": 8.67696153846154e-06,
      "loss": 4.6799,
      "step": 1074400
    },
    {
      "epoch": 2.4806763508084995,
      "grad_norm": 5.121989727020264,
      "learning_rate": 8.673115384615385e-06,
      "loss": 4.6482,
      "step": 1074500
    },
    {
      "epoch": 2.4809072187797243,
      "grad_norm": 7.463449001312256,
      "learning_rate": 8.669269230769231e-06,
      "loss": 4.7272,
      "step": 1074600
    },
    {
      "epoch": 2.4811380867509487,
      "grad_norm": 5.663121700286865,
      "learning_rate": 8.665423076923078e-06,
      "loss": 4.716,
      "step": 1074700
    },
    {
      "epoch": 2.4813689547221736,
      "grad_norm": 6.131681442260742,
      "learning_rate": 8.661576923076924e-06,
      "loss": 4.7266,
      "step": 1074800
    },
    {
      "epoch": 2.481599822693398,
      "grad_norm": 5.611341953277588,
      "learning_rate": 8.65773076923077e-06,
      "loss": 4.7612,
      "step": 1074900
    },
    {
      "epoch": 2.481830690664623,
      "grad_norm": 5.30927848815918,
      "learning_rate": 8.653884615384617e-06,
      "loss": 4.7207,
      "step": 1075000
    },
    {
      "epoch": 2.482061558635847,
      "grad_norm": 6.447800636291504,
      "learning_rate": 8.650038461538462e-06,
      "loss": 4.7194,
      "step": 1075100
    },
    {
      "epoch": 2.482292426607072,
      "grad_norm": 6.3015546798706055,
      "learning_rate": 8.646192307692308e-06,
      "loss": 4.7202,
      "step": 1075200
    },
    {
      "epoch": 2.4825232945782965,
      "grad_norm": 5.134161472320557,
      "learning_rate": 8.642346153846153e-06,
      "loss": 4.7441,
      "step": 1075300
    },
    {
      "epoch": 2.4827541625495213,
      "grad_norm": 6.765268802642822,
      "learning_rate": 8.6385e-06,
      "loss": 4.7203,
      "step": 1075400
    },
    {
      "epoch": 2.4829850305207457,
      "grad_norm": 6.844722270965576,
      "learning_rate": 8.634653846153846e-06,
      "loss": 4.744,
      "step": 1075500
    },
    {
      "epoch": 2.4832158984919706,
      "grad_norm": 6.547153949737549,
      "learning_rate": 8.630807692307691e-06,
      "loss": 4.7033,
      "step": 1075600
    },
    {
      "epoch": 2.483446766463195,
      "grad_norm": 6.670027256011963,
      "learning_rate": 8.626961538461539e-06,
      "loss": 4.7612,
      "step": 1075700
    },
    {
      "epoch": 2.48367763443442,
      "grad_norm": 6.51768159866333,
      "learning_rate": 8.623115384615384e-06,
      "loss": 4.7291,
      "step": 1075800
    },
    {
      "epoch": 2.483908502405644,
      "grad_norm": 5.524089813232422,
      "learning_rate": 8.619269230769231e-06,
      "loss": 4.7579,
      "step": 1075900
    },
    {
      "epoch": 2.484139370376869,
      "grad_norm": 5.541207313537598,
      "learning_rate": 8.615423076923079e-06,
      "loss": 4.7591,
      "step": 1076000
    },
    {
      "epoch": 2.4843702383480935,
      "grad_norm": 5.653524398803711,
      "learning_rate": 8.611576923076924e-06,
      "loss": 4.7021,
      "step": 1076100
    },
    {
      "epoch": 2.4846011063193183,
      "grad_norm": 6.009559154510498,
      "learning_rate": 8.60773076923077e-06,
      "loss": 4.7364,
      "step": 1076200
    },
    {
      "epoch": 2.4848319742905427,
      "grad_norm": 5.806958198547363,
      "learning_rate": 8.603884615384617e-06,
      "loss": 4.7171,
      "step": 1076300
    },
    {
      "epoch": 2.485062842261767,
      "grad_norm": 6.566535949707031,
      "learning_rate": 8.600038461538463e-06,
      "loss": 4.6825,
      "step": 1076400
    },
    {
      "epoch": 2.485293710232992,
      "grad_norm": 7.558495044708252,
      "learning_rate": 8.596192307692308e-06,
      "loss": 4.7754,
      "step": 1076500
    },
    {
      "epoch": 2.485524578204217,
      "grad_norm": 5.669667720794678,
      "learning_rate": 8.592346153846154e-06,
      "loss": 4.7326,
      "step": 1076600
    },
    {
      "epoch": 2.485755446175441,
      "grad_norm": 6.209847927093506,
      "learning_rate": 8.588500000000001e-06,
      "loss": 4.7111,
      "step": 1076700
    },
    {
      "epoch": 2.4859863141466656,
      "grad_norm": 7.266294002532959,
      "learning_rate": 8.584653846153846e-06,
      "loss": 4.6992,
      "step": 1076800
    },
    {
      "epoch": 2.4862171821178904,
      "grad_norm": 6.660445690155029,
      "learning_rate": 8.580807692307692e-06,
      "loss": 4.726,
      "step": 1076900
    },
    {
      "epoch": 2.486448050089115,
      "grad_norm": 6.029411792755127,
      "learning_rate": 8.57696153846154e-06,
      "loss": 4.7632,
      "step": 1077000
    },
    {
      "epoch": 2.4866789180603397,
      "grad_norm": 5.104260444641113,
      "learning_rate": 8.573115384615385e-06,
      "loss": 4.6725,
      "step": 1077100
    },
    {
      "epoch": 2.486909786031564,
      "grad_norm": 6.4543938636779785,
      "learning_rate": 8.56926923076923e-06,
      "loss": 4.7125,
      "step": 1077200
    },
    {
      "epoch": 2.487140654002789,
      "grad_norm": 5.259544372558594,
      "learning_rate": 8.565423076923077e-06,
      "loss": 4.7325,
      "step": 1077300
    },
    {
      "epoch": 2.4873715219740133,
      "grad_norm": 5.4377851486206055,
      "learning_rate": 8.561576923076923e-06,
      "loss": 4.7491,
      "step": 1077400
    },
    {
      "epoch": 2.487602389945238,
      "grad_norm": 7.042457580566406,
      "learning_rate": 8.557730769230769e-06,
      "loss": 4.735,
      "step": 1077500
    },
    {
      "epoch": 2.4878332579164626,
      "grad_norm": 6.290253162384033,
      "learning_rate": 8.553884615384616e-06,
      "loss": 4.7223,
      "step": 1077600
    },
    {
      "epoch": 2.4880641258876874,
      "grad_norm": 5.625354766845703,
      "learning_rate": 8.550038461538461e-06,
      "loss": 4.7507,
      "step": 1077700
    },
    {
      "epoch": 2.488294993858912,
      "grad_norm": 5.900050163269043,
      "learning_rate": 8.546192307692309e-06,
      "loss": 4.739,
      "step": 1077800
    },
    {
      "epoch": 2.4885258618301367,
      "grad_norm": 5.886505126953125,
      "learning_rate": 8.542346153846154e-06,
      "loss": 4.7532,
      "step": 1077900
    },
    {
      "epoch": 2.488756729801361,
      "grad_norm": 5.845418930053711,
      "learning_rate": 8.538500000000001e-06,
      "loss": 4.7374,
      "step": 1078000
    },
    {
      "epoch": 2.488987597772586,
      "grad_norm": 6.205918788909912,
      "learning_rate": 8.534653846153847e-06,
      "loss": 4.7202,
      "step": 1078100
    },
    {
      "epoch": 2.4892184657438103,
      "grad_norm": 6.583683013916016,
      "learning_rate": 8.530807692307692e-06,
      "loss": 4.7082,
      "step": 1078200
    },
    {
      "epoch": 2.489449333715035,
      "grad_norm": 6.916409492492676,
      "learning_rate": 8.52696153846154e-06,
      "loss": 4.7379,
      "step": 1078300
    },
    {
      "epoch": 2.4896802016862596,
      "grad_norm": 6.356986999511719,
      "learning_rate": 8.523115384615385e-06,
      "loss": 4.7478,
      "step": 1078400
    },
    {
      "epoch": 2.4899110696574844,
      "grad_norm": 5.378293514251709,
      "learning_rate": 8.51926923076923e-06,
      "loss": 4.7101,
      "step": 1078500
    },
    {
      "epoch": 2.490141937628709,
      "grad_norm": 5.366555690765381,
      "learning_rate": 8.515423076923078e-06,
      "loss": 4.6804,
      "step": 1078600
    },
    {
      "epoch": 2.4903728055999337,
      "grad_norm": 8.103912353515625,
      "learning_rate": 8.511576923076924e-06,
      "loss": 4.7396,
      "step": 1078700
    },
    {
      "epoch": 2.490603673571158,
      "grad_norm": 8.701977729797363,
      "learning_rate": 8.507730769230769e-06,
      "loss": 4.7451,
      "step": 1078800
    },
    {
      "epoch": 2.490834541542383,
      "grad_norm": 5.166780948638916,
      "learning_rate": 8.503884615384616e-06,
      "loss": 4.7155,
      "step": 1078900
    },
    {
      "epoch": 2.4910654095136073,
      "grad_norm": 5.355058193206787,
      "learning_rate": 8.500038461538462e-06,
      "loss": 4.7101,
      "step": 1079000
    },
    {
      "epoch": 2.4912962774848317,
      "grad_norm": 6.8217692375183105,
      "learning_rate": 8.496192307692307e-06,
      "loss": 4.6878,
      "step": 1079100
    },
    {
      "epoch": 2.4915271454560566,
      "grad_norm": 8.21935749053955,
      "learning_rate": 8.492346153846155e-06,
      "loss": 4.7132,
      "step": 1079200
    },
    {
      "epoch": 2.4917580134272814,
      "grad_norm": 5.461580276489258,
      "learning_rate": 8.4885e-06,
      "loss": 4.6929,
      "step": 1079300
    },
    {
      "epoch": 2.491988881398506,
      "grad_norm": 6.305825710296631,
      "learning_rate": 8.484653846153846e-06,
      "loss": 4.6979,
      "step": 1079400
    },
    {
      "epoch": 2.4922197493697302,
      "grad_norm": 6.439722537994385,
      "learning_rate": 8.480807692307693e-06,
      "loss": 4.7127,
      "step": 1079500
    },
    {
      "epoch": 2.492450617340955,
      "grad_norm": 6.853301525115967,
      "learning_rate": 8.476961538461538e-06,
      "loss": 4.7078,
      "step": 1079600
    },
    {
      "epoch": 2.4926814853121795,
      "grad_norm": 4.981500148773193,
      "learning_rate": 8.473115384615386e-06,
      "loss": 4.7323,
      "step": 1079700
    },
    {
      "epoch": 2.4929123532834043,
      "grad_norm": 6.512912750244141,
      "learning_rate": 8.469269230769231e-06,
      "loss": 4.7461,
      "step": 1079800
    },
    {
      "epoch": 2.4931432212546287,
      "grad_norm": 5.69999885559082,
      "learning_rate": 8.465423076923078e-06,
      "loss": 4.7616,
      "step": 1079900
    },
    {
      "epoch": 2.4933740892258536,
      "grad_norm": 5.29785680770874,
      "learning_rate": 8.461576923076924e-06,
      "loss": 4.6913,
      "step": 1080000
    },
    {
      "epoch": 2.493604957197078,
      "grad_norm": 5.926019668579102,
      "learning_rate": 8.45773076923077e-06,
      "loss": 4.6981,
      "step": 1080100
    },
    {
      "epoch": 2.493835825168303,
      "grad_norm": 6.176147937774658,
      "learning_rate": 8.453884615384617e-06,
      "loss": 4.7309,
      "step": 1080200
    },
    {
      "epoch": 2.4940666931395272,
      "grad_norm": 5.829010486602783,
      "learning_rate": 8.450038461538462e-06,
      "loss": 4.7553,
      "step": 1080300
    },
    {
      "epoch": 2.494297561110752,
      "grad_norm": 5.862816333770752,
      "learning_rate": 8.446192307692308e-06,
      "loss": 4.6884,
      "step": 1080400
    },
    {
      "epoch": 2.4945284290819765,
      "grad_norm": 6.078411102294922,
      "learning_rate": 8.442346153846155e-06,
      "loss": 4.683,
      "step": 1080500
    },
    {
      "epoch": 2.4947592970532013,
      "grad_norm": 5.221292972564697,
      "learning_rate": 8.4385e-06,
      "loss": 4.686,
      "step": 1080600
    },
    {
      "epoch": 2.4949901650244257,
      "grad_norm": 8.283705711364746,
      "learning_rate": 8.434653846153846e-06,
      "loss": 4.7453,
      "step": 1080700
    },
    {
      "epoch": 2.4952210329956506,
      "grad_norm": 7.67403507232666,
      "learning_rate": 8.430807692307692e-06,
      "loss": 4.7132,
      "step": 1080800
    },
    {
      "epoch": 2.495451900966875,
      "grad_norm": 5.904220104217529,
      "learning_rate": 8.426961538461539e-06,
      "loss": 4.7227,
      "step": 1080900
    },
    {
      "epoch": 2.4956827689381,
      "grad_norm": 7.789488315582275,
      "learning_rate": 8.423115384615384e-06,
      "loss": 4.7541,
      "step": 1081000
    },
    {
      "epoch": 2.495913636909324,
      "grad_norm": 6.182951927185059,
      "learning_rate": 8.41926923076923e-06,
      "loss": 4.7323,
      "step": 1081100
    },
    {
      "epoch": 2.496144504880549,
      "grad_norm": 5.196172714233398,
      "learning_rate": 8.415423076923077e-06,
      "loss": 4.7375,
      "step": 1081200
    },
    {
      "epoch": 2.4963753728517735,
      "grad_norm": 7.17120885848999,
      "learning_rate": 8.411576923076923e-06,
      "loss": 4.6981,
      "step": 1081300
    },
    {
      "epoch": 2.4966062408229983,
      "grad_norm": 7.1465654373168945,
      "learning_rate": 8.407730769230768e-06,
      "loss": 4.6592,
      "step": 1081400
    },
    {
      "epoch": 2.4968371087942227,
      "grad_norm": 5.757106304168701,
      "learning_rate": 8.403884615384616e-06,
      "loss": 4.7158,
      "step": 1081500
    },
    {
      "epoch": 2.4970679767654476,
      "grad_norm": 6.812759876251221,
      "learning_rate": 8.400038461538463e-06,
      "loss": 4.7337,
      "step": 1081600
    },
    {
      "epoch": 2.497298844736672,
      "grad_norm": 8.72077465057373,
      "learning_rate": 8.396192307692308e-06,
      "loss": 4.7135,
      "step": 1081700
    },
    {
      "epoch": 2.4975297127078964,
      "grad_norm": 8.493518829345703,
      "learning_rate": 8.392346153846156e-06,
      "loss": 4.7076,
      "step": 1081800
    },
    {
      "epoch": 2.497760580679121,
      "grad_norm": 5.785113334655762,
      "learning_rate": 8.388500000000001e-06,
      "loss": 4.7428,
      "step": 1081900
    },
    {
      "epoch": 2.497991448650346,
      "grad_norm": 6.091776371002197,
      "learning_rate": 8.384653846153847e-06,
      "loss": 4.6827,
      "step": 1082000
    },
    {
      "epoch": 2.4982223166215705,
      "grad_norm": 5.265056133270264,
      "learning_rate": 8.380807692307694e-06,
      "loss": 4.7226,
      "step": 1082100
    },
    {
      "epoch": 2.498453184592795,
      "grad_norm": 6.649998188018799,
      "learning_rate": 8.37696153846154e-06,
      "loss": 4.7649,
      "step": 1082200
    },
    {
      "epoch": 2.4986840525640197,
      "grad_norm": 5.104866027832031,
      "learning_rate": 8.373115384615385e-06,
      "loss": 4.7383,
      "step": 1082300
    },
    {
      "epoch": 2.498914920535244,
      "grad_norm": 7.3207688331604,
      "learning_rate": 8.36926923076923e-06,
      "loss": 4.7043,
      "step": 1082400
    },
    {
      "epoch": 2.499145788506469,
      "grad_norm": 5.578672409057617,
      "learning_rate": 8.365423076923078e-06,
      "loss": 4.7399,
      "step": 1082500
    },
    {
      "epoch": 2.4993766564776934,
      "grad_norm": 6.92471170425415,
      "learning_rate": 8.361576923076923e-06,
      "loss": 4.6982,
      "step": 1082600
    },
    {
      "epoch": 2.499607524448918,
      "grad_norm": 6.115197658538818,
      "learning_rate": 8.357730769230769e-06,
      "loss": 4.6827,
      "step": 1082700
    },
    {
      "epoch": 2.4998383924201426,
      "grad_norm": 8.885970115661621,
      "learning_rate": 8.353884615384616e-06,
      "loss": 4.7412,
      "step": 1082800
    },
    {
      "epoch": 2.5000692603913675,
      "grad_norm": 5.0669636726379395,
      "learning_rate": 8.350038461538462e-06,
      "loss": 4.6953,
      "step": 1082900
    },
    {
      "epoch": 2.500300128362592,
      "grad_norm": 7.077240467071533,
      "learning_rate": 8.346192307692307e-06,
      "loss": 4.7076,
      "step": 1083000
    },
    {
      "epoch": 2.5005309963338167,
      "grad_norm": 5.494563579559326,
      "learning_rate": 8.342346153846154e-06,
      "loss": 4.6952,
      "step": 1083100
    },
    {
      "epoch": 2.500761864305041,
      "grad_norm": 5.095261096954346,
      "learning_rate": 8.3385e-06,
      "loss": 4.6863,
      "step": 1083200
    },
    {
      "epoch": 2.500992732276266,
      "grad_norm": 5.717129707336426,
      "learning_rate": 8.334653846153845e-06,
      "loss": 4.7316,
      "step": 1083300
    },
    {
      "epoch": 2.5012236002474904,
      "grad_norm": 5.148199081420898,
      "learning_rate": 8.330807692307693e-06,
      "loss": 4.7027,
      "step": 1083400
    },
    {
      "epoch": 2.501454468218715,
      "grad_norm": 8.271669387817383,
      "learning_rate": 8.32696153846154e-06,
      "loss": 4.6904,
      "step": 1083500
    },
    {
      "epoch": 2.5016853361899396,
      "grad_norm": 7.041079998016357,
      "learning_rate": 8.323115384615385e-06,
      "loss": 4.6821,
      "step": 1083600
    },
    {
      "epoch": 2.5019162041611644,
      "grad_norm": 5.729174613952637,
      "learning_rate": 8.319269230769231e-06,
      "loss": 4.7186,
      "step": 1083700
    },
    {
      "epoch": 2.502147072132389,
      "grad_norm": 5.947240829467773,
      "learning_rate": 8.315423076923078e-06,
      "loss": 4.7103,
      "step": 1083800
    },
    {
      "epoch": 2.5023779401036137,
      "grad_norm": 5.397453308105469,
      "learning_rate": 8.311576923076924e-06,
      "loss": 4.727,
      "step": 1083900
    },
    {
      "epoch": 2.502608808074838,
      "grad_norm": 5.4285759925842285,
      "learning_rate": 8.30773076923077e-06,
      "loss": 4.7015,
      "step": 1084000
    },
    {
      "epoch": 2.5028396760460625,
      "grad_norm": 7.211451530456543,
      "learning_rate": 8.303884615384616e-06,
      "loss": 4.6953,
      "step": 1084100
    },
    {
      "epoch": 2.5030705440172873,
      "grad_norm": 6.380644798278809,
      "learning_rate": 8.300038461538462e-06,
      "loss": 4.6739,
      "step": 1084200
    },
    {
      "epoch": 2.503301411988512,
      "grad_norm": 5.556374549865723,
      "learning_rate": 8.296192307692308e-06,
      "loss": 4.6997,
      "step": 1084300
    },
    {
      "epoch": 2.5035322799597366,
      "grad_norm": 6.141529560089111,
      "learning_rate": 8.292346153846155e-06,
      "loss": 4.7011,
      "step": 1084400
    },
    {
      "epoch": 2.503763147930961,
      "grad_norm": 7.750216960906982,
      "learning_rate": 8.2885e-06,
      "loss": 4.7059,
      "step": 1084500
    },
    {
      "epoch": 2.503994015902186,
      "grad_norm": 5.536269664764404,
      "learning_rate": 8.284653846153846e-06,
      "loss": 4.7349,
      "step": 1084600
    },
    {
      "epoch": 2.5042248838734107,
      "grad_norm": 6.145575523376465,
      "learning_rate": 8.280807692307693e-06,
      "loss": 4.6816,
      "step": 1084700
    },
    {
      "epoch": 2.504455751844635,
      "grad_norm": 5.947544097900391,
      "learning_rate": 8.276961538461539e-06,
      "loss": 4.6994,
      "step": 1084800
    },
    {
      "epoch": 2.5046866198158595,
      "grad_norm": 5.717543125152588,
      "learning_rate": 8.273115384615384e-06,
      "loss": 4.6673,
      "step": 1084900
    },
    {
      "epoch": 2.5049174877870843,
      "grad_norm": 6.11472225189209,
      "learning_rate": 8.269269230769231e-06,
      "loss": 4.6671,
      "step": 1085000
    },
    {
      "epoch": 2.505148355758309,
      "grad_norm": 5.905416488647461,
      "learning_rate": 8.265423076923077e-06,
      "loss": 4.6713,
      "step": 1085100
    },
    {
      "epoch": 2.5053792237295336,
      "grad_norm": 6.355477333068848,
      "learning_rate": 8.261576923076923e-06,
      "loss": 4.7055,
      "step": 1085200
    },
    {
      "epoch": 2.505610091700758,
      "grad_norm": 5.598907470703125,
      "learning_rate": 8.25773076923077e-06,
      "loss": 4.6983,
      "step": 1085300
    },
    {
      "epoch": 2.505840959671983,
      "grad_norm": 6.38567590713501,
      "learning_rate": 8.253884615384615e-06,
      "loss": 4.6863,
      "step": 1085400
    },
    {
      "epoch": 2.5060718276432072,
      "grad_norm": 6.036622047424316,
      "learning_rate": 8.250038461538463e-06,
      "loss": 4.7045,
      "step": 1085500
    },
    {
      "epoch": 2.506302695614432,
      "grad_norm": 6.118834972381592,
      "learning_rate": 8.246192307692308e-06,
      "loss": 4.6951,
      "step": 1085600
    },
    {
      "epoch": 2.5065335635856565,
      "grad_norm": 7.3459153175354,
      "learning_rate": 8.242346153846155e-06,
      "loss": 4.724,
      "step": 1085700
    },
    {
      "epoch": 2.5067644315568813,
      "grad_norm": 5.377624988555908,
      "learning_rate": 8.2385e-06,
      "loss": 4.6705,
      "step": 1085800
    },
    {
      "epoch": 2.5069952995281057,
      "grad_norm": 6.286096096038818,
      "learning_rate": 8.234653846153846e-06,
      "loss": 4.6904,
      "step": 1085900
    },
    {
      "epoch": 2.5072261674993306,
      "grad_norm": 4.902608871459961,
      "learning_rate": 8.230807692307694e-06,
      "loss": 4.7116,
      "step": 1086000
    },
    {
      "epoch": 2.507457035470555,
      "grad_norm": 5.031330585479736,
      "learning_rate": 8.226961538461539e-06,
      "loss": 4.7189,
      "step": 1086100
    },
    {
      "epoch": 2.50768790344178,
      "grad_norm": 5.61232328414917,
      "learning_rate": 8.223115384615385e-06,
      "loss": 4.7472,
      "step": 1086200
    },
    {
      "epoch": 2.5079187714130042,
      "grad_norm": 5.509322643280029,
      "learning_rate": 8.219269230769232e-06,
      "loss": 4.6619,
      "step": 1086300
    },
    {
      "epoch": 2.508149639384229,
      "grad_norm": 6.234911918640137,
      "learning_rate": 8.215423076923077e-06,
      "loss": 4.6917,
      "step": 1086400
    },
    {
      "epoch": 2.5083805073554535,
      "grad_norm": 7.634627342224121,
      "learning_rate": 8.211576923076923e-06,
      "loss": 4.6809,
      "step": 1086500
    },
    {
      "epoch": 2.5086113753266783,
      "grad_norm": 5.696427822113037,
      "learning_rate": 8.207730769230769e-06,
      "loss": 4.7195,
      "step": 1086600
    },
    {
      "epoch": 2.5088422432979027,
      "grad_norm": 8.154646873474121,
      "learning_rate": 8.203884615384616e-06,
      "loss": 4.7608,
      "step": 1086700
    },
    {
      "epoch": 2.5090731112691276,
      "grad_norm": 5.966305732727051,
      "learning_rate": 8.200038461538461e-06,
      "loss": 4.6758,
      "step": 1086800
    },
    {
      "epoch": 2.509303979240352,
      "grad_norm": 6.5553879737854,
      "learning_rate": 8.196192307692307e-06,
      "loss": 4.7487,
      "step": 1086900
    },
    {
      "epoch": 2.509534847211577,
      "grad_norm": 7.023654937744141,
      "learning_rate": 8.192346153846154e-06,
      "loss": 4.6225,
      "step": 1087000
    },
    {
      "epoch": 2.5097657151828012,
      "grad_norm": 7.499586582183838,
      "learning_rate": 8.1885e-06,
      "loss": 4.7138,
      "step": 1087100
    },
    {
      "epoch": 2.5099965831540256,
      "grad_norm": 5.545510768890381,
      "learning_rate": 8.184653846153847e-06,
      "loss": 4.6905,
      "step": 1087200
    },
    {
      "epoch": 2.5102274511252505,
      "grad_norm": 5.284024238586426,
      "learning_rate": 8.180807692307692e-06,
      "loss": 4.6727,
      "step": 1087300
    },
    {
      "epoch": 2.5104583190964753,
      "grad_norm": 7.077063083648682,
      "learning_rate": 8.17696153846154e-06,
      "loss": 4.7023,
      "step": 1087400
    },
    {
      "epoch": 2.5106891870676997,
      "grad_norm": 5.276401042938232,
      "learning_rate": 8.173115384615385e-06,
      "loss": 4.6829,
      "step": 1087500
    },
    {
      "epoch": 2.510920055038924,
      "grad_norm": 5.679516315460205,
      "learning_rate": 8.169269230769232e-06,
      "loss": 4.6937,
      "step": 1087600
    },
    {
      "epoch": 2.511150923010149,
      "grad_norm": 6.578956604003906,
      "learning_rate": 8.165423076923078e-06,
      "loss": 4.7187,
      "step": 1087700
    },
    {
      "epoch": 2.511381790981374,
      "grad_norm": 6.165951728820801,
      "learning_rate": 8.161576923076923e-06,
      "loss": 4.654,
      "step": 1087800
    },
    {
      "epoch": 2.511612658952598,
      "grad_norm": 5.650866985321045,
      "learning_rate": 8.15773076923077e-06,
      "loss": 4.6397,
      "step": 1087900
    },
    {
      "epoch": 2.5118435269238226,
      "grad_norm": 6.242573261260986,
      "learning_rate": 8.153884615384616e-06,
      "loss": 4.6918,
      "step": 1088000
    },
    {
      "epoch": 2.5120743948950475,
      "grad_norm": 5.366567611694336,
      "learning_rate": 8.150038461538462e-06,
      "loss": 4.744,
      "step": 1088100
    },
    {
      "epoch": 2.512305262866272,
      "grad_norm": 6.764358997344971,
      "learning_rate": 8.146192307692307e-06,
      "loss": 4.7084,
      "step": 1088200
    },
    {
      "epoch": 2.5125361308374967,
      "grad_norm": 7.627740383148193,
      "learning_rate": 8.142346153846155e-06,
      "loss": 4.6965,
      "step": 1088300
    },
    {
      "epoch": 2.512766998808721,
      "grad_norm": 6.46082878112793,
      "learning_rate": 8.1385e-06,
      "loss": 4.6949,
      "step": 1088400
    },
    {
      "epoch": 2.512997866779946,
      "grad_norm": 5.9443745613098145,
      "learning_rate": 8.134653846153846e-06,
      "loss": 4.676,
      "step": 1088500
    },
    {
      "epoch": 2.5132287347511704,
      "grad_norm": 6.30594539642334,
      "learning_rate": 8.130807692307693e-06,
      "loss": 4.7658,
      "step": 1088600
    },
    {
      "epoch": 2.513459602722395,
      "grad_norm": 6.245987892150879,
      "learning_rate": 8.126961538461538e-06,
      "loss": 4.7141,
      "step": 1088700
    },
    {
      "epoch": 2.5136904706936196,
      "grad_norm": 4.944717884063721,
      "learning_rate": 8.123115384615384e-06,
      "loss": 4.6713,
      "step": 1088800
    },
    {
      "epoch": 2.5139213386648445,
      "grad_norm": 5.987293243408203,
      "learning_rate": 8.119269230769231e-06,
      "loss": 4.6528,
      "step": 1088900
    },
    {
      "epoch": 2.514152206636069,
      "grad_norm": 6.262804985046387,
      "learning_rate": 8.115423076923077e-06,
      "loss": 4.7027,
      "step": 1089000
    },
    {
      "epoch": 2.5143830746072937,
      "grad_norm": 6.249660968780518,
      "learning_rate": 8.111576923076924e-06,
      "loss": 4.704,
      "step": 1089100
    },
    {
      "epoch": 2.514613942578518,
      "grad_norm": 6.515023231506348,
      "learning_rate": 8.10773076923077e-06,
      "loss": 4.6519,
      "step": 1089200
    },
    {
      "epoch": 2.514844810549743,
      "grad_norm": 5.146068096160889,
      "learning_rate": 8.103884615384617e-06,
      "loss": 4.73,
      "step": 1089300
    },
    {
      "epoch": 2.5150756785209674,
      "grad_norm": 5.975722312927246,
      "learning_rate": 8.100038461538462e-06,
      "loss": 4.6752,
      "step": 1089400
    },
    {
      "epoch": 2.515306546492192,
      "grad_norm": 6.061029434204102,
      "learning_rate": 8.096192307692308e-06,
      "loss": 4.6979,
      "step": 1089500
    },
    {
      "epoch": 2.5155374144634166,
      "grad_norm": 5.448739051818848,
      "learning_rate": 8.092346153846155e-06,
      "loss": 4.6837,
      "step": 1089600
    },
    {
      "epoch": 2.5157682824346415,
      "grad_norm": 5.186996936798096,
      "learning_rate": 8.0885e-06,
      "loss": 4.7067,
      "step": 1089700
    },
    {
      "epoch": 2.515999150405866,
      "grad_norm": 5.935093879699707,
      "learning_rate": 8.084653846153846e-06,
      "loss": 4.7033,
      "step": 1089800
    },
    {
      "epoch": 2.5162300183770903,
      "grad_norm": 5.08854866027832,
      "learning_rate": 8.080807692307693e-06,
      "loss": 4.7282,
      "step": 1089900
    },
    {
      "epoch": 2.516460886348315,
      "grad_norm": 5.720367908477783,
      "learning_rate": 8.076961538461539e-06,
      "loss": 4.6783,
      "step": 1090000
    },
    {
      "epoch": 2.51669175431954,
      "grad_norm": 7.489949703216553,
      "learning_rate": 8.073115384615384e-06,
      "loss": 4.6634,
      "step": 1090100
    },
    {
      "epoch": 2.5169226222907644,
      "grad_norm": 6.295482158660889,
      "learning_rate": 8.069269230769232e-06,
      "loss": 4.7047,
      "step": 1090200
    },
    {
      "epoch": 2.5171534902619888,
      "grad_norm": 6.693918228149414,
      "learning_rate": 8.065423076923077e-06,
      "loss": 4.6673,
      "step": 1090300
    },
    {
      "epoch": 2.5173843582332136,
      "grad_norm": 6.289270401000977,
      "learning_rate": 8.061576923076923e-06,
      "loss": 4.7112,
      "step": 1090400
    },
    {
      "epoch": 2.5176152262044384,
      "grad_norm": 6.111064910888672,
      "learning_rate": 8.05773076923077e-06,
      "loss": 4.7056,
      "step": 1090500
    },
    {
      "epoch": 2.517846094175663,
      "grad_norm": 6.945882320404053,
      "learning_rate": 8.053884615384616e-06,
      "loss": 4.7013,
      "step": 1090600
    },
    {
      "epoch": 2.5180769621468873,
      "grad_norm": 6.7099995613098145,
      "learning_rate": 8.050038461538461e-06,
      "loss": 4.7315,
      "step": 1090700
    },
    {
      "epoch": 2.518307830118112,
      "grad_norm": 6.179964065551758,
      "learning_rate": 8.046192307692308e-06,
      "loss": 4.6711,
      "step": 1090800
    },
    {
      "epoch": 2.518538698089337,
      "grad_norm": 5.645125865936279,
      "learning_rate": 8.042346153846154e-06,
      "loss": 4.6891,
      "step": 1090900
    },
    {
      "epoch": 2.5187695660605613,
      "grad_norm": 5.504622459411621,
      "learning_rate": 8.0385e-06,
      "loss": 4.6751,
      "step": 1091000
    },
    {
      "epoch": 2.5190004340317858,
      "grad_norm": 7.28372859954834,
      "learning_rate": 8.034653846153847e-06,
      "loss": 4.7155,
      "step": 1091100
    },
    {
      "epoch": 2.5192313020030106,
      "grad_norm": 5.928168773651123,
      "learning_rate": 8.030807692307694e-06,
      "loss": 4.6911,
      "step": 1091200
    },
    {
      "epoch": 2.519462169974235,
      "grad_norm": 5.955520153045654,
      "learning_rate": 8.02696153846154e-06,
      "loss": 4.6719,
      "step": 1091300
    },
    {
      "epoch": 2.51969303794546,
      "grad_norm": 7.375112533569336,
      "learning_rate": 8.023115384615385e-06,
      "loss": 4.6683,
      "step": 1091400
    },
    {
      "epoch": 2.5199239059166842,
      "grad_norm": 6.059545516967773,
      "learning_rate": 8.019269230769232e-06,
      "loss": 4.6887,
      "step": 1091500
    },
    {
      "epoch": 2.520154773887909,
      "grad_norm": 5.015564918518066,
      "learning_rate": 8.015423076923078e-06,
      "loss": 4.669,
      "step": 1091600
    },
    {
      "epoch": 2.5203856418591335,
      "grad_norm": 7.796191215515137,
      "learning_rate": 8.011576923076923e-06,
      "loss": 4.6827,
      "step": 1091700
    },
    {
      "epoch": 2.5206165098303583,
      "grad_norm": 5.855520248413086,
      "learning_rate": 8.00773076923077e-06,
      "loss": 4.6892,
      "step": 1091800
    },
    {
      "epoch": 2.5208473778015827,
      "grad_norm": 6.188665390014648,
      "learning_rate": 8.003884615384616e-06,
      "loss": 4.6714,
      "step": 1091900
    },
    {
      "epoch": 2.5210782457728076,
      "grad_norm": 5.724023342132568,
      "learning_rate": 8.000038461538462e-06,
      "loss": 4.6818,
      "step": 1092000
    },
    {
      "epoch": 2.521309113744032,
      "grad_norm": 5.217788219451904,
      "learning_rate": 7.996192307692309e-06,
      "loss": 4.6772,
      "step": 1092100
    },
    {
      "epoch": 2.521539981715257,
      "grad_norm": 5.089122295379639,
      "learning_rate": 7.992346153846154e-06,
      "loss": 4.7285,
      "step": 1092200
    },
    {
      "epoch": 2.5217708496864812,
      "grad_norm": 7.317885398864746,
      "learning_rate": 7.9885e-06,
      "loss": 4.7075,
      "step": 1092300
    },
    {
      "epoch": 2.522001717657706,
      "grad_norm": 5.497722625732422,
      "learning_rate": 7.984653846153845e-06,
      "loss": 4.6956,
      "step": 1092400
    },
    {
      "epoch": 2.5222325856289305,
      "grad_norm": 5.858470916748047,
      "learning_rate": 7.980807692307693e-06,
      "loss": 4.6756,
      "step": 1092500
    },
    {
      "epoch": 2.522463453600155,
      "grad_norm": 5.678149700164795,
      "learning_rate": 7.976961538461538e-06,
      "loss": 4.6553,
      "step": 1092600
    },
    {
      "epoch": 2.5226943215713797,
      "grad_norm": 7.68992805480957,
      "learning_rate": 7.973115384615384e-06,
      "loss": 4.6533,
      "step": 1092700
    },
    {
      "epoch": 2.5229251895426046,
      "grad_norm": 5.803164482116699,
      "learning_rate": 7.969269230769231e-06,
      "loss": 4.6691,
      "step": 1092800
    },
    {
      "epoch": 2.523156057513829,
      "grad_norm": 6.681570053100586,
      "learning_rate": 7.965423076923076e-06,
      "loss": 4.6947,
      "step": 1092900
    },
    {
      "epoch": 2.5233869254850534,
      "grad_norm": 6.137105464935303,
      "learning_rate": 7.961576923076924e-06,
      "loss": 4.6757,
      "step": 1093000
    },
    {
      "epoch": 2.5236177934562782,
      "grad_norm": 5.69950008392334,
      "learning_rate": 7.957730769230771e-06,
      "loss": 4.6849,
      "step": 1093100
    },
    {
      "epoch": 2.523848661427503,
      "grad_norm": 6.091769695281982,
      "learning_rate": 7.953884615384616e-06,
      "loss": 4.6671,
      "step": 1093200
    },
    {
      "epoch": 2.5240795293987275,
      "grad_norm": 6.228732109069824,
      "learning_rate": 7.950038461538462e-06,
      "loss": 4.6669,
      "step": 1093300
    },
    {
      "epoch": 2.524310397369952,
      "grad_norm": 5.639925479888916,
      "learning_rate": 7.94619230769231e-06,
      "loss": 4.6662,
      "step": 1093400
    },
    {
      "epoch": 2.5245412653411767,
      "grad_norm": 7.606318473815918,
      "learning_rate": 7.942346153846155e-06,
      "loss": 4.7085,
      "step": 1093500
    },
    {
      "epoch": 2.5247721333124016,
      "grad_norm": 5.481118679046631,
      "learning_rate": 7.9385e-06,
      "loss": 4.6829,
      "step": 1093600
    },
    {
      "epoch": 2.525003001283626,
      "grad_norm": 6.464376926422119,
      "learning_rate": 7.934653846153848e-06,
      "loss": 4.6598,
      "step": 1093700
    },
    {
      "epoch": 2.5252338692548504,
      "grad_norm": 5.533239364624023,
      "learning_rate": 7.930807692307693e-06,
      "loss": 4.6461,
      "step": 1093800
    },
    {
      "epoch": 2.5254647372260752,
      "grad_norm": 5.94521427154541,
      "learning_rate": 7.926961538461539e-06,
      "loss": 4.6516,
      "step": 1093900
    },
    {
      "epoch": 2.5256956051972996,
      "grad_norm": 6.109067440032959,
      "learning_rate": 7.923115384615384e-06,
      "loss": 4.6789,
      "step": 1094000
    },
    {
      "epoch": 2.5259264731685245,
      "grad_norm": 5.472485065460205,
      "learning_rate": 7.919269230769231e-06,
      "loss": 4.701,
      "step": 1094100
    },
    {
      "epoch": 2.526157341139749,
      "grad_norm": 5.813121795654297,
      "learning_rate": 7.915423076923077e-06,
      "loss": 4.6809,
      "step": 1094200
    },
    {
      "epoch": 2.5263882091109737,
      "grad_norm": 9.418952941894531,
      "learning_rate": 7.911576923076922e-06,
      "loss": 4.7271,
      "step": 1094300
    },
    {
      "epoch": 2.526619077082198,
      "grad_norm": 6.015612602233887,
      "learning_rate": 7.90773076923077e-06,
      "loss": 4.6432,
      "step": 1094400
    },
    {
      "epoch": 2.526849945053423,
      "grad_norm": 5.776474475860596,
      "learning_rate": 7.903884615384615e-06,
      "loss": 4.7019,
      "step": 1094500
    },
    {
      "epoch": 2.5270808130246474,
      "grad_norm": 5.585885047912598,
      "learning_rate": 7.90003846153846e-06,
      "loss": 4.6959,
      "step": 1094600
    },
    {
      "epoch": 2.5273116809958722,
      "grad_norm": 5.426590919494629,
      "learning_rate": 7.896192307692308e-06,
      "loss": 4.6776,
      "step": 1094700
    },
    {
      "epoch": 2.5275425489670966,
      "grad_norm": 7.8854827880859375,
      "learning_rate": 7.892346153846154e-06,
      "loss": 4.6854,
      "step": 1094800
    },
    {
      "epoch": 2.5277734169383215,
      "grad_norm": 6.437156677246094,
      "learning_rate": 7.8885e-06,
      "loss": 4.68,
      "step": 1094900
    },
    {
      "epoch": 2.528004284909546,
      "grad_norm": 5.80014705657959,
      "learning_rate": 7.884653846153846e-06,
      "loss": 4.6922,
      "step": 1095000
    },
    {
      "epoch": 2.5282351528807707,
      "grad_norm": 5.45651912689209,
      "learning_rate": 7.880807692307694e-06,
      "loss": 4.6652,
      "step": 1095100
    },
    {
      "epoch": 2.528466020851995,
      "grad_norm": 5.867551803588867,
      "learning_rate": 7.876961538461539e-06,
      "loss": 4.7033,
      "step": 1095200
    },
    {
      "epoch": 2.5286968888232195,
      "grad_norm": 5.422336101531982,
      "learning_rate": 7.873115384615385e-06,
      "loss": 4.6203,
      "step": 1095300
    },
    {
      "epoch": 2.5289277567944444,
      "grad_norm": 5.416679859161377,
      "learning_rate": 7.869269230769232e-06,
      "loss": 4.629,
      "step": 1095400
    },
    {
      "epoch": 2.529158624765669,
      "grad_norm": 5.951381683349609,
      "learning_rate": 7.865423076923077e-06,
      "loss": 4.6791,
      "step": 1095500
    },
    {
      "epoch": 2.5293894927368936,
      "grad_norm": 5.725890159606934,
      "learning_rate": 7.861576923076923e-06,
      "loss": 4.6882,
      "step": 1095600
    },
    {
      "epoch": 2.529620360708118,
      "grad_norm": 7.1076483726501465,
      "learning_rate": 7.85773076923077e-06,
      "loss": 4.6818,
      "step": 1095700
    },
    {
      "epoch": 2.529851228679343,
      "grad_norm": 7.461616516113281,
      "learning_rate": 7.853884615384616e-06,
      "loss": 4.7061,
      "step": 1095800
    },
    {
      "epoch": 2.5300820966505677,
      "grad_norm": 7.374478340148926,
      "learning_rate": 7.850038461538461e-06,
      "loss": 4.6869,
      "step": 1095900
    },
    {
      "epoch": 2.530312964621792,
      "grad_norm": 6.086855411529541,
      "learning_rate": 7.846192307692308e-06,
      "loss": 4.7035,
      "step": 1096000
    },
    {
      "epoch": 2.5305438325930165,
      "grad_norm": 5.4167704582214355,
      "learning_rate": 7.842346153846154e-06,
      "loss": 4.7042,
      "step": 1096100
    },
    {
      "epoch": 2.5307747005642414,
      "grad_norm": 4.9659295082092285,
      "learning_rate": 7.8385e-06,
      "loss": 4.6518,
      "step": 1096200
    },
    {
      "epoch": 2.531005568535466,
      "grad_norm": 6.009027481079102,
      "learning_rate": 7.834653846153847e-06,
      "loss": 4.6522,
      "step": 1096300
    },
    {
      "epoch": 2.5312364365066906,
      "grad_norm": 5.48121976852417,
      "learning_rate": 7.830807692307692e-06,
      "loss": 4.6934,
      "step": 1096400
    },
    {
      "epoch": 2.531467304477915,
      "grad_norm": 5.773444175720215,
      "learning_rate": 7.826961538461538e-06,
      "loss": 4.6997,
      "step": 1096500
    },
    {
      "epoch": 2.53169817244914,
      "grad_norm": 5.708419322967529,
      "learning_rate": 7.823115384615385e-06,
      "loss": 4.6665,
      "step": 1096600
    },
    {
      "epoch": 2.5319290404203643,
      "grad_norm": 8.33138656616211,
      "learning_rate": 7.81926923076923e-06,
      "loss": 4.6764,
      "step": 1096700
    },
    {
      "epoch": 2.532159908391589,
      "grad_norm": 6.5168609619140625,
      "learning_rate": 7.815423076923078e-06,
      "loss": 4.7097,
      "step": 1096800
    },
    {
      "epoch": 2.5323907763628135,
      "grad_norm": 5.713618278503418,
      "learning_rate": 7.811576923076923e-06,
      "loss": 4.6678,
      "step": 1096900
    },
    {
      "epoch": 2.5326216443340384,
      "grad_norm": 5.953513145446777,
      "learning_rate": 7.80773076923077e-06,
      "loss": 4.6478,
      "step": 1097000
    },
    {
      "epoch": 2.5328525123052628,
      "grad_norm": 6.508925437927246,
      "learning_rate": 7.803884615384616e-06,
      "loss": 4.6768,
      "step": 1097100
    },
    {
      "epoch": 2.5330833802764876,
      "grad_norm": 5.981989860534668,
      "learning_rate": 7.800038461538462e-06,
      "loss": 4.6663,
      "step": 1097200
    },
    {
      "epoch": 2.533314248247712,
      "grad_norm": 8.784101486206055,
      "learning_rate": 7.796192307692309e-06,
      "loss": 4.6425,
      "step": 1097300
    },
    {
      "epoch": 2.533545116218937,
      "grad_norm": 6.13938045501709,
      "learning_rate": 7.792346153846155e-06,
      "loss": 4.7198,
      "step": 1097400
    },
    {
      "epoch": 2.5337759841901613,
      "grad_norm": 6.87965726852417,
      "learning_rate": 7.7885e-06,
      "loss": 4.6448,
      "step": 1097500
    },
    {
      "epoch": 2.534006852161386,
      "grad_norm": 6.393479347229004,
      "learning_rate": 7.784653846153847e-06,
      "loss": 4.6675,
      "step": 1097600
    },
    {
      "epoch": 2.5342377201326105,
      "grad_norm": 8.405909538269043,
      "learning_rate": 7.780807692307693e-06,
      "loss": 4.6891,
      "step": 1097700
    },
    {
      "epoch": 2.5344685881038354,
      "grad_norm": 8.981277465820312,
      "learning_rate": 7.776961538461538e-06,
      "loss": 4.6595,
      "step": 1097800
    },
    {
      "epoch": 2.5346994560750598,
      "grad_norm": 5.159241676330566,
      "learning_rate": 7.773115384615386e-06,
      "loss": 4.6683,
      "step": 1097900
    },
    {
      "epoch": 2.534930324046284,
      "grad_norm": 5.348891735076904,
      "learning_rate": 7.769269230769231e-06,
      "loss": 4.7111,
      "step": 1098000
    },
    {
      "epoch": 2.535161192017509,
      "grad_norm": 4.742798328399658,
      "learning_rate": 7.765423076923077e-06,
      "loss": 4.6568,
      "step": 1098100
    },
    {
      "epoch": 2.535392059988734,
      "grad_norm": 5.475077152252197,
      "learning_rate": 7.761576923076922e-06,
      "loss": 4.6512,
      "step": 1098200
    },
    {
      "epoch": 2.5356229279599583,
      "grad_norm": 5.909837245941162,
      "learning_rate": 7.75773076923077e-06,
      "loss": 4.6777,
      "step": 1098300
    },
    {
      "epoch": 2.5358537959311827,
      "grad_norm": 5.354619979858398,
      "learning_rate": 7.753884615384615e-06,
      "loss": 4.6762,
      "step": 1098400
    },
    {
      "epoch": 2.5360846639024075,
      "grad_norm": 5.297696113586426,
      "learning_rate": 7.75003846153846e-06,
      "loss": 4.6721,
      "step": 1098500
    },
    {
      "epoch": 2.5363155318736323,
      "grad_norm": 5.553512096405029,
      "learning_rate": 7.746192307692308e-06,
      "loss": 4.6612,
      "step": 1098600
    },
    {
      "epoch": 2.5365463998448567,
      "grad_norm": 6.5189104080200195,
      "learning_rate": 7.742346153846155e-06,
      "loss": 4.6818,
      "step": 1098700
    },
    {
      "epoch": 2.536777267816081,
      "grad_norm": 6.134567737579346,
      "learning_rate": 7.7385e-06,
      "loss": 4.6628,
      "step": 1098800
    },
    {
      "epoch": 2.537008135787306,
      "grad_norm": 6.945367336273193,
      "learning_rate": 7.734653846153848e-06,
      "loss": 4.6814,
      "step": 1098900
    },
    {
      "epoch": 2.537239003758531,
      "grad_norm": 6.714107036590576,
      "learning_rate": 7.730807692307693e-06,
      "loss": 4.6758,
      "step": 1099000
    },
    {
      "epoch": 2.5374698717297552,
      "grad_norm": 6.026492118835449,
      "learning_rate": 7.726961538461539e-06,
      "loss": 4.6229,
      "step": 1099100
    },
    {
      "epoch": 2.5377007397009796,
      "grad_norm": 6.863582134246826,
      "learning_rate": 7.723115384615386e-06,
      "loss": 4.6539,
      "step": 1099200
    },
    {
      "epoch": 2.5379316076722045,
      "grad_norm": 6.8088059425354,
      "learning_rate": 7.719269230769232e-06,
      "loss": 4.6695,
      "step": 1099300
    },
    {
      "epoch": 2.538162475643429,
      "grad_norm": 5.784916400909424,
      "learning_rate": 7.715423076923077e-06,
      "loss": 4.6629,
      "step": 1099400
    },
    {
      "epoch": 2.5383933436146537,
      "grad_norm": 6.166120529174805,
      "learning_rate": 7.711576923076923e-06,
      "loss": 4.7326,
      "step": 1099500
    },
    {
      "epoch": 2.538624211585878,
      "grad_norm": 6.29475736618042,
      "learning_rate": 7.70773076923077e-06,
      "loss": 4.6054,
      "step": 1099600
    },
    {
      "epoch": 2.538855079557103,
      "grad_norm": 6.422042369842529,
      "learning_rate": 7.703884615384615e-06,
      "loss": 4.6822,
      "step": 1099700
    },
    {
      "epoch": 2.5390859475283274,
      "grad_norm": 6.162586212158203,
      "learning_rate": 7.700038461538461e-06,
      "loss": 4.6409,
      "step": 1099800
    },
    {
      "epoch": 2.5393168154995522,
      "grad_norm": 5.624545574188232,
      "learning_rate": 7.696192307692308e-06,
      "loss": 4.6666,
      "step": 1099900
    },
    {
      "epoch": 2.5395476834707766,
      "grad_norm": 5.792885780334473,
      "learning_rate": 7.692346153846154e-06,
      "loss": 4.6808,
      "step": 1100000
    },
    {
      "epoch": 2.5397785514420015,
      "grad_norm": 5.569228172302246,
      "learning_rate": 7.6885e-06,
      "loss": 4.668,
      "step": 1100100
    },
    {
      "epoch": 2.540009419413226,
      "grad_norm": 6.206923484802246,
      "learning_rate": 7.684653846153847e-06,
      "loss": 4.6573,
      "step": 1100200
    },
    {
      "epoch": 2.5402402873844507,
      "grad_norm": 5.081302165985107,
      "learning_rate": 7.680807692307692e-06,
      "loss": 4.682,
      "step": 1100300
    },
    {
      "epoch": 2.540471155355675,
      "grad_norm": 7.11236047744751,
      "learning_rate": 7.676961538461538e-06,
      "loss": 4.6674,
      "step": 1100400
    },
    {
      "epoch": 2.5407020233269,
      "grad_norm": 6.9890666007995605,
      "learning_rate": 7.673115384615385e-06,
      "loss": 4.6635,
      "step": 1100500
    },
    {
      "epoch": 2.5409328912981244,
      "grad_norm": 5.510751724243164,
      "learning_rate": 7.669269230769232e-06,
      "loss": 4.6559,
      "step": 1100600
    },
    {
      "epoch": 2.541163759269349,
      "grad_norm": 6.6093549728393555,
      "learning_rate": 7.665423076923078e-06,
      "loss": 4.6774,
      "step": 1100700
    },
    {
      "epoch": 2.5413946272405736,
      "grad_norm": 6.69638204574585,
      "learning_rate": 7.661576923076925e-06,
      "loss": 4.6642,
      "step": 1100800
    },
    {
      "epoch": 2.5416254952117985,
      "grad_norm": 6.776202201843262,
      "learning_rate": 7.65773076923077e-06,
      "loss": 4.6909,
      "step": 1100900
    },
    {
      "epoch": 2.541856363183023,
      "grad_norm": 5.971412181854248,
      "learning_rate": 7.653884615384616e-06,
      "loss": 4.6009,
      "step": 1101000
    },
    {
      "epoch": 2.5420872311542473,
      "grad_norm": 5.7588419914245605,
      "learning_rate": 7.650038461538461e-06,
      "loss": 4.6651,
      "step": 1101100
    },
    {
      "epoch": 2.542318099125472,
      "grad_norm": 5.760491847991943,
      "learning_rate": 7.646192307692309e-06,
      "loss": 4.6874,
      "step": 1101200
    },
    {
      "epoch": 2.542548967096697,
      "grad_norm": 7.236048698425293,
      "learning_rate": 7.642346153846154e-06,
      "loss": 4.6249,
      "step": 1101300
    },
    {
      "epoch": 2.5427798350679214,
      "grad_norm": 5.578652381896973,
      "learning_rate": 7.6385e-06,
      "loss": 4.6617,
      "step": 1101400
    },
    {
      "epoch": 2.543010703039146,
      "grad_norm": 5.946843147277832,
      "learning_rate": 7.634653846153847e-06,
      "loss": 4.6711,
      "step": 1101500
    },
    {
      "epoch": 2.5432415710103706,
      "grad_norm": 6.2813944816589355,
      "learning_rate": 7.630807692307693e-06,
      "loss": 4.67,
      "step": 1101600
    },
    {
      "epoch": 2.5434724389815955,
      "grad_norm": 5.739607810974121,
      "learning_rate": 7.626961538461538e-06,
      "loss": 4.6926,
      "step": 1101700
    },
    {
      "epoch": 2.54370330695282,
      "grad_norm": 7.0765838623046875,
      "learning_rate": 7.623115384615385e-06,
      "loss": 4.6789,
      "step": 1101800
    },
    {
      "epoch": 2.5439341749240443,
      "grad_norm": 5.390957355499268,
      "learning_rate": 7.619269230769231e-06,
      "loss": 4.6788,
      "step": 1101900
    },
    {
      "epoch": 2.544165042895269,
      "grad_norm": 7.1947784423828125,
      "learning_rate": 7.615423076923077e-06,
      "loss": 4.6412,
      "step": 1102000
    },
    {
      "epoch": 2.5443959108664935,
      "grad_norm": 6.235762119293213,
      "learning_rate": 7.611576923076924e-06,
      "loss": 4.6823,
      "step": 1102100
    },
    {
      "epoch": 2.5446267788377184,
      "grad_norm": 5.863368034362793,
      "learning_rate": 7.60773076923077e-06,
      "loss": 4.6915,
      "step": 1102200
    },
    {
      "epoch": 2.5448576468089428,
      "grad_norm": 5.838420391082764,
      "learning_rate": 7.603884615384616e-06,
      "loss": 4.6371,
      "step": 1102300
    },
    {
      "epoch": 2.5450885147801676,
      "grad_norm": 6.664885520935059,
      "learning_rate": 7.600038461538461e-06,
      "loss": 4.6912,
      "step": 1102400
    },
    {
      "epoch": 2.545319382751392,
      "grad_norm": 5.700107097625732,
      "learning_rate": 7.596192307692308e-06,
      "loss": 4.664,
      "step": 1102500
    },
    {
      "epoch": 2.545550250722617,
      "grad_norm": 5.683457374572754,
      "learning_rate": 7.592346153846154e-06,
      "loss": 4.6873,
      "step": 1102600
    },
    {
      "epoch": 2.5457811186938413,
      "grad_norm": 6.255384922027588,
      "learning_rate": 7.5884999999999994e-06,
      "loss": 4.6761,
      "step": 1102700
    },
    {
      "epoch": 2.546011986665066,
      "grad_norm": 6.297535419464111,
      "learning_rate": 7.584653846153847e-06,
      "loss": 4.6553,
      "step": 1102800
    },
    {
      "epoch": 2.5462428546362905,
      "grad_norm": 4.978271484375,
      "learning_rate": 7.580807692307692e-06,
      "loss": 4.6552,
      "step": 1102900
    },
    {
      "epoch": 2.5464737226075154,
      "grad_norm": 9.296594619750977,
      "learning_rate": 7.576961538461539e-06,
      "loss": 4.6907,
      "step": 1103000
    },
    {
      "epoch": 2.5467045905787398,
      "grad_norm": 5.717981815338135,
      "learning_rate": 7.573115384615386e-06,
      "loss": 4.6452,
      "step": 1103100
    },
    {
      "epoch": 2.5469354585499646,
      "grad_norm": 5.282708644866943,
      "learning_rate": 7.569269230769231e-06,
      "loss": 4.6872,
      "step": 1103200
    },
    {
      "epoch": 2.547166326521189,
      "grad_norm": 5.781152725219727,
      "learning_rate": 7.565423076923077e-06,
      "loss": 4.6309,
      "step": 1103300
    },
    {
      "epoch": 2.5473971944924134,
      "grad_norm": 6.73910665512085,
      "learning_rate": 7.561576923076924e-06,
      "loss": 4.634,
      "step": 1103400
    },
    {
      "epoch": 2.5476280624636383,
      "grad_norm": 6.676779747009277,
      "learning_rate": 7.55773076923077e-06,
      "loss": 4.62,
      "step": 1103500
    },
    {
      "epoch": 2.547858930434863,
      "grad_norm": 5.294862747192383,
      "learning_rate": 7.553884615384615e-06,
      "loss": 4.6956,
      "step": 1103600
    },
    {
      "epoch": 2.5480897984060875,
      "grad_norm": 6.285812854766846,
      "learning_rate": 7.5500384615384624e-06,
      "loss": 4.6633,
      "step": 1103700
    },
    {
      "epoch": 2.548320666377312,
      "grad_norm": 6.307514667510986,
      "learning_rate": 7.546192307692308e-06,
      "loss": 4.6389,
      "step": 1103800
    },
    {
      "epoch": 2.5485515343485368,
      "grad_norm": 5.848395824432373,
      "learning_rate": 7.542346153846154e-06,
      "loss": 4.6528,
      "step": 1103900
    },
    {
      "epoch": 2.5487824023197616,
      "grad_norm": 6.331240653991699,
      "learning_rate": 7.5385e-06,
      "loss": 4.7062,
      "step": 1104000
    },
    {
      "epoch": 2.549013270290986,
      "grad_norm": 5.785828590393066,
      "learning_rate": 7.534653846153847e-06,
      "loss": 4.6392,
      "step": 1104100
    },
    {
      "epoch": 2.5492441382622104,
      "grad_norm": 6.0777177810668945,
      "learning_rate": 7.530807692307693e-06,
      "loss": 4.6718,
      "step": 1104200
    },
    {
      "epoch": 2.5494750062334353,
      "grad_norm": 7.270312309265137,
      "learning_rate": 7.526961538461538e-06,
      "loss": 4.6594,
      "step": 1104300
    },
    {
      "epoch": 2.54970587420466,
      "grad_norm": 5.94016695022583,
      "learning_rate": 7.5231153846153855e-06,
      "loss": 4.6637,
      "step": 1104400
    },
    {
      "epoch": 2.5499367421758845,
      "grad_norm": 6.506700038909912,
      "learning_rate": 7.519269230769231e-06,
      "loss": 4.7019,
      "step": 1104500
    },
    {
      "epoch": 2.550167610147109,
      "grad_norm": 5.47559118270874,
      "learning_rate": 7.5154230769230765e-06,
      "loss": 4.6609,
      "step": 1104600
    },
    {
      "epoch": 2.5503984781183338,
      "grad_norm": 6.413811683654785,
      "learning_rate": 7.511576923076924e-06,
      "loss": 4.679,
      "step": 1104700
    },
    {
      "epoch": 2.550629346089558,
      "grad_norm": 5.421149730682373,
      "learning_rate": 7.507730769230769e-06,
      "loss": 4.655,
      "step": 1104800
    },
    {
      "epoch": 2.550860214060783,
      "grad_norm": 9.643204689025879,
      "learning_rate": 7.503884615384616e-06,
      "loss": 4.6461,
      "step": 1104900
    },
    {
      "epoch": 2.5510910820320074,
      "grad_norm": 5.66576623916626,
      "learning_rate": 7.500038461538463e-06,
      "loss": 4.6749,
      "step": 1105000
    },
    {
      "epoch": 2.5513219500032323,
      "grad_norm": 5.472967147827148,
      "learning_rate": 7.4961923076923085e-06,
      "loss": 4.6654,
      "step": 1105100
    },
    {
      "epoch": 2.5515528179744567,
      "grad_norm": 9.374032974243164,
      "learning_rate": 7.492346153846154e-06,
      "loss": 4.674,
      "step": 1105200
    },
    {
      "epoch": 2.5517836859456815,
      "grad_norm": 5.693887710571289,
      "learning_rate": 7.4884999999999995e-06,
      "loss": 4.6498,
      "step": 1105300
    },
    {
      "epoch": 2.552014553916906,
      "grad_norm": 6.536951541900635,
      "learning_rate": 7.484653846153847e-06,
      "loss": 4.6412,
      "step": 1105400
    },
    {
      "epoch": 2.5522454218881307,
      "grad_norm": 5.820354461669922,
      "learning_rate": 7.480807692307692e-06,
      "loss": 4.6725,
      "step": 1105500
    },
    {
      "epoch": 2.552476289859355,
      "grad_norm": 5.248282432556152,
      "learning_rate": 7.476961538461538e-06,
      "loss": 4.7003,
      "step": 1105600
    },
    {
      "epoch": 2.55270715783058,
      "grad_norm": 6.288875579833984,
      "learning_rate": 7.473115384615385e-06,
      "loss": 4.6672,
      "step": 1105700
    },
    {
      "epoch": 2.5529380258018044,
      "grad_norm": 5.12559175491333,
      "learning_rate": 7.4692692307692315e-06,
      "loss": 4.6983,
      "step": 1105800
    },
    {
      "epoch": 2.5531688937730292,
      "grad_norm": 5.266486167907715,
      "learning_rate": 7.465423076923077e-06,
      "loss": 4.6138,
      "step": 1105900
    },
    {
      "epoch": 2.5533997617442536,
      "grad_norm": 8.363484382629395,
      "learning_rate": 7.461576923076924e-06,
      "loss": 4.6295,
      "step": 1106000
    },
    {
      "epoch": 2.553630629715478,
      "grad_norm": 5.507945537567139,
      "learning_rate": 7.45773076923077e-06,
      "loss": 4.6371,
      "step": 1106100
    },
    {
      "epoch": 2.553861497686703,
      "grad_norm": 6.964896202087402,
      "learning_rate": 7.453884615384615e-06,
      "loss": 4.6823,
      "step": 1106200
    },
    {
      "epoch": 2.5540923656579277,
      "grad_norm": 7.013787269592285,
      "learning_rate": 7.4500384615384626e-06,
      "loss": 4.6897,
      "step": 1106300
    },
    {
      "epoch": 2.554323233629152,
      "grad_norm": 6.7859601974487305,
      "learning_rate": 7.446192307692308e-06,
      "loss": 4.6603,
      "step": 1106400
    },
    {
      "epoch": 2.5545541016003765,
      "grad_norm": 7.097718715667725,
      "learning_rate": 7.442346153846154e-06,
      "loss": 4.6629,
      "step": 1106500
    },
    {
      "epoch": 2.5547849695716014,
      "grad_norm": 5.139368534088135,
      "learning_rate": 7.438500000000001e-06,
      "loss": 4.67,
      "step": 1106600
    },
    {
      "epoch": 2.5550158375428262,
      "grad_norm": 5.695727348327637,
      "learning_rate": 7.434653846153846e-06,
      "loss": 4.6432,
      "step": 1106700
    },
    {
      "epoch": 2.5552467055140506,
      "grad_norm": 5.405413627624512,
      "learning_rate": 7.430807692307693e-06,
      "loss": 4.618,
      "step": 1106800
    },
    {
      "epoch": 2.555477573485275,
      "grad_norm": 6.2481608390808105,
      "learning_rate": 7.426961538461538e-06,
      "loss": 4.6174,
      "step": 1106900
    },
    {
      "epoch": 2.5557084414565,
      "grad_norm": 7.1369733810424805,
      "learning_rate": 7.4231153846153856e-06,
      "loss": 4.6661,
      "step": 1107000
    },
    {
      "epoch": 2.5559393094277247,
      "grad_norm": 8.000985145568848,
      "learning_rate": 7.419269230769231e-06,
      "loss": 4.6252,
      "step": 1107100
    },
    {
      "epoch": 2.556170177398949,
      "grad_norm": 5.39471435546875,
      "learning_rate": 7.415423076923077e-06,
      "loss": 4.6283,
      "step": 1107200
    },
    {
      "epoch": 2.5564010453701735,
      "grad_norm": 5.2894511222839355,
      "learning_rate": 7.411576923076924e-06,
      "loss": 4.6272,
      "step": 1107300
    },
    {
      "epoch": 2.5566319133413984,
      "grad_norm": 6.259653568267822,
      "learning_rate": 7.407730769230769e-06,
      "loss": 4.6392,
      "step": 1107400
    },
    {
      "epoch": 2.556862781312623,
      "grad_norm": 5.892636299133301,
      "learning_rate": 7.403884615384615e-06,
      "loss": 4.6616,
      "step": 1107500
    },
    {
      "epoch": 2.5570936492838476,
      "grad_norm": 5.502834796905518,
      "learning_rate": 7.400038461538462e-06,
      "loss": 4.6941,
      "step": 1107600
    },
    {
      "epoch": 2.557324517255072,
      "grad_norm": 5.4024739265441895,
      "learning_rate": 7.396192307692308e-06,
      "loss": 4.6776,
      "step": 1107700
    },
    {
      "epoch": 2.557555385226297,
      "grad_norm": 5.705023288726807,
      "learning_rate": 7.392346153846154e-06,
      "loss": 4.6116,
      "step": 1107800
    },
    {
      "epoch": 2.5577862531975213,
      "grad_norm": 6.614461898803711,
      "learning_rate": 7.388500000000001e-06,
      "loss": 4.661,
      "step": 1107900
    },
    {
      "epoch": 2.558017121168746,
      "grad_norm": 6.61961555480957,
      "learning_rate": 7.384653846153847e-06,
      "loss": 4.6662,
      "step": 1108000
    },
    {
      "epoch": 2.5582479891399705,
      "grad_norm": 6.5882568359375,
      "learning_rate": 7.380807692307692e-06,
      "loss": 4.6406,
      "step": 1108100
    },
    {
      "epoch": 2.5584788571111954,
      "grad_norm": 6.912416458129883,
      "learning_rate": 7.376961538461538e-06,
      "loss": 4.628,
      "step": 1108200
    },
    {
      "epoch": 2.55870972508242,
      "grad_norm": 6.349175453186035,
      "learning_rate": 7.373115384615385e-06,
      "loss": 4.6251,
      "step": 1108300
    },
    {
      "epoch": 2.5589405930536446,
      "grad_norm": 4.894800186157227,
      "learning_rate": 7.369269230769231e-06,
      "loss": 4.6192,
      "step": 1108400
    },
    {
      "epoch": 2.559171461024869,
      "grad_norm": 5.955481052398682,
      "learning_rate": 7.365423076923077e-06,
      "loss": 4.6579,
      "step": 1108500
    },
    {
      "epoch": 2.559402328996094,
      "grad_norm": 6.009964942932129,
      "learning_rate": 7.3615769230769235e-06,
      "loss": 4.6299,
      "step": 1108600
    },
    {
      "epoch": 2.5596331969673183,
      "grad_norm": 5.4564208984375,
      "learning_rate": 7.35773076923077e-06,
      "loss": 4.666,
      "step": 1108700
    },
    {
      "epoch": 2.5598640649385427,
      "grad_norm": 6.076679706573486,
      "learning_rate": 7.3538846153846154e-06,
      "loss": 4.6542,
      "step": 1108800
    },
    {
      "epoch": 2.5600949329097675,
      "grad_norm": 5.685737609863281,
      "learning_rate": 7.350038461538463e-06,
      "loss": 4.6468,
      "step": 1108900
    },
    {
      "epoch": 2.5603258008809924,
      "grad_norm": 5.848730087280273,
      "learning_rate": 7.346192307692308e-06,
      "loss": 4.669,
      "step": 1109000
    },
    {
      "epoch": 2.5605566688522168,
      "grad_norm": 6.093011379241943,
      "learning_rate": 7.342346153846154e-06,
      "loss": 4.6287,
      "step": 1109100
    },
    {
      "epoch": 2.560787536823441,
      "grad_norm": 6.884397029876709,
      "learning_rate": 7.338500000000001e-06,
      "loss": 4.6424,
      "step": 1109200
    },
    {
      "epoch": 2.561018404794666,
      "grad_norm": 5.981021881103516,
      "learning_rate": 7.3346538461538465e-06,
      "loss": 4.6487,
      "step": 1109300
    },
    {
      "epoch": 2.561249272765891,
      "grad_norm": 6.114333152770996,
      "learning_rate": 7.330807692307692e-06,
      "loss": 4.6574,
      "step": 1109400
    },
    {
      "epoch": 2.5614801407371153,
      "grad_norm": 5.4960551261901855,
      "learning_rate": 7.326961538461539e-06,
      "loss": 4.6243,
      "step": 1109500
    },
    {
      "epoch": 2.5617110087083397,
      "grad_norm": 5.673609733581543,
      "learning_rate": 7.323115384615385e-06,
      "loss": 4.679,
      "step": 1109600
    },
    {
      "epoch": 2.5619418766795645,
      "grad_norm": 7.041971683502197,
      "learning_rate": 7.319269230769231e-06,
      "loss": 4.6978,
      "step": 1109700
    },
    {
      "epoch": 2.5621727446507894,
      "grad_norm": 5.319178581237793,
      "learning_rate": 7.315423076923077e-06,
      "loss": 4.6162,
      "step": 1109800
    },
    {
      "epoch": 2.5624036126220138,
      "grad_norm": 6.228233814239502,
      "learning_rate": 7.311576923076924e-06,
      "loss": 4.635,
      "step": 1109900
    },
    {
      "epoch": 2.562634480593238,
      "grad_norm": 5.751675128936768,
      "learning_rate": 7.3077307692307695e-06,
      "loss": 4.6313,
      "step": 1110000
    },
    {
      "epoch": 2.562865348564463,
      "grad_norm": 5.877601623535156,
      "learning_rate": 7.303884615384615e-06,
      "loss": 4.6741,
      "step": 1110100
    },
    {
      "epoch": 2.5630962165356874,
      "grad_norm": 5.748383522033691,
      "learning_rate": 7.300038461538462e-06,
      "loss": 4.6471,
      "step": 1110200
    },
    {
      "epoch": 2.5633270845069123,
      "grad_norm": 6.7142720222473145,
      "learning_rate": 7.296192307692308e-06,
      "loss": 4.6324,
      "step": 1110300
    },
    {
      "epoch": 2.5635579524781367,
      "grad_norm": 6.851410865783691,
      "learning_rate": 7.292346153846153e-06,
      "loss": 4.6583,
      "step": 1110400
    },
    {
      "epoch": 2.5637888204493615,
      "grad_norm": 5.580756664276123,
      "learning_rate": 7.288500000000001e-06,
      "loss": 4.6738,
      "step": 1110500
    },
    {
      "epoch": 2.564019688420586,
      "grad_norm": 5.566095352172852,
      "learning_rate": 7.284653846153847e-06,
      "loss": 4.6572,
      "step": 1110600
    },
    {
      "epoch": 2.5642505563918108,
      "grad_norm": 7.044559001922607,
      "learning_rate": 7.2808076923076925e-06,
      "loss": 4.688,
      "step": 1110700
    },
    {
      "epoch": 2.564481424363035,
      "grad_norm": 7.711679458618164,
      "learning_rate": 7.27696153846154e-06,
      "loss": 4.6491,
      "step": 1110800
    },
    {
      "epoch": 2.56471229233426,
      "grad_norm": 5.799066543579102,
      "learning_rate": 7.273115384615385e-06,
      "loss": 4.6377,
      "step": 1110900
    },
    {
      "epoch": 2.5649431603054844,
      "grad_norm": 7.120490074157715,
      "learning_rate": 7.269269230769231e-06,
      "loss": 4.6587,
      "step": 1111000
    },
    {
      "epoch": 2.5651740282767093,
      "grad_norm": 5.442714214324951,
      "learning_rate": 7.265423076923076e-06,
      "loss": 4.6547,
      "step": 1111100
    },
    {
      "epoch": 2.5654048962479337,
      "grad_norm": 6.032235145568848,
      "learning_rate": 7.261576923076924e-06,
      "loss": 4.6736,
      "step": 1111200
    },
    {
      "epoch": 2.5656357642191585,
      "grad_norm": 5.804880142211914,
      "learning_rate": 7.257730769230769e-06,
      "loss": 4.6669,
      "step": 1111300
    },
    {
      "epoch": 2.565866632190383,
      "grad_norm": 6.723594665527344,
      "learning_rate": 7.2538846153846155e-06,
      "loss": 4.6595,
      "step": 1111400
    },
    {
      "epoch": 2.5660975001616073,
      "grad_norm": 6.3940229415893555,
      "learning_rate": 7.250038461538462e-06,
      "loss": 4.6375,
      "step": 1111500
    },
    {
      "epoch": 2.566328368132832,
      "grad_norm": 6.779038906097412,
      "learning_rate": 7.246192307692308e-06,
      "loss": 4.6309,
      "step": 1111600
    },
    {
      "epoch": 2.566559236104057,
      "grad_norm": 6.1038432121276855,
      "learning_rate": 7.242346153846154e-06,
      "loss": 4.6426,
      "step": 1111700
    },
    {
      "epoch": 2.5667901040752814,
      "grad_norm": 7.384216785430908,
      "learning_rate": 7.238500000000001e-06,
      "loss": 4.6501,
      "step": 1111800
    },
    {
      "epoch": 2.567020972046506,
      "grad_norm": 5.666510105133057,
      "learning_rate": 7.234653846153847e-06,
      "loss": 4.6093,
      "step": 1111900
    },
    {
      "epoch": 2.5672518400177307,
      "grad_norm": 6.668756008148193,
      "learning_rate": 7.230807692307692e-06,
      "loss": 4.6831,
      "step": 1112000
    },
    {
      "epoch": 2.5674827079889555,
      "grad_norm": 5.9423136711120605,
      "learning_rate": 7.226961538461539e-06,
      "loss": 4.6729,
      "step": 1112100
    },
    {
      "epoch": 2.56771357596018,
      "grad_norm": 6.914087772369385,
      "learning_rate": 7.223115384615385e-06,
      "loss": 4.628,
      "step": 1112200
    },
    {
      "epoch": 2.5679444439314043,
      "grad_norm": 5.668954849243164,
      "learning_rate": 7.2192692307692305e-06,
      "loss": 4.6484,
      "step": 1112300
    },
    {
      "epoch": 2.568175311902629,
      "grad_norm": 6.4246110916137695,
      "learning_rate": 7.215423076923078e-06,
      "loss": 4.6864,
      "step": 1112400
    },
    {
      "epoch": 2.568406179873854,
      "grad_norm": 5.326198577880859,
      "learning_rate": 7.211576923076924e-06,
      "loss": 4.6758,
      "step": 1112500
    },
    {
      "epoch": 2.5686370478450784,
      "grad_norm": 6.746312141418457,
      "learning_rate": 7.20773076923077e-06,
      "loss": 4.6266,
      "step": 1112600
    },
    {
      "epoch": 2.568867915816303,
      "grad_norm": 6.057248592376709,
      "learning_rate": 7.203884615384615e-06,
      "loss": 4.6317,
      "step": 1112700
    },
    {
      "epoch": 2.5690987837875277,
      "grad_norm": 6.6555352210998535,
      "learning_rate": 7.200038461538462e-06,
      "loss": 4.6364,
      "step": 1112800
    },
    {
      "epoch": 2.569329651758752,
      "grad_norm": 6.229861259460449,
      "learning_rate": 7.196192307692308e-06,
      "loss": 4.6966,
      "step": 1112900
    },
    {
      "epoch": 2.569560519729977,
      "grad_norm": 6.3430705070495605,
      "learning_rate": 7.1923461538461535e-06,
      "loss": 4.6206,
      "step": 1113000
    },
    {
      "epoch": 2.5697913877012013,
      "grad_norm": 5.564675807952881,
      "learning_rate": 7.188500000000001e-06,
      "loss": 4.6243,
      "step": 1113100
    },
    {
      "epoch": 2.570022255672426,
      "grad_norm": 6.764963150024414,
      "learning_rate": 7.184653846153846e-06,
      "loss": 4.5848,
      "step": 1113200
    },
    {
      "epoch": 2.5702531236436506,
      "grad_norm": 5.409678936004639,
      "learning_rate": 7.180807692307693e-06,
      "loss": 4.6444,
      "step": 1113300
    },
    {
      "epoch": 2.5704839916148754,
      "grad_norm": 5.753258228302002,
      "learning_rate": 7.176961538461539e-06,
      "loss": 4.6548,
      "step": 1113400
    },
    {
      "epoch": 2.5707148595861,
      "grad_norm": 5.839659690856934,
      "learning_rate": 7.173115384615385e-06,
      "loss": 4.6444,
      "step": 1113500
    },
    {
      "epoch": 2.5709457275573246,
      "grad_norm": 6.572466850280762,
      "learning_rate": 7.169269230769231e-06,
      "loss": 4.69,
      "step": 1113600
    },
    {
      "epoch": 2.571176595528549,
      "grad_norm": 5.273560047149658,
      "learning_rate": 7.165423076923078e-06,
      "loss": 4.6605,
      "step": 1113700
    },
    {
      "epoch": 2.571407463499774,
      "grad_norm": 5.7803053855896,
      "learning_rate": 7.161576923076924e-06,
      "loss": 4.6103,
      "step": 1113800
    },
    {
      "epoch": 2.5716383314709983,
      "grad_norm": 5.8317952156066895,
      "learning_rate": 7.157730769230769e-06,
      "loss": 4.6348,
      "step": 1113900
    },
    {
      "epoch": 2.571869199442223,
      "grad_norm": 6.430695056915283,
      "learning_rate": 7.153884615384615e-06,
      "loss": 4.6451,
      "step": 1114000
    },
    {
      "epoch": 2.5721000674134475,
      "grad_norm": 5.726781368255615,
      "learning_rate": 7.150038461538462e-06,
      "loss": 4.6517,
      "step": 1114100
    },
    {
      "epoch": 2.572330935384672,
      "grad_norm": 5.488142490386963,
      "learning_rate": 7.1461923076923076e-06,
      "loss": 4.6458,
      "step": 1114200
    },
    {
      "epoch": 2.572561803355897,
      "grad_norm": 5.467831134796143,
      "learning_rate": 7.142346153846154e-06,
      "loss": 4.6883,
      "step": 1114300
    },
    {
      "epoch": 2.5727926713271216,
      "grad_norm": 6.429312229156494,
      "learning_rate": 7.1385e-06,
      "loss": 4.6192,
      "step": 1114400
    },
    {
      "epoch": 2.573023539298346,
      "grad_norm": 5.501313209533691,
      "learning_rate": 7.134653846153847e-06,
      "loss": 4.6177,
      "step": 1114500
    },
    {
      "epoch": 2.5732544072695704,
      "grad_norm": 6.087646484375,
      "learning_rate": 7.130807692307692e-06,
      "loss": 4.6566,
      "step": 1114600
    },
    {
      "epoch": 2.5734852752407953,
      "grad_norm": 5.691512584686279,
      "learning_rate": 7.1269615384615395e-06,
      "loss": 4.6289,
      "step": 1114700
    },
    {
      "epoch": 2.57371614321202,
      "grad_norm": 9.566763877868652,
      "learning_rate": 7.123115384615385e-06,
      "loss": 4.6284,
      "step": 1114800
    },
    {
      "epoch": 2.5739470111832445,
      "grad_norm": 5.716721057891846,
      "learning_rate": 7.119269230769231e-06,
      "loss": 4.6031,
      "step": 1114900
    },
    {
      "epoch": 2.574177879154469,
      "grad_norm": 6.694827556610107,
      "learning_rate": 7.115423076923078e-06,
      "loss": 4.6645,
      "step": 1115000
    },
    {
      "epoch": 2.574408747125694,
      "grad_norm": 5.878190994262695,
      "learning_rate": 7.111576923076923e-06,
      "loss": 4.627,
      "step": 1115100
    },
    {
      "epoch": 2.5746396150969186,
      "grad_norm": 6.447806358337402,
      "learning_rate": 7.107730769230769e-06,
      "loss": 4.644,
      "step": 1115200
    },
    {
      "epoch": 2.574870483068143,
      "grad_norm": 7.561313629150391,
      "learning_rate": 7.103884615384616e-06,
      "loss": 4.6596,
      "step": 1115300
    },
    {
      "epoch": 2.5751013510393674,
      "grad_norm": 6.788549900054932,
      "learning_rate": 7.1000384615384625e-06,
      "loss": 4.6615,
      "step": 1115400
    },
    {
      "epoch": 2.5753322190105923,
      "grad_norm": 7.401182651519775,
      "learning_rate": 7.096192307692308e-06,
      "loss": 4.6405,
      "step": 1115500
    },
    {
      "epoch": 2.5755630869818167,
      "grad_norm": 5.435157775878906,
      "learning_rate": 7.092346153846154e-06,
      "loss": 4.6109,
      "step": 1115600
    },
    {
      "epoch": 2.5757939549530415,
      "grad_norm": 5.6533708572387695,
      "learning_rate": 7.088500000000001e-06,
      "loss": 4.6004,
      "step": 1115700
    },
    {
      "epoch": 2.576024822924266,
      "grad_norm": 6.024336338043213,
      "learning_rate": 7.084653846153846e-06,
      "loss": 4.6336,
      "step": 1115800
    },
    {
      "epoch": 2.576255690895491,
      "grad_norm": 8.335923194885254,
      "learning_rate": 7.080807692307692e-06,
      "loss": 4.6885,
      "step": 1115900
    },
    {
      "epoch": 2.576486558866715,
      "grad_norm": 6.801823616027832,
      "learning_rate": 7.076961538461539e-06,
      "loss": 4.6641,
      "step": 1116000
    },
    {
      "epoch": 2.57671742683794,
      "grad_norm": 6.766439914703369,
      "learning_rate": 7.073115384615385e-06,
      "loss": 4.6437,
      "step": 1116100
    },
    {
      "epoch": 2.5769482948091644,
      "grad_norm": 6.703958988189697,
      "learning_rate": 7.069269230769231e-06,
      "loss": 4.605,
      "step": 1116200
    },
    {
      "epoch": 2.5771791627803893,
      "grad_norm": 5.119759559631348,
      "learning_rate": 7.0654230769230774e-06,
      "loss": 4.6326,
      "step": 1116300
    },
    {
      "epoch": 2.5774100307516137,
      "grad_norm": 7.014640808105469,
      "learning_rate": 7.061576923076924e-06,
      "loss": 4.6135,
      "step": 1116400
    },
    {
      "epoch": 2.5776408987228385,
      "grad_norm": 6.74793004989624,
      "learning_rate": 7.057730769230769e-06,
      "loss": 4.659,
      "step": 1116500
    },
    {
      "epoch": 2.577871766694063,
      "grad_norm": 6.003613471984863,
      "learning_rate": 7.053884615384617e-06,
      "loss": 4.6538,
      "step": 1116600
    },
    {
      "epoch": 2.5781026346652878,
      "grad_norm": 6.263541221618652,
      "learning_rate": 7.050038461538462e-06,
      "loss": 4.6086,
      "step": 1116700
    },
    {
      "epoch": 2.578333502636512,
      "grad_norm": 5.789710521697998,
      "learning_rate": 7.046192307692308e-06,
      "loss": 4.6809,
      "step": 1116800
    },
    {
      "epoch": 2.5785643706077366,
      "grad_norm": 6.01837158203125,
      "learning_rate": 7.042346153846153e-06,
      "loss": 4.6398,
      "step": 1116900
    },
    {
      "epoch": 2.5787952385789614,
      "grad_norm": 5.387373924255371,
      "learning_rate": 7.0385000000000005e-06,
      "loss": 4.6392,
      "step": 1117000
    },
    {
      "epoch": 2.5790261065501863,
      "grad_norm": 6.517993450164795,
      "learning_rate": 7.034653846153846e-06,
      "loss": 4.6317,
      "step": 1117100
    },
    {
      "epoch": 2.5792569745214107,
      "grad_norm": 5.708402633666992,
      "learning_rate": 7.030807692307692e-06,
      "loss": 4.5752,
      "step": 1117200
    },
    {
      "epoch": 2.579487842492635,
      "grad_norm": 6.539990425109863,
      "learning_rate": 7.02696153846154e-06,
      "loss": 4.6563,
      "step": 1117300
    },
    {
      "epoch": 2.57971871046386,
      "grad_norm": 10.256089210510254,
      "learning_rate": 7.023115384615385e-06,
      "loss": 4.6321,
      "step": 1117400
    },
    {
      "epoch": 2.5799495784350848,
      "grad_norm": 5.200093746185303,
      "learning_rate": 7.019269230769231e-06,
      "loss": 4.6334,
      "step": 1117500
    },
    {
      "epoch": 2.580180446406309,
      "grad_norm": 5.762847423553467,
      "learning_rate": 7.015423076923078e-06,
      "loss": 4.6216,
      "step": 1117600
    },
    {
      "epoch": 2.5804113143775336,
      "grad_norm": 5.513103008270264,
      "learning_rate": 7.0115769230769235e-06,
      "loss": 4.6307,
      "step": 1117700
    },
    {
      "epoch": 2.5806421823487584,
      "grad_norm": 5.4916276931762695,
      "learning_rate": 7.007730769230769e-06,
      "loss": 4.6273,
      "step": 1117800
    },
    {
      "epoch": 2.5808730503199833,
      "grad_norm": 6.454307556152344,
      "learning_rate": 7.003884615384616e-06,
      "loss": 4.6658,
      "step": 1117900
    },
    {
      "epoch": 2.5811039182912077,
      "grad_norm": 6.359661102294922,
      "learning_rate": 7.000038461538462e-06,
      "loss": 4.6676,
      "step": 1118000
    },
    {
      "epoch": 2.581334786262432,
      "grad_norm": 5.596408367156982,
      "learning_rate": 6.996192307692308e-06,
      "loss": 4.6881,
      "step": 1118100
    },
    {
      "epoch": 2.581565654233657,
      "grad_norm": 6.215575695037842,
      "learning_rate": 6.992346153846154e-06,
      "loss": 4.6673,
      "step": 1118200
    },
    {
      "epoch": 2.5817965222048813,
      "grad_norm": 7.301687717437744,
      "learning_rate": 6.988500000000001e-06,
      "loss": 4.6416,
      "step": 1118300
    },
    {
      "epoch": 2.582027390176106,
      "grad_norm": 5.165409088134766,
      "learning_rate": 6.9846538461538465e-06,
      "loss": 4.6249,
      "step": 1118400
    },
    {
      "epoch": 2.5822582581473306,
      "grad_norm": 7.855154991149902,
      "learning_rate": 6.980807692307692e-06,
      "loss": 4.6041,
      "step": 1118500
    },
    {
      "epoch": 2.5824891261185554,
      "grad_norm": 5.60549259185791,
      "learning_rate": 6.976961538461539e-06,
      "loss": 4.6874,
      "step": 1118600
    },
    {
      "epoch": 2.58271999408978,
      "grad_norm": 8.139260292053223,
      "learning_rate": 6.973115384615385e-06,
      "loss": 4.644,
      "step": 1118700
    },
    {
      "epoch": 2.5829508620610047,
      "grad_norm": 6.541540145874023,
      "learning_rate": 6.96926923076923e-06,
      "loss": 4.6402,
      "step": 1118800
    },
    {
      "epoch": 2.583181730032229,
      "grad_norm": 5.357305526733398,
      "learning_rate": 6.9654230769230776e-06,
      "loss": 4.6336,
      "step": 1118900
    },
    {
      "epoch": 2.583412598003454,
      "grad_norm": 5.4309210777282715,
      "learning_rate": 6.961576923076923e-06,
      "loss": 4.6361,
      "step": 1119000
    },
    {
      "epoch": 2.5836434659746783,
      "grad_norm": 8.534098625183105,
      "learning_rate": 6.9577307692307695e-06,
      "loss": 4.6735,
      "step": 1119100
    },
    {
      "epoch": 2.583874333945903,
      "grad_norm": 5.091400146484375,
      "learning_rate": 6.953884615384616e-06,
      "loss": 4.6548,
      "step": 1119200
    },
    {
      "epoch": 2.5841052019171276,
      "grad_norm": 5.473172187805176,
      "learning_rate": 6.950038461538462e-06,
      "loss": 4.5766,
      "step": 1119300
    },
    {
      "epoch": 2.5843360698883524,
      "grad_norm": 5.400793075561523,
      "learning_rate": 6.946192307692308e-06,
      "loss": 4.6568,
      "step": 1119400
    },
    {
      "epoch": 2.584566937859577,
      "grad_norm": 6.911993026733398,
      "learning_rate": 6.942346153846155e-06,
      "loss": 4.6631,
      "step": 1119500
    },
    {
      "epoch": 2.584797805830801,
      "grad_norm": 5.68344259262085,
      "learning_rate": 6.9385000000000006e-06,
      "loss": 4.6328,
      "step": 1119600
    },
    {
      "epoch": 2.585028673802026,
      "grad_norm": 5.754604816436768,
      "learning_rate": 6.934653846153846e-06,
      "loss": 4.6165,
      "step": 1119700
    },
    {
      "epoch": 2.585259541773251,
      "grad_norm": 6.565208435058594,
      "learning_rate": 6.930807692307692e-06,
      "loss": 4.6428,
      "step": 1119800
    },
    {
      "epoch": 2.5854904097444753,
      "grad_norm": 6.284149169921875,
      "learning_rate": 6.926961538461539e-06,
      "loss": 4.6215,
      "step": 1119900
    },
    {
      "epoch": 2.5857212777156997,
      "grad_norm": 5.465636253356934,
      "learning_rate": 6.923115384615384e-06,
      "loss": 4.6089,
      "step": 1120000
    },
    {
      "epoch": 2.5859521456869246,
      "grad_norm": 5.148323059082031,
      "learning_rate": 6.919269230769231e-06,
      "loss": 4.6135,
      "step": 1120100
    },
    {
      "epoch": 2.5861830136581494,
      "grad_norm": 5.367412090301514,
      "learning_rate": 6.915423076923078e-06,
      "loss": 4.6432,
      "step": 1120200
    },
    {
      "epoch": 2.586413881629374,
      "grad_norm": 5.438456058502197,
      "learning_rate": 6.911576923076924e-06,
      "loss": 4.6108,
      "step": 1120300
    },
    {
      "epoch": 2.586644749600598,
      "grad_norm": 6.882632255554199,
      "learning_rate": 6.907730769230769e-06,
      "loss": 4.6497,
      "step": 1120400
    },
    {
      "epoch": 2.586875617571823,
      "grad_norm": 6.124143123626709,
      "learning_rate": 6.903884615384616e-06,
      "loss": 4.6403,
      "step": 1120500
    },
    {
      "epoch": 2.587106485543048,
      "grad_norm": 5.595274448394775,
      "learning_rate": 6.900038461538462e-06,
      "loss": 4.6357,
      "step": 1120600
    },
    {
      "epoch": 2.5873373535142723,
      "grad_norm": 6.914165019989014,
      "learning_rate": 6.8961923076923074e-06,
      "loss": 4.6509,
      "step": 1120700
    },
    {
      "epoch": 2.5875682214854967,
      "grad_norm": 8.030165672302246,
      "learning_rate": 6.892346153846155e-06,
      "loss": 4.6434,
      "step": 1120800
    },
    {
      "epoch": 2.5877990894567215,
      "grad_norm": 6.263325214385986,
      "learning_rate": 6.8885e-06,
      "loss": 4.6066,
      "step": 1120900
    },
    {
      "epoch": 2.588029957427946,
      "grad_norm": 6.486035346984863,
      "learning_rate": 6.884653846153847e-06,
      "loss": 4.6463,
      "step": 1121000
    },
    {
      "epoch": 2.588260825399171,
      "grad_norm": 6.345425605773926,
      "learning_rate": 6.880807692307692e-06,
      "loss": 4.6147,
      "step": 1121100
    },
    {
      "epoch": 2.588491693370395,
      "grad_norm": 5.716912746429443,
      "learning_rate": 6.876961538461539e-06,
      "loss": 4.6269,
      "step": 1121200
    },
    {
      "epoch": 2.58872256134162,
      "grad_norm": 5.916669845581055,
      "learning_rate": 6.873115384615385e-06,
      "loss": 4.5886,
      "step": 1121300
    },
    {
      "epoch": 2.5889534293128444,
      "grad_norm": 6.0289225578308105,
      "learning_rate": 6.8692692307692304e-06,
      "loss": 4.6027,
      "step": 1121400
    },
    {
      "epoch": 2.5891842972840693,
      "grad_norm": 5.422337532043457,
      "learning_rate": 6.865423076923078e-06,
      "loss": 4.5944,
      "step": 1121500
    },
    {
      "epoch": 2.5894151652552937,
      "grad_norm": 6.6769819259643555,
      "learning_rate": 6.861576923076923e-06,
      "loss": 4.6425,
      "step": 1121600
    },
    {
      "epoch": 2.5896460332265185,
      "grad_norm": 5.697197437286377,
      "learning_rate": 6.857730769230769e-06,
      "loss": 4.6176,
      "step": 1121700
    },
    {
      "epoch": 2.589876901197743,
      "grad_norm": 5.1964921951293945,
      "learning_rate": 6.853884615384616e-06,
      "loss": 4.6909,
      "step": 1121800
    },
    {
      "epoch": 2.590107769168968,
      "grad_norm": 5.7304816246032715,
      "learning_rate": 6.8500384615384615e-06,
      "loss": 4.6685,
      "step": 1121900
    },
    {
      "epoch": 2.590338637140192,
      "grad_norm": 6.838639259338379,
      "learning_rate": 6.846192307692308e-06,
      "loss": 4.5979,
      "step": 1122000
    },
    {
      "epoch": 2.590569505111417,
      "grad_norm": 6.239196300506592,
      "learning_rate": 6.842346153846155e-06,
      "loss": 4.6239,
      "step": 1122100
    },
    {
      "epoch": 2.5908003730826414,
      "grad_norm": 6.508171081542969,
      "learning_rate": 6.838500000000001e-06,
      "loss": 4.7144,
      "step": 1122200
    },
    {
      "epoch": 2.591031241053866,
      "grad_norm": 6.702906608581543,
      "learning_rate": 6.834653846153846e-06,
      "loss": 4.6066,
      "step": 1122300
    },
    {
      "epoch": 2.5912621090250907,
      "grad_norm": 5.732738494873047,
      "learning_rate": 6.8308076923076935e-06,
      "loss": 4.5951,
      "step": 1122400
    },
    {
      "epoch": 2.5914929769963155,
      "grad_norm": 5.498129844665527,
      "learning_rate": 6.826961538461539e-06,
      "loss": 4.6183,
      "step": 1122500
    },
    {
      "epoch": 2.59172384496754,
      "grad_norm": 6.465541362762451,
      "learning_rate": 6.8231153846153845e-06,
      "loss": 4.6136,
      "step": 1122600
    },
    {
      "epoch": 2.5919547129387643,
      "grad_norm": 6.447116851806641,
      "learning_rate": 6.81926923076923e-06,
      "loss": 4.6225,
      "step": 1122700
    },
    {
      "epoch": 2.592185580909989,
      "grad_norm": 6.148109436035156,
      "learning_rate": 6.815423076923077e-06,
      "loss": 4.6422,
      "step": 1122800
    },
    {
      "epoch": 2.592416448881214,
      "grad_norm": 6.328669548034668,
      "learning_rate": 6.811576923076924e-06,
      "loss": 4.6579,
      "step": 1122900
    },
    {
      "epoch": 2.5926473168524384,
      "grad_norm": 6.639339447021484,
      "learning_rate": 6.807730769230769e-06,
      "loss": 4.6063,
      "step": 1123000
    },
    {
      "epoch": 2.592878184823663,
      "grad_norm": 5.837856292724609,
      "learning_rate": 6.8038846153846165e-06,
      "loss": 4.6146,
      "step": 1123100
    },
    {
      "epoch": 2.5931090527948877,
      "grad_norm": 7.045108795166016,
      "learning_rate": 6.800038461538462e-06,
      "loss": 4.6392,
      "step": 1123200
    },
    {
      "epoch": 2.5933399207661125,
      "grad_norm": 5.740973472595215,
      "learning_rate": 6.7961923076923075e-06,
      "loss": 4.5768,
      "step": 1123300
    },
    {
      "epoch": 2.593570788737337,
      "grad_norm": 5.958710670471191,
      "learning_rate": 6.792346153846155e-06,
      "loss": 4.6394,
      "step": 1123400
    },
    {
      "epoch": 2.5938016567085613,
      "grad_norm": 5.6141157150268555,
      "learning_rate": 6.7885e-06,
      "loss": 4.6226,
      "step": 1123500
    },
    {
      "epoch": 2.594032524679786,
      "grad_norm": 7.039780616760254,
      "learning_rate": 6.784653846153846e-06,
      "loss": 4.6049,
      "step": 1123600
    },
    {
      "epoch": 2.5942633926510106,
      "grad_norm": 6.61928129196167,
      "learning_rate": 6.780807692307693e-06,
      "loss": 4.6455,
      "step": 1123700
    },
    {
      "epoch": 2.5944942606222354,
      "grad_norm": 6.8533430099487305,
      "learning_rate": 6.776961538461539e-06,
      "loss": 4.5854,
      "step": 1123800
    },
    {
      "epoch": 2.59472512859346,
      "grad_norm": 6.235055923461914,
      "learning_rate": 6.773115384615385e-06,
      "loss": 4.6443,
      "step": 1123900
    },
    {
      "epoch": 2.5949559965646847,
      "grad_norm": 5.660865306854248,
      "learning_rate": 6.7692692307692305e-06,
      "loss": 4.6173,
      "step": 1124000
    },
    {
      "epoch": 2.595186864535909,
      "grad_norm": 7.357894420623779,
      "learning_rate": 6.765423076923078e-06,
      "loss": 4.5811,
      "step": 1124100
    },
    {
      "epoch": 2.595417732507134,
      "grad_norm": 5.993887424468994,
      "learning_rate": 6.761576923076923e-06,
      "loss": 4.6352,
      "step": 1124200
    },
    {
      "epoch": 2.5956486004783583,
      "grad_norm": 6.595607757568359,
      "learning_rate": 6.757730769230769e-06,
      "loss": 4.645,
      "step": 1124300
    },
    {
      "epoch": 2.595879468449583,
      "grad_norm": 5.842720031738281,
      "learning_rate": 6.753884615384616e-06,
      "loss": 4.6339,
      "step": 1124400
    },
    {
      "epoch": 2.5961103364208076,
      "grad_norm": 6.527168273925781,
      "learning_rate": 6.750038461538462e-06,
      "loss": 4.6301,
      "step": 1124500
    },
    {
      "epoch": 2.5963412043920324,
      "grad_norm": 5.2839274406433105,
      "learning_rate": 6.746192307692307e-06,
      "loss": 4.6737,
      "step": 1124600
    },
    {
      "epoch": 2.596572072363257,
      "grad_norm": 6.435173511505127,
      "learning_rate": 6.742346153846154e-06,
      "loss": 4.6556,
      "step": 1124700
    },
    {
      "epoch": 2.5968029403344817,
      "grad_norm": 7.02259635925293,
      "learning_rate": 6.738500000000001e-06,
      "loss": 4.631,
      "step": 1124800
    },
    {
      "epoch": 2.597033808305706,
      "grad_norm": 5.489924907684326,
      "learning_rate": 6.734653846153846e-06,
      "loss": 4.633,
      "step": 1124900
    },
    {
      "epoch": 2.5972646762769305,
      "grad_norm": 6.8246588706970215,
      "learning_rate": 6.7308076923076936e-06,
      "loss": 4.636,
      "step": 1125000
    },
    {
      "epoch": 2.5974955442481553,
      "grad_norm": 6.4108781814575195,
      "learning_rate": 6.726961538461539e-06,
      "loss": 4.6381,
      "step": 1125100
    },
    {
      "epoch": 2.59772641221938,
      "grad_norm": 6.172124862670898,
      "learning_rate": 6.723115384615385e-06,
      "loss": 4.6436,
      "step": 1125200
    },
    {
      "epoch": 2.5979572801906046,
      "grad_norm": 5.888926029205322,
      "learning_rate": 6.719269230769232e-06,
      "loss": 4.6118,
      "step": 1125300
    },
    {
      "epoch": 2.598188148161829,
      "grad_norm": 5.84799337387085,
      "learning_rate": 6.715423076923077e-06,
      "loss": 4.6279,
      "step": 1125400
    },
    {
      "epoch": 2.598419016133054,
      "grad_norm": 5.797723770141602,
      "learning_rate": 6.711576923076923e-06,
      "loss": 4.6283,
      "step": 1125500
    },
    {
      "epoch": 2.5986498841042787,
      "grad_norm": 6.13834810256958,
      "learning_rate": 6.707730769230769e-06,
      "loss": 4.6299,
      "step": 1125600
    },
    {
      "epoch": 2.598880752075503,
      "grad_norm": 5.65101957321167,
      "learning_rate": 6.703884615384616e-06,
      "loss": 4.6158,
      "step": 1125700
    },
    {
      "epoch": 2.5991116200467275,
      "grad_norm": 5.85149621963501,
      "learning_rate": 6.700038461538462e-06,
      "loss": 4.589,
      "step": 1125800
    },
    {
      "epoch": 2.5993424880179523,
      "grad_norm": 7.784127712249756,
      "learning_rate": 6.696192307692308e-06,
      "loss": 4.6429,
      "step": 1125900
    },
    {
      "epoch": 2.599573355989177,
      "grad_norm": 5.809202671051025,
      "learning_rate": 6.692346153846155e-06,
      "loss": 4.6423,
      "step": 1126000
    },
    {
      "epoch": 2.5998042239604016,
      "grad_norm": 5.292194366455078,
      "learning_rate": 6.6885e-06,
      "loss": 4.6796,
      "step": 1126100
    },
    {
      "epoch": 2.600035091931626,
      "grad_norm": 5.294939041137695,
      "learning_rate": 6.684653846153846e-06,
      "loss": 4.6349,
      "step": 1126200
    },
    {
      "epoch": 2.600265959902851,
      "grad_norm": 6.358399391174316,
      "learning_rate": 6.680807692307693e-06,
      "loss": 4.6193,
      "step": 1126300
    },
    {
      "epoch": 2.600496827874075,
      "grad_norm": 6.4983229637146,
      "learning_rate": 6.676961538461539e-06,
      "loss": 4.5715,
      "step": 1126400
    },
    {
      "epoch": 2.6007276958453,
      "grad_norm": 8.010441780090332,
      "learning_rate": 6.673115384615384e-06,
      "loss": 4.6231,
      "step": 1126500
    },
    {
      "epoch": 2.6009585638165245,
      "grad_norm": 5.255354404449463,
      "learning_rate": 6.6692692307692315e-06,
      "loss": 4.5494,
      "step": 1126600
    },
    {
      "epoch": 2.6011894317877493,
      "grad_norm": 5.411899089813232,
      "learning_rate": 6.665423076923077e-06,
      "loss": 4.6001,
      "step": 1126700
    },
    {
      "epoch": 2.6014202997589737,
      "grad_norm": 6.958702087402344,
      "learning_rate": 6.6615769230769234e-06,
      "loss": 4.6423,
      "step": 1126800
    },
    {
      "epoch": 2.6016511677301986,
      "grad_norm": 6.644082069396973,
      "learning_rate": 6.657730769230769e-06,
      "loss": 4.6001,
      "step": 1126900
    },
    {
      "epoch": 2.601882035701423,
      "grad_norm": 5.608126163482666,
      "learning_rate": 6.653884615384616e-06,
      "loss": 4.6222,
      "step": 1127000
    },
    {
      "epoch": 2.602112903672648,
      "grad_norm": 9.348357200622559,
      "learning_rate": 6.650038461538462e-06,
      "loss": 4.656,
      "step": 1127100
    },
    {
      "epoch": 2.602343771643872,
      "grad_norm": 5.376818656921387,
      "learning_rate": 6.646192307692307e-06,
      "loss": 4.6526,
      "step": 1127200
    },
    {
      "epoch": 2.602574639615097,
      "grad_norm": 5.7326884269714355,
      "learning_rate": 6.6423461538461545e-06,
      "loss": 4.6418,
      "step": 1127300
    },
    {
      "epoch": 2.6028055075863215,
      "grad_norm": 6.435953140258789,
      "learning_rate": 6.6385e-06,
      "loss": 4.6342,
      "step": 1127400
    },
    {
      "epoch": 2.6030363755575463,
      "grad_norm": 5.801665782928467,
      "learning_rate": 6.634653846153846e-06,
      "loss": 4.6048,
      "step": 1127500
    },
    {
      "epoch": 2.6032672435287707,
      "grad_norm": 5.813058376312256,
      "learning_rate": 6.630807692307693e-06,
      "loss": 4.6256,
      "step": 1127600
    },
    {
      "epoch": 2.6034981114999955,
      "grad_norm": 5.579229354858398,
      "learning_rate": 6.626961538461539e-06,
      "loss": 4.6255,
      "step": 1127700
    },
    {
      "epoch": 2.60372897947122,
      "grad_norm": 5.795482158660889,
      "learning_rate": 6.623115384615385e-06,
      "loss": 4.5782,
      "step": 1127800
    },
    {
      "epoch": 2.603959847442445,
      "grad_norm": 5.869901657104492,
      "learning_rate": 6.619269230769232e-06,
      "loss": 4.6297,
      "step": 1127900
    },
    {
      "epoch": 2.604190715413669,
      "grad_norm": 6.611833095550537,
      "learning_rate": 6.6154230769230775e-06,
      "loss": 4.6113,
      "step": 1128000
    },
    {
      "epoch": 2.6044215833848936,
      "grad_norm": 6.679018974304199,
      "learning_rate": 6.611576923076923e-06,
      "loss": 4.6299,
      "step": 1128100
    },
    {
      "epoch": 2.6046524513561184,
      "grad_norm": 5.85385274887085,
      "learning_rate": 6.60773076923077e-06,
      "loss": 4.6013,
      "step": 1128200
    },
    {
      "epoch": 2.6048833193273433,
      "grad_norm": 5.806308746337891,
      "learning_rate": 6.603884615384616e-06,
      "loss": 4.5645,
      "step": 1128300
    },
    {
      "epoch": 2.6051141872985677,
      "grad_norm": 6.024408340454102,
      "learning_rate": 6.600038461538461e-06,
      "loss": 4.5804,
      "step": 1128400
    },
    {
      "epoch": 2.605345055269792,
      "grad_norm": 5.321135997772217,
      "learning_rate": 6.596192307692308e-06,
      "loss": 4.6316,
      "step": 1128500
    },
    {
      "epoch": 2.605575923241017,
      "grad_norm": 7.351127624511719,
      "learning_rate": 6.592346153846154e-06,
      "loss": 4.592,
      "step": 1128600
    },
    {
      "epoch": 2.605806791212242,
      "grad_norm": 5.778989791870117,
      "learning_rate": 6.5885000000000005e-06,
      "loss": 4.6333,
      "step": 1128700
    },
    {
      "epoch": 2.606037659183466,
      "grad_norm": 6.386127471923828,
      "learning_rate": 6.584653846153846e-06,
      "loss": 4.6312,
      "step": 1128800
    },
    {
      "epoch": 2.6062685271546906,
      "grad_norm": 6.847332954406738,
      "learning_rate": 6.580807692307693e-06,
      "loss": 4.654,
      "step": 1128900
    },
    {
      "epoch": 2.6064993951259154,
      "grad_norm": 5.541748523712158,
      "learning_rate": 6.576961538461539e-06,
      "loss": 4.6103,
      "step": 1129000
    },
    {
      "epoch": 2.60673026309714,
      "grad_norm": 5.605461597442627,
      "learning_rate": 6.573115384615384e-06,
      "loss": 4.5999,
      "step": 1129100
    },
    {
      "epoch": 2.6069611310683647,
      "grad_norm": 7.022756099700928,
      "learning_rate": 6.569269230769232e-06,
      "loss": 4.5998,
      "step": 1129200
    },
    {
      "epoch": 2.607191999039589,
      "grad_norm": 5.45261287689209,
      "learning_rate": 6.565423076923077e-06,
      "loss": 4.5721,
      "step": 1129300
    },
    {
      "epoch": 2.607422867010814,
      "grad_norm": 6.621981143951416,
      "learning_rate": 6.561576923076923e-06,
      "loss": 4.6081,
      "step": 1129400
    },
    {
      "epoch": 2.6076537349820383,
      "grad_norm": 5.797094821929932,
      "learning_rate": 6.55773076923077e-06,
      "loss": 4.5883,
      "step": 1129500
    },
    {
      "epoch": 2.607884602953263,
      "grad_norm": 5.989932537078857,
      "learning_rate": 6.553884615384616e-06,
      "loss": 4.6155,
      "step": 1129600
    },
    {
      "epoch": 2.6081154709244876,
      "grad_norm": 5.992272853851318,
      "learning_rate": 6.550038461538462e-06,
      "loss": 4.6395,
      "step": 1129700
    },
    {
      "epoch": 2.6083463388957124,
      "grad_norm": 5.8771233558654785,
      "learning_rate": 6.546192307692307e-06,
      "loss": 4.5954,
      "step": 1129800
    },
    {
      "epoch": 2.608577206866937,
      "grad_norm": 6.476869106292725,
      "learning_rate": 6.542346153846155e-06,
      "loss": 4.5902,
      "step": 1129900
    },
    {
      "epoch": 2.6088080748381617,
      "grad_norm": 5.900103569030762,
      "learning_rate": 6.5385e-06,
      "loss": 4.5926,
      "step": 1130000
    },
    {
      "epoch": 2.609038942809386,
      "grad_norm": 7.328470230102539,
      "learning_rate": 6.534653846153846e-06,
      "loss": 4.6342,
      "step": 1130100
    },
    {
      "epoch": 2.609269810780611,
      "grad_norm": 5.970292091369629,
      "learning_rate": 6.530807692307693e-06,
      "loss": 4.6049,
      "step": 1130200
    },
    {
      "epoch": 2.6095006787518353,
      "grad_norm": 6.535192489624023,
      "learning_rate": 6.5269615384615385e-06,
      "loss": 4.6074,
      "step": 1130300
    },
    {
      "epoch": 2.60973154672306,
      "grad_norm": 6.056039810180664,
      "learning_rate": 6.523115384615385e-06,
      "loss": 4.6011,
      "step": 1130400
    },
    {
      "epoch": 2.6099624146942846,
      "grad_norm": 6.444319725036621,
      "learning_rate": 6.519269230769231e-06,
      "loss": 4.669,
      "step": 1130500
    },
    {
      "epoch": 2.6101932826655094,
      "grad_norm": 7.446422576904297,
      "learning_rate": 6.515423076923078e-06,
      "loss": 4.6202,
      "step": 1130600
    },
    {
      "epoch": 2.610424150636734,
      "grad_norm": 8.285820960998535,
      "learning_rate": 6.511576923076923e-06,
      "loss": 4.6318,
      "step": 1130700
    },
    {
      "epoch": 2.6106550186079582,
      "grad_norm": 9.34852409362793,
      "learning_rate": 6.50773076923077e-06,
      "loss": 4.6164,
      "step": 1130800
    },
    {
      "epoch": 2.610885886579183,
      "grad_norm": 5.518189430236816,
      "learning_rate": 6.503884615384616e-06,
      "loss": 4.6317,
      "step": 1130900
    },
    {
      "epoch": 2.611116754550408,
      "grad_norm": 5.5676751136779785,
      "learning_rate": 6.5000384615384615e-06,
      "loss": 4.5971,
      "step": 1131000
    },
    {
      "epoch": 2.6113476225216323,
      "grad_norm": 8.090367317199707,
      "learning_rate": 6.496192307692309e-06,
      "loss": 4.6586,
      "step": 1131100
    },
    {
      "epoch": 2.6115784904928567,
      "grad_norm": 5.529645919799805,
      "learning_rate": 6.492346153846154e-06,
      "loss": 4.6361,
      "step": 1131200
    },
    {
      "epoch": 2.6118093584640816,
      "grad_norm": 6.247756004333496,
      "learning_rate": 6.4885e-06,
      "loss": 4.5863,
      "step": 1131300
    },
    {
      "epoch": 2.6120402264353064,
      "grad_norm": 6.049452781677246,
      "learning_rate": 6.484653846153846e-06,
      "loss": 4.582,
      "step": 1131400
    },
    {
      "epoch": 2.612271094406531,
      "grad_norm": 7.93963623046875,
      "learning_rate": 6.4808076923076926e-06,
      "loss": 4.6038,
      "step": 1131500
    },
    {
      "epoch": 2.6125019623777552,
      "grad_norm": 5.911839008331299,
      "learning_rate": 6.476961538461539e-06,
      "loss": 4.6445,
      "step": 1131600
    },
    {
      "epoch": 2.61273283034898,
      "grad_norm": 8.742053985595703,
      "learning_rate": 6.4731153846153845e-06,
      "loss": 4.6246,
      "step": 1131700
    },
    {
      "epoch": 2.6129636983202045,
      "grad_norm": 7.645793437957764,
      "learning_rate": 6.469269230769232e-06,
      "loss": 4.5746,
      "step": 1131800
    },
    {
      "epoch": 2.6131945662914293,
      "grad_norm": 5.80030632019043,
      "learning_rate": 6.465423076923077e-06,
      "loss": 4.626,
      "step": 1131900
    },
    {
      "epoch": 2.6134254342626537,
      "grad_norm": 8.551260948181152,
      "learning_rate": 6.461576923076923e-06,
      "loss": 4.5679,
      "step": 1132000
    },
    {
      "epoch": 2.6136563022338786,
      "grad_norm": 5.833493232727051,
      "learning_rate": 6.45773076923077e-06,
      "loss": 4.5936,
      "step": 1132100
    },
    {
      "epoch": 2.613887170205103,
      "grad_norm": 8.013364791870117,
      "learning_rate": 6.4538846153846156e-06,
      "loss": 4.65,
      "step": 1132200
    },
    {
      "epoch": 2.614118038176328,
      "grad_norm": 6.02638053894043,
      "learning_rate": 6.450038461538461e-06,
      "loss": 4.6229,
      "step": 1132300
    },
    {
      "epoch": 2.614348906147552,
      "grad_norm": 5.59518575668335,
      "learning_rate": 6.446192307692308e-06,
      "loss": 4.592,
      "step": 1132400
    },
    {
      "epoch": 2.614579774118777,
      "grad_norm": 5.7627363204956055,
      "learning_rate": 6.442346153846155e-06,
      "loss": 4.6431,
      "step": 1132500
    },
    {
      "epoch": 2.6148106420900015,
      "grad_norm": 7.395132541656494,
      "learning_rate": 6.4385e-06,
      "loss": 4.6356,
      "step": 1132600
    },
    {
      "epoch": 2.6150415100612263,
      "grad_norm": 5.761249542236328,
      "learning_rate": 6.434653846153846e-06,
      "loss": 4.5763,
      "step": 1132700
    },
    {
      "epoch": 2.6152723780324507,
      "grad_norm": 5.496517181396484,
      "learning_rate": 6.430807692307693e-06,
      "loss": 4.6059,
      "step": 1132800
    },
    {
      "epoch": 2.6155032460036756,
      "grad_norm": 5.810365676879883,
      "learning_rate": 6.426961538461539e-06,
      "loss": 4.6294,
      "step": 1132900
    },
    {
      "epoch": 2.6157341139749,
      "grad_norm": 5.5506157875061035,
      "learning_rate": 6.423115384615384e-06,
      "loss": 4.6353,
      "step": 1133000
    },
    {
      "epoch": 2.615964981946125,
      "grad_norm": 6.236181735992432,
      "learning_rate": 6.419269230769231e-06,
      "loss": 4.6267,
      "step": 1133100
    },
    {
      "epoch": 2.616195849917349,
      "grad_norm": 5.5697174072265625,
      "learning_rate": 6.415423076923077e-06,
      "loss": 4.6006,
      "step": 1133200
    },
    {
      "epoch": 2.616426717888574,
      "grad_norm": 7.031830310821533,
      "learning_rate": 6.411576923076923e-06,
      "loss": 4.6295,
      "step": 1133300
    },
    {
      "epoch": 2.6166575858597985,
      "grad_norm": 5.414368629455566,
      "learning_rate": 6.40773076923077e-06,
      "loss": 4.5614,
      "step": 1133400
    },
    {
      "epoch": 2.616888453831023,
      "grad_norm": 5.177987098693848,
      "learning_rate": 6.403884615384616e-06,
      "loss": 4.6344,
      "step": 1133500
    },
    {
      "epoch": 2.6171193218022477,
      "grad_norm": 6.1231465339660645,
      "learning_rate": 6.400038461538462e-06,
      "loss": 4.6254,
      "step": 1133600
    },
    {
      "epoch": 2.6173501897734726,
      "grad_norm": 5.1846232414245605,
      "learning_rate": 6.396192307692309e-06,
      "loss": 4.5896,
      "step": 1133700
    },
    {
      "epoch": 2.617581057744697,
      "grad_norm": 6.4084625244140625,
      "learning_rate": 6.392346153846154e-06,
      "loss": 4.6025,
      "step": 1133800
    },
    {
      "epoch": 2.6178119257159214,
      "grad_norm": 7.197921276092529,
      "learning_rate": 6.3885e-06,
      "loss": 4.5866,
      "step": 1133900
    },
    {
      "epoch": 2.618042793687146,
      "grad_norm": 5.460020065307617,
      "learning_rate": 6.384653846153847e-06,
      "loss": 4.675,
      "step": 1134000
    },
    {
      "epoch": 2.618273661658371,
      "grad_norm": 5.794233798980713,
      "learning_rate": 6.380807692307693e-06,
      "loss": 4.6417,
      "step": 1134100
    },
    {
      "epoch": 2.6185045296295955,
      "grad_norm": 6.934865474700928,
      "learning_rate": 6.376961538461538e-06,
      "loss": 4.61,
      "step": 1134200
    },
    {
      "epoch": 2.61873539760082,
      "grad_norm": 6.357961177825928,
      "learning_rate": 6.373115384615385e-06,
      "loss": 4.5989,
      "step": 1134300
    },
    {
      "epoch": 2.6189662655720447,
      "grad_norm": 6.125694751739502,
      "learning_rate": 6.369269230769232e-06,
      "loss": 4.6468,
      "step": 1134400
    },
    {
      "epoch": 2.6191971335432696,
      "grad_norm": 8.520666122436523,
      "learning_rate": 6.365423076923077e-06,
      "loss": 4.6394,
      "step": 1134500
    },
    {
      "epoch": 2.619428001514494,
      "grad_norm": 5.936623573303223,
      "learning_rate": 6.361576923076923e-06,
      "loss": 4.6212,
      "step": 1134600
    },
    {
      "epoch": 2.6196588694857184,
      "grad_norm": 5.615409851074219,
      "learning_rate": 6.35773076923077e-06,
      "loss": 4.6129,
      "step": 1134700
    },
    {
      "epoch": 2.619889737456943,
      "grad_norm": 5.1184210777282715,
      "learning_rate": 6.353884615384616e-06,
      "loss": 4.5947,
      "step": 1134800
    },
    {
      "epoch": 2.6201206054281676,
      "grad_norm": 5.967523574829102,
      "learning_rate": 6.350038461538461e-06,
      "loss": 4.5776,
      "step": 1134900
    },
    {
      "epoch": 2.6203514733993925,
      "grad_norm": 6.124667644500732,
      "learning_rate": 6.3461923076923085e-06,
      "loss": 4.6463,
      "step": 1135000
    },
    {
      "epoch": 2.620582341370617,
      "grad_norm": 6.162820816040039,
      "learning_rate": 6.342346153846154e-06,
      "loss": 4.5333,
      "step": 1135100
    },
    {
      "epoch": 2.6208132093418417,
      "grad_norm": 6.057112216949463,
      "learning_rate": 6.3385e-06,
      "loss": 4.5976,
      "step": 1135200
    },
    {
      "epoch": 2.621044077313066,
      "grad_norm": 6.952250957489014,
      "learning_rate": 6.334653846153847e-06,
      "loss": 4.601,
      "step": 1135300
    },
    {
      "epoch": 2.621274945284291,
      "grad_norm": 6.058785915374756,
      "learning_rate": 6.330807692307693e-06,
      "loss": 4.5928,
      "step": 1135400
    },
    {
      "epoch": 2.6215058132555153,
      "grad_norm": 5.588677883148193,
      "learning_rate": 6.326961538461539e-06,
      "loss": 4.565,
      "step": 1135500
    },
    {
      "epoch": 2.62173668122674,
      "grad_norm": 5.705528259277344,
      "learning_rate": 6.323115384615384e-06,
      "loss": 4.5927,
      "step": 1135600
    },
    {
      "epoch": 2.6219675491979646,
      "grad_norm": 6.941496849060059,
      "learning_rate": 6.3192692307692315e-06,
      "loss": 4.5581,
      "step": 1135700
    },
    {
      "epoch": 2.6221984171691894,
      "grad_norm": 5.875405311584473,
      "learning_rate": 6.315423076923077e-06,
      "loss": 4.6367,
      "step": 1135800
    },
    {
      "epoch": 2.622429285140414,
      "grad_norm": 8.548762321472168,
      "learning_rate": 6.3115769230769225e-06,
      "loss": 4.6261,
      "step": 1135900
    },
    {
      "epoch": 2.6226601531116387,
      "grad_norm": 5.9951171875,
      "learning_rate": 6.30773076923077e-06,
      "loss": 4.6461,
      "step": 1136000
    },
    {
      "epoch": 2.622891021082863,
      "grad_norm": 5.851071357727051,
      "learning_rate": 6.303884615384615e-06,
      "loss": 4.6173,
      "step": 1136100
    },
    {
      "epoch": 2.6231218890540875,
      "grad_norm": 6.673145771026611,
      "learning_rate": 6.300038461538462e-06,
      "loss": 4.6299,
      "step": 1136200
    },
    {
      "epoch": 2.6233527570253123,
      "grad_norm": 5.279755592346191,
      "learning_rate": 6.296192307692308e-06,
      "loss": 4.6337,
      "step": 1136300
    },
    {
      "epoch": 2.623583624996537,
      "grad_norm": 8.05111312866211,
      "learning_rate": 6.2923461538461545e-06,
      "loss": 4.6212,
      "step": 1136400
    },
    {
      "epoch": 2.6238144929677616,
      "grad_norm": 6.492903709411621,
      "learning_rate": 6.2885e-06,
      "loss": 4.5916,
      "step": 1136500
    },
    {
      "epoch": 2.624045360938986,
      "grad_norm": 6.203310489654541,
      "learning_rate": 6.284653846153847e-06,
      "loss": 4.6169,
      "step": 1136600
    },
    {
      "epoch": 2.624276228910211,
      "grad_norm": 6.015117168426514,
      "learning_rate": 6.280807692307693e-06,
      "loss": 4.6064,
      "step": 1136700
    },
    {
      "epoch": 2.6245070968814357,
      "grad_norm": 6.076644420623779,
      "learning_rate": 6.276961538461538e-06,
      "loss": 4.6047,
      "step": 1136800
    },
    {
      "epoch": 2.62473796485266,
      "grad_norm": 5.106466293334961,
      "learning_rate": 6.273115384615384e-06,
      "loss": 4.6251,
      "step": 1136900
    },
    {
      "epoch": 2.6249688328238845,
      "grad_norm": 5.550331115722656,
      "learning_rate": 6.269269230769231e-06,
      "loss": 4.5668,
      "step": 1137000
    },
    {
      "epoch": 2.6251997007951093,
      "grad_norm": 5.405240535736084,
      "learning_rate": 6.2654230769230775e-06,
      "loss": 4.5932,
      "step": 1137100
    },
    {
      "epoch": 2.625430568766334,
      "grad_norm": 5.102819919586182,
      "learning_rate": 6.261576923076923e-06,
      "loss": 4.6255,
      "step": 1137200
    },
    {
      "epoch": 2.6256614367375586,
      "grad_norm": 5.855877876281738,
      "learning_rate": 6.25773076923077e-06,
      "loss": 4.5885,
      "step": 1137300
    },
    {
      "epoch": 2.625892304708783,
      "grad_norm": 7.063370704650879,
      "learning_rate": 6.253884615384616e-06,
      "loss": 4.5978,
      "step": 1137400
    },
    {
      "epoch": 2.626123172680008,
      "grad_norm": 5.874835014343262,
      "learning_rate": 6.250038461538461e-06,
      "loss": 4.6206,
      "step": 1137500
    },
    {
      "epoch": 2.6263540406512322,
      "grad_norm": 5.943883419036865,
      "learning_rate": 6.246192307692308e-06,
      "loss": 4.5962,
      "step": 1137600
    },
    {
      "epoch": 2.626584908622457,
      "grad_norm": 5.67340612411499,
      "learning_rate": 6.242346153846154e-06,
      "loss": 4.6304,
      "step": 1137700
    },
    {
      "epoch": 2.6268157765936815,
      "grad_norm": 7.628098487854004,
      "learning_rate": 6.2385000000000005e-06,
      "loss": 4.6203,
      "step": 1137800
    },
    {
      "epoch": 2.6270466445649063,
      "grad_norm": 5.531359672546387,
      "learning_rate": 6.234653846153846e-06,
      "loss": 4.5989,
      "step": 1137900
    },
    {
      "epoch": 2.6272775125361307,
      "grad_norm": 5.880748271942139,
      "learning_rate": 6.230807692307692e-06,
      "loss": 4.6281,
      "step": 1138000
    },
    {
      "epoch": 2.6275083805073556,
      "grad_norm": 6.668099403381348,
      "learning_rate": 6.226961538461539e-06,
      "loss": 4.6012,
      "step": 1138100
    },
    {
      "epoch": 2.62773924847858,
      "grad_norm": 8.126740455627441,
      "learning_rate": 6.223115384615385e-06,
      "loss": 4.6353,
      "step": 1138200
    },
    {
      "epoch": 2.627970116449805,
      "grad_norm": 5.237033843994141,
      "learning_rate": 6.219269230769232e-06,
      "loss": 4.6166,
      "step": 1138300
    },
    {
      "epoch": 2.6282009844210292,
      "grad_norm": 7.166578769683838,
      "learning_rate": 6.215423076923077e-06,
      "loss": 4.583,
      "step": 1138400
    },
    {
      "epoch": 2.628431852392254,
      "grad_norm": 5.940316200256348,
      "learning_rate": 6.2115769230769235e-06,
      "loss": 4.6093,
      "step": 1138500
    },
    {
      "epoch": 2.6286627203634785,
      "grad_norm": 6.714372634887695,
      "learning_rate": 6.20773076923077e-06,
      "loss": 4.5737,
      "step": 1138600
    },
    {
      "epoch": 2.6288935883347033,
      "grad_norm": 5.948895454406738,
      "learning_rate": 6.2038846153846154e-06,
      "loss": 4.6006,
      "step": 1138700
    },
    {
      "epoch": 2.6291244563059277,
      "grad_norm": 5.960202217102051,
      "learning_rate": 6.200038461538462e-06,
      "loss": 4.5595,
      "step": 1138800
    },
    {
      "epoch": 2.629355324277152,
      "grad_norm": 5.825650691986084,
      "learning_rate": 6.196192307692307e-06,
      "loss": 4.5788,
      "step": 1138900
    },
    {
      "epoch": 2.629586192248377,
      "grad_norm": 5.743068218231201,
      "learning_rate": 6.192346153846154e-06,
      "loss": 4.6014,
      "step": 1139000
    },
    {
      "epoch": 2.629817060219602,
      "grad_norm": 6.955103874206543,
      "learning_rate": 6.1885e-06,
      "loss": 4.6145,
      "step": 1139100
    },
    {
      "epoch": 2.6300479281908262,
      "grad_norm": 5.693925380706787,
      "learning_rate": 6.1846538461538465e-06,
      "loss": 4.5449,
      "step": 1139200
    },
    {
      "epoch": 2.6302787961620506,
      "grad_norm": 7.614748954772949,
      "learning_rate": 6.180807692307693e-06,
      "loss": 4.602,
      "step": 1139300
    },
    {
      "epoch": 2.6305096641332755,
      "grad_norm": 5.833863258361816,
      "learning_rate": 6.1769615384615384e-06,
      "loss": 4.5919,
      "step": 1139400
    },
    {
      "epoch": 2.6307405321045003,
      "grad_norm": 6.2398881912231445,
      "learning_rate": 6.173115384615385e-06,
      "loss": 4.6502,
      "step": 1139500
    },
    {
      "epoch": 2.6309714000757247,
      "grad_norm": 6.150816440582275,
      "learning_rate": 6.169269230769231e-06,
      "loss": 4.5951,
      "step": 1139600
    },
    {
      "epoch": 2.631202268046949,
      "grad_norm": 5.863711357116699,
      "learning_rate": 6.165423076923077e-06,
      "loss": 4.5986,
      "step": 1139700
    },
    {
      "epoch": 2.631433136018174,
      "grad_norm": 5.486724853515625,
      "learning_rate": 6.161576923076923e-06,
      "loss": 4.639,
      "step": 1139800
    },
    {
      "epoch": 2.631664003989399,
      "grad_norm": 7.076770782470703,
      "learning_rate": 6.1577307692307695e-06,
      "loss": 4.5845,
      "step": 1139900
    },
    {
      "epoch": 2.631894871960623,
      "grad_norm": 5.720422267913818,
      "learning_rate": 6.153884615384616e-06,
      "loss": 4.6244,
      "step": 1140000
    },
    {
      "epoch": 2.6321257399318476,
      "grad_norm": 5.199675559997559,
      "learning_rate": 6.150038461538462e-06,
      "loss": 4.6328,
      "step": 1140100
    },
    {
      "epoch": 2.6323566079030725,
      "grad_norm": 6.710171222686768,
      "learning_rate": 6.146192307692308e-06,
      "loss": 4.6226,
      "step": 1140200
    },
    {
      "epoch": 2.632587475874297,
      "grad_norm": 5.844330787658691,
      "learning_rate": 6.142346153846154e-06,
      "loss": 4.6339,
      "step": 1140300
    },
    {
      "epoch": 2.6328183438455217,
      "grad_norm": 7.102224826812744,
      "learning_rate": 6.138500000000001e-06,
      "loss": 4.5962,
      "step": 1140400
    },
    {
      "epoch": 2.633049211816746,
      "grad_norm": 5.958399772644043,
      "learning_rate": 6.134653846153846e-06,
      "loss": 4.5743,
      "step": 1140500
    },
    {
      "epoch": 2.633280079787971,
      "grad_norm": 6.031943321228027,
      "learning_rate": 6.1308076923076925e-06,
      "loss": 4.6314,
      "step": 1140600
    },
    {
      "epoch": 2.6335109477591954,
      "grad_norm": 6.071807861328125,
      "learning_rate": 6.126961538461539e-06,
      "loss": 4.6112,
      "step": 1140700
    },
    {
      "epoch": 2.63374181573042,
      "grad_norm": 7.169225692749023,
      "learning_rate": 6.1231153846153845e-06,
      "loss": 4.5886,
      "step": 1140800
    },
    {
      "epoch": 2.6339726837016446,
      "grad_norm": 6.270476341247559,
      "learning_rate": 6.119269230769231e-06,
      "loss": 4.6205,
      "step": 1140900
    },
    {
      "epoch": 2.6342035516728695,
      "grad_norm": 7.360788822174072,
      "learning_rate": 6.115423076923077e-06,
      "loss": 4.6188,
      "step": 1141000
    },
    {
      "epoch": 2.634434419644094,
      "grad_norm": 6.0305962562561035,
      "learning_rate": 6.111576923076924e-06,
      "loss": 4.6055,
      "step": 1141100
    },
    {
      "epoch": 2.6346652876153187,
      "grad_norm": 6.109908103942871,
      "learning_rate": 6.10773076923077e-06,
      "loss": 4.6172,
      "step": 1141200
    },
    {
      "epoch": 2.634896155586543,
      "grad_norm": 5.758654594421387,
      "learning_rate": 6.1038846153846155e-06,
      "loss": 4.6071,
      "step": 1141300
    },
    {
      "epoch": 2.635127023557768,
      "grad_norm": 5.358509540557861,
      "learning_rate": 6.100038461538462e-06,
      "loss": 4.6151,
      "step": 1141400
    },
    {
      "epoch": 2.6353578915289924,
      "grad_norm": 5.097407817840576,
      "learning_rate": 6.096192307692308e-06,
      "loss": 4.6225,
      "step": 1141500
    },
    {
      "epoch": 2.6355887595002168,
      "grad_norm": 8.273259162902832,
      "learning_rate": 6.092346153846154e-06,
      "loss": 4.6455,
      "step": 1141600
    },
    {
      "epoch": 2.6358196274714416,
      "grad_norm": 5.438462734222412,
      "learning_rate": 6.0885e-06,
      "loss": 4.6171,
      "step": 1141700
    },
    {
      "epoch": 2.6360504954426665,
      "grad_norm": 7.040985584259033,
      "learning_rate": 6.084653846153846e-06,
      "loss": 4.5643,
      "step": 1141800
    },
    {
      "epoch": 2.636281363413891,
      "grad_norm": 6.047891139984131,
      "learning_rate": 6.080807692307693e-06,
      "loss": 4.5921,
      "step": 1141900
    },
    {
      "epoch": 2.6365122313851153,
      "grad_norm": 6.145970821380615,
      "learning_rate": 6.076961538461539e-06,
      "loss": 4.5814,
      "step": 1142000
    },
    {
      "epoch": 2.63674309935634,
      "grad_norm": 7.175294399261475,
      "learning_rate": 6.073115384615385e-06,
      "loss": 4.5933,
      "step": 1142100
    },
    {
      "epoch": 2.636973967327565,
      "grad_norm": 8.95953369140625,
      "learning_rate": 6.069269230769231e-06,
      "loss": 4.5681,
      "step": 1142200
    },
    {
      "epoch": 2.6372048352987894,
      "grad_norm": 6.025686264038086,
      "learning_rate": 6.065423076923077e-06,
      "loss": 4.6138,
      "step": 1142300
    },
    {
      "epoch": 2.6374357032700138,
      "grad_norm": 5.615450382232666,
      "learning_rate": 6.061576923076923e-06,
      "loss": 4.6191,
      "step": 1142400
    },
    {
      "epoch": 2.6376665712412386,
      "grad_norm": 5.793150424957275,
      "learning_rate": 6.05773076923077e-06,
      "loss": 4.61,
      "step": 1142500
    },
    {
      "epoch": 2.6378974392124634,
      "grad_norm": 5.6257758140563965,
      "learning_rate": 6.053884615384615e-06,
      "loss": 4.5959,
      "step": 1142600
    },
    {
      "epoch": 2.638128307183688,
      "grad_norm": 6.07775354385376,
      "learning_rate": 6.0500384615384616e-06,
      "loss": 4.5964,
      "step": 1142700
    },
    {
      "epoch": 2.6383591751549123,
      "grad_norm": 5.933465003967285,
      "learning_rate": 6.046192307692308e-06,
      "loss": 4.6163,
      "step": 1142800
    },
    {
      "epoch": 2.638590043126137,
      "grad_norm": 6.638937473297119,
      "learning_rate": 6.042346153846154e-06,
      "loss": 4.6039,
      "step": 1142900
    },
    {
      "epoch": 2.6388209110973615,
      "grad_norm": 6.813966274261475,
      "learning_rate": 6.038500000000001e-06,
      "loss": 4.5918,
      "step": 1143000
    },
    {
      "epoch": 2.6390517790685863,
      "grad_norm": 11.759498596191406,
      "learning_rate": 6.034653846153846e-06,
      "loss": 4.6654,
      "step": 1143100
    },
    {
      "epoch": 2.6392826470398107,
      "grad_norm": 5.787928104400635,
      "learning_rate": 6.030807692307693e-06,
      "loss": 4.6287,
      "step": 1143200
    },
    {
      "epoch": 2.6395135150110356,
      "grad_norm": 5.721101760864258,
      "learning_rate": 6.026961538461539e-06,
      "loss": 4.5644,
      "step": 1143300
    },
    {
      "epoch": 2.63974438298226,
      "grad_norm": 6.137598037719727,
      "learning_rate": 6.0231153846153846e-06,
      "loss": 4.6339,
      "step": 1143400
    },
    {
      "epoch": 2.639975250953485,
      "grad_norm": 7.041064739227295,
      "learning_rate": 6.019269230769231e-06,
      "loss": 4.5925,
      "step": 1143500
    },
    {
      "epoch": 2.6402061189247092,
      "grad_norm": 5.2686872482299805,
      "learning_rate": 6.015423076923077e-06,
      "loss": 4.6139,
      "step": 1143600
    },
    {
      "epoch": 2.640436986895934,
      "grad_norm": 5.165757179260254,
      "learning_rate": 6.011576923076923e-06,
      "loss": 4.5527,
      "step": 1143700
    },
    {
      "epoch": 2.6406678548671585,
      "grad_norm": 6.383805274963379,
      "learning_rate": 6.007730769230769e-06,
      "loss": 4.6227,
      "step": 1143800
    },
    {
      "epoch": 2.6408987228383833,
      "grad_norm": 5.940663814544678,
      "learning_rate": 6.003884615384616e-06,
      "loss": 4.5825,
      "step": 1143900
    },
    {
      "epoch": 2.6411295908096077,
      "grad_norm": 7.029284477233887,
      "learning_rate": 6.000038461538462e-06,
      "loss": 4.5676,
      "step": 1144000
    },
    {
      "epoch": 2.6413604587808326,
      "grad_norm": 5.596332550048828,
      "learning_rate": 5.996192307692308e-06,
      "loss": 4.5762,
      "step": 1144100
    },
    {
      "epoch": 2.641591326752057,
      "grad_norm": 5.5975260734558105,
      "learning_rate": 5.992346153846154e-06,
      "loss": 4.6323,
      "step": 1144200
    },
    {
      "epoch": 2.6418221947232814,
      "grad_norm": 6.7220916748046875,
      "learning_rate": 5.9885e-06,
      "loss": 4.5757,
      "step": 1144300
    },
    {
      "epoch": 2.6420530626945062,
      "grad_norm": 8.827366828918457,
      "learning_rate": 5.984653846153847e-06,
      "loss": 4.5796,
      "step": 1144400
    },
    {
      "epoch": 2.642283930665731,
      "grad_norm": 7.731281757354736,
      "learning_rate": 5.980807692307692e-06,
      "loss": 4.611,
      "step": 1144500
    },
    {
      "epoch": 2.6425147986369555,
      "grad_norm": 5.609256267547607,
      "learning_rate": 5.976961538461539e-06,
      "loss": 4.6033,
      "step": 1144600
    },
    {
      "epoch": 2.64274566660818,
      "grad_norm": 5.626757621765137,
      "learning_rate": 5.973115384615385e-06,
      "loss": 4.597,
      "step": 1144700
    },
    {
      "epoch": 2.6429765345794047,
      "grad_norm": 5.51717472076416,
      "learning_rate": 5.9692692307692314e-06,
      "loss": 4.5993,
      "step": 1144800
    },
    {
      "epoch": 2.6432074025506296,
      "grad_norm": 6.137684345245361,
      "learning_rate": 5.965423076923078e-06,
      "loss": 4.6084,
      "step": 1144900
    },
    {
      "epoch": 2.643438270521854,
      "grad_norm": 5.710581302642822,
      "learning_rate": 5.961576923076923e-06,
      "loss": 4.5801,
      "step": 1145000
    },
    {
      "epoch": 2.6436691384930784,
      "grad_norm": 5.62742280960083,
      "learning_rate": 5.95773076923077e-06,
      "loss": 4.6151,
      "step": 1145100
    },
    {
      "epoch": 2.6439000064643032,
      "grad_norm": 5.513373374938965,
      "learning_rate": 5.953884615384615e-06,
      "loss": 4.6397,
      "step": 1145200
    },
    {
      "epoch": 2.644130874435528,
      "grad_norm": 5.750004291534424,
      "learning_rate": 5.950038461538462e-06,
      "loss": 4.5852,
      "step": 1145300
    },
    {
      "epoch": 2.6443617424067525,
      "grad_norm": 6.428973197937012,
      "learning_rate": 5.946192307692308e-06,
      "loss": 4.6118,
      "step": 1145400
    },
    {
      "epoch": 2.644592610377977,
      "grad_norm": 6.793677806854248,
      "learning_rate": 5.942346153846154e-06,
      "loss": 4.5817,
      "step": 1145500
    },
    {
      "epoch": 2.6448234783492017,
      "grad_norm": 5.740260601043701,
      "learning_rate": 5.9385e-06,
      "loss": 4.5644,
      "step": 1145600
    },
    {
      "epoch": 2.645054346320426,
      "grad_norm": 5.456676959991455,
      "learning_rate": 5.934653846153846e-06,
      "loss": 4.6073,
      "step": 1145700
    },
    {
      "epoch": 2.645285214291651,
      "grad_norm": 6.197357177734375,
      "learning_rate": 5.930807692307693e-06,
      "loss": 4.5942,
      "step": 1145800
    },
    {
      "epoch": 2.6455160822628754,
      "grad_norm": 7.11569881439209,
      "learning_rate": 5.926961538461539e-06,
      "loss": 4.6162,
      "step": 1145900
    },
    {
      "epoch": 2.6457469502341002,
      "grad_norm": 5.355040073394775,
      "learning_rate": 5.923115384615385e-06,
      "loss": 4.623,
      "step": 1146000
    },
    {
      "epoch": 2.6459778182053246,
      "grad_norm": 5.822505474090576,
      "learning_rate": 5.919269230769231e-06,
      "loss": 4.6071,
      "step": 1146100
    },
    {
      "epoch": 2.6462086861765495,
      "grad_norm": 7.1537065505981445,
      "learning_rate": 5.9154230769230774e-06,
      "loss": 4.6113,
      "step": 1146200
    },
    {
      "epoch": 2.646439554147774,
      "grad_norm": 6.567570209503174,
      "learning_rate": 5.911576923076923e-06,
      "loss": 4.6471,
      "step": 1146300
    },
    {
      "epoch": 2.6466704221189987,
      "grad_norm": 5.948604583740234,
      "learning_rate": 5.907730769230769e-06,
      "loss": 4.6225,
      "step": 1146400
    },
    {
      "epoch": 2.646901290090223,
      "grad_norm": 5.948800563812256,
      "learning_rate": 5.903884615384616e-06,
      "loss": 4.6048,
      "step": 1146500
    },
    {
      "epoch": 2.647132158061448,
      "grad_norm": 6.227224349975586,
      "learning_rate": 5.900038461538461e-06,
      "loss": 4.6143,
      "step": 1146600
    },
    {
      "epoch": 2.6473630260326724,
      "grad_norm": 5.457129955291748,
      "learning_rate": 5.8961923076923085e-06,
      "loss": 4.5686,
      "step": 1146700
    },
    {
      "epoch": 2.647593894003897,
      "grad_norm": 5.930593967437744,
      "learning_rate": 5.892346153846154e-06,
      "loss": 4.5957,
      "step": 1146800
    },
    {
      "epoch": 2.6478247619751216,
      "grad_norm": 6.872560977935791,
      "learning_rate": 5.8885000000000005e-06,
      "loss": 4.6056,
      "step": 1146900
    },
    {
      "epoch": 2.648055629946346,
      "grad_norm": 10.745585441589355,
      "learning_rate": 5.884653846153847e-06,
      "loss": 4.6099,
      "step": 1147000
    },
    {
      "epoch": 2.648286497917571,
      "grad_norm": 6.378479480743408,
      "learning_rate": 5.880807692307692e-06,
      "loss": 4.6059,
      "step": 1147100
    },
    {
      "epoch": 2.6485173658887957,
      "grad_norm": 6.37655782699585,
      "learning_rate": 5.876961538461539e-06,
      "loss": 4.5838,
      "step": 1147200
    },
    {
      "epoch": 2.64874823386002,
      "grad_norm": 7.503279685974121,
      "learning_rate": 5.873115384615385e-06,
      "loss": 4.599,
      "step": 1147300
    },
    {
      "epoch": 2.6489791018312445,
      "grad_norm": 5.5767951011657715,
      "learning_rate": 5.869269230769231e-06,
      "loss": 4.6109,
      "step": 1147400
    },
    {
      "epoch": 2.6492099698024694,
      "grad_norm": 6.632805347442627,
      "learning_rate": 5.865423076923077e-06,
      "loss": 4.6365,
      "step": 1147500
    },
    {
      "epoch": 2.649440837773694,
      "grad_norm": 5.775228977203369,
      "learning_rate": 5.8615769230769235e-06,
      "loss": 4.6257,
      "step": 1147600
    },
    {
      "epoch": 2.6496717057449186,
      "grad_norm": 5.650686740875244,
      "learning_rate": 5.85773076923077e-06,
      "loss": 4.5928,
      "step": 1147700
    },
    {
      "epoch": 2.649902573716143,
      "grad_norm": 8.475406646728516,
      "learning_rate": 5.853884615384616e-06,
      "loss": 4.5666,
      "step": 1147800
    },
    {
      "epoch": 2.650133441687368,
      "grad_norm": 5.2553863525390625,
      "learning_rate": 5.850038461538462e-06,
      "loss": 4.5662,
      "step": 1147900
    },
    {
      "epoch": 2.6503643096585927,
      "grad_norm": 9.465728759765625,
      "learning_rate": 5.846192307692308e-06,
      "loss": 4.566,
      "step": 1148000
    },
    {
      "epoch": 2.650595177629817,
      "grad_norm": 6.508630275726318,
      "learning_rate": 5.842346153846154e-06,
      "loss": 4.6495,
      "step": 1148100
    },
    {
      "epoch": 2.6508260456010415,
      "grad_norm": 6.361420154571533,
      "learning_rate": 5.8385e-06,
      "loss": 4.568,
      "step": 1148200
    },
    {
      "epoch": 2.6510569135722664,
      "grad_norm": 5.8235602378845215,
      "learning_rate": 5.8346538461538465e-06,
      "loss": 4.5953,
      "step": 1148300
    },
    {
      "epoch": 2.6512877815434908,
      "grad_norm": 7.102438449859619,
      "learning_rate": 5.830807692307692e-06,
      "loss": 4.5412,
      "step": 1148400
    },
    {
      "epoch": 2.6515186495147156,
      "grad_norm": 5.7578606605529785,
      "learning_rate": 5.826961538461538e-06,
      "loss": 4.614,
      "step": 1148500
    },
    {
      "epoch": 2.65174951748594,
      "grad_norm": 6.048397541046143,
      "learning_rate": 5.823115384615385e-06,
      "loss": 4.5938,
      "step": 1148600
    },
    {
      "epoch": 2.651980385457165,
      "grad_norm": 6.107083320617676,
      "learning_rate": 5.819269230769231e-06,
      "loss": 4.6158,
      "step": 1148700
    },
    {
      "epoch": 2.6522112534283893,
      "grad_norm": 5.818136215209961,
      "learning_rate": 5.8154230769230776e-06,
      "loss": 4.5608,
      "step": 1148800
    },
    {
      "epoch": 2.652442121399614,
      "grad_norm": 6.0289459228515625,
      "learning_rate": 5.811576923076923e-06,
      "loss": 4.6249,
      "step": 1148900
    },
    {
      "epoch": 2.6526729893708385,
      "grad_norm": 6.51557731628418,
      "learning_rate": 5.8077307692307695e-06,
      "loss": 4.6041,
      "step": 1149000
    },
    {
      "epoch": 2.6529038573420634,
      "grad_norm": 6.035068511962891,
      "learning_rate": 5.803884615384616e-06,
      "loss": 4.621,
      "step": 1149100
    },
    {
      "epoch": 2.6531347253132878,
      "grad_norm": 6.263927459716797,
      "learning_rate": 5.800038461538461e-06,
      "loss": 4.5879,
      "step": 1149200
    },
    {
      "epoch": 2.6533655932845126,
      "grad_norm": 5.538919925689697,
      "learning_rate": 5.796192307692308e-06,
      "loss": 4.5614,
      "step": 1149300
    },
    {
      "epoch": 2.653596461255737,
      "grad_norm": 5.460351943969727,
      "learning_rate": 5.792346153846154e-06,
      "loss": 4.6665,
      "step": 1149400
    },
    {
      "epoch": 2.653827329226962,
      "grad_norm": 5.304315567016602,
      "learning_rate": 5.7885000000000006e-06,
      "loss": 4.6094,
      "step": 1149500
    },
    {
      "epoch": 2.6540581971981863,
      "grad_norm": 6.314706802368164,
      "learning_rate": 5.784653846153847e-06,
      "loss": 4.5984,
      "step": 1149600
    },
    {
      "epoch": 2.6542890651694107,
      "grad_norm": 5.954013347625732,
      "learning_rate": 5.7808076923076925e-06,
      "loss": 4.5686,
      "step": 1149700
    },
    {
      "epoch": 2.6545199331406355,
      "grad_norm": 5.103060722351074,
      "learning_rate": 5.776961538461539e-06,
      "loss": 4.5698,
      "step": 1149800
    },
    {
      "epoch": 2.6547508011118603,
      "grad_norm": 6.296014785766602,
      "learning_rate": 5.773115384615385e-06,
      "loss": 4.5168,
      "step": 1149900
    },
    {
      "epoch": 2.6549816690830847,
      "grad_norm": 6.510451793670654,
      "learning_rate": 5.769269230769231e-06,
      "loss": 4.6224,
      "step": 1150000
    },
    {
      "epoch": 2.655212537054309,
      "grad_norm": 6.351698398590088,
      "learning_rate": 5.765423076923077e-06,
      "loss": 4.578,
      "step": 1150100
    },
    {
      "epoch": 2.655443405025534,
      "grad_norm": 6.210583209991455,
      "learning_rate": 5.761576923076923e-06,
      "loss": 4.5837,
      "step": 1150200
    },
    {
      "epoch": 2.655674272996759,
      "grad_norm": 8.56289005279541,
      "learning_rate": 5.757730769230769e-06,
      "loss": 4.6,
      "step": 1150300
    },
    {
      "epoch": 2.6559051409679832,
      "grad_norm": 7.068514347076416,
      "learning_rate": 5.7538846153846155e-06,
      "loss": 4.6029,
      "step": 1150400
    },
    {
      "epoch": 2.6561360089392076,
      "grad_norm": 6.354221820831299,
      "learning_rate": 5.750038461538462e-06,
      "loss": 4.5662,
      "step": 1150500
    },
    {
      "epoch": 2.6563668769104325,
      "grad_norm": 6.266217231750488,
      "learning_rate": 5.746192307692308e-06,
      "loss": 4.617,
      "step": 1150600
    },
    {
      "epoch": 2.6565977448816573,
      "grad_norm": 6.16117000579834,
      "learning_rate": 5.742346153846155e-06,
      "loss": 4.5359,
      "step": 1150700
    },
    {
      "epoch": 2.6568286128528817,
      "grad_norm": 6.129704475402832,
      "learning_rate": 5.7385e-06,
      "loss": 4.5688,
      "step": 1150800
    },
    {
      "epoch": 2.657059480824106,
      "grad_norm": 5.827393054962158,
      "learning_rate": 5.734653846153847e-06,
      "loss": 4.6127,
      "step": 1150900
    },
    {
      "epoch": 2.657290348795331,
      "grad_norm": 5.883533000946045,
      "learning_rate": 5.730807692307692e-06,
      "loss": 4.6071,
      "step": 1151000
    },
    {
      "epoch": 2.6575212167665554,
      "grad_norm": 5.726599216461182,
      "learning_rate": 5.7269615384615385e-06,
      "loss": 4.5549,
      "step": 1151100
    },
    {
      "epoch": 2.6577520847377802,
      "grad_norm": 6.390722751617432,
      "learning_rate": 5.723115384615385e-06,
      "loss": 4.6173,
      "step": 1151200
    },
    {
      "epoch": 2.6579829527090046,
      "grad_norm": 5.711450099945068,
      "learning_rate": 5.7192692307692304e-06,
      "loss": 4.6023,
      "step": 1151300
    },
    {
      "epoch": 2.6582138206802295,
      "grad_norm": 5.640965461730957,
      "learning_rate": 5.715423076923077e-06,
      "loss": 4.5865,
      "step": 1151400
    },
    {
      "epoch": 2.658444688651454,
      "grad_norm": 6.649728775024414,
      "learning_rate": 5.711576923076924e-06,
      "loss": 4.5818,
      "step": 1151500
    },
    {
      "epoch": 2.6586755566226787,
      "grad_norm": 5.577934741973877,
      "learning_rate": 5.70773076923077e-06,
      "loss": 4.5549,
      "step": 1151600
    },
    {
      "epoch": 2.658906424593903,
      "grad_norm": 5.897771835327148,
      "learning_rate": 5.703884615384616e-06,
      "loss": 4.6374,
      "step": 1151700
    },
    {
      "epoch": 2.659137292565128,
      "grad_norm": 6.049086570739746,
      "learning_rate": 5.7000384615384615e-06,
      "loss": 4.629,
      "step": 1151800
    },
    {
      "epoch": 2.6593681605363524,
      "grad_norm": 5.741835117340088,
      "learning_rate": 5.696192307692308e-06,
      "loss": 4.6313,
      "step": 1151900
    },
    {
      "epoch": 2.6595990285075772,
      "grad_norm": 7.07002592086792,
      "learning_rate": 5.692346153846154e-06,
      "loss": 4.5362,
      "step": 1152000
    },
    {
      "epoch": 2.6598298964788016,
      "grad_norm": 6.3447136878967285,
      "learning_rate": 5.6885e-06,
      "loss": 4.6135,
      "step": 1152100
    },
    {
      "epoch": 2.6600607644500265,
      "grad_norm": 6.10767126083374,
      "learning_rate": 5.684653846153846e-06,
      "loss": 4.6218,
      "step": 1152200
    },
    {
      "epoch": 2.660291632421251,
      "grad_norm": 7.057210922241211,
      "learning_rate": 5.680807692307693e-06,
      "loss": 4.5788,
      "step": 1152300
    },
    {
      "epoch": 2.6605225003924753,
      "grad_norm": 5.717965126037598,
      "learning_rate": 5.676961538461539e-06,
      "loss": 4.5643,
      "step": 1152400
    },
    {
      "epoch": 2.6607533683637,
      "grad_norm": 7.471529006958008,
      "learning_rate": 5.673115384615385e-06,
      "loss": 4.6334,
      "step": 1152500
    },
    {
      "epoch": 2.660984236334925,
      "grad_norm": 6.242431163787842,
      "learning_rate": 5.669269230769231e-06,
      "loss": 4.6,
      "step": 1152600
    },
    {
      "epoch": 2.6612151043061494,
      "grad_norm": 5.369986534118652,
      "learning_rate": 5.665423076923077e-06,
      "loss": 4.5897,
      "step": 1152700
    },
    {
      "epoch": 2.661445972277374,
      "grad_norm": 6.063516139984131,
      "learning_rate": 5.661576923076924e-06,
      "loss": 4.5647,
      "step": 1152800
    },
    {
      "epoch": 2.6616768402485986,
      "grad_norm": 5.819179058074951,
      "learning_rate": 5.657730769230769e-06,
      "loss": 4.633,
      "step": 1152900
    },
    {
      "epoch": 2.6619077082198235,
      "grad_norm": 5.228287220001221,
      "learning_rate": 5.653884615384616e-06,
      "loss": 4.5699,
      "step": 1153000
    },
    {
      "epoch": 2.662138576191048,
      "grad_norm": 7.9302144050598145,
      "learning_rate": 5.650038461538461e-06,
      "loss": 4.5804,
      "step": 1153100
    },
    {
      "epoch": 2.6623694441622723,
      "grad_norm": 5.188127517700195,
      "learning_rate": 5.6461923076923075e-06,
      "loss": 4.5961,
      "step": 1153200
    },
    {
      "epoch": 2.662600312133497,
      "grad_norm": 6.139073371887207,
      "learning_rate": 5.642346153846154e-06,
      "loss": 4.609,
      "step": 1153300
    },
    {
      "epoch": 2.662831180104722,
      "grad_norm": 6.2682647705078125,
      "learning_rate": 5.6385e-06,
      "loss": 4.5555,
      "step": 1153400
    },
    {
      "epoch": 2.6630620480759464,
      "grad_norm": 5.440744876861572,
      "learning_rate": 5.634653846153847e-06,
      "loss": 4.597,
      "step": 1153500
    },
    {
      "epoch": 2.6632929160471708,
      "grad_norm": 5.413341522216797,
      "learning_rate": 5.630807692307693e-06,
      "loss": 4.5956,
      "step": 1153600
    },
    {
      "epoch": 2.6635237840183956,
      "grad_norm": 6.171175479888916,
      "learning_rate": 5.626961538461539e-06,
      "loss": 4.6245,
      "step": 1153700
    },
    {
      "epoch": 2.66375465198962,
      "grad_norm": 6.630688667297363,
      "learning_rate": 5.623115384615385e-06,
      "loss": 4.579,
      "step": 1153800
    },
    {
      "epoch": 2.663985519960845,
      "grad_norm": 5.83845853805542,
      "learning_rate": 5.6192692307692305e-06,
      "loss": 4.6274,
      "step": 1153900
    },
    {
      "epoch": 2.6642163879320693,
      "grad_norm": 6.134106636047363,
      "learning_rate": 5.615423076923077e-06,
      "loss": 4.5959,
      "step": 1154000
    },
    {
      "epoch": 2.664447255903294,
      "grad_norm": 6.206569194793701,
      "learning_rate": 5.611576923076923e-06,
      "loss": 4.6123,
      "step": 1154100
    },
    {
      "epoch": 2.6646781238745185,
      "grad_norm": 6.190959453582764,
      "learning_rate": 5.60773076923077e-06,
      "loss": 4.5461,
      "step": 1154200
    },
    {
      "epoch": 2.6649089918457434,
      "grad_norm": 5.626429557800293,
      "learning_rate": 5.603884615384616e-06,
      "loss": 4.5715,
      "step": 1154300
    },
    {
      "epoch": 2.6651398598169678,
      "grad_norm": 7.2677483558654785,
      "learning_rate": 5.6000384615384625e-06,
      "loss": 4.6275,
      "step": 1154400
    },
    {
      "epoch": 2.6653707277881926,
      "grad_norm": 5.619269847869873,
      "learning_rate": 5.596192307692308e-06,
      "loss": 4.6054,
      "step": 1154500
    },
    {
      "epoch": 2.665601595759417,
      "grad_norm": 7.636053085327148,
      "learning_rate": 5.592346153846154e-06,
      "loss": 4.6319,
      "step": 1154600
    },
    {
      "epoch": 2.665832463730642,
      "grad_norm": 5.688714981079102,
      "learning_rate": 5.5885e-06,
      "loss": 4.6107,
      "step": 1154700
    },
    {
      "epoch": 2.6660633317018663,
      "grad_norm": 6.853294372558594,
      "learning_rate": 5.584653846153846e-06,
      "loss": 4.5918,
      "step": 1154800
    },
    {
      "epoch": 2.666294199673091,
      "grad_norm": 6.146968364715576,
      "learning_rate": 5.580807692307693e-06,
      "loss": 4.5184,
      "step": 1154900
    },
    {
      "epoch": 2.6665250676443155,
      "grad_norm": 6.525213718414307,
      "learning_rate": 5.576961538461538e-06,
      "loss": 4.5971,
      "step": 1155000
    },
    {
      "epoch": 2.66675593561554,
      "grad_norm": 5.773026466369629,
      "learning_rate": 5.573115384615385e-06,
      "loss": 4.5518,
      "step": 1155100
    },
    {
      "epoch": 2.6669868035867648,
      "grad_norm": 6.5846638679504395,
      "learning_rate": 5.569269230769231e-06,
      "loss": 4.552,
      "step": 1155200
    },
    {
      "epoch": 2.6672176715579896,
      "grad_norm": 6.703915596008301,
      "learning_rate": 5.565423076923077e-06,
      "loss": 4.5946,
      "step": 1155300
    },
    {
      "epoch": 2.667448539529214,
      "grad_norm": 5.423802852630615,
      "learning_rate": 5.561576923076924e-06,
      "loss": 4.578,
      "step": 1155400
    },
    {
      "epoch": 2.6676794075004384,
      "grad_norm": 6.14579963684082,
      "learning_rate": 5.557730769230769e-06,
      "loss": 4.578,
      "step": 1155500
    },
    {
      "epoch": 2.6679102754716633,
      "grad_norm": 5.524372100830078,
      "learning_rate": 5.553884615384616e-06,
      "loss": 4.6014,
      "step": 1155600
    },
    {
      "epoch": 2.668141143442888,
      "grad_norm": 5.4594831466674805,
      "learning_rate": 5.550038461538462e-06,
      "loss": 4.5682,
      "step": 1155700
    },
    {
      "epoch": 2.6683720114141125,
      "grad_norm": 7.508424282073975,
      "learning_rate": 5.546192307692308e-06,
      "loss": 4.5883,
      "step": 1155800
    },
    {
      "epoch": 2.668602879385337,
      "grad_norm": 5.742066383361816,
      "learning_rate": 5.542346153846154e-06,
      "loss": 4.5742,
      "step": 1155900
    },
    {
      "epoch": 2.6688337473565618,
      "grad_norm": 6.582572937011719,
      "learning_rate": 5.5384999999999996e-06,
      "loss": 4.6012,
      "step": 1156000
    },
    {
      "epoch": 2.6690646153277866,
      "grad_norm": 6.390713214874268,
      "learning_rate": 5.534653846153846e-06,
      "loss": 4.6078,
      "step": 1156100
    },
    {
      "epoch": 2.669295483299011,
      "grad_norm": 5.566352844238281,
      "learning_rate": 5.530807692307693e-06,
      "loss": 4.6196,
      "step": 1156200
    },
    {
      "epoch": 2.6695263512702354,
      "grad_norm": 5.783066749572754,
      "learning_rate": 5.526961538461539e-06,
      "loss": 4.587,
      "step": 1156300
    },
    {
      "epoch": 2.6697572192414603,
      "grad_norm": 6.181545734405518,
      "learning_rate": 5.523115384615385e-06,
      "loss": 4.5762,
      "step": 1156400
    },
    {
      "epoch": 2.6699880872126847,
      "grad_norm": 6.074690341949463,
      "learning_rate": 5.5192692307692315e-06,
      "loss": 4.5574,
      "step": 1156500
    },
    {
      "epoch": 2.6702189551839095,
      "grad_norm": 5.624835968017578,
      "learning_rate": 5.515423076923077e-06,
      "loss": 4.611,
      "step": 1156600
    },
    {
      "epoch": 2.670449823155134,
      "grad_norm": 5.474942684173584,
      "learning_rate": 5.5115769230769234e-06,
      "loss": 4.5829,
      "step": 1156700
    },
    {
      "epoch": 2.6706806911263588,
      "grad_norm": 7.024411201477051,
      "learning_rate": 5.507730769230769e-06,
      "loss": 4.5659,
      "step": 1156800
    },
    {
      "epoch": 2.670911559097583,
      "grad_norm": 6.596469402313232,
      "learning_rate": 5.503884615384615e-06,
      "loss": 4.5795,
      "step": 1156900
    },
    {
      "epoch": 2.671142427068808,
      "grad_norm": 5.983609676361084,
      "learning_rate": 5.500038461538462e-06,
      "loss": 4.5843,
      "step": 1157000
    },
    {
      "epoch": 2.6713732950400324,
      "grad_norm": 5.52172327041626,
      "learning_rate": 5.496192307692308e-06,
      "loss": 4.5806,
      "step": 1157100
    },
    {
      "epoch": 2.6716041630112572,
      "grad_norm": 6.648263454437256,
      "learning_rate": 5.4923461538461545e-06,
      "loss": 4.5308,
      "step": 1157200
    },
    {
      "epoch": 2.6718350309824817,
      "grad_norm": 6.0425705909729,
      "learning_rate": 5.488500000000001e-06,
      "loss": 4.6252,
      "step": 1157300
    },
    {
      "epoch": 2.6720658989537065,
      "grad_norm": 5.836409568786621,
      "learning_rate": 5.4846538461538464e-06,
      "loss": 4.6065,
      "step": 1157400
    },
    {
      "epoch": 2.672296766924931,
      "grad_norm": 5.10738468170166,
      "learning_rate": 5.480807692307693e-06,
      "loss": 4.5517,
      "step": 1157500
    },
    {
      "epoch": 2.6725276348961557,
      "grad_norm": 5.475467205047607,
      "learning_rate": 5.476961538461538e-06,
      "loss": 4.5878,
      "step": 1157600
    },
    {
      "epoch": 2.67275850286738,
      "grad_norm": 6.169427394866943,
      "learning_rate": 5.473115384615385e-06,
      "loss": 4.6348,
      "step": 1157700
    },
    {
      "epoch": 2.6729893708386046,
      "grad_norm": 6.13181209564209,
      "learning_rate": 5.469269230769231e-06,
      "loss": 4.5519,
      "step": 1157800
    },
    {
      "epoch": 2.6732202388098294,
      "grad_norm": 5.662928104400635,
      "learning_rate": 5.465423076923077e-06,
      "loss": 4.6031,
      "step": 1157900
    },
    {
      "epoch": 2.6734511067810542,
      "grad_norm": 5.659206390380859,
      "learning_rate": 5.461576923076923e-06,
      "loss": 4.5747,
      "step": 1158000
    },
    {
      "epoch": 2.6736819747522786,
      "grad_norm": 5.856066703796387,
      "learning_rate": 5.4577307692307694e-06,
      "loss": 4.5477,
      "step": 1158100
    },
    {
      "epoch": 2.673912842723503,
      "grad_norm": 5.284942626953125,
      "learning_rate": 5.453884615384616e-06,
      "loss": 4.6019,
      "step": 1158200
    },
    {
      "epoch": 2.674143710694728,
      "grad_norm": 5.547715187072754,
      "learning_rate": 5.450038461538462e-06,
      "loss": 4.5999,
      "step": 1158300
    },
    {
      "epoch": 2.6743745786659527,
      "grad_norm": 7.111536026000977,
      "learning_rate": 5.446192307692308e-06,
      "loss": 4.6052,
      "step": 1158400
    },
    {
      "epoch": 2.674605446637177,
      "grad_norm": 6.833667278289795,
      "learning_rate": 5.442346153846154e-06,
      "loss": 4.6161,
      "step": 1158500
    },
    {
      "epoch": 2.6748363146084015,
      "grad_norm": 5.957667350769043,
      "learning_rate": 5.4385000000000005e-06,
      "loss": 4.5669,
      "step": 1158600
    },
    {
      "epoch": 2.6750671825796264,
      "grad_norm": 7.163536071777344,
      "learning_rate": 5.434653846153846e-06,
      "loss": 4.5759,
      "step": 1158700
    },
    {
      "epoch": 2.6752980505508512,
      "grad_norm": 7.055347919464111,
      "learning_rate": 5.4308076923076925e-06,
      "loss": 4.5741,
      "step": 1158800
    },
    {
      "epoch": 2.6755289185220756,
      "grad_norm": 6.589362621307373,
      "learning_rate": 5.426961538461538e-06,
      "loss": 4.6071,
      "step": 1158900
    },
    {
      "epoch": 2.6757597864933,
      "grad_norm": 6.465494632720947,
      "learning_rate": 5.423115384615385e-06,
      "loss": 4.5975,
      "step": 1159000
    },
    {
      "epoch": 2.675990654464525,
      "grad_norm": 6.125578880310059,
      "learning_rate": 5.419269230769232e-06,
      "loss": 4.5393,
      "step": 1159100
    },
    {
      "epoch": 2.6762215224357493,
      "grad_norm": 5.84697961807251,
      "learning_rate": 5.415423076923077e-06,
      "loss": 4.6114,
      "step": 1159200
    },
    {
      "epoch": 2.676452390406974,
      "grad_norm": 7.356420516967773,
      "learning_rate": 5.4115769230769235e-06,
      "loss": 4.6307,
      "step": 1159300
    },
    {
      "epoch": 2.6766832583781985,
      "grad_norm": 5.342960357666016,
      "learning_rate": 5.40773076923077e-06,
      "loss": 4.5687,
      "step": 1159400
    },
    {
      "epoch": 2.6769141263494234,
      "grad_norm": 5.566362380981445,
      "learning_rate": 5.4038846153846155e-06,
      "loss": 4.5355,
      "step": 1159500
    },
    {
      "epoch": 2.677144994320648,
      "grad_norm": 6.798802375793457,
      "learning_rate": 5.400038461538462e-06,
      "loss": 4.5875,
      "step": 1159600
    },
    {
      "epoch": 2.6773758622918726,
      "grad_norm": 7.131104469299316,
      "learning_rate": 5.396192307692307e-06,
      "loss": 4.5612,
      "step": 1159700
    },
    {
      "epoch": 2.677606730263097,
      "grad_norm": 6.0999298095703125,
      "learning_rate": 5.392346153846154e-06,
      "loss": 4.5725,
      "step": 1159800
    },
    {
      "epoch": 2.677837598234322,
      "grad_norm": 5.711843490600586,
      "learning_rate": 5.3885e-06,
      "loss": 4.611,
      "step": 1159900
    },
    {
      "epoch": 2.6780684662055463,
      "grad_norm": 6.304359436035156,
      "learning_rate": 5.3846538461538465e-06,
      "loss": 4.6244,
      "step": 1160000
    },
    {
      "epoch": 2.678299334176771,
      "grad_norm": 6.067633152008057,
      "learning_rate": 5.380807692307693e-06,
      "loss": 4.5947,
      "step": 1160100
    },
    {
      "epoch": 2.6785302021479955,
      "grad_norm": 7.133291721343994,
      "learning_rate": 5.376961538461539e-06,
      "loss": 4.6064,
      "step": 1160200
    },
    {
      "epoch": 2.6787610701192204,
      "grad_norm": 5.381283283233643,
      "learning_rate": 5.373115384615385e-06,
      "loss": 4.5489,
      "step": 1160300
    },
    {
      "epoch": 2.678991938090445,
      "grad_norm": 6.231795310974121,
      "learning_rate": 5.369269230769231e-06,
      "loss": 4.5922,
      "step": 1160400
    },
    {
      "epoch": 2.679222806061669,
      "grad_norm": 8.23100471496582,
      "learning_rate": 5.365423076923077e-06,
      "loss": 4.5873,
      "step": 1160500
    },
    {
      "epoch": 2.679453674032894,
      "grad_norm": 6.871706485748291,
      "learning_rate": 5.361576923076923e-06,
      "loss": 4.511,
      "step": 1160600
    },
    {
      "epoch": 2.679684542004119,
      "grad_norm": 5.795687198638916,
      "learning_rate": 5.3577307692307696e-06,
      "loss": 4.5836,
      "step": 1160700
    },
    {
      "epoch": 2.6799154099753433,
      "grad_norm": 5.782908916473389,
      "learning_rate": 5.353884615384615e-06,
      "loss": 4.5647,
      "step": 1160800
    },
    {
      "epoch": 2.6801462779465677,
      "grad_norm": 6.261905193328857,
      "learning_rate": 5.3500384615384615e-06,
      "loss": 4.5752,
      "step": 1160900
    },
    {
      "epoch": 2.6803771459177925,
      "grad_norm": 5.970925331115723,
      "learning_rate": 5.346192307692308e-06,
      "loss": 4.5437,
      "step": 1161000
    },
    {
      "epoch": 2.6806080138890174,
      "grad_norm": 6.651040077209473,
      "learning_rate": 5.342346153846154e-06,
      "loss": 4.5726,
      "step": 1161100
    },
    {
      "epoch": 2.6808388818602418,
      "grad_norm": 5.288600444793701,
      "learning_rate": 5.338500000000001e-06,
      "loss": 4.5707,
      "step": 1161200
    },
    {
      "epoch": 2.681069749831466,
      "grad_norm": 6.49885368347168,
      "learning_rate": 5.334653846153846e-06,
      "loss": 4.57,
      "step": 1161300
    },
    {
      "epoch": 2.681300617802691,
      "grad_norm": 7.343451499938965,
      "learning_rate": 5.3308076923076926e-06,
      "loss": 4.5596,
      "step": 1161400
    },
    {
      "epoch": 2.681531485773916,
      "grad_norm": 6.293933868408203,
      "learning_rate": 5.326961538461539e-06,
      "loss": 4.5576,
      "step": 1161500
    },
    {
      "epoch": 2.6817623537451403,
      "grad_norm": 6.260909557342529,
      "learning_rate": 5.3231153846153845e-06,
      "loss": 4.5823,
      "step": 1161600
    },
    {
      "epoch": 2.6819932217163647,
      "grad_norm": 5.631511211395264,
      "learning_rate": 5.319269230769231e-06,
      "loss": 4.5782,
      "step": 1161700
    },
    {
      "epoch": 2.6822240896875895,
      "grad_norm": 6.2554497718811035,
      "learning_rate": 5.315423076923077e-06,
      "loss": 4.5678,
      "step": 1161800
    },
    {
      "epoch": 2.682454957658814,
      "grad_norm": 7.127565383911133,
      "learning_rate": 5.311576923076924e-06,
      "loss": 4.5432,
      "step": 1161900
    },
    {
      "epoch": 2.6826858256300388,
      "grad_norm": 5.310524940490723,
      "learning_rate": 5.30773076923077e-06,
      "loss": 4.6092,
      "step": 1162000
    },
    {
      "epoch": 2.682916693601263,
      "grad_norm": 5.992315292358398,
      "learning_rate": 5.3038846153846156e-06,
      "loss": 4.5472,
      "step": 1162100
    },
    {
      "epoch": 2.683147561572488,
      "grad_norm": 5.666263580322266,
      "learning_rate": 5.300038461538462e-06,
      "loss": 4.5568,
      "step": 1162200
    },
    {
      "epoch": 2.6833784295437124,
      "grad_norm": 6.277202129364014,
      "learning_rate": 5.296192307692308e-06,
      "loss": 4.5732,
      "step": 1162300
    },
    {
      "epoch": 2.6836092975149373,
      "grad_norm": 6.55912971496582,
      "learning_rate": 5.292346153846154e-06,
      "loss": 4.617,
      "step": 1162400
    },
    {
      "epoch": 2.6838401654861617,
      "grad_norm": 6.0154948234558105,
      "learning_rate": 5.2885e-06,
      "loss": 4.544,
      "step": 1162500
    },
    {
      "epoch": 2.6840710334573865,
      "grad_norm": 5.376554012298584,
      "learning_rate": 5.284653846153846e-06,
      "loss": 4.5388,
      "step": 1162600
    },
    {
      "epoch": 2.684301901428611,
      "grad_norm": 5.601325035095215,
      "learning_rate": 5.280807692307692e-06,
      "loss": 4.5561,
      "step": 1162700
    },
    {
      "epoch": 2.6845327693998358,
      "grad_norm": 5.723585605621338,
      "learning_rate": 5.276961538461539e-06,
      "loss": 4.591,
      "step": 1162800
    },
    {
      "epoch": 2.68476363737106,
      "grad_norm": 6.330595016479492,
      "learning_rate": 5.273115384615385e-06,
      "loss": 4.5713,
      "step": 1162900
    },
    {
      "epoch": 2.684994505342285,
      "grad_norm": 8.713724136352539,
      "learning_rate": 5.269269230769231e-06,
      "loss": 4.5902,
      "step": 1163000
    },
    {
      "epoch": 2.6852253733135094,
      "grad_norm": 7.25184440612793,
      "learning_rate": 5.265423076923078e-06,
      "loss": 4.6401,
      "step": 1163100
    },
    {
      "epoch": 2.685456241284734,
      "grad_norm": 6.457337379455566,
      "learning_rate": 5.261576923076923e-06,
      "loss": 4.5962,
      "step": 1163200
    },
    {
      "epoch": 2.6856871092559587,
      "grad_norm": 5.909477710723877,
      "learning_rate": 5.25773076923077e-06,
      "loss": 4.5306,
      "step": 1163300
    },
    {
      "epoch": 2.6859179772271835,
      "grad_norm": 6.74714469909668,
      "learning_rate": 5.253884615384615e-06,
      "loss": 4.5854,
      "step": 1163400
    },
    {
      "epoch": 2.686148845198408,
      "grad_norm": 6.5077619552612305,
      "learning_rate": 5.250038461538462e-06,
      "loss": 4.5812,
      "step": 1163500
    },
    {
      "epoch": 2.6863797131696323,
      "grad_norm": 6.4817681312561035,
      "learning_rate": 5.246192307692308e-06,
      "loss": 4.5484,
      "step": 1163600
    },
    {
      "epoch": 2.686610581140857,
      "grad_norm": 6.840216636657715,
      "learning_rate": 5.2423461538461535e-06,
      "loss": 4.5681,
      "step": 1163700
    },
    {
      "epoch": 2.686841449112082,
      "grad_norm": 5.481594562530518,
      "learning_rate": 5.238500000000001e-06,
      "loss": 4.5465,
      "step": 1163800
    },
    {
      "epoch": 2.6870723170833064,
      "grad_norm": 5.33528995513916,
      "learning_rate": 5.234653846153846e-06,
      "loss": 4.5098,
      "step": 1163900
    },
    {
      "epoch": 2.687303185054531,
      "grad_norm": 7.362955093383789,
      "learning_rate": 5.230807692307693e-06,
      "loss": 4.5765,
      "step": 1164000
    },
    {
      "epoch": 2.6875340530257557,
      "grad_norm": 9.13963794708252,
      "learning_rate": 5.226961538461539e-06,
      "loss": 4.5414,
      "step": 1164100
    },
    {
      "epoch": 2.6877649209969805,
      "grad_norm": 6.122569561004639,
      "learning_rate": 5.223115384615385e-06,
      "loss": 4.5903,
      "step": 1164200
    },
    {
      "epoch": 2.687995788968205,
      "grad_norm": 6.202209949493408,
      "learning_rate": 5.219269230769231e-06,
      "loss": 4.5876,
      "step": 1164300
    },
    {
      "epoch": 2.6882266569394293,
      "grad_norm": 6.358719825744629,
      "learning_rate": 5.215423076923077e-06,
      "loss": 4.6114,
      "step": 1164400
    },
    {
      "epoch": 2.688457524910654,
      "grad_norm": 5.999908447265625,
      "learning_rate": 5.211576923076923e-06,
      "loss": 4.5753,
      "step": 1164500
    },
    {
      "epoch": 2.6886883928818786,
      "grad_norm": 6.004038333892822,
      "learning_rate": 5.207730769230769e-06,
      "loss": 4.5754,
      "step": 1164600
    },
    {
      "epoch": 2.6889192608531034,
      "grad_norm": 5.433160781860352,
      "learning_rate": 5.203884615384616e-06,
      "loss": 4.5733,
      "step": 1164700
    },
    {
      "epoch": 2.689150128824328,
      "grad_norm": 6.562106609344482,
      "learning_rate": 5.200038461538462e-06,
      "loss": 4.5373,
      "step": 1164800
    },
    {
      "epoch": 2.6893809967955526,
      "grad_norm": 5.785248279571533,
      "learning_rate": 5.1961923076923085e-06,
      "loss": 4.5585,
      "step": 1164900
    },
    {
      "epoch": 2.689611864766777,
      "grad_norm": 6.2551045417785645,
      "learning_rate": 5.192346153846154e-06,
      "loss": 4.5415,
      "step": 1165000
    },
    {
      "epoch": 2.689842732738002,
      "grad_norm": 5.633389472961426,
      "learning_rate": 5.1885e-06,
      "loss": 4.553,
      "step": 1165100
    },
    {
      "epoch": 2.6900736007092263,
      "grad_norm": 6.882682800292969,
      "learning_rate": 5.184653846153847e-06,
      "loss": 4.6388,
      "step": 1165200
    },
    {
      "epoch": 2.690304468680451,
      "grad_norm": 6.092987060546875,
      "learning_rate": 5.180807692307692e-06,
      "loss": 4.5708,
      "step": 1165300
    },
    {
      "epoch": 2.6905353366516755,
      "grad_norm": 6.199638366699219,
      "learning_rate": 5.176961538461539e-06,
      "loss": 4.5842,
      "step": 1165400
    },
    {
      "epoch": 2.6907662046229004,
      "grad_norm": 6.983051776885986,
      "learning_rate": 5.173115384615384e-06,
      "loss": 4.5613,
      "step": 1165500
    },
    {
      "epoch": 2.690997072594125,
      "grad_norm": 6.034374713897705,
      "learning_rate": 5.169269230769231e-06,
      "loss": 4.5975,
      "step": 1165600
    },
    {
      "epoch": 2.6912279405653496,
      "grad_norm": 6.200113296508789,
      "learning_rate": 5.165423076923077e-06,
      "loss": 4.5602,
      "step": 1165700
    },
    {
      "epoch": 2.691458808536574,
      "grad_norm": 7.835197448730469,
      "learning_rate": 5.161576923076923e-06,
      "loss": 4.5778,
      "step": 1165800
    },
    {
      "epoch": 2.6916896765077984,
      "grad_norm": 7.111330032348633,
      "learning_rate": 5.15773076923077e-06,
      "loss": 4.6192,
      "step": 1165900
    },
    {
      "epoch": 2.6919205444790233,
      "grad_norm": 6.299790859222412,
      "learning_rate": 5.153884615384616e-06,
      "loss": 4.5594,
      "step": 1166000
    },
    {
      "epoch": 2.692151412450248,
      "grad_norm": 7.1325249671936035,
      "learning_rate": 5.150038461538462e-06,
      "loss": 4.5911,
      "step": 1166100
    },
    {
      "epoch": 2.6923822804214725,
      "grad_norm": 5.352997779846191,
      "learning_rate": 5.146192307692308e-06,
      "loss": 4.5942,
      "step": 1166200
    },
    {
      "epoch": 2.692613148392697,
      "grad_norm": 5.692698955535889,
      "learning_rate": 5.142346153846154e-06,
      "loss": 4.5662,
      "step": 1166300
    },
    {
      "epoch": 2.692844016363922,
      "grad_norm": 6.524324893951416,
      "learning_rate": 5.1385e-06,
      "loss": 4.5875,
      "step": 1166400
    },
    {
      "epoch": 2.6930748843351466,
      "grad_norm": 6.312582015991211,
      "learning_rate": 5.134653846153846e-06,
      "loss": 4.5481,
      "step": 1166500
    },
    {
      "epoch": 2.693305752306371,
      "grad_norm": 5.829681396484375,
      "learning_rate": 5.130807692307693e-06,
      "loss": 4.5424,
      "step": 1166600
    },
    {
      "epoch": 2.6935366202775954,
      "grad_norm": 6.166510581970215,
      "learning_rate": 5.126961538461539e-06,
      "loss": 4.5755,
      "step": 1166700
    },
    {
      "epoch": 2.6937674882488203,
      "grad_norm": 5.621584415435791,
      "learning_rate": 5.123115384615385e-06,
      "loss": 4.598,
      "step": 1166800
    },
    {
      "epoch": 2.693998356220045,
      "grad_norm": 7.0489182472229,
      "learning_rate": 5.119269230769231e-06,
      "loss": 4.5881,
      "step": 1166900
    },
    {
      "epoch": 2.6942292241912695,
      "grad_norm": 7.456872940063477,
      "learning_rate": 5.1154230769230775e-06,
      "loss": 4.547,
      "step": 1167000
    },
    {
      "epoch": 2.694460092162494,
      "grad_norm": 6.75029993057251,
      "learning_rate": 5.111576923076923e-06,
      "loss": 4.5836,
      "step": 1167100
    },
    {
      "epoch": 2.694690960133719,
      "grad_norm": 5.461265563964844,
      "learning_rate": 5.107730769230769e-06,
      "loss": 4.5996,
      "step": 1167200
    },
    {
      "epoch": 2.694921828104943,
      "grad_norm": 5.930307388305664,
      "learning_rate": 5.103884615384616e-06,
      "loss": 4.5822,
      "step": 1167300
    },
    {
      "epoch": 2.695152696076168,
      "grad_norm": 5.523780822753906,
      "learning_rate": 5.100038461538461e-06,
      "loss": 4.6128,
      "step": 1167400
    },
    {
      "epoch": 2.6953835640473924,
      "grad_norm": 6.249533176422119,
      "learning_rate": 5.096192307692308e-06,
      "loss": 4.601,
      "step": 1167500
    },
    {
      "epoch": 2.6956144320186173,
      "grad_norm": 7.043259143829346,
      "learning_rate": 5.092346153846154e-06,
      "loss": 4.5594,
      "step": 1167600
    },
    {
      "epoch": 2.6958452999898417,
      "grad_norm": 6.297285556793213,
      "learning_rate": 5.0885000000000005e-06,
      "loss": 4.5795,
      "step": 1167700
    },
    {
      "epoch": 2.6960761679610665,
      "grad_norm": 7.372097969055176,
      "learning_rate": 5.084653846153847e-06,
      "loss": 4.564,
      "step": 1167800
    },
    {
      "epoch": 2.696307035932291,
      "grad_norm": 5.964949607849121,
      "learning_rate": 5.080807692307692e-06,
      "loss": 4.5998,
      "step": 1167900
    },
    {
      "epoch": 2.6965379039035158,
      "grad_norm": 5.390130996704102,
      "learning_rate": 5.076961538461539e-06,
      "loss": 4.5409,
      "step": 1168000
    },
    {
      "epoch": 2.69676877187474,
      "grad_norm": 5.40074348449707,
      "learning_rate": 5.073115384615385e-06,
      "loss": 4.6218,
      "step": 1168100
    },
    {
      "epoch": 2.696999639845965,
      "grad_norm": 6.5201544761657715,
      "learning_rate": 5.069269230769231e-06,
      "loss": 4.5926,
      "step": 1168200
    },
    {
      "epoch": 2.6972305078171894,
      "grad_norm": 5.968080520629883,
      "learning_rate": 5.065423076923077e-06,
      "loss": 4.6377,
      "step": 1168300
    },
    {
      "epoch": 2.6974613757884143,
      "grad_norm": 6.847702980041504,
      "learning_rate": 5.061576923076923e-06,
      "loss": 4.5696,
      "step": 1168400
    },
    {
      "epoch": 2.6976922437596387,
      "grad_norm": 5.30531644821167,
      "learning_rate": 5.05773076923077e-06,
      "loss": 4.5873,
      "step": 1168500
    },
    {
      "epoch": 2.697923111730863,
      "grad_norm": 6.813413619995117,
      "learning_rate": 5.053884615384616e-06,
      "loss": 4.5651,
      "step": 1168600
    },
    {
      "epoch": 2.698153979702088,
      "grad_norm": 5.364729881286621,
      "learning_rate": 5.050038461538462e-06,
      "loss": 4.5517,
      "step": 1168700
    },
    {
      "epoch": 2.6983848476733128,
      "grad_norm": 5.745990753173828,
      "learning_rate": 5.046192307692308e-06,
      "loss": 4.5678,
      "step": 1168800
    },
    {
      "epoch": 2.698615715644537,
      "grad_norm": 6.606147289276123,
      "learning_rate": 5.042346153846154e-06,
      "loss": 4.5955,
      "step": 1168900
    },
    {
      "epoch": 2.6988465836157616,
      "grad_norm": 5.724309921264648,
      "learning_rate": 5.0385e-06,
      "loss": 4.5813,
      "step": 1169000
    },
    {
      "epoch": 2.6990774515869864,
      "grad_norm": 5.96321439743042,
      "learning_rate": 5.0346538461538465e-06,
      "loss": 4.5864,
      "step": 1169100
    },
    {
      "epoch": 2.6993083195582113,
      "grad_norm": 6.337908744812012,
      "learning_rate": 5.030807692307692e-06,
      "loss": 4.5529,
      "step": 1169200
    },
    {
      "epoch": 2.6995391875294357,
      "grad_norm": 6.135162830352783,
      "learning_rate": 5.0269615384615384e-06,
      "loss": 4.5269,
      "step": 1169300
    },
    {
      "epoch": 2.69977005550066,
      "grad_norm": 5.263788223266602,
      "learning_rate": 5.023115384615385e-06,
      "loss": 4.5514,
      "step": 1169400
    },
    {
      "epoch": 2.700000923471885,
      "grad_norm": 6.4665751457214355,
      "learning_rate": 5.019269230769231e-06,
      "loss": 4.6111,
      "step": 1169500
    },
    {
      "epoch": 2.7002317914431098,
      "grad_norm": 6.231091022491455,
      "learning_rate": 5.015423076923078e-06,
      "loss": 4.5865,
      "step": 1169600
    },
    {
      "epoch": 2.700462659414334,
      "grad_norm": 6.038573741912842,
      "learning_rate": 5.011576923076923e-06,
      "loss": 4.5814,
      "step": 1169700
    },
    {
      "epoch": 2.7006935273855586,
      "grad_norm": 6.597385406494141,
      "learning_rate": 5.0077307692307695e-06,
      "loss": 4.5867,
      "step": 1169800
    },
    {
      "epoch": 2.7009243953567834,
      "grad_norm": 6.719357013702393,
      "learning_rate": 5.003884615384616e-06,
      "loss": 4.6175,
      "step": 1169900
    },
    {
      "epoch": 2.701155263328008,
      "grad_norm": 5.323585510253906,
      "learning_rate": 5.0000384615384614e-06,
      "loss": 4.5967,
      "step": 1170000
    },
    {
      "epoch": 2.7013861312992327,
      "grad_norm": 6.484561920166016,
      "learning_rate": 4.996192307692308e-06,
      "loss": 4.571,
      "step": 1170100
    },
    {
      "epoch": 2.701616999270457,
      "grad_norm": 5.21085262298584,
      "learning_rate": 4.992346153846154e-06,
      "loss": 4.5467,
      "step": 1170200
    },
    {
      "epoch": 2.701847867241682,
      "grad_norm": 7.25051736831665,
      "learning_rate": 4.9885e-06,
      "loss": 4.6051,
      "step": 1170300
    },
    {
      "epoch": 2.7020787352129063,
      "grad_norm": 5.597653388977051,
      "learning_rate": 4.984653846153846e-06,
      "loss": 4.5499,
      "step": 1170400
    },
    {
      "epoch": 2.702309603184131,
      "grad_norm": 5.884560585021973,
      "learning_rate": 4.9808076923076925e-06,
      "loss": 4.5178,
      "step": 1170500
    },
    {
      "epoch": 2.7025404711553556,
      "grad_norm": 6.045828342437744,
      "learning_rate": 4.976961538461539e-06,
      "loss": 4.6111,
      "step": 1170600
    },
    {
      "epoch": 2.7027713391265804,
      "grad_norm": 5.766244411468506,
      "learning_rate": 4.973115384615385e-06,
      "loss": 4.5545,
      "step": 1170700
    },
    {
      "epoch": 2.703002207097805,
      "grad_norm": 5.3472089767456055,
      "learning_rate": 4.969269230769231e-06,
      "loss": 4.5734,
      "step": 1170800
    },
    {
      "epoch": 2.7032330750690297,
      "grad_norm": 8.412311553955078,
      "learning_rate": 4.965423076923077e-06,
      "loss": 4.5937,
      "step": 1170900
    },
    {
      "epoch": 2.703463943040254,
      "grad_norm": 6.109172344207764,
      "learning_rate": 4.961576923076924e-06,
      "loss": 4.5175,
      "step": 1171000
    },
    {
      "epoch": 2.703694811011479,
      "grad_norm": 6.699642181396484,
      "learning_rate": 4.957730769230769e-06,
      "loss": 4.6088,
      "step": 1171100
    },
    {
      "epoch": 2.7039256789827033,
      "grad_norm": 5.759321212768555,
      "learning_rate": 4.9538846153846155e-06,
      "loss": 4.5769,
      "step": 1171200
    },
    {
      "epoch": 2.704156546953928,
      "grad_norm": 6.187424659729004,
      "learning_rate": 4.950038461538462e-06,
      "loss": 4.6008,
      "step": 1171300
    },
    {
      "epoch": 2.7043874149251526,
      "grad_norm": 6.999932289123535,
      "learning_rate": 4.946192307692308e-06,
      "loss": 4.5378,
      "step": 1171400
    },
    {
      "epoch": 2.7046182828963774,
      "grad_norm": 8.977445602416992,
      "learning_rate": 4.942346153846155e-06,
      "loss": 4.5877,
      "step": 1171500
    },
    {
      "epoch": 2.704849150867602,
      "grad_norm": 5.500254154205322,
      "learning_rate": 4.9385e-06,
      "loss": 4.5377,
      "step": 1171600
    },
    {
      "epoch": 2.705080018838826,
      "grad_norm": 5.856317520141602,
      "learning_rate": 4.934653846153847e-06,
      "loss": 4.6176,
      "step": 1171700
    },
    {
      "epoch": 2.705310886810051,
      "grad_norm": 7.807414531707764,
      "learning_rate": 4.930807692307692e-06,
      "loss": 4.6301,
      "step": 1171800
    },
    {
      "epoch": 2.705541754781276,
      "grad_norm": 9.282750129699707,
      "learning_rate": 4.9269615384615385e-06,
      "loss": 4.5673,
      "step": 1171900
    },
    {
      "epoch": 2.7057726227525003,
      "grad_norm": 6.885701656341553,
      "learning_rate": 4.923115384615385e-06,
      "loss": 4.5428,
      "step": 1172000
    },
    {
      "epoch": 2.7060034907237247,
      "grad_norm": 6.802892684936523,
      "learning_rate": 4.9192692307692305e-06,
      "loss": 4.5901,
      "step": 1172100
    },
    {
      "epoch": 2.7062343586949495,
      "grad_norm": 5.721009731292725,
      "learning_rate": 4.915423076923077e-06,
      "loss": 4.5942,
      "step": 1172200
    },
    {
      "epoch": 2.7064652266661744,
      "grad_norm": 7.116865158081055,
      "learning_rate": 4.911576923076923e-06,
      "loss": 4.6217,
      "step": 1172300
    },
    {
      "epoch": 2.706696094637399,
      "grad_norm": 5.993443965911865,
      "learning_rate": 4.90773076923077e-06,
      "loss": 4.573,
      "step": 1172400
    },
    {
      "epoch": 2.706926962608623,
      "grad_norm": 6.51585578918457,
      "learning_rate": 4.903884615384616e-06,
      "loss": 4.6059,
      "step": 1172500
    },
    {
      "epoch": 2.707157830579848,
      "grad_norm": 6.123737335205078,
      "learning_rate": 4.9000384615384616e-06,
      "loss": 4.5901,
      "step": 1172600
    },
    {
      "epoch": 2.7073886985510724,
      "grad_norm": 7.5874552726745605,
      "learning_rate": 4.896192307692308e-06,
      "loss": 4.4858,
      "step": 1172700
    },
    {
      "epoch": 2.7076195665222973,
      "grad_norm": 8.26585578918457,
      "learning_rate": 4.892346153846154e-06,
      "loss": 4.5796,
      "step": 1172800
    },
    {
      "epoch": 2.7078504344935217,
      "grad_norm": 6.463087558746338,
      "learning_rate": 4.8885e-06,
      "loss": 4.5912,
      "step": 1172900
    },
    {
      "epoch": 2.7080813024647465,
      "grad_norm": 5.797952175140381,
      "learning_rate": 4.884653846153846e-06,
      "loss": 4.6089,
      "step": 1173000
    },
    {
      "epoch": 2.708312170435971,
      "grad_norm": 6.201519966125488,
      "learning_rate": 4.880807692307693e-06,
      "loss": 4.5799,
      "step": 1173100
    },
    {
      "epoch": 2.708543038407196,
      "grad_norm": 5.2713422775268555,
      "learning_rate": 4.876961538461538e-06,
      "loss": 4.5788,
      "step": 1173200
    },
    {
      "epoch": 2.70877390637842,
      "grad_norm": 6.016960144042969,
      "learning_rate": 4.873115384615385e-06,
      "loss": 4.5954,
      "step": 1173300
    },
    {
      "epoch": 2.709004774349645,
      "grad_norm": 5.297661781311035,
      "learning_rate": 4.869269230769231e-06,
      "loss": 4.5939,
      "step": 1173400
    },
    {
      "epoch": 2.7092356423208694,
      "grad_norm": 5.235980987548828,
      "learning_rate": 4.865423076923077e-06,
      "loss": 4.5771,
      "step": 1173500
    },
    {
      "epoch": 2.7094665102920943,
      "grad_norm": 5.7138519287109375,
      "learning_rate": 4.861576923076924e-06,
      "loss": 4.6005,
      "step": 1173600
    },
    {
      "epoch": 2.7096973782633187,
      "grad_norm": 5.229606628417969,
      "learning_rate": 4.857730769230769e-06,
      "loss": 4.5447,
      "step": 1173700
    },
    {
      "epoch": 2.7099282462345435,
      "grad_norm": 6.765290260314941,
      "learning_rate": 4.853884615384616e-06,
      "loss": 4.5827,
      "step": 1173800
    },
    {
      "epoch": 2.710159114205768,
      "grad_norm": 7.786624908447266,
      "learning_rate": 4.850038461538462e-06,
      "loss": 4.5681,
      "step": 1173900
    },
    {
      "epoch": 2.710389982176993,
      "grad_norm": 6.063436985015869,
      "learning_rate": 4.8461923076923076e-06,
      "loss": 4.5234,
      "step": 1174000
    },
    {
      "epoch": 2.710620850148217,
      "grad_norm": 6.206210613250732,
      "learning_rate": 4.842346153846154e-06,
      "loss": 4.5584,
      "step": 1174100
    },
    {
      "epoch": 2.710851718119442,
      "grad_norm": 5.961799621582031,
      "learning_rate": 4.8385e-06,
      "loss": 4.5789,
      "step": 1174200
    },
    {
      "epoch": 2.7110825860906664,
      "grad_norm": 7.020892143249512,
      "learning_rate": 4.834653846153847e-06,
      "loss": 4.5549,
      "step": 1174300
    },
    {
      "epoch": 2.711313454061891,
      "grad_norm": 5.406471252441406,
      "learning_rate": 4.830807692307693e-06,
      "loss": 4.6014,
      "step": 1174400
    },
    {
      "epoch": 2.7115443220331157,
      "grad_norm": 6.028871059417725,
      "learning_rate": 4.826961538461539e-06,
      "loss": 4.5623,
      "step": 1174500
    },
    {
      "epoch": 2.7117751900043405,
      "grad_norm": 5.572830677032471,
      "learning_rate": 4.823115384615385e-06,
      "loss": 4.5482,
      "step": 1174600
    },
    {
      "epoch": 2.712006057975565,
      "grad_norm": 5.630256652832031,
      "learning_rate": 4.819269230769231e-06,
      "loss": 4.5681,
      "step": 1174700
    },
    {
      "epoch": 2.7122369259467893,
      "grad_norm": 5.9010162353515625,
      "learning_rate": 4.815423076923077e-06,
      "loss": 4.5847,
      "step": 1174800
    },
    {
      "epoch": 2.712467793918014,
      "grad_norm": 6.1685471534729,
      "learning_rate": 4.811576923076923e-06,
      "loss": 4.6131,
      "step": 1174900
    },
    {
      "epoch": 2.712698661889239,
      "grad_norm": 6.774785041809082,
      "learning_rate": 4.807730769230769e-06,
      "loss": 4.567,
      "step": 1175000
    },
    {
      "epoch": 2.7129295298604634,
      "grad_norm": 6.2377543449401855,
      "learning_rate": 4.803884615384615e-06,
      "loss": 4.5452,
      "step": 1175100
    },
    {
      "epoch": 2.713160397831688,
      "grad_norm": 6.161922454833984,
      "learning_rate": 4.800038461538462e-06,
      "loss": 4.5662,
      "step": 1175200
    },
    {
      "epoch": 2.7133912658029127,
      "grad_norm": 5.935946464538574,
      "learning_rate": 4.796192307692308e-06,
      "loss": 4.5706,
      "step": 1175300
    },
    {
      "epoch": 2.7136221337741375,
      "grad_norm": 7.567905902862549,
      "learning_rate": 4.7923461538461544e-06,
      "loss": 4.6092,
      "step": 1175400
    },
    {
      "epoch": 2.713853001745362,
      "grad_norm": 5.7447710037231445,
      "learning_rate": 4.7885e-06,
      "loss": 4.5573,
      "step": 1175500
    },
    {
      "epoch": 2.7140838697165863,
      "grad_norm": 8.381192207336426,
      "learning_rate": 4.784653846153846e-06,
      "loss": 4.5674,
      "step": 1175600
    },
    {
      "epoch": 2.714314737687811,
      "grad_norm": 8.028331756591797,
      "learning_rate": 4.780807692307693e-06,
      "loss": 4.5724,
      "step": 1175700
    },
    {
      "epoch": 2.7145456056590356,
      "grad_norm": 8.500767707824707,
      "learning_rate": 4.776961538461538e-06,
      "loss": 4.589,
      "step": 1175800
    },
    {
      "epoch": 2.7147764736302604,
      "grad_norm": 6.8713483810424805,
      "learning_rate": 4.773115384615385e-06,
      "loss": 4.5443,
      "step": 1175900
    },
    {
      "epoch": 2.715007341601485,
      "grad_norm": 5.622989654541016,
      "learning_rate": 4.769269230769231e-06,
      "loss": 4.5701,
      "step": 1176000
    },
    {
      "epoch": 2.7152382095727097,
      "grad_norm": 7.279954433441162,
      "learning_rate": 4.7654230769230774e-06,
      "loss": 4.5275,
      "step": 1176100
    },
    {
      "epoch": 2.715469077543934,
      "grad_norm": 5.631007671356201,
      "learning_rate": 4.761576923076924e-06,
      "loss": 4.6086,
      "step": 1176200
    },
    {
      "epoch": 2.715699945515159,
      "grad_norm": 5.9727349281311035,
      "learning_rate": 4.757730769230769e-06,
      "loss": 4.5559,
      "step": 1176300
    },
    {
      "epoch": 2.7159308134863833,
      "grad_norm": 5.666209697723389,
      "learning_rate": 4.753884615384616e-06,
      "loss": 4.5795,
      "step": 1176400
    },
    {
      "epoch": 2.716161681457608,
      "grad_norm": 7.251192092895508,
      "learning_rate": 4.750038461538462e-06,
      "loss": 4.5739,
      "step": 1176500
    },
    {
      "epoch": 2.7163925494288326,
      "grad_norm": 6.339956760406494,
      "learning_rate": 4.746192307692308e-06,
      "loss": 4.5718,
      "step": 1176600
    },
    {
      "epoch": 2.7166234174000574,
      "grad_norm": 5.383035182952881,
      "learning_rate": 4.742346153846154e-06,
      "loss": 4.5722,
      "step": 1176700
    },
    {
      "epoch": 2.716854285371282,
      "grad_norm": 8.060442924499512,
      "learning_rate": 4.7385e-06,
      "loss": 4.5801,
      "step": 1176800
    },
    {
      "epoch": 2.7170851533425067,
      "grad_norm": 5.647273063659668,
      "learning_rate": 4.734653846153846e-06,
      "loss": 4.5325,
      "step": 1176900
    },
    {
      "epoch": 2.717316021313731,
      "grad_norm": 7.0058722496032715,
      "learning_rate": 4.730807692307692e-06,
      "loss": 4.5997,
      "step": 1177000
    },
    {
      "epoch": 2.7175468892849555,
      "grad_norm": 5.854611873626709,
      "learning_rate": 4.726961538461539e-06,
      "loss": 4.523,
      "step": 1177100
    },
    {
      "epoch": 2.7177777572561803,
      "grad_norm": 8.123509407043457,
      "learning_rate": 4.723115384615385e-06,
      "loss": 4.5671,
      "step": 1177200
    },
    {
      "epoch": 2.718008625227405,
      "grad_norm": 6.285974979400635,
      "learning_rate": 4.7192692307692315e-06,
      "loss": 4.5623,
      "step": 1177300
    },
    {
      "epoch": 2.7182394931986296,
      "grad_norm": 6.366151809692383,
      "learning_rate": 4.715423076923077e-06,
      "loss": 4.5583,
      "step": 1177400
    },
    {
      "epoch": 2.718470361169854,
      "grad_norm": 6.825849533081055,
      "learning_rate": 4.7115769230769235e-06,
      "loss": 4.5899,
      "step": 1177500
    },
    {
      "epoch": 2.718701229141079,
      "grad_norm": 7.033501148223877,
      "learning_rate": 4.707730769230769e-06,
      "loss": 4.5612,
      "step": 1177600
    },
    {
      "epoch": 2.7189320971123037,
      "grad_norm": 5.93801736831665,
      "learning_rate": 4.703884615384615e-06,
      "loss": 4.5807,
      "step": 1177700
    },
    {
      "epoch": 2.719162965083528,
      "grad_norm": 5.701897621154785,
      "learning_rate": 4.700038461538462e-06,
      "loss": 4.5769,
      "step": 1177800
    },
    {
      "epoch": 2.7193938330547525,
      "grad_norm": 5.934304714202881,
      "learning_rate": 4.696192307692307e-06,
      "loss": 4.6162,
      "step": 1177900
    },
    {
      "epoch": 2.7196247010259773,
      "grad_norm": 6.05886173248291,
      "learning_rate": 4.692346153846154e-06,
      "loss": 4.5666,
      "step": 1178000
    },
    {
      "epoch": 2.719855568997202,
      "grad_norm": 6.531981468200684,
      "learning_rate": 4.688500000000001e-06,
      "loss": 4.611,
      "step": 1178100
    },
    {
      "epoch": 2.7200864369684266,
      "grad_norm": 5.332348346710205,
      "learning_rate": 4.6846538461538465e-06,
      "loss": 4.5798,
      "step": 1178200
    },
    {
      "epoch": 2.720317304939651,
      "grad_norm": 5.738990306854248,
      "learning_rate": 4.680807692307693e-06,
      "loss": 4.5183,
      "step": 1178300
    },
    {
      "epoch": 2.720548172910876,
      "grad_norm": 8.279479026794434,
      "learning_rate": 4.676961538461538e-06,
      "loss": 4.553,
      "step": 1178400
    },
    {
      "epoch": 2.7207790408821,
      "grad_norm": 5.769455432891846,
      "learning_rate": 4.673115384615385e-06,
      "loss": 4.5715,
      "step": 1178500
    },
    {
      "epoch": 2.721009908853325,
      "grad_norm": 5.518131732940674,
      "learning_rate": 4.669269230769231e-06,
      "loss": 4.5287,
      "step": 1178600
    },
    {
      "epoch": 2.7212407768245495,
      "grad_norm": 5.916011810302734,
      "learning_rate": 4.665423076923077e-06,
      "loss": 4.5634,
      "step": 1178700
    },
    {
      "epoch": 2.7214716447957743,
      "grad_norm": 6.556906700134277,
      "learning_rate": 4.661576923076923e-06,
      "loss": 4.5987,
      "step": 1178800
    },
    {
      "epoch": 2.7217025127669987,
      "grad_norm": 5.922726631164551,
      "learning_rate": 4.6577307692307695e-06,
      "loss": 4.5845,
      "step": 1178900
    },
    {
      "epoch": 2.7219333807382236,
      "grad_norm": 6.524659156799316,
      "learning_rate": 4.653884615384616e-06,
      "loss": 4.5811,
      "step": 1179000
    },
    {
      "epoch": 2.722164248709448,
      "grad_norm": 8.348543167114258,
      "learning_rate": 4.650038461538462e-06,
      "loss": 4.5765,
      "step": 1179100
    },
    {
      "epoch": 2.722395116680673,
      "grad_norm": 5.7805328369140625,
      "learning_rate": 4.646192307692308e-06,
      "loss": 4.565,
      "step": 1179200
    },
    {
      "epoch": 2.722625984651897,
      "grad_norm": 5.922635078430176,
      "learning_rate": 4.642346153846154e-06,
      "loss": 4.5629,
      "step": 1179300
    },
    {
      "epoch": 2.722856852623122,
      "grad_norm": 8.337464332580566,
      "learning_rate": 4.6385000000000006e-06,
      "loss": 4.5598,
      "step": 1179400
    },
    {
      "epoch": 2.7230877205943465,
      "grad_norm": 6.106130599975586,
      "learning_rate": 4.634653846153846e-06,
      "loss": 4.5874,
      "step": 1179500
    },
    {
      "epoch": 2.7233185885655713,
      "grad_norm": 5.903988838195801,
      "learning_rate": 4.6308076923076925e-06,
      "loss": 4.5378,
      "step": 1179600
    },
    {
      "epoch": 2.7235494565367957,
      "grad_norm": 5.5078125,
      "learning_rate": 4.626961538461538e-06,
      "loss": 4.5551,
      "step": 1179700
    },
    {
      "epoch": 2.72378032450802,
      "grad_norm": 6.523238182067871,
      "learning_rate": 4.623115384615384e-06,
      "loss": 4.5773,
      "step": 1179800
    },
    {
      "epoch": 2.724011192479245,
      "grad_norm": 7.085691928863525,
      "learning_rate": 4.619269230769231e-06,
      "loss": 4.5477,
      "step": 1179900
    },
    {
      "epoch": 2.72424206045047,
      "grad_norm": 7.364629745483398,
      "learning_rate": 4.615423076923077e-06,
      "loss": 4.5744,
      "step": 1180000
    },
    {
      "epoch": 2.724472928421694,
      "grad_norm": 6.076695919036865,
      "learning_rate": 4.6115769230769236e-06,
      "loss": 4.609,
      "step": 1180100
    },
    {
      "epoch": 2.7247037963929186,
      "grad_norm": 6.0976386070251465,
      "learning_rate": 4.60773076923077e-06,
      "loss": 4.6322,
      "step": 1180200
    },
    {
      "epoch": 2.7249346643641434,
      "grad_norm": 5.600433349609375,
      "learning_rate": 4.6038846153846155e-06,
      "loss": 4.5597,
      "step": 1180300
    },
    {
      "epoch": 2.7251655323353683,
      "grad_norm": 6.617905139923096,
      "learning_rate": 4.600038461538462e-06,
      "loss": 4.5245,
      "step": 1180400
    },
    {
      "epoch": 2.7253964003065927,
      "grad_norm": 5.765529632568359,
      "learning_rate": 4.5961923076923074e-06,
      "loss": 4.5894,
      "step": 1180500
    },
    {
      "epoch": 2.725627268277817,
      "grad_norm": 5.656552314758301,
      "learning_rate": 4.592346153846154e-06,
      "loss": 4.5624,
      "step": 1180600
    },
    {
      "epoch": 2.725858136249042,
      "grad_norm": 5.293391704559326,
      "learning_rate": 4.5885e-06,
      "loss": 4.5667,
      "step": 1180700
    },
    {
      "epoch": 2.726089004220267,
      "grad_norm": 6.0406975746154785,
      "learning_rate": 4.584653846153846e-06,
      "loss": 4.5767,
      "step": 1180800
    },
    {
      "epoch": 2.726319872191491,
      "grad_norm": 6.495266437530518,
      "learning_rate": 4.580807692307693e-06,
      "loss": 4.6008,
      "step": 1180900
    },
    {
      "epoch": 2.7265507401627156,
      "grad_norm": 6.216119289398193,
      "learning_rate": 4.576961538461539e-06,
      "loss": 4.5515,
      "step": 1181000
    },
    {
      "epoch": 2.7267816081339404,
      "grad_norm": 6.315058708190918,
      "learning_rate": 4.573115384615385e-06,
      "loss": 4.5954,
      "step": 1181100
    },
    {
      "epoch": 2.727012476105165,
      "grad_norm": 6.260495662689209,
      "learning_rate": 4.569269230769231e-06,
      "loss": 4.596,
      "step": 1181200
    },
    {
      "epoch": 2.7272433440763897,
      "grad_norm": 5.271072864532471,
      "learning_rate": 4.565423076923077e-06,
      "loss": 4.556,
      "step": 1181300
    },
    {
      "epoch": 2.727474212047614,
      "grad_norm": 5.764013290405273,
      "learning_rate": 4.561576923076923e-06,
      "loss": 4.5401,
      "step": 1181400
    },
    {
      "epoch": 2.727705080018839,
      "grad_norm": 5.407341957092285,
      "learning_rate": 4.55773076923077e-06,
      "loss": 4.5813,
      "step": 1181500
    },
    {
      "epoch": 2.7279359479900633,
      "grad_norm": 5.395582675933838,
      "learning_rate": 4.553884615384615e-06,
      "loss": 4.6132,
      "step": 1181600
    },
    {
      "epoch": 2.728166815961288,
      "grad_norm": 7.5094099044799805,
      "learning_rate": 4.5500384615384615e-06,
      "loss": 4.5708,
      "step": 1181700
    },
    {
      "epoch": 2.7283976839325126,
      "grad_norm": 5.804068565368652,
      "learning_rate": 4.546192307692308e-06,
      "loss": 4.5836,
      "step": 1181800
    },
    {
      "epoch": 2.7286285519037374,
      "grad_norm": 5.847285747528076,
      "learning_rate": 4.542346153846154e-06,
      "loss": 4.5704,
      "step": 1181900
    },
    {
      "epoch": 2.728859419874962,
      "grad_norm": 5.63040828704834,
      "learning_rate": 4.538500000000001e-06,
      "loss": 4.5441,
      "step": 1182000
    },
    {
      "epoch": 2.7290902878461867,
      "grad_norm": 5.541187286376953,
      "learning_rate": 4.534653846153846e-06,
      "loss": 4.5414,
      "step": 1182100
    },
    {
      "epoch": 2.729321155817411,
      "grad_norm": 5.473632335662842,
      "learning_rate": 4.530807692307693e-06,
      "loss": 4.5835,
      "step": 1182200
    },
    {
      "epoch": 2.729552023788636,
      "grad_norm": 6.353168964385986,
      "learning_rate": 4.526961538461539e-06,
      "loss": 4.5864,
      "step": 1182300
    },
    {
      "epoch": 2.7297828917598603,
      "grad_norm": 5.2468085289001465,
      "learning_rate": 4.5231153846153845e-06,
      "loss": 4.6025,
      "step": 1182400
    },
    {
      "epoch": 2.7300137597310847,
      "grad_norm": 6.266976356506348,
      "learning_rate": 4.519269230769231e-06,
      "loss": 4.592,
      "step": 1182500
    },
    {
      "epoch": 2.7302446277023096,
      "grad_norm": 6.908746719360352,
      "learning_rate": 4.5154230769230765e-06,
      "loss": 4.5793,
      "step": 1182600
    },
    {
      "epoch": 2.7304754956735344,
      "grad_norm": 6.775980472564697,
      "learning_rate": 4.511576923076923e-06,
      "loss": 4.5846,
      "step": 1182700
    },
    {
      "epoch": 2.730706363644759,
      "grad_norm": 6.020334243774414,
      "learning_rate": 4.507730769230769e-06,
      "loss": 4.5635,
      "step": 1182800
    },
    {
      "epoch": 2.7309372316159832,
      "grad_norm": 6.964818000793457,
      "learning_rate": 4.503884615384616e-06,
      "loss": 4.5907,
      "step": 1182900
    },
    {
      "epoch": 2.731168099587208,
      "grad_norm": 5.486399173736572,
      "learning_rate": 4.500038461538462e-06,
      "loss": 4.5856,
      "step": 1183000
    },
    {
      "epoch": 2.731398967558433,
      "grad_norm": 7.8931403160095215,
      "learning_rate": 4.496192307692308e-06,
      "loss": 4.5605,
      "step": 1183100
    },
    {
      "epoch": 2.7316298355296573,
      "grad_norm": 6.297665596008301,
      "learning_rate": 4.492346153846154e-06,
      "loss": 4.5566,
      "step": 1183200
    },
    {
      "epoch": 2.7318607035008817,
      "grad_norm": 6.846577167510986,
      "learning_rate": 4.4885e-06,
      "loss": 4.6039,
      "step": 1183300
    },
    {
      "epoch": 2.7320915714721066,
      "grad_norm": 5.959845542907715,
      "learning_rate": 4.484653846153846e-06,
      "loss": 4.5483,
      "step": 1183400
    },
    {
      "epoch": 2.7323224394433314,
      "grad_norm": 6.414483070373535,
      "learning_rate": 4.480807692307692e-06,
      "loss": 4.5812,
      "step": 1183500
    },
    {
      "epoch": 2.732553307414556,
      "grad_norm": 5.22228479385376,
      "learning_rate": 4.476961538461539e-06,
      "loss": 4.5465,
      "step": 1183600
    },
    {
      "epoch": 2.7327841753857802,
      "grad_norm": 7.0701518058776855,
      "learning_rate": 4.473115384615385e-06,
      "loss": 4.5719,
      "step": 1183700
    },
    {
      "epoch": 2.733015043357005,
      "grad_norm": 6.379547595977783,
      "learning_rate": 4.469269230769231e-06,
      "loss": 4.5201,
      "step": 1183800
    },
    {
      "epoch": 2.7332459113282295,
      "grad_norm": 5.951126575469971,
      "learning_rate": 4.465423076923078e-06,
      "loss": 4.5877,
      "step": 1183900
    },
    {
      "epoch": 2.7334767792994543,
      "grad_norm": 7.083292007446289,
      "learning_rate": 4.461576923076923e-06,
      "loss": 4.5717,
      "step": 1184000
    },
    {
      "epoch": 2.7337076472706787,
      "grad_norm": 5.683341026306152,
      "learning_rate": 4.45773076923077e-06,
      "loss": 4.5873,
      "step": 1184100
    },
    {
      "epoch": 2.7339385152419036,
      "grad_norm": 8.583471298217773,
      "learning_rate": 4.453884615384615e-06,
      "loss": 4.5306,
      "step": 1184200
    },
    {
      "epoch": 2.734169383213128,
      "grad_norm": 6.337554931640625,
      "learning_rate": 4.450038461538462e-06,
      "loss": 4.5693,
      "step": 1184300
    },
    {
      "epoch": 2.734400251184353,
      "grad_norm": 6.569629192352295,
      "learning_rate": 4.446192307692308e-06,
      "loss": 4.5783,
      "step": 1184400
    },
    {
      "epoch": 2.734631119155577,
      "grad_norm": 6.373188018798828,
      "learning_rate": 4.4423461538461536e-06,
      "loss": 4.563,
      "step": 1184500
    },
    {
      "epoch": 2.734861987126802,
      "grad_norm": 6.668705463409424,
      "learning_rate": 4.4385e-06,
      "loss": 4.5468,
      "step": 1184600
    },
    {
      "epoch": 2.7350928550980265,
      "grad_norm": 9.894440650939941,
      "learning_rate": 4.434653846153846e-06,
      "loss": 4.5474,
      "step": 1184700
    },
    {
      "epoch": 2.7353237230692513,
      "grad_norm": 6.7767863273620605,
      "learning_rate": 4.430807692307693e-06,
      "loss": 4.5237,
      "step": 1184800
    },
    {
      "epoch": 2.7355545910404757,
      "grad_norm": 6.373222351074219,
      "learning_rate": 4.426961538461539e-06,
      "loss": 4.5322,
      "step": 1184900
    },
    {
      "epoch": 2.7357854590117006,
      "grad_norm": 7.8054423332214355,
      "learning_rate": 4.423115384615385e-06,
      "loss": 4.5615,
      "step": 1185000
    },
    {
      "epoch": 2.736016326982925,
      "grad_norm": 5.2981390953063965,
      "learning_rate": 4.419269230769231e-06,
      "loss": 4.5743,
      "step": 1185100
    },
    {
      "epoch": 2.7362471949541494,
      "grad_norm": 5.835665702819824,
      "learning_rate": 4.415423076923077e-06,
      "loss": 4.5584,
      "step": 1185200
    },
    {
      "epoch": 2.736478062925374,
      "grad_norm": 7.2215142250061035,
      "learning_rate": 4.411576923076923e-06,
      "loss": 4.5622,
      "step": 1185300
    },
    {
      "epoch": 2.736708930896599,
      "grad_norm": 6.370073318481445,
      "learning_rate": 4.407730769230769e-06,
      "loss": 4.582,
      "step": 1185400
    },
    {
      "epoch": 2.7369397988678235,
      "grad_norm": 8.79006290435791,
      "learning_rate": 4.403884615384615e-06,
      "loss": 4.5558,
      "step": 1185500
    },
    {
      "epoch": 2.737170666839048,
      "grad_norm": 8.238909721374512,
      "learning_rate": 4.400038461538462e-06,
      "loss": 4.5646,
      "step": 1185600
    },
    {
      "epoch": 2.7374015348102727,
      "grad_norm": 6.168186664581299,
      "learning_rate": 4.3961923076923085e-06,
      "loss": 4.5594,
      "step": 1185700
    },
    {
      "epoch": 2.7376324027814976,
      "grad_norm": 5.53317928314209,
      "learning_rate": 4.392346153846154e-06,
      "loss": 4.5648,
      "step": 1185800
    },
    {
      "epoch": 2.737863270752722,
      "grad_norm": 5.749795436859131,
      "learning_rate": 4.3885e-06,
      "loss": 4.5989,
      "step": 1185900
    },
    {
      "epoch": 2.7380941387239464,
      "grad_norm": 7.353539943695068,
      "learning_rate": 4.384653846153847e-06,
      "loss": 4.5554,
      "step": 1186000
    },
    {
      "epoch": 2.738325006695171,
      "grad_norm": 6.431292533874512,
      "learning_rate": 4.380807692307692e-06,
      "loss": 4.5987,
      "step": 1186100
    },
    {
      "epoch": 2.738555874666396,
      "grad_norm": 6.787583351135254,
      "learning_rate": 4.376961538461539e-06,
      "loss": 4.5624,
      "step": 1186200
    },
    {
      "epoch": 2.7387867426376205,
      "grad_norm": 5.606853008270264,
      "learning_rate": 4.373115384615384e-06,
      "loss": 4.5814,
      "step": 1186300
    },
    {
      "epoch": 2.739017610608845,
      "grad_norm": 7.297966480255127,
      "learning_rate": 4.369269230769231e-06,
      "loss": 4.581,
      "step": 1186400
    },
    {
      "epoch": 2.7392484785800697,
      "grad_norm": 5.61321496963501,
      "learning_rate": 4.365423076923077e-06,
      "loss": 4.5649,
      "step": 1186500
    },
    {
      "epoch": 2.739479346551294,
      "grad_norm": 7.503799915313721,
      "learning_rate": 4.3615769230769234e-06,
      "loss": 4.5642,
      "step": 1186600
    },
    {
      "epoch": 2.739710214522519,
      "grad_norm": 6.303709983825684,
      "learning_rate": 4.35773076923077e-06,
      "loss": 4.612,
      "step": 1186700
    },
    {
      "epoch": 2.7399410824937434,
      "grad_norm": 7.872646808624268,
      "learning_rate": 4.353884615384616e-06,
      "loss": 4.527,
      "step": 1186800
    },
    {
      "epoch": 2.740171950464968,
      "grad_norm": 5.281113624572754,
      "learning_rate": 4.350038461538462e-06,
      "loss": 4.5519,
      "step": 1186900
    },
    {
      "epoch": 2.7404028184361926,
      "grad_norm": 8.228837966918945,
      "learning_rate": 4.346192307692308e-06,
      "loss": 4.5097,
      "step": 1187000
    },
    {
      "epoch": 2.7406336864074174,
      "grad_norm": 5.884286403656006,
      "learning_rate": 4.342346153846154e-06,
      "loss": 4.5978,
      "step": 1187100
    },
    {
      "epoch": 2.740864554378642,
      "grad_norm": 6.473492622375488,
      "learning_rate": 4.3385e-06,
      "loss": 4.5709,
      "step": 1187200
    },
    {
      "epoch": 2.7410954223498667,
      "grad_norm": 5.700556755065918,
      "learning_rate": 4.3346538461538464e-06,
      "loss": 4.5256,
      "step": 1187300
    },
    {
      "epoch": 2.741326290321091,
      "grad_norm": 6.118844985961914,
      "learning_rate": 4.330807692307692e-06,
      "loss": 4.4699,
      "step": 1187400
    },
    {
      "epoch": 2.741557158292316,
      "grad_norm": 6.76072359085083,
      "learning_rate": 4.326961538461538e-06,
      "loss": 4.5703,
      "step": 1187500
    },
    {
      "epoch": 2.7417880262635403,
      "grad_norm": 5.943179130554199,
      "learning_rate": 4.323115384615385e-06,
      "loss": 4.555,
      "step": 1187600
    },
    {
      "epoch": 2.742018894234765,
      "grad_norm": 5.482557773590088,
      "learning_rate": 4.319269230769231e-06,
      "loss": 4.5731,
      "step": 1187700
    },
    {
      "epoch": 2.7422497622059896,
      "grad_norm": 7.012495517730713,
      "learning_rate": 4.3154230769230775e-06,
      "loss": 4.5649,
      "step": 1187800
    },
    {
      "epoch": 2.742480630177214,
      "grad_norm": 6.5198259353637695,
      "learning_rate": 4.311576923076923e-06,
      "loss": 4.5615,
      "step": 1187900
    },
    {
      "epoch": 2.742711498148439,
      "grad_norm": 6.519043445587158,
      "learning_rate": 4.3077307692307694e-06,
      "loss": 4.5689,
      "step": 1188000
    },
    {
      "epoch": 2.7429423661196637,
      "grad_norm": 5.70523738861084,
      "learning_rate": 4.303884615384616e-06,
      "loss": 4.5236,
      "step": 1188100
    },
    {
      "epoch": 2.743173234090888,
      "grad_norm": 6.861206531524658,
      "learning_rate": 4.300038461538461e-06,
      "loss": 4.5446,
      "step": 1188200
    },
    {
      "epoch": 2.7434041020621125,
      "grad_norm": 7.683046340942383,
      "learning_rate": 4.296192307692308e-06,
      "loss": 4.6142,
      "step": 1188300
    },
    {
      "epoch": 2.7436349700333373,
      "grad_norm": 5.877613544464111,
      "learning_rate": 4.292346153846154e-06,
      "loss": 4.5657,
      "step": 1188400
    },
    {
      "epoch": 2.743865838004562,
      "grad_norm": 5.7952399253845215,
      "learning_rate": 4.2885000000000005e-06,
      "loss": 4.5681,
      "step": 1188500
    },
    {
      "epoch": 2.7440967059757866,
      "grad_norm": 6.862385272979736,
      "learning_rate": 4.284653846153847e-06,
      "loss": 4.5528,
      "step": 1188600
    },
    {
      "epoch": 2.744327573947011,
      "grad_norm": 7.779675483703613,
      "learning_rate": 4.2808076923076925e-06,
      "loss": 4.5674,
      "step": 1188700
    },
    {
      "epoch": 2.744558441918236,
      "grad_norm": 9.379741668701172,
      "learning_rate": 4.276961538461539e-06,
      "loss": 4.5599,
      "step": 1188800
    },
    {
      "epoch": 2.7447893098894607,
      "grad_norm": 7.392699241638184,
      "learning_rate": 4.273115384615385e-06,
      "loss": 4.5052,
      "step": 1188900
    },
    {
      "epoch": 2.745020177860685,
      "grad_norm": 7.458576679229736,
      "learning_rate": 4.269269230769231e-06,
      "loss": 4.5447,
      "step": 1189000
    },
    {
      "epoch": 2.7452510458319095,
      "grad_norm": 5.674448490142822,
      "learning_rate": 4.265423076923077e-06,
      "loss": 4.5652,
      "step": 1189100
    },
    {
      "epoch": 2.7454819138031343,
      "grad_norm": 5.452151775360107,
      "learning_rate": 4.261576923076923e-06,
      "loss": 4.5829,
      "step": 1189200
    },
    {
      "epoch": 2.7457127817743587,
      "grad_norm": 6.176711082458496,
      "learning_rate": 4.257730769230769e-06,
      "loss": 4.5783,
      "step": 1189300
    },
    {
      "epoch": 2.7459436497455836,
      "grad_norm": 5.695946216583252,
      "learning_rate": 4.2538846153846155e-06,
      "loss": 4.5523,
      "step": 1189400
    },
    {
      "epoch": 2.746174517716808,
      "grad_norm": 5.561420917510986,
      "learning_rate": 4.250038461538462e-06,
      "loss": 4.549,
      "step": 1189500
    },
    {
      "epoch": 2.746405385688033,
      "grad_norm": 6.068274974822998,
      "learning_rate": 4.246192307692308e-06,
      "loss": 4.5569,
      "step": 1189600
    },
    {
      "epoch": 2.7466362536592572,
      "grad_norm": 6.46867561340332,
      "learning_rate": 4.242346153846155e-06,
      "loss": 4.5709,
      "step": 1189700
    },
    {
      "epoch": 2.746867121630482,
      "grad_norm": 6.682496547698975,
      "learning_rate": 4.2385e-06,
      "loss": 4.5044,
      "step": 1189800
    },
    {
      "epoch": 2.7470979896017065,
      "grad_norm": 6.228562355041504,
      "learning_rate": 4.2346538461538465e-06,
      "loss": 4.5995,
      "step": 1189900
    },
    {
      "epoch": 2.7473288575729313,
      "grad_norm": 6.216666221618652,
      "learning_rate": 4.230807692307692e-06,
      "loss": 4.5919,
      "step": 1190000
    },
    {
      "epoch": 2.7475597255441557,
      "grad_norm": 6.29218864440918,
      "learning_rate": 4.2269615384615385e-06,
      "loss": 4.5413,
      "step": 1190100
    },
    {
      "epoch": 2.7477905935153806,
      "grad_norm": 6.809164524078369,
      "learning_rate": 4.223115384615385e-06,
      "loss": 4.5383,
      "step": 1190200
    },
    {
      "epoch": 2.748021461486605,
      "grad_norm": 6.400777816772461,
      "learning_rate": 4.21926923076923e-06,
      "loss": 4.566,
      "step": 1190300
    },
    {
      "epoch": 2.74825232945783,
      "grad_norm": 6.103621482849121,
      "learning_rate": 4.215423076923078e-06,
      "loss": 4.588,
      "step": 1190400
    },
    {
      "epoch": 2.7484831974290542,
      "grad_norm": 5.257715225219727,
      "learning_rate": 4.211576923076923e-06,
      "loss": 4.5881,
      "step": 1190500
    },
    {
      "epoch": 2.7487140654002786,
      "grad_norm": 7.0123138427734375,
      "learning_rate": 4.2077307692307696e-06,
      "loss": 4.5795,
      "step": 1190600
    },
    {
      "epoch": 2.7489449333715035,
      "grad_norm": 6.401630401611328,
      "learning_rate": 4.203884615384616e-06,
      "loss": 4.5773,
      "step": 1190700
    },
    {
      "epoch": 2.7491758013427283,
      "grad_norm": 7.362802505493164,
      "learning_rate": 4.2000384615384615e-06,
      "loss": 4.5678,
      "step": 1190800
    },
    {
      "epoch": 2.7494066693139527,
      "grad_norm": 7.262117385864258,
      "learning_rate": 4.196192307692308e-06,
      "loss": 4.5571,
      "step": 1190900
    },
    {
      "epoch": 2.749637537285177,
      "grad_norm": 6.084285736083984,
      "learning_rate": 4.192346153846154e-06,
      "loss": 4.5415,
      "step": 1191000
    },
    {
      "epoch": 2.749868405256402,
      "grad_norm": 8.280475616455078,
      "learning_rate": 4.1885e-06,
      "loss": 4.5723,
      "step": 1191100
    },
    {
      "epoch": 2.750099273227627,
      "grad_norm": 7.374931812286377,
      "learning_rate": 4.184653846153846e-06,
      "loss": 4.5319,
      "step": 1191200
    },
    {
      "epoch": 2.750330141198851,
      "grad_norm": 6.4936604499816895,
      "learning_rate": 4.1808076923076926e-06,
      "loss": 4.553,
      "step": 1191300
    },
    {
      "epoch": 2.7505610091700756,
      "grad_norm": 5.4815239906311035,
      "learning_rate": 4.176961538461539e-06,
      "loss": 4.5786,
      "step": 1191400
    },
    {
      "epoch": 2.7507918771413005,
      "grad_norm": 5.861995697021484,
      "learning_rate": 4.173115384615385e-06,
      "loss": 4.5687,
      "step": 1191500
    },
    {
      "epoch": 2.7510227451125253,
      "grad_norm": 5.767940044403076,
      "learning_rate": 4.169269230769231e-06,
      "loss": 4.5738,
      "step": 1191600
    },
    {
      "epoch": 2.7512536130837497,
      "grad_norm": 6.419427871704102,
      "learning_rate": 4.165423076923077e-06,
      "loss": 4.5435,
      "step": 1191700
    },
    {
      "epoch": 2.751484481054974,
      "grad_norm": 5.843478202819824,
      "learning_rate": 4.161576923076924e-06,
      "loss": 4.5507,
      "step": 1191800
    },
    {
      "epoch": 2.751715349026199,
      "grad_norm": 7.225835800170898,
      "learning_rate": 4.157730769230769e-06,
      "loss": 4.5695,
      "step": 1191900
    },
    {
      "epoch": 2.7519462169974234,
      "grad_norm": 6.211607933044434,
      "learning_rate": 4.1538846153846156e-06,
      "loss": 4.5273,
      "step": 1192000
    },
    {
      "epoch": 2.752177084968648,
      "grad_norm": 6.8099141120910645,
      "learning_rate": 4.150038461538461e-06,
      "loss": 4.6156,
      "step": 1192100
    },
    {
      "epoch": 2.7524079529398726,
      "grad_norm": 6.218361854553223,
      "learning_rate": 4.1461923076923075e-06,
      "loss": 4.5795,
      "step": 1192200
    },
    {
      "epoch": 2.7526388209110975,
      "grad_norm": 7.082250595092773,
      "learning_rate": 4.142346153846154e-06,
      "loss": 4.5783,
      "step": 1192300
    },
    {
      "epoch": 2.752869688882322,
      "grad_norm": 5.589537143707275,
      "learning_rate": 4.1385e-06,
      "loss": 4.5568,
      "step": 1192400
    },
    {
      "epoch": 2.7531005568535467,
      "grad_norm": 5.684010028839111,
      "learning_rate": 4.134653846153847e-06,
      "loss": 4.5442,
      "step": 1192500
    },
    {
      "epoch": 2.753331424824771,
      "grad_norm": 7.066370010375977,
      "learning_rate": 4.130807692307693e-06,
      "loss": 4.5429,
      "step": 1192600
    },
    {
      "epoch": 2.753562292795996,
      "grad_norm": 7.351663112640381,
      "learning_rate": 4.126961538461539e-06,
      "loss": 4.5652,
      "step": 1192700
    },
    {
      "epoch": 2.7537931607672204,
      "grad_norm": 5.534571647644043,
      "learning_rate": 4.123115384615385e-06,
      "loss": 4.5591,
      "step": 1192800
    },
    {
      "epoch": 2.754024028738445,
      "grad_norm": 5.5436930656433105,
      "learning_rate": 4.1192692307692305e-06,
      "loss": 4.6062,
      "step": 1192900
    },
    {
      "epoch": 2.7542548967096696,
      "grad_norm": 6.563962936401367,
      "learning_rate": 4.115423076923077e-06,
      "loss": 4.5631,
      "step": 1193000
    },
    {
      "epoch": 2.7544857646808945,
      "grad_norm": 6.055979251861572,
      "learning_rate": 4.111576923076923e-06,
      "loss": 4.5386,
      "step": 1193100
    },
    {
      "epoch": 2.754716632652119,
      "grad_norm": 6.84821081161499,
      "learning_rate": 4.10773076923077e-06,
      "loss": 4.5634,
      "step": 1193200
    },
    {
      "epoch": 2.7549475006233433,
      "grad_norm": 5.42341947555542,
      "learning_rate": 4.103884615384616e-06,
      "loss": 4.6141,
      "step": 1193300
    },
    {
      "epoch": 2.755178368594568,
      "grad_norm": 10.888416290283203,
      "learning_rate": 4.100038461538462e-06,
      "loss": 4.516,
      "step": 1193400
    },
    {
      "epoch": 2.755409236565793,
      "grad_norm": 7.254912376403809,
      "learning_rate": 4.096192307692308e-06,
      "loss": 4.5224,
      "step": 1193500
    },
    {
      "epoch": 2.7556401045370174,
      "grad_norm": 6.214484214782715,
      "learning_rate": 4.092346153846154e-06,
      "loss": 4.5197,
      "step": 1193600
    },
    {
      "epoch": 2.7558709725082418,
      "grad_norm": 5.270130634307861,
      "learning_rate": 4.0885e-06,
      "loss": 4.5379,
      "step": 1193700
    },
    {
      "epoch": 2.7561018404794666,
      "grad_norm": 5.752839088439941,
      "learning_rate": 4.084653846153846e-06,
      "loss": 4.5351,
      "step": 1193800
    },
    {
      "epoch": 2.7563327084506914,
      "grad_norm": 7.614979267120361,
      "learning_rate": 4.080807692307693e-06,
      "loss": 4.533,
      "step": 1193900
    },
    {
      "epoch": 2.756563576421916,
      "grad_norm": 6.030319690704346,
      "learning_rate": 4.076961538461538e-06,
      "loss": 4.5397,
      "step": 1194000
    },
    {
      "epoch": 2.7567944443931403,
      "grad_norm": 6.03522253036499,
      "learning_rate": 4.073115384615385e-06,
      "loss": 4.5648,
      "step": 1194100
    },
    {
      "epoch": 2.757025312364365,
      "grad_norm": 5.672430038452148,
      "learning_rate": 4.069269230769231e-06,
      "loss": 4.589,
      "step": 1194200
    },
    {
      "epoch": 2.75725618033559,
      "grad_norm": 9.23687744140625,
      "learning_rate": 4.065423076923077e-06,
      "loss": 4.5542,
      "step": 1194300
    },
    {
      "epoch": 2.7574870483068143,
      "grad_norm": 6.400026798248291,
      "learning_rate": 4.061576923076924e-06,
      "loss": 4.5438,
      "step": 1194400
    },
    {
      "epoch": 2.7577179162780388,
      "grad_norm": 5.327157020568848,
      "learning_rate": 4.057730769230769e-06,
      "loss": 4.549,
      "step": 1194500
    },
    {
      "epoch": 2.7579487842492636,
      "grad_norm": 6.861208438873291,
      "learning_rate": 4.053884615384616e-06,
      "loss": 4.5773,
      "step": 1194600
    },
    {
      "epoch": 2.758179652220488,
      "grad_norm": 5.824344158172607,
      "learning_rate": 4.050038461538462e-06,
      "loss": 4.5811,
      "step": 1194700
    },
    {
      "epoch": 2.758410520191713,
      "grad_norm": 7.173868656158447,
      "learning_rate": 4.046192307692308e-06,
      "loss": 4.5491,
      "step": 1194800
    },
    {
      "epoch": 2.7586413881629372,
      "grad_norm": 6.553785800933838,
      "learning_rate": 4.042346153846154e-06,
      "loss": 4.5377,
      "step": 1194900
    },
    {
      "epoch": 2.758872256134162,
      "grad_norm": 5.813446521759033,
      "learning_rate": 4.0384999999999995e-06,
      "loss": 4.5434,
      "step": 1195000
    },
    {
      "epoch": 2.7591031241053865,
      "grad_norm": 7.2512593269348145,
      "learning_rate": 4.034653846153846e-06,
      "loss": 4.5257,
      "step": 1195100
    },
    {
      "epoch": 2.7593339920766113,
      "grad_norm": 6.378771781921387,
      "learning_rate": 4.030807692307693e-06,
      "loss": 4.5693,
      "step": 1195200
    },
    {
      "epoch": 2.7595648600478357,
      "grad_norm": 5.808582782745361,
      "learning_rate": 4.026961538461539e-06,
      "loss": 4.5631,
      "step": 1195300
    },
    {
      "epoch": 2.7597957280190606,
      "grad_norm": 5.648507595062256,
      "learning_rate": 4.023115384615385e-06,
      "loss": 4.5551,
      "step": 1195400
    },
    {
      "epoch": 2.760026595990285,
      "grad_norm": 6.506046295166016,
      "learning_rate": 4.019269230769231e-06,
      "loss": 4.5833,
      "step": 1195500
    },
    {
      "epoch": 2.76025746396151,
      "grad_norm": 5.11133337020874,
      "learning_rate": 4.015423076923077e-06,
      "loss": 4.5408,
      "step": 1195600
    },
    {
      "epoch": 2.7604883319327342,
      "grad_norm": 5.883778095245361,
      "learning_rate": 4.011576923076923e-06,
      "loss": 4.5216,
      "step": 1195700
    },
    {
      "epoch": 2.760719199903959,
      "grad_norm": 6.9104156494140625,
      "learning_rate": 4.007730769230769e-06,
      "loss": 4.5588,
      "step": 1195800
    },
    {
      "epoch": 2.7609500678751835,
      "grad_norm": 6.301976680755615,
      "learning_rate": 4.003884615384615e-06,
      "loss": 4.477,
      "step": 1195900
    },
    {
      "epoch": 2.761180935846408,
      "grad_norm": 6.410419940948486,
      "learning_rate": 4.000038461538462e-06,
      "loss": 4.5579,
      "step": 1196000
    },
    {
      "epoch": 2.7614118038176327,
      "grad_norm": 6.036133766174316,
      "learning_rate": 3.996192307692308e-06,
      "loss": 4.5928,
      "step": 1196100
    },
    {
      "epoch": 2.7616426717888576,
      "grad_norm": 7.6746344566345215,
      "learning_rate": 3.9923461538461545e-06,
      "loss": 4.5841,
      "step": 1196200
    },
    {
      "epoch": 2.761873539760082,
      "grad_norm": 5.386216640472412,
      "learning_rate": 3.9885e-06,
      "loss": 4.5433,
      "step": 1196300
    },
    {
      "epoch": 2.7621044077313064,
      "grad_norm": 6.087439060211182,
      "learning_rate": 3.984653846153846e-06,
      "loss": 4.5646,
      "step": 1196400
    },
    {
      "epoch": 2.7623352757025312,
      "grad_norm": 6.728044033050537,
      "learning_rate": 3.980807692307693e-06,
      "loss": 4.591,
      "step": 1196500
    },
    {
      "epoch": 2.762566143673756,
      "grad_norm": 5.54529333114624,
      "learning_rate": 3.976961538461538e-06,
      "loss": 4.5892,
      "step": 1196600
    },
    {
      "epoch": 2.7627970116449805,
      "grad_norm": 6.333499908447266,
      "learning_rate": 3.973115384615385e-06,
      "loss": 4.5174,
      "step": 1196700
    },
    {
      "epoch": 2.763027879616205,
      "grad_norm": 5.404489994049072,
      "learning_rate": 3.969269230769231e-06,
      "loss": 4.5344,
      "step": 1196800
    },
    {
      "epoch": 2.7632587475874297,
      "grad_norm": 5.793302059173584,
      "learning_rate": 3.965423076923077e-06,
      "loss": 4.5186,
      "step": 1196900
    },
    {
      "epoch": 2.7634896155586546,
      "grad_norm": 7.003917217254639,
      "learning_rate": 3.961576923076923e-06,
      "loss": 4.5078,
      "step": 1197000
    },
    {
      "epoch": 2.763720483529879,
      "grad_norm": 5.969775199890137,
      "learning_rate": 3.957730769230769e-06,
      "loss": 4.5325,
      "step": 1197100
    },
    {
      "epoch": 2.7639513515011034,
      "grad_norm": 7.1117401123046875,
      "learning_rate": 3.953884615384616e-06,
      "loss": 4.5596,
      "step": 1197200
    },
    {
      "epoch": 2.7641822194723282,
      "grad_norm": 5.693763256072998,
      "learning_rate": 3.950038461538462e-06,
      "loss": 4.528,
      "step": 1197300
    },
    {
      "epoch": 2.7644130874435526,
      "grad_norm": 5.527405738830566,
      "learning_rate": 3.946192307692308e-06,
      "loss": 4.545,
      "step": 1197400
    },
    {
      "epoch": 2.7646439554147775,
      "grad_norm": 6.219064235687256,
      "learning_rate": 3.942346153846154e-06,
      "loss": 4.5213,
      "step": 1197500
    },
    {
      "epoch": 2.764874823386002,
      "grad_norm": 5.530368804931641,
      "learning_rate": 3.9385000000000005e-06,
      "loss": 4.5491,
      "step": 1197600
    },
    {
      "epoch": 2.7651056913572267,
      "grad_norm": 7.555694580078125,
      "learning_rate": 3.934653846153846e-06,
      "loss": 4.5187,
      "step": 1197700
    },
    {
      "epoch": 2.765336559328451,
      "grad_norm": 6.072011470794678,
      "learning_rate": 3.930807692307692e-06,
      "loss": 4.5927,
      "step": 1197800
    },
    {
      "epoch": 2.765567427299676,
      "grad_norm": 5.777915954589844,
      "learning_rate": 3.926961538461539e-06,
      "loss": 4.5678,
      "step": 1197900
    },
    {
      "epoch": 2.7657982952709004,
      "grad_norm": 5.7207112312316895,
      "learning_rate": 3.923115384615385e-06,
      "loss": 4.5828,
      "step": 1198000
    },
    {
      "epoch": 2.766029163242125,
      "grad_norm": 7.116499423980713,
      "learning_rate": 3.9192692307692316e-06,
      "loss": 4.558,
      "step": 1198100
    },
    {
      "epoch": 2.7662600312133496,
      "grad_norm": 5.758762836456299,
      "learning_rate": 3.915423076923077e-06,
      "loss": 4.5382,
      "step": 1198200
    },
    {
      "epoch": 2.7664908991845745,
      "grad_norm": 5.302974700927734,
      "learning_rate": 3.9115769230769235e-06,
      "loss": 4.613,
      "step": 1198300
    },
    {
      "epoch": 2.766721767155799,
      "grad_norm": 8.097393035888672,
      "learning_rate": 3.907730769230769e-06,
      "loss": 4.5254,
      "step": 1198400
    },
    {
      "epoch": 2.7669526351270237,
      "grad_norm": 7.44985818862915,
      "learning_rate": 3.9038846153846154e-06,
      "loss": 4.5882,
      "step": 1198500
    },
    {
      "epoch": 2.767183503098248,
      "grad_norm": 7.698529243469238,
      "learning_rate": 3.900038461538462e-06,
      "loss": 4.5731,
      "step": 1198600
    },
    {
      "epoch": 2.7674143710694725,
      "grad_norm": 6.0163726806640625,
      "learning_rate": 3.896192307692307e-06,
      "loss": 4.5382,
      "step": 1198700
    },
    {
      "epoch": 2.7676452390406974,
      "grad_norm": 5.901362419128418,
      "learning_rate": 3.892346153846154e-06,
      "loss": 4.5629,
      "step": 1198800
    },
    {
      "epoch": 2.767876107011922,
      "grad_norm": 5.51706075668335,
      "learning_rate": 3.8885e-06,
      "loss": 4.5117,
      "step": 1198900
    },
    {
      "epoch": 2.7681069749831466,
      "grad_norm": 6.141120433807373,
      "learning_rate": 3.8846538461538465e-06,
      "loss": 4.5616,
      "step": 1199000
    },
    {
      "epoch": 2.768337842954371,
      "grad_norm": 5.76144552230835,
      "learning_rate": 3.880807692307693e-06,
      "loss": 4.575,
      "step": 1199100
    },
    {
      "epoch": 2.768568710925596,
      "grad_norm": 6.085196018218994,
      "learning_rate": 3.8769615384615384e-06,
      "loss": 4.5523,
      "step": 1199200
    },
    {
      "epoch": 2.7687995788968207,
      "grad_norm": 8.204510688781738,
      "learning_rate": 3.873115384615385e-06,
      "loss": 4.5544,
      "step": 1199300
    },
    {
      "epoch": 2.769030446868045,
      "grad_norm": 7.498712539672852,
      "learning_rate": 3.869269230769231e-06,
      "loss": 4.5544,
      "step": 1199400
    },
    {
      "epoch": 2.7692613148392695,
      "grad_norm": 5.230258464813232,
      "learning_rate": 3.865423076923077e-06,
      "loss": 4.5848,
      "step": 1199500
    },
    {
      "epoch": 2.7694921828104944,
      "grad_norm": 6.968529224395752,
      "learning_rate": 3.861576923076923e-06,
      "loss": 4.5569,
      "step": 1199600
    },
    {
      "epoch": 2.769723050781719,
      "grad_norm": 5.8914947509765625,
      "learning_rate": 3.8577307692307695e-06,
      "loss": 4.5424,
      "step": 1199700
    },
    {
      "epoch": 2.7699539187529436,
      "grad_norm": 5.887913227081299,
      "learning_rate": 3.853884615384615e-06,
      "loss": 4.5774,
      "step": 1199800
    },
    {
      "epoch": 2.770184786724168,
      "grad_norm": 5.912846088409424,
      "learning_rate": 3.850038461538462e-06,
      "loss": 4.5224,
      "step": 1199900
    },
    {
      "epoch": 2.770415654695393,
      "grad_norm": 5.340514659881592,
      "learning_rate": 3.846192307692308e-06,
      "loss": 4.539,
      "step": 1200000
    },
    {
      "epoch": 2.7706465226666173,
      "grad_norm": 5.691800594329834,
      "learning_rate": 3.842346153846154e-06,
      "loss": 4.5397,
      "step": 1200100
    },
    {
      "epoch": 2.770877390637842,
      "grad_norm": 7.5105156898498535,
      "learning_rate": 3.838500000000001e-06,
      "loss": 4.5808,
      "step": 1200200
    },
    {
      "epoch": 2.7711082586090665,
      "grad_norm": 5.957464218139648,
      "learning_rate": 3.834653846153846e-06,
      "loss": 4.5608,
      "step": 1200300
    },
    {
      "epoch": 2.7713391265802914,
      "grad_norm": 6.658435821533203,
      "learning_rate": 3.8308076923076925e-06,
      "loss": 4.5436,
      "step": 1200400
    },
    {
      "epoch": 2.7715699945515158,
      "grad_norm": 6.909130573272705,
      "learning_rate": 3.826961538461539e-06,
      "loss": 4.5444,
      "step": 1200500
    },
    {
      "epoch": 2.7718008625227406,
      "grad_norm": 9.060531616210938,
      "learning_rate": 3.8231153846153845e-06,
      "loss": 4.5733,
      "step": 1200600
    },
    {
      "epoch": 2.772031730493965,
      "grad_norm": 6.825743198394775,
      "learning_rate": 3.819269230769231e-06,
      "loss": 4.5205,
      "step": 1200700
    },
    {
      "epoch": 2.77226259846519,
      "grad_norm": 6.413716793060303,
      "learning_rate": 3.815423076923077e-06,
      "loss": 4.6006,
      "step": 1200800
    },
    {
      "epoch": 2.7724934664364143,
      "grad_norm": 5.698605060577393,
      "learning_rate": 3.811576923076923e-06,
      "loss": 4.4883,
      "step": 1200900
    },
    {
      "epoch": 2.772724334407639,
      "grad_norm": 6.447134017944336,
      "learning_rate": 3.8077307692307696e-06,
      "loss": 4.5563,
      "step": 1201000
    },
    {
      "epoch": 2.7729552023788635,
      "grad_norm": 8.31643009185791,
      "learning_rate": 3.8038846153846155e-06,
      "loss": 4.6062,
      "step": 1201100
    },
    {
      "epoch": 2.7731860703500884,
      "grad_norm": 5.689211845397949,
      "learning_rate": 3.800038461538462e-06,
      "loss": 4.5416,
      "step": 1201200
    },
    {
      "epoch": 2.7734169383213128,
      "grad_norm": 9.098660469055176,
      "learning_rate": 3.7961923076923075e-06,
      "loss": 4.5506,
      "step": 1201300
    },
    {
      "epoch": 2.773647806292537,
      "grad_norm": 5.6786675453186035,
      "learning_rate": 3.792346153846154e-06,
      "loss": 4.565,
      "step": 1201400
    },
    {
      "epoch": 2.773878674263762,
      "grad_norm": 5.628605842590332,
      "learning_rate": 3.7885000000000002e-06,
      "loss": 4.5317,
      "step": 1201500
    },
    {
      "epoch": 2.774109542234987,
      "grad_norm": 6.52126932144165,
      "learning_rate": 3.784653846153846e-06,
      "loss": 4.5323,
      "step": 1201600
    },
    {
      "epoch": 2.7743404102062112,
      "grad_norm": 7.190359592437744,
      "learning_rate": 3.7808076923076926e-06,
      "loss": 4.4827,
      "step": 1201700
    },
    {
      "epoch": 2.7745712781774357,
      "grad_norm": 5.589947700500488,
      "learning_rate": 3.776961538461539e-06,
      "loss": 4.5515,
      "step": 1201800
    },
    {
      "epoch": 2.7748021461486605,
      "grad_norm": 5.22429084777832,
      "learning_rate": 3.7731153846153845e-06,
      "loss": 4.5017,
      "step": 1201900
    },
    {
      "epoch": 2.7750330141198853,
      "grad_norm": 6.639791488647461,
      "learning_rate": 3.769269230769231e-06,
      "loss": 4.5659,
      "step": 1202000
    },
    {
      "epoch": 2.7752638820911097,
      "grad_norm": 6.312199115753174,
      "learning_rate": 3.765423076923077e-06,
      "loss": 4.5626,
      "step": 1202100
    },
    {
      "epoch": 2.775494750062334,
      "grad_norm": 8.116081237792969,
      "learning_rate": 3.7615769230769232e-06,
      "loss": 4.5373,
      "step": 1202200
    },
    {
      "epoch": 2.775725618033559,
      "grad_norm": 6.276678085327148,
      "learning_rate": 3.7577307692307696e-06,
      "loss": 4.5165,
      "step": 1202300
    },
    {
      "epoch": 2.775956486004784,
      "grad_norm": 6.357284069061279,
      "learning_rate": 3.753884615384615e-06,
      "loss": 4.5333,
      "step": 1202400
    },
    {
      "epoch": 2.7761873539760082,
      "grad_norm": 6.319303512573242,
      "learning_rate": 3.750038461538462e-06,
      "loss": 4.5537,
      "step": 1202500
    },
    {
      "epoch": 2.7764182219472326,
      "grad_norm": 7.030848026275635,
      "learning_rate": 3.7461923076923084e-06,
      "loss": 4.5649,
      "step": 1202600
    },
    {
      "epoch": 2.7766490899184575,
      "grad_norm": 5.5154290199279785,
      "learning_rate": 3.742346153846154e-06,
      "loss": 4.5618,
      "step": 1202700
    },
    {
      "epoch": 2.776879957889682,
      "grad_norm": 7.421781063079834,
      "learning_rate": 3.7385000000000003e-06,
      "loss": 4.541,
      "step": 1202800
    },
    {
      "epoch": 2.7771108258609067,
      "grad_norm": 6.944743633270264,
      "learning_rate": 3.7346538461538463e-06,
      "loss": 4.576,
      "step": 1202900
    },
    {
      "epoch": 2.777341693832131,
      "grad_norm": 6.347556114196777,
      "learning_rate": 3.7308076923076926e-06,
      "loss": 4.5943,
      "step": 1203000
    },
    {
      "epoch": 2.777572561803356,
      "grad_norm": 7.720951557159424,
      "learning_rate": 3.726961538461539e-06,
      "loss": 4.5726,
      "step": 1203100
    },
    {
      "epoch": 2.7778034297745804,
      "grad_norm": 6.021005630493164,
      "learning_rate": 3.7231153846153846e-06,
      "loss": 4.5136,
      "step": 1203200
    },
    {
      "epoch": 2.7780342977458052,
      "grad_norm": 6.784780025482178,
      "learning_rate": 3.719269230769231e-06,
      "loss": 4.5624,
      "step": 1203300
    },
    {
      "epoch": 2.7782651657170296,
      "grad_norm": 6.192239761352539,
      "learning_rate": 3.715423076923077e-06,
      "loss": 4.5772,
      "step": 1203400
    },
    {
      "epoch": 2.7784960336882545,
      "grad_norm": 8.297822952270508,
      "learning_rate": 3.7115769230769233e-06,
      "loss": 4.466,
      "step": 1203500
    },
    {
      "epoch": 2.778726901659479,
      "grad_norm": 6.933975696563721,
      "learning_rate": 3.7077307692307697e-06,
      "loss": 4.5331,
      "step": 1203600
    },
    {
      "epoch": 2.7789577696307037,
      "grad_norm": 9.131138801574707,
      "learning_rate": 3.7038846153846152e-06,
      "loss": 4.5705,
      "step": 1203700
    },
    {
      "epoch": 2.779188637601928,
      "grad_norm": 6.2147321701049805,
      "learning_rate": 3.7000384615384616e-06,
      "loss": 4.5496,
      "step": 1203800
    },
    {
      "epoch": 2.779419505573153,
      "grad_norm": 5.874480247497559,
      "learning_rate": 3.696192307692308e-06,
      "loss": 4.5528,
      "step": 1203900
    },
    {
      "epoch": 2.7796503735443774,
      "grad_norm": 5.5298380851745605,
      "learning_rate": 3.692346153846154e-06,
      "loss": 4.5614,
      "step": 1204000
    },
    {
      "epoch": 2.779881241515602,
      "grad_norm": 5.234644412994385,
      "learning_rate": 3.6885000000000003e-06,
      "loss": 4.5609,
      "step": 1204100
    },
    {
      "epoch": 2.7801121094868266,
      "grad_norm": 6.133688449859619,
      "learning_rate": 3.684653846153846e-06,
      "loss": 4.5751,
      "step": 1204200
    },
    {
      "epoch": 2.7803429774580515,
      "grad_norm": 6.732495307922363,
      "learning_rate": 3.6808076923076923e-06,
      "loss": 4.5257,
      "step": 1204300
    },
    {
      "epoch": 2.780573845429276,
      "grad_norm": 8.612754821777344,
      "learning_rate": 3.6769615384615387e-06,
      "loss": 4.5257,
      "step": 1204400
    },
    {
      "epoch": 2.7808047134005003,
      "grad_norm": 7.880711555480957,
      "learning_rate": 3.6731153846153846e-06,
      "loss": 4.5479,
      "step": 1204500
    },
    {
      "epoch": 2.781035581371725,
      "grad_norm": 7.625741958618164,
      "learning_rate": 3.669269230769231e-06,
      "loss": 4.5694,
      "step": 1204600
    },
    {
      "epoch": 2.78126644934295,
      "grad_norm": 6.430412769317627,
      "learning_rate": 3.6654230769230774e-06,
      "loss": 4.5393,
      "step": 1204700
    },
    {
      "epoch": 2.7814973173141744,
      "grad_norm": 5.584704875946045,
      "learning_rate": 3.661576923076923e-06,
      "loss": 4.569,
      "step": 1204800
    },
    {
      "epoch": 2.781728185285399,
      "grad_norm": 6.3478288650512695,
      "learning_rate": 3.6577307692307697e-06,
      "loss": 4.5293,
      "step": 1204900
    },
    {
      "epoch": 2.7819590532566236,
      "grad_norm": 5.289479732513428,
      "learning_rate": 3.6538846153846153e-06,
      "loss": 4.5353,
      "step": 1205000
    }
  ],
  "logging_steps": 100,
  "max_steps": 1300000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.893116817493197e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
